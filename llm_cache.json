{"\nFor the project `Tutorial-Codebase-Knowledge`:\n\nCodebase Context:\n--- File Index 0: README.md ---\n<h1 align=\"center\">Turns Codebase into Easy Tutorial with AI</h1>\n\n![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)\n\n> *Ever stared at a new codebase written by others feeling completely lost? This tutorial shows you how to build an AI agent that analyzes GitHub repositories and creates beginner-friendly tutorials explaining exactly how the code works.*\n\n<p align=\"center\">\n  <img \n    src=\"./assets/banner.png\" width=\"800\"\n  />\n</p>\n\nThis is a tutorial project of [Pocket Flow](https://github.com/The-Pocket/PocketFlow), a 100-line LLM framework. It crawls GitHub repositories and build a knowledge base from the code. It analyzes entire codebases to identify core abstractions and how they interact, and transforms complex code into beginner-friendly tutorials with clear visualizations.\n\n- Check out the [YouTube Development Tutorial](https://youtu.be/AFY67zOpbSo) for more!\n\n- Check out the [Substack Post Tutorial](https://zacharyhuang.substack.com/p/ai-codebase-knowledge-builder-full) for more!\n\n## \u2b50 Example Results for Popular GitHub Repositories!\n\n<p align=\"center\">\n    <img \n      src=\"./assets/example.png\" width=\"600\"\n    />\n</p>\n\n\ud83e\udd2f All these tutorials are generated **entirely by AI** by crawling the GitHub repo!\n\n- [AutoGen Core](https://the-pocket.github.io/Tutorial-Codebase-Knowledge/AutoGen%20Core) - Build AI teams that talk, think, and solve problems together like coworkers!\n\n- [Browser Use](https://the-pocket.github.io/Tutorial-Codebase-Knowledge/Browser%20Use) - Let AI surf the web for you, clicking buttons and filling forms like a digital assistant!\n\n- [Celery](https://the-pocket.github.io/Tutorial-Codebase-Knowledge/Celery) - Supercharge your app with background tasks that run while you sleep!\n\n- [Click](https://the-pocket.github.io/Tutorial-Codebase-Knowledge/Click) - Turn Python functions into slick command-line tools with just a decorator!\n\n- [Crawl4AI](https://the-pocket.github.io/Tutorial-Codebase-Knowledge/Crawl4AI) - Train your AI to extract exactly what matters from any website!\n\n- [CrewAI](https://the-pocket.github.io/Tutorial-Codebase-Knowledge/CrewAI) - Assemble a dream team of AI specialists to tackle impossible problems!\n\n- [DSPy](https://the-pocket.github.io/Tutorial-Codebase-Knowledge/DSPy) - Build LLM apps like Lego blocks that optimize themselves!\n\n- [FastAPI](https://the-pocket.github.io/Tutorial-Codebase-Knowledge/FastAPI) - Create APIs at lightning speed with automatic docs that clients will love!\n\n- [Flask](https://the-pocket.github.io/Tutorial-Codebase-Knowledge/Flask) - Craft web apps with minimal code that scales from prototype to production!\n\n- [Google A2A](https://the-pocket.github.io/Tutorial-Codebase-Knowledge/Google%20A2A) - The universal language that lets AI agents collaborate across borders!\n\n- [LangGraph](https://the-pocket.github.io/Tutorial-Codebase-Knowledge/LangGraph) - Design AI agents as flowcharts where each step remembers what happened before!\n\n- [LevelDB](https://the-pocket.github.io/Tutorial-Codebase-Knowledge/LevelDB) - Store data at warp speed with Google's engine that powers blockchains!\n\n- [MCP Python SDK](https://the-pocket.github.io/Tutorial-Codebase-Knowledge/MCP%20Python%20SDK) - Build powerful apps that communicate through an elegant protocol without sweating the details!\n\n- [NumPy Core](https://the-pocket.github.io/Tutorial-Codebase-Knowledge/NumPy%20Core) - Master the engine behind data science that makes Python as fast as C!\n\n- [OpenManus](https://the-pocket.github.io/Tutorial-Codebase-Knowledge/OpenManus) - Build AI agents with digital brains that think, learn, and use tools just like humans do!\n\n- [Pydantic Core](https://the-pocket.github.io/Tutorial-Codebase-Knowledge/Pydantic%20Core) - Validate data at rocket speed with just Python type hints!\n\n- [Requests](https://the-pocket.github.io/Tutorial-Codebase-Knowledge/Requests) - Talk to the internet in Python with code so simple it feels like cheating!\n\n- [SmolaAgents](https://the-pocket.github.io/Tutorial-Codebase-Knowledge/SmolaAgents) - Build tiny AI agents that punch way above their weight class!\n\n\n## \ud83d\ude80 Getting Started\n\n1. Clone this repository\n\n2. Install dependencies: \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. Set up LLM in [`utils/call_llm.py`](./utils/call_llm.py) by providing credentials. By default, you can use the AI Studio key with this client for Gemini Pro 2.5:\n\n   ```python\n   client = genai.Client(\n     api_key=os.getenv(\"GEMINI_API_KEY\", \"your-api_key\"),\n   )\n   ```\n  \n   You can use your own models. We highly recommend the latest models with thinking capabilities (Claude 3.7 with thinking, O1). You can verify that it is correctly set up by running:\n   ```bash\n   python utils/call_llm.py\n   ```\n\n7. Generate a complete codebase tutorial by running the main script:\n    ```bash\n    # Analyze a GitHub repository\n    python main.py --repo https://github.com/username/repo --include \"*.py\" \"*.js\" --exclude \"tests/*\" --max-size 50000\n\n    # Or, analyze a local directory\n    python main.py --dir /path/to/your/codebase --include \"*.py\" --exclude \"*test*\"\n    ```\n    - `--repo` or `--dir` - Specify either a GitHub repo URL or a local directory path (required, mutually exclusive)\n    - `-n, --name` - Project name (optional, derived from URL/directory if omitted)\n    - `-t, --token` - GitHub token (or set GITHUB_TOKEN environment variable)\n    - `-o, --output` - Output directory (default: ./output)\n    - `-i, --include` - Files to include (e.g., \"*.py\" \"*.js\")\n    - `-e, --exclude` - Files to exclude (e.g., \"tests/*\" \"docs/*\")\n    - `-s, --max-size` - Maximum file size in bytes (default: 100KB)\n      \nThe application will crawl the repository, analyze the codebase structure, generate tutorial content, and save the output in the specified directory (default: ./output).\n\n\n## \ud83d\udca1 Development Tutorial\n\n- I built using [**Agentic Coding**](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to), the fastest development paradigm, where humans simply [design](docs/design.md) and agents [code](flow.py).\n\n- The secret weapon is [Pocket Flow](https://github.com/The-Pocket/PocketFlow), a 100-line LLM framework that lets Agents (e.g., Cursor AI) build for you\n  \n- Check out the Step-by-step YouTube development tutorial: \n\n<br>\n<div align=\"center\">\n  <a href=\"https://youtu.be/AFY67zOpbSo\" target=\"_blank\">\n    <img src=\"./assets/youtube_thumbnail.png\" width=\"500\" alt=\"IMAGE ALT TEXT\" style=\"cursor: pointer;\">\n  </a>\n</div>\n<br>\n\n\n--- File Index 1: flow.py ---\nfrom pocketflow import Flow\n# Import all node classes from nodes.py\nfrom nodes import (\n    FetchRepo,\n    IdentifyAbstractions,\n    AnalyzeRelationships,\n    OrderChapters,\n    WriteChapters,\n    CombineTutorial\n)\n\ndef create_tutorial_flow():\n    \"\"\"Creates and returns the codebase tutorial generation flow.\"\"\"\n\n    # Instantiate nodes\n    fetch_repo = FetchRepo()\n    identify_abstractions = IdentifyAbstractions(max_retries=3, wait=10)\n    analyze_relationships = AnalyzeRelationships(max_retries=3, wait=10)\n    order_chapters = OrderChapters(max_retries=3, wait=10)\n    write_chapters = WriteChapters(max_retries=3, wait=10) # This is a BatchNode\n    combine_tutorial = CombineTutorial()\n\n    # Connect nodes in sequence based on the design\n    fetch_repo >> identify_abstractions\n    identify_abstractions >> analyze_relationships\n    analyze_relationships >> order_chapters\n    order_chapters >> write_chapters\n    write_chapters >> combine_tutorial\n\n    # Create the flow starting with FetchRepo\n    tutorial_flow = Flow(start=fetch_repo)\n\n    return tutorial_flow\n\n--- File Index 2: main.py ---\nimport dotenv\nimport os\nimport argparse\n# Import the function that creates the flow\nfrom flow import create_tutorial_flow\n\ndotenv.load_dotenv()\n\n# Default file patterns\nDEFAULT_INCLUDE_PATTERNS = {\n    \"*.py\", \"*.js\", \"*.jsx\", \"*.ts\", \"*.tsx\", \"*.go\", \"*.java\", \"*.pyi\", \"*.pyx\", \n    \"*.c\", \"*.cc\", \"*.cpp\", \"*.h\", \"*.md\", \"*.rst\", \"Dockerfile\", \n    \"Makefile\", \"*.yaml\", \"*.yml\",\n}\n\nDEFAULT_EXCLUDE_PATTERNS = {\n    \"*test*\", \"tests/*\", \"docs/*\", \"examples/*\", \"v1/*\", \n    \"dist/*\", \"build/*\", \"experimental/*\", \"deprecated/*\", \n    \"legacy/*\", \".git/*\", \".github/*\", \".next/*\", \".vscode/*\", \"obj/*\", \"bin/*\", \"node_modules/*\", \"*.log\"\n}\n\n# --- Main Function ---\ndef main():\n    parser = argparse.ArgumentParser(description=\"Generate a tutorial for a GitHub codebase or local directory.\")\n    \n    # Create mutually exclusive group for source\n    source_group = parser.add_mutually_exclusive_group(required=True)\n    source_group.add_argument(\"--repo\", help=\"URL of the public GitHub repository.\")\n    source_group.add_argument(\"--dir\", help=\"Path to local directory.\")\n    \n    parser.add_argument(\"-n\", \"--name\", help=\"Project name (optional, derived from repo/directory if omitted).\")\n    parser.add_argument(\"-t\", \"--token\", help=\"GitHub personal access token (optional, reads from GITHUB_TOKEN env var if not provided).\")\n    parser.add_argument(\"-o\", \"--output\", default=\"output\", help=\"Base directory for output (default: ./output).\")\n    parser.add_argument(\"-i\", \"--include\", nargs=\"+\", help=\"Include file patterns (e.g. '*.py' '*.js'). Defaults to common code files if not specified.\")\n    parser.add_argument(\"-e\", \"--exclude\", nargs=\"+\", help=\"Exclude file patterns (e.g. 'tests/*' 'docs/*'). Defaults to test/build directories if not specified.\")\n    parser.add_argument(\"-s\", \"--max-size\", type=int, default=100000, help=\"Maximum file size in bytes (default: 100000, about 100KB).\")\n\n    args = parser.parse_args()\n\n    # Get GitHub token from argument or environment variable if using repo\n    github_token = None\n    if args.repo:\n        github_token = args.token or os.environ.get('GITHUB_TOKEN')\n        if not github_token:\n            print(\"Warning: No GitHub token provided. You might hit rate limits for public repositories.\")\n\n    # Initialize the shared dictionary with inputs\n    shared = {\n        \"repo_url\": args.repo,\n        \"local_dir\": args.dir,\n        \"project_name\": args.name, # Can be None, FetchRepo will derive it\n        \"github_token\": github_token,\n        \"output_dir\": args.output, # Base directory for CombineTutorial output\n\n        # Add include/exclude patterns and max file size\n        \"include_patterns\": set(args.include) if args.include else DEFAULT_INCLUDE_PATTERNS,\n        \"exclude_patterns\": set(args.exclude) if args.exclude else DEFAULT_EXCLUDE_PATTERNS,\n        \"max_file_size\": args.max_size,\n\n        # Outputs will be populated by the nodes\n        \"files\": [],\n        \"abstractions\": [],\n        \"relationships\": {},\n        \"chapter_order\": [],\n        \"chapters\": [],\n        \"final_output_dir\": None\n    }\n\n    print(f\"Starting tutorial generation for: {args.repo or args.dir}\")\n\n    # Create the flow instance\n    tutorial_flow = create_tutorial_flow()\n\n    # Run the flow\n    tutorial_flow.run(shared)\n    \nif __name__ == \"__main__\":\n    main()\n\n--- File Index 3: nodes.py ---\nimport os\nimport yaml\nfrom pocketflow import Node, BatchNode\nfrom utils.crawl_github_files import crawl_github_files\nfrom utils.call_llm import call_llm \nfrom utils.crawl_local_files import crawl_local_files\n\n# Helper to get content for specific file indices\ndef get_content_for_indices(files_data, indices):\n    content_map = {}\n    for i in indices:\n        if 0 <= i < len(files_data):\n            path, content = files_data[i]\n            content_map[f\"{i} # {path}\"] = content # Use index + path as key for context\n    return content_map\n\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"local_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        if not project_name:\n            # Basic name derivation from URL or directory\n            if repo_url:\n                project_name = repo_url.split('/')[-1].replace('.git', '')\n            else:\n                project_name = os.path.basename(os.path.abspath(local_dir))\n            shared[\"project_name\"] = project_name\n\n        # Get file patterns directly from shared\n        include_patterns = shared[\"include_patterns\"]\n        exclude_patterns = shared[\"exclude_patterns\"]\n        max_file_size = shared[\"max_file_size\"]\n\n        return {\n            \"repo_url\": repo_url,\n            \"local_dir\": local_dir,\n            \"token\": shared.get(\"github_token\"),\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns,\n            \"max_file_size\": max_file_size,\n            \"use_relative_paths\": True\n        }\n\n    def exec(self, prep_res):\n        if prep_res[\"repo_url\"]:\n            print(f\"Crawling repository: {prep_res['repo_url']}...\")\n            result = crawl_github_files(\n                repo_url=prep_res[\"repo_url\"],\n                token=prep_res[\"token\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        else:\n            print(f\"Crawling directory: {prep_res['local_dir']}...\")\n            result = crawl_local_files(\n                directory=prep_res[\"local_dir\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n            \n        # Convert dict to list of tuples: [(path, content), ...]\n        files_list = list(result.get(\"files\", {}).items())\n        print(f\"Fetched {len(files_list)} files.\")\n        return files_list\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"files\"] = exec_res # List of (path, content) tuples\n\nclass IdentifyAbstractions(Node):\n    def prep(self, shared):\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        \n        # Helper to create context from files, respecting limits (basic example)\n        def create_llm_context(files_data):\n            context = \"\"\n            file_info = [] # Store tuples of (index, path)\n            for i, (path, content) in enumerate(files_data):\n                entry = f\"--- File Index {i}: {path} ---\\n{content}\\n\\n\"\n                context += entry\n                file_info.append((i, path))\n\n            return context, file_info # file_info is list of (index, path)\n\n        context, file_info = create_llm_context(files_data)\n        # Format file info for the prompt (comment is just a hint for LLM)\n        file_listing_for_prompt = \"\\n\".join([f\"- {idx} # {path}\" for idx, path in file_info])\n        return context, file_listing_for_prompt, len(files_data), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, file_listing_for_prompt, file_count, project_name = prep_res  # Unpack project name\n        print(\"Identifying abstractions using LLM...\")\n        prompt = f\"\"\"\nFor the project `{project_name}`:\n\nCodebase Context:\n{context}\n\nAnalyze the codebase context.\nIdentify the top 5-10 core most important abstractions to help those new to the codebase.\n\nFor each abstraction, provide:\n1. A concise `name`.\n2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\nList of file indices and paths present in the context:\n{file_listing_for_prompt}\n\nFormat the output as a YAML list of dictionaries:\n\n```yaml\n- name: Query Processing\n  description: | \n    Explains what the abstraction does.\n    It's like a central dispatcher routing requests.\n  file_indices:\n    - 0 # path/to/file1.py\n    - 3 # path/to/related.py\n- name: Query Optimization\n  description: |\n    Another core concept, similar to a blueprint for objects.\n  file_indices:\n    - 5 # path/to/another.js\n# ... up to 10 abstractions\n```\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        abstractions = yaml.safe_load(yaml_str)\n\n        if not isinstance(abstractions, list):\n            raise ValueError(\"LLM Output is not a list\")\n\n        validated_abstractions = []\n        for item in abstractions:\n            if not isinstance(item, dict) or not all(k in item for k in [\"name\", \"description\", \"file_indices\"]):\n                raise ValueError(f\"Missing keys in abstraction item: {item}\")\n            if not isinstance(item[\"description\"], str):\n                 raise ValueError(f\"description is not a string in item: {item}\")\n            if not isinstance(item[\"file_indices\"], list):\n                 raise ValueError(f\"file_indices is not a list in item: {item}\")\n\n            # Validate indices\n            validated_indices = []\n            for idx_entry in item[\"file_indices\"]:\n                 try:\n                     if isinstance(idx_entry, int):\n                         idx = idx_entry\n                     elif isinstance(idx_entry, str) and '#' in idx_entry:\n                          idx = int(idx_entry.split('#')[0].strip())\n                     else:\n                          idx = int(str(idx_entry).strip())\n\n                     if not (0 <= idx < file_count):\n                         raise ValueError(f\"Invalid file index {idx} found in item {item['name']}. Max index is {file_count - 1}.\")\n                     validated_indices.append(idx)\n                 except (ValueError, TypeError):\n                      raise ValueError(f\"Could not parse index from entry: {idx_entry} in item {item['name']}\")\n\n            item[\"files\"] = sorted(list(set(validated_indices)))\n            # Store only the required fields\n            validated_abstractions.append({\n                \"name\": item[\"name\"],\n                \"description\": item[\"description\"],\n                \"files\": item[\"files\"]\n            })\n\n        print(f\"Identified {len(validated_abstractions)} abstractions.\")\n        return validated_abstractions\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"abstractions\"] = exec_res # List of {\"name\": str, \"description\": str, \"files\": [int]}\n\nclass AnalyzeRelationships(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"] # Now contains 'files' list of indices\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n\n        # Create context with abstraction names, indices, descriptions, and relevant file snippets\n        context = \"Identified Abstractions:\\n\"\n        all_relevant_indices = set()\n        abstraction_info_for_prompt = []\n        for i, abstr in enumerate(abstractions):\n            # Use 'files' which contains indices directly\n            file_indices_str = \", \".join(map(str, abstr['files']))\n            info_line = f\"- Index {i}: {abstr['name']} (Relevant file indices: [{file_indices_str}])\\n  Description: {abstr['description']}\"\n            context += info_line + \"\\n\"\n            abstraction_info_for_prompt.append(f\"{i} # {abstr['name']}\")\n            all_relevant_indices.update(abstr['files'])\n\n        context += \"\\nRelevant File Snippets (Referenced by Index and Path):\\n\"\n        # Get content for relevant files using helper\n        relevant_files_content_map = get_content_for_indices(\n            files_data,\n            sorted(list(all_relevant_indices))\n        )\n        # Format file content for context\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path} ---\\n{content}\"\n            for idx_path, content in relevant_files_content_map.items()\n        )\n        context += file_context_str\n\n        return context, \"\\n\".join(abstraction_info_for_prompt), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, abstraction_listing, project_name = prep_res  # Unpack project name\n        print(\"Analyzing relationships using LLM...\")\n        prompt = f\"\"\"\nBased on the following abstractions and relevant code snippets from the project `{project_name}`:\n\nList of Abstraction Indices and Names:\n{abstraction_listing}\n\nContext (Abstractions, Descriptions, Code):\n{context}\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        relationships_data = yaml.safe_load(yaml_str)\n\n        if not isinstance(relationships_data, dict) or not all(k in relationships_data for k in [\"summary\", \"relationships\"]):\n            raise ValueError(\"LLM output is not a dict or missing keys ('summary', 'relationships')\")\n        if not isinstance(relationships_data[\"summary\"], str):\n             raise ValueError(\"summary is not a string\")\n        if not isinstance(relationships_data[\"relationships\"], list):\n             raise ValueError(\"relationships is not a list\")\n\n        # Validate relationships structure\n        validated_relationships = []\n        num_abstractions = len(abstraction_listing.split('\\n'))\n        for rel in relationships_data[\"relationships\"]:\n             # Check for 'label' key\n             if not isinstance(rel, dict) or not all(k in rel for k in [\"from_abstraction\", \"to_abstraction\", \"label\"]):\n                  raise ValueError(f\"Missing keys (expected from_abstraction, to_abstraction, label) in relationship item: {rel}\")\n             # Validate 'label' is a string\n             if not isinstance(rel[\"label\"], str):\n                  raise ValueError(f\"Relationship label is not a string: {rel}\")\n\n             # Validate indices\n             try:\n                 from_idx = int(str(rel[\"from_abstraction\"]).split('#')[0].strip())\n                 to_idx = int(str(rel[\"to_abstraction\"]).split('#')[0].strip())\n                 if not (0 <= from_idx < num_abstractions and 0 <= to_idx < num_abstractions):\n                      raise ValueError(f\"Invalid index in relationship: from={from_idx}, to={to_idx}. Max index is {num_abstractions-1}.\")\n                 validated_relationships.append({\n                     \"from\": from_idx,\n                     \"to\": to_idx,\n                     \"label\": rel[\"label\"] \n                 })\n             except (ValueError, TypeError):\n                  raise ValueError(f\"Could not parse indices from relationship: {rel}\")\n\n        print(\"Generated project summary and relationship details.\")\n        return {\n            \"summary\": relationships_data[\"summary\"],\n            \"details\": validated_relationships # Store validated, index-based relationships\n        }\n\n\n    def post(self, shared, prep_res, exec_res):\n        # Structure is now {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        shared[\"relationships\"] = exec_res\n\nclass OrderChapters(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"]\n        relationships = shared[\"relationships\"]\n        project_name = shared[\"project_name\"]  # Get project name\n\n        # Prepare context for the LLM\n        abstraction_info_for_prompt = []\n        for i, a in enumerate(abstractions):\n            abstraction_info_for_prompt.append(f\"- {i} # {a['name']}\")\n        abstraction_listing = \"\\n\".join(abstraction_info_for_prompt)\n\n        context = f\"Project Summary:\\n{relationships['summary']}\\n\\n\"\n        context += \"Relationships (Indices refer to abstractions above):\\n\"\n        for rel in relationships['details']:\n             from_name = abstractions[rel['from']]['name']\n             to_name = abstractions[rel['to']]['name']\n             # Use 'label' instead of 'desc'\n             context += f\"- From {rel['from']} ({from_name}) to {rel['to']} ({to_name}): {rel['label']}\\n\"\n\n        return abstraction_listing, context, len(abstractions), project_name\n\n    def exec(self, prep_res):\n        abstraction_listing, context, num_abstractions, project_name = prep_res\n        print(\"Determining chapter order using LLM...\")\n        prompt = f\"\"\"\nGiven the following project abstractions and their relationships for the project ```` {project_name} ````:\n\nAbstractions (Index # Name):\n{abstraction_listing}\n\nContext about relationships and project summary:\n{context}\n\nIf you are going to make a tutorial for ```` {project_name} ````, what is the best order to explain these abstractions, from first to last?\nIdeally, first explain those that are the most important or foundational, perhaps user-facing concepts or entry points. Then move to more detailed, lower-level implementation details or supporting concepts.\n\nOutput the ordered list of abstraction indices, including the name in a comment for clarity. Use the format `idx # AbstractionName`.\n\n```yaml\n- 2 # FoundationalConcept\n- 0 # CoreClassA\n- 1 # CoreClassB (uses CoreClassA)\n- ...\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        # Rely on Node's built-in retry/fallback\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        ordered_indices_raw = yaml.safe_load(yaml_str)\n\n        if not isinstance(ordered_indices_raw, list):\n            raise ValueError(\"LLM output is not a list\")\n\n        ordered_indices = []\n        seen_indices = set()\n        for entry in ordered_indices_raw:\n            try:\n                 if isinstance(entry, int):\n                     idx = entry\n                 elif isinstance(entry, str) and '#' in entry:\n                      idx = int(entry.split('#')[0].strip())\n                 else:\n                      idx = int(str(entry).strip())\n\n                 if not (0 <= idx < num_abstractions):\n                      raise ValueError(f\"Invalid index {idx} in ordered list. Max index is {num_abstractions-1}.\")\n                 if idx in seen_indices:\n                     raise ValueError(f\"Duplicate index {idx} found in ordered list.\")\n                 ordered_indices.append(idx)\n                 seen_indices.add(idx)\n\n            except (ValueError, TypeError):\n                 raise ValueError(f\"Could not parse index from ordered list entry: {entry}\")\n\n        # Check if all abstractions are included\n        if len(ordered_indices) != num_abstractions:\n             raise ValueError(f\"Ordered list length ({len(ordered_indices)}) does not match number of abstractions ({num_abstractions}). Missing indices: {set(range(num_abstractions)) - seen_indices}\")\n\n        print(f\"Determined chapter order (indices): {ordered_indices}\")\n        return ordered_indices # Return the list of indices\n\n    def post(self, shared, prep_res, exec_res):\n        # exec_res is already the list of ordered indices\n        shared[\"chapter_order\"] = exec_res # List of indices\n\nclass WriteChapters(BatchNode):\n    def prep(self, shared):\n        chapter_order = shared[\"chapter_order\"] # List of indices\n        abstractions = shared[\"abstractions\"]   # List of dicts, now using 'files' with indices\n        files_data = shared[\"files\"]\n        # Get already written chapters to provide context\n        # We store them temporarily during the batch run, not in shared memory yet\n        # The 'previous_chapters_summary' will be built progressively in the exec context\n        self.chapters_written_so_far = [] # Use instance variable for temporary storage across exec calls\n\n        # Create a complete list of all chapters\n        all_chapters = []\n        chapter_filenames = {} # Store chapter filename mapping for linking\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                chapter_num = i + 1\n                chapter_name = abstractions[abstraction_index][\"name\"]\n                # Create safe filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in chapter_name).lower()\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                # Format with link\n                all_chapters.append(f\"{chapter_num}. [{chapter_name}]({filename})\")\n                # Store mapping of chapter index to filename for linking\n                chapter_filenames[abstraction_index] = {\"num\": chapter_num, \"name\": chapter_name, \"filename\": filename}\n        \n        # Create a formatted string with all chapters\n        full_chapter_listing = \"\\n\".join(all_chapters)\n\n        items_to_process = []\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                abstraction_details = abstractions[abstraction_index]\n                # Use 'files' (list of indices) directly\n                related_file_indices = abstraction_details.get(\"files\", [])\n                # Get content using helper, passing indices\n                related_files_content_map = get_content_for_indices(files_data, related_file_indices)\n                \n                # Get previous chapter info for transitions\n                prev_chapter = None\n                if i > 0:\n                    prev_idx = chapter_order[i-1]\n                    prev_chapter = chapter_filenames[prev_idx]\n                \n                # Get next chapter info for transitions\n                next_chapter = None\n                if i < len(chapter_order) - 1:\n                    next_idx = chapter_order[i+1]\n                    next_chapter = chapter_filenames[next_idx]\n\n                items_to_process.append({\n                    \"chapter_num\": i + 1,\n                    \"abstraction_index\": abstraction_index,\n                    \"abstraction_details\": abstraction_details,\n                    \"related_files_content_map\": related_files_content_map,\n                    \"project_name\": shared[\"project_name\"],  # Add project name\n                    \"full_chapter_listing\": full_chapter_listing,  # Add the full chapter listing\n                    \"chapter_filenames\": chapter_filenames,  # Add chapter filenames mapping\n                    \"prev_chapter\": prev_chapter,  # Add previous chapter info\n                    \"next_chapter\": next_chapter,  # Add next chapter info\n                    # previous_chapters_summary will be added dynamically in exec\n                })\n            else:\n                print(f\"Warning: Invalid abstraction index {abstraction_index} in chapter_order. Skipping.\")\n\n        print(f\"Preparing to write {len(items_to_process)} chapters...\")\n        return items_to_process # Iterable for BatchNode\n\n    def exec(self, item):\n        # This runs for each item prepared above\n        abstraction_name = item[\"abstraction_details\"][\"name\"]\n        chapter_num = item[\"chapter_num\"]\n        project_name = item.get(\"project_name\")  # Get from item\n        print(f\"Writing chapter {chapter_num} for: {abstraction_name} using LLM...\")\n\n        # Prepare file context string from the map\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path.split('# ')[1] if '# ' in idx_path else idx_path} ---\\n{content}\"\n            for idx_path, content in item[\"related_files_content_map\"].items()\n        )\n\n        # Get summary of chapters written *before* this one\n        # Use the temporary instance variable\n        previous_chapters_summary = \"\\n---\\n\".join(self.chapters_written_so_far)\n\n\n        prompt = f\"\"\"\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `{project_name}` about the concept: \"{abstraction_name}\". This is Chapter {chapter_num}.\n\nConcept Details:\n- Description:\n{item[\"abstraction_details\"][\"description\"]}\n\nComplete Tutorial Structure:\n{item[\"full_chapter_listing\"]}\n\nContext from previous chapters (summary):\n{previous_chapters_summary if previous_chapters_summary else \"This is the first chapter.\"}\n\nRelevant Code Snippets:\n{file_context_str if file_context_str else \"No specific code snippets provided for this abstraction.\"}\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter {chapter_num}: {abstraction_name}`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n\"\"\"\n        chapter_content = call_llm(prompt)\n        # Basic validation/cleanup\n        actual_heading = f\"# Chapter {chapter_num}: {abstraction_name}\"\n        if not chapter_content.strip().startswith(f\"# Chapter {chapter_num}\"):\n             # Add heading if missing or incorrect, trying to preserve content\n             lines = chapter_content.strip().split('\\n')\n             if lines and lines[0].strip().startswith(\"#\"): # If there's some heading, replace it\n                 lines[0] = actual_heading\n                 chapter_content = \"\\n\".join(lines)\n             else: # Otherwise, prepend it\n                 chapter_content = f\"{actual_heading}\\n\\n{chapter_content}\"\n\n        # Add the generated content to our temporary list for the next iteration's context\n        self.chapters_written_so_far.append(chapter_content)\n\n        return chapter_content # Return the Markdown string\n\n    def post(self, shared, prep_res, exec_res_list):\n        # exec_res_list contains the generated Markdown for each chapter, in order\n        shared[\"chapters\"] = exec_res_list\n        # Clean up the temporary instance variable\n        del self.chapters_written_so_far\n        print(f\"Finished writing {len(exec_res_list)} chapters.\")\n\nclass CombineTutorial(Node):\n    def prep(self, shared):\n        project_name = shared[\"project_name\"]\n        output_base_dir = shared.get(\"output_dir\", \"output\") # Default output dir\n        output_path = os.path.join(output_base_dir, project_name)\n        repo_url = shared[\"repo_url\"]  # Get the repository URL\n\n        # Use 'label' from relationships_data['details']\n        relationships_data = shared[\"relationships\"] # {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        chapter_order = shared[\"chapter_order\"] # indices\n        abstractions = shared[\"abstractions\"]   # list of dicts\n        chapters_content = shared[\"chapters\"]   # list of strings\n\n        # --- Generate Mermaid Diagram ---\n        mermaid_lines = [\"flowchart TD\"]\n        # Add nodes for each abstraction\n        for i, abstr in enumerate(abstractions):\n            # Sanitize name for Mermaid ID and label\n            node_id = f\"A{i}\"\n            sanitized_name = abstr['name'].replace('\"', '')\n            node_label = sanitized_name # Using sanitized name only, no index\n            mermaid_lines.append(f'    {node_id}[\"{node_label}\"]')\n        # Add edges for relationships using 'label'\n        for rel in relationships_data['details']:\n            from_node_id = f\"A{rel['from']}\"\n            to_node_id = f\"A{rel['to']}\"\n            # Sanitize 'label' for edge label\n            edge_label = rel['label'].replace('\"', '').replace('\\n', ' ') # Basic sanitization\n            # Limit edge label length for readability (optional, but good for diagrams)\n            max_label_len = 30 # Make it shorter for labels\n            if len(edge_label) > max_label_len:\n                edge_label = edge_label[:max_label_len-3] + \"...\"\n            mermaid_lines.append(f'    {from_node_id} -- \"{edge_label}\" --> {to_node_id}')\n\n        mermaid_diagram = \"\\n\".join(mermaid_lines)\n        # --- End Mermaid ---\n\n\n        # Prepare index.md content\n        index_content = f\"# Tutorial: {project_name}\\n\\n\"\n        index_content += f\"{relationships_data['summary']}\\n\\n\"\n        index_content += f\"**Source Repository:** [{repo_url}]({repo_url})\\n\\n\"\n\n        # Add Mermaid diagram for relationships\n        index_content += \"```mermaid\\n\"\n        index_content += mermaid_diagram + \"\\n\"\n        index_content += \"```\\n\\n\"\n\n        index_content += \"## Chapters\\n\\n\"\n\n        chapter_files = []\n        # Generate chapter links based on the determined order\n        for i, abstraction_index in enumerate(chapter_order):\n            # Ensure index is valid and we have content for it\n            if 0 <= abstraction_index < len(abstractions) and i < len(chapters_content):\n                abstraction_name = abstractions[abstraction_index][\"name\"]\n                # Sanitize name for filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in abstraction_name).lower()\n                # Use chapter number (i+1) for ordering filename\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                index_content += f\"{i+1}. [{abstraction_name}]({filename})\\n\"\n                \n                # Add attribution to chapter content\n                chapter_content = chapters_content[i]\n                if not chapter_content.endswith(\"\\n\\n\"):\n                    chapter_content += \"\\n\\n\"\n                chapter_content += \"---\\n\\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\"\n                \n                # Store filename and corresponding content\n                chapter_files.append({\"filename\": filename, \"content\": chapter_content})\n            else:\n                 print(f\"Warning: Mismatch between chapter order, abstractions, or content at index {i} (abstraction index {abstraction_index}). Skipping file generation for this entry.\")\n\n        # Add attribution to index content\n        index_content += \"\\n\\n---\\n\\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\"\n\n        return {\n            \"output_path\": output_path,\n            \"index_content\": index_content,\n            \"chapter_files\": chapter_files # List of {\"filename\": str, \"content\": str}\n        }\n\n    def exec(self, prep_res):\n        output_path = prep_res[\"output_path\"]\n        index_content = prep_res[\"index_content\"]\n        chapter_files = prep_res[\"chapter_files\"]\n\n        print(f\"Combining tutorial into directory: {output_path}\")\n        # Rely on Node's built-in retry/fallback\n        os.makedirs(output_path, exist_ok=True)\n\n        # Write index.md\n        index_filepath = os.path.join(output_path, \"index.md\")\n        with open(index_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(index_content)\n        print(f\"  - Wrote {index_filepath}\")\n\n        # Write chapter files\n        for chapter_info in chapter_files:\n            chapter_filepath = os.path.join(output_path, chapter_info[\"filename\"])\n            with open(chapter_filepath, \"w\", encoding=\"utf-8\") as f:\n                f.write(chapter_info[\"content\"])\n            print(f\"  - Wrote {chapter_filepath}\")\n\n        return output_path # Return the final path\n\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"final_output_dir\"] = exec_res # Store the output path\n        print(f\"\\nTutorial generation complete! Files are in: {exec_res}\")\n\n--- File Index 4: utils/__init__.py ---\n\n\n--- File Index 5: utils/call_llm.py ---\nfrom google import genai\nimport os\nimport logging\nimport json\nfrom datetime import datetime\n\n# Configure logging\nlog_directory = os.getenv(\"LOG_DIR\", \"logs\")\nos.makedirs(log_directory, exist_ok=True)\nlog_file = os.path.join(log_directory, f\"llm_calls_{datetime.now().strftime('%Y%m%d')}.log\")\n\n# Set up logger\nlogger = logging.getLogger(\"llm_logger\")\nlogger.setLevel(logging.INFO)\nlogger.propagate = False  # Prevent propagation to root logger\nfile_handler = logging.FileHandler(log_file)\nfile_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\nlogger.addHandler(file_handler)\n\n# Simple cache configuration\ncache_file = \"llm_cache.json\"\n\n# By default, we Google Gemini 2.5 pro, as it shows great performance for code understanding\ndef call_llm(prompt: str, use_cache: bool = True) -> str:\n    # Log the prompt\n    logger.info(f\"PROMPT: {prompt}\")\n    \n    # Check cache if enabled\n    if use_cache:\n        # Load cache from disk\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, 'r') as f:\n                    cache = json.load(f)\n            except:\n                logger.warning(f\"Failed to load cache, starting with empty cache\")\n        \n        # Return from cache if exists\n        if prompt in cache:\n            logger.info(f\"RESPONSE: {cache[prompt]}\")\n            return cache[prompt]\n    \n    # Call the LLM if not in cache or cache disabled\n    client = genai.Client(\n        vertexai=True, \n        # TODO: change to your own project id and location\n        project=os.getenv(\"GEMINI_PROJECT_ID\", \"your-project-id\"),\n        location=os.getenv(\"GEMINI_LOCATION\", \"us-central1\")\n    )\n    # You can comment the previous line and use the AI Studio key instead:\n    # client = genai.Client(\n    #     api_key=os.getenv(\"GEMINI_API_KEY\", \"your-api_key\"),\n    # )\n    model = os.getenv(\"GEMINI_MODEL\", \"gemini-2.5-pro-exp-03-25\")\n    response = client.models.generate_content(\n        model=model,\n        contents=[prompt]\n    )\n    response_text = response.text\n    \n    # Log the response\n    logger.info(f\"RESPONSE: {response_text}\")\n    \n    # Update cache if enabled\n    if use_cache:\n        # Load cache again to avoid overwrites\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, 'r') as f:\n                    cache = json.load(f)\n            except:\n                pass\n        \n        # Add to cache and save\n        cache[prompt] = response_text\n        try:\n            with open(cache_file, 'w') as f:\n                json.dump(cache, f)\n        except Exception as e:\n            logger.error(f\"Failed to save cache: {e}\")\n    \n    return response_text\n\n# # Use Anthropic Claude 3.7 Sonnet Extended Thinking\n# def call_llm(prompt, use_cache: bool = True):\n#     from anthropic import Anthropic\n#     client = Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\", \"your-api-key\"))\n#     response = client.messages.create(\n#         model=\"claude-3-7-sonnet-20250219\",\n#         max_tokens=21000,\n#         thinking={\n#             \"type\": \"enabled\",\n#             \"budget_tokens\": 20000\n#         },\n#         messages=[\n#             {\"role\": \"user\", \"content\": prompt}\n#         ]\n#     )\n#     return response.content[1].text\n\n# # Use OpenAI o1\n# def call_llm(prompt, use_cache: bool = True):    \n#     from openai import OpenAI\n#     client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n#     r = client.chat.completions.create(\n#         model=\"o1\",\n#         messages=[{\"role\": \"user\", \"content\": prompt}],\n#         response_format={\n#             \"type\": \"text\"\n#         },\n#         reasoning_effort=\"medium\",\n#         store=False\n#     )\n#     return r.choices[0].message.content\n\nif __name__ == \"__main__\":\n    test_prompt = \"Hello, how are you?\"\n    \n    # First call - should hit the API\n    print(\"Making call...\")\n    response1 = call_llm(test_prompt, use_cache=False)\n    print(f\"Response: {response1}\")\n    \n\n\n--- File Index 6: utils/crawl_github_files.py ---\nimport requests\nimport base64\nimport os\nimport tempfile\nimport git\nimport time\nimport fnmatch\nfrom typing import Union, Set, List, Dict, Tuple, Any\nfrom urllib.parse import urlparse\n\ndef crawl_github_files(\n    repo_url, \n    token=None, \n    max_file_size: int = 1 * 1024 * 1024,  # 1 MB\n    use_relative_paths: bool = False,\n    include_patterns: Union[str, Set[str]] = None,\n    exclude_patterns: Union[str, Set[str]] = None\n):\n    \"\"\"\n    Crawl files from a specific path in a GitHub repository at a specific commit.\n\n    Args:\n        repo_url (str): URL of the GitHub repository with specific path and commit\n                        (e.g., 'https://github.com/microsoft/autogen/tree/e45a15766746d95f8cfaaa705b0371267bec812e/python/packages/autogen-core/src/autogen_core')\n        token (str, optional): **GitHub personal access token.**\n            - **Required for private repositories.**\n            - **Recommended for public repos to avoid rate limits.**\n            - Can be passed explicitly or set via the `GITHUB_TOKEN` environment variable.\n        max_file_size (int, optional): Maximum file size in bytes to download (default: 1 MB)\n        use_relative_paths (bool, optional): If True, file paths will be relative to the specified subdirectory\n        include_patterns (str or set of str, optional): Pattern or set of patterns specifying which files to include (e.g., \"*.py\", {\"*.md\", \"*.txt\"}).\n                                                       If None, all files are included.\n        exclude_patterns (str or set of str, optional): Pattern or set of patterns specifying which files to exclude.\n                                                       If None, no files are excluded.\n\n    Returns:\n        dict: Dictionary with files and statistics\n    \"\"\"\n    # Convert single pattern to set\n    if include_patterns and isinstance(include_patterns, str):\n        include_patterns = {include_patterns}\n    if exclude_patterns and isinstance(exclude_patterns, str):\n        exclude_patterns = {exclude_patterns}\n\n    def should_include_file(file_path: str, file_name: str) -> bool:\n        \"\"\"Determine if a file should be included based on patterns\"\"\"\n        # If no include patterns are specified, include all files\n        if not include_patterns:\n            include_file = True\n        else:\n            # Check if file matches any include pattern\n            include_file = any(fnmatch.fnmatch(file_name, pattern) for pattern in include_patterns)\n\n        # If exclude patterns are specified, check if file should be excluded\n        if exclude_patterns and include_file:\n            # Exclude if file matches any exclude pattern\n            exclude_file = any(fnmatch.fnmatch(file_path, pattern) for pattern in exclude_patterns)\n            return not exclude_file\n\n        return include_file\n\n    # Detect SSH URL (git@ or .git suffix)\n    is_ssh_url = repo_url.startswith(\"git@\") or repo_url.endswith(\".git\")\n\n    if is_ssh_url:\n        # Clone repo via SSH to temp dir\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            print(f\"Cloning SSH repo {repo_url} to temp dir {tmpdirname} ...\")\n            try:\n                repo = git.Repo.clone_from(repo_url, tmpdirname)\n            except Exception as e:\n                print(f\"Error cloning repo: {e}\")\n                return {\"files\": {}, \"stats\": {\"error\": str(e)}}\n\n            # Attempt to checkout specific commit/branch if in URL\n            # Parse ref and subdir from SSH URL? SSH URLs don't have branch info embedded\n            # So rely on default branch, or user can checkout manually later\n            # Optionally, user can pass ref explicitly in future API\n\n            # Walk directory\n            files = {}\n            skipped_files = []\n\n            for root, dirs, filenames in os.walk(tmpdirname):\n                for filename in filenames:\n                    abs_path = os.path.join(root, filename)\n                    rel_path = os.path.relpath(abs_path, tmpdirname)\n\n                    # Check file size\n                    try:\n                        file_size = os.path.getsize(abs_path)\n                    except OSError:\n                        continue\n\n                    if file_size > max_file_size:\n                        skipped_files.append((rel_path, file_size))\n                        print(f\"Skipping {rel_path}: size {file_size} exceeds limit {max_file_size}\")\n                        continue\n\n                    # Check include/exclude patterns\n                    if not should_include_file(rel_path, filename):\n                        print(f\"Skipping {rel_path}: does not match include/exclude patterns\")\n                        continue\n\n                    # Read content\n                    try:\n                        with open(abs_path, \"r\", encoding=\"utf-8\") as f:\n                            content = f.read()\n                        files[rel_path] = content\n                        print(f\"Added {rel_path} ({file_size} bytes)\")\n                    except Exception as e:\n                        print(f\"Failed to read {rel_path}: {e}\")\n\n            return {\n                \"files\": files,\n                \"stats\": {\n                    \"downloaded_count\": len(files),\n                    \"skipped_count\": len(skipped_files),\n                    \"skipped_files\": skipped_files,\n                    \"base_path\": None,\n                    \"include_patterns\": include_patterns,\n                    \"exclude_patterns\": exclude_patterns,\n                    \"source\": \"ssh_clone\"\n                }\n            }\n\n    # Parse GitHub URL to extract owner, repo, commit/branch, and path\n    parsed_url = urlparse(repo_url)\n    path_parts = parsed_url.path.strip('/').split('/')\n    \n    if len(path_parts) < 2:\n        raise ValueError(f\"Invalid GitHub URL: {repo_url}\")\n    \n    # Extract the basic components\n    owner = path_parts[0]\n    repo = path_parts[1]\n    \n    # Check if URL contains a specific branch/commit\n    if 'tree' in path_parts:\n        tree_index = path_parts.index('tree')\n        ref = path_parts[tree_index + 1]\n        # Combine all parts after the ref as the path\n        path_start = tree_index + 2\n        specific_path = '/'.join(path_parts[path_start:]) if path_start < len(path_parts) else \"\"\n    else:\n        ref = \"main\"  # Default branch\n        specific_path = \"\"\n    \n    # Setup for GitHub API\n    headers = {\"Accept\": \"application/vnd.github.v3+json\"}\n    if token:\n        headers[\"Authorization\"] = f\"token {token}\"\n    \n    # Dictionary to store path -> content mapping\n    files = {}\n    skipped_files = []\n    \n    def should_include_file(file_path: str, file_name: str) -> bool:\n        \"\"\"Determine if a file should be included based on patterns\"\"\"\n        # If no include patterns are specified, include all files\n        if not include_patterns:\n            include_file = True\n        else:\n            # Check if file matches any include pattern\n            include_file = any(fnmatch.fnmatch(file_name, pattern) for pattern in include_patterns)\n        \n        # If exclude patterns are specified, check if file should be excluded\n        if exclude_patterns and include_file:\n            # Exclude if file matches any exclude pattern\n            exclude_file = any(fnmatch.fnmatch(file_path, pattern) for pattern in exclude_patterns)\n            return not exclude_file\n        \n        return include_file\n    \n    def fetch_contents(path):\n        \"\"\"Fetch contents of the repository at a specific path and commit\"\"\"\n        url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{path}\"\n        params = {\"ref\": ref}\n        \n        response = requests.get(url, headers=headers, params=params)\n        \n        if response.status_code == 403 and 'rate limit exceeded' in response.text.lower():\n            reset_time = int(response.headers.get('X-RateLimit-Reset', 0))\n            wait_time = max(reset_time - time.time(), 0) + 1\n            print(f\"Rate limit exceeded. Waiting for {wait_time:.0f} seconds...\")\n            time.sleep(wait_time)\n            return fetch_contents(path)\n            \n        if response.status_code == 404:\n            if not token:\n                print(f\"Error 404: Repository not found or is private.\\n\"\n                      f\"If this is a private repository, please provide a valid GitHub token via the 'token' argument or set the GITHUB_TOKEN environment variable.\")\n            else:\n                print(f\"Error 404: Path '{path}' not found in repository or insufficient permissions with the provided token.\\n\"\n                      f\"Please verify the token has access to this repository and the path exists.\")\n            return\n            \n        if response.status_code != 200:\n            print(f\"Error fetching {path}: {response.status_code} - {response.text}\")\n            return\n        \n        contents = response.json()\n        \n        # Handle both single file and directory responses\n        if not isinstance(contents, list):\n            contents = [contents]\n        \n        for item in contents:\n            item_path = item[\"path\"]\n            \n            # Calculate relative path if requested\n            if use_relative_paths and specific_path:\n                # Make sure the path is relative to the specified subdirectory\n                if item_path.startswith(specific_path):\n                    rel_path = item_path[len(specific_path):].lstrip('/')\n                else:\n                    rel_path = item_path\n            else:\n                rel_path = item_path\n            \n            if item[\"type\"] == \"file\":\n                # Check if file should be included based on patterns\n                if not should_include_file(rel_path, item[\"name\"]):\n                    print(f\"Skipping {rel_path}: Does not match include/exclude patterns\")\n                    continue\n                \n                # Check file size if available\n                file_size = item.get(\"size\", 0)\n                if file_size > max_file_size:\n                    skipped_files.append((item_path, file_size))\n                    print(f\"Skipping {rel_path}: File size ({file_size} bytes) exceeds limit ({max_file_size} bytes)\")\n                    continue\n                \n                # For files, get raw content\n                if \"download_url\" in item and item[\"download_url\"]:\n                    file_url = item[\"download_url\"]\n                    file_response = requests.get(file_url, headers=headers)\n                    \n                    # Final size check in case content-length header is available but differs from metadata\n                    content_length = int(file_response.headers.get('content-length', 0))\n                    if content_length > max_file_size:\n                        skipped_files.append((item_path, content_length))\n                        print(f\"Skipping {rel_path}: Content length ({content_length} bytes) exceeds limit ({max_file_size} bytes)\")\n                        continue\n                        \n                    if file_response.status_code == 200:\n                        files[rel_path] = file_response.text\n                        print(f\"Downloaded: {rel_path} ({file_size} bytes) \")\n                    else:\n                        print(f\"Failed to download {rel_path}: {file_response.status_code}\")\n                else:\n                    # Alternative method if download_url is not available\n                    content_response = requests.get(item[\"url\"], headers=headers)\n                    if content_response.status_code == 200:\n                        content_data = content_response.json()\n                        if content_data.get(\"encoding\") == \"base64\" and \"content\" in content_data:\n                            # Check size of base64 content before decoding\n                            if len(content_data[\"content\"]) * 0.75 > max_file_size:  # Approximate size calculation\n                                estimated_size = int(len(content_data[\"content\"]) * 0.75)\n                                skipped_files.append((item_path, estimated_size))\n                                print(f\"Skipping {rel_path}: Encoded content exceeds size limit\")\n                                continue\n                                \n                            file_content = base64.b64decode(content_data[\"content\"]).decode('utf-8')\n                            files[rel_path] = file_content\n                            print(f\"Downloaded: {rel_path} ({file_size} bytes)\")\n                        else:\n                            print(f\"Unexpected content format for {rel_path}\")\n                    else:\n                        print(f\"Failed to get content for {rel_path}: {content_response.status_code}\")\n            \n            elif item[\"type\"] == \"dir\":\n                # Recursively process subdirectories\n                fetch_contents(item_path)\n    \n    # Start crawling from the specified path\n    fetch_contents(specific_path)\n    \n    return {\n        \"files\": files,\n        \"stats\": {\n            \"downloaded_count\": len(files),\n            \"skipped_count\": len(skipped_files),\n            \"skipped_files\": skipped_files,\n            \"base_path\": specific_path if use_relative_paths else None,\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns\n        }\n    }\n\n# Example usage\nif __name__ == \"__main__\":\n    # Get token from environment variable (recommended for private repos)\n    github_token = os.environ.get(\"GITHUB_TOKEN\")\n    if not github_token:\n        print(\"Warning: No GitHub token found in environment variable 'GITHUB_TOKEN'.\\n\"\n              \"Private repositories will not be accessible without a token.\\n\"\n              \"To access private repos, set the environment variable or pass the token explicitly.\")\n    \n    repo_url = \"https://github.com/pydantic/pydantic/tree/6c38dc93f40a47f4d1350adca9ec0d72502e223f/pydantic\"\n    \n    # Example: Get Python and Markdown files, but exclude test files\n    result = crawl_github_files(\n        repo_url, \n        token=github_token,\n        max_file_size=1 * 1024 * 1024,  # 1 MB in bytes\n        use_relative_paths=True,  # Enable relative paths\n        include_patterns={\"*.py\", \"*.md\"},  # Include Python and Markdown files\n    )\n    \n    files = result[\"files\"]\n    stats = result[\"stats\"]\n    \n    print(f\"\\nDownloaded {stats['downloaded_count']} files.\")\n    print(f\"Skipped {stats['skipped_count']} files due to size limits or patterns.\")\n    print(f\"Base path for relative paths: {stats['base_path']}\")\n    print(f\"Include patterns: {stats['include_patterns']}\")\n    print(f\"Exclude patterns: {stats['exclude_patterns']}\")\n    \n    # Display all file paths in the dictionary\n    print(\"\\nFiles in dictionary:\")\n    for file_path in sorted(files.keys()):\n        print(f\"  {file_path}\")\n    \n    # Example: accessing content of a specific file\n    if files:\n        sample_file = next(iter(files))\n        print(f\"\\nSample file: {sample_file}\")\n        print(f\"Content preview: {files[sample_file][:200]}...\")\n\n--- File Index 7: utils/crawl_local_files.py ---\nimport os\nimport fnmatch\n\ndef crawl_local_files(directory, include_patterns=None, exclude_patterns=None, max_file_size=None, use_relative_paths=True):\n    \"\"\"\n    Crawl files in a local directory with similar interface as crawl_github_files.\n    \n    Args:\n        directory (str): Path to local directory\n        include_patterns (set): File patterns to include (e.g. {\"*.py\", \"*.js\"})\n        exclude_patterns (set): File patterns to exclude (e.g. {\"tests/*\"})\n        max_file_size (int): Maximum file size in bytes\n        use_relative_paths (bool): Whether to use paths relative to directory\n        \n    Returns:\n        dict: {\"files\": {filepath: content}}\n    \"\"\"\n    if not os.path.isdir(directory):\n        raise ValueError(f\"Directory does not exist: {directory}\")\n        \n    files_dict = {}\n    \n    for root, _, files in os.walk(directory):\n        for filename in files:\n            filepath = os.path.join(root, filename)\n            \n            # Get path relative to directory if requested\n            if use_relative_paths:\n                relpath = os.path.relpath(filepath, directory)\n            else:\n                relpath = filepath\n                \n            # Check if file matches any include pattern\n            included = False\n            if include_patterns:\n                for pattern in include_patterns:\n                    if fnmatch.fnmatch(relpath, pattern):\n                        included = True\n                        break\n            else:\n                included = True\n                \n            # Check if file matches any exclude pattern\n            excluded = False\n            if exclude_patterns:\n                for pattern in exclude_patterns:\n                    if fnmatch.fnmatch(relpath, pattern):\n                        excluded = True\n                        break\n                        \n            if not included or excluded:\n                continue\n                \n            # Check file size\n            if max_file_size and os.path.getsize(filepath) > max_file_size:\n                continue\n                \n            try:\n                with open(filepath, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                files_dict[relpath] = content\n            except Exception as e:\n                print(f\"Warning: Could not read file {filepath}: {e}\")\n                \n    return {\"files\": files_dict}\n\nif __name__ == \"__main__\":\n    print(\"--- Crawling parent directory ('..') ---\")\n    files_data = crawl_local_files(\"..\", exclude_patterns={\"*.pyc\", \"__pycache__/*\", \".git/*\", \"output/*\"})\n    print(f\"Found {len(files_data['files'])} files:\")\n    for path in files_data[\"files\"]:\n        print(f\"  {path}\")\n\n\n\nAnalyze the codebase context.\nIdentify the top 5-10 core most important abstractions to help those new to the codebase.\n\nFor each abstraction, provide:\n1. A concise `name`.\n2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\nList of file indices and paths present in the context:\n- 0 # README.md\n- 1 # flow.py\n- 2 # main.py\n- 3 # nodes.py\n- 4 # utils/__init__.py\n- 5 # utils/call_llm.py\n- 6 # utils/crawl_github_files.py\n- 7 # utils/crawl_local_files.py\n\nFormat the output as a YAML list of dictionaries:\n\n```yaml\n- name: Query Processing\n  description: | \n    Explains what the abstraction does.\n    It's like a central dispatcher routing requests.\n  file_indices:\n    - 0 # path/to/file1.py\n    - 3 # path/to/related.py\n- name: Query Optimization\n  description: |\n    Another core concept, similar to a blueprint for objects.\n  file_indices:\n    - 5 # path/to/another.js\n# ... up to 10 abstractions\n```": "```yaml\n- name: Codebase Crawling\n  description: |\n    This abstraction handles fetching and filtering code files from a repository, either a GitHub repository or a local directory. It's like a librarian searching for specific books based on certain criteria (include/exclude patterns) and size restrictions, ensuring only relevant materials are collected for analysis.\n  file_indices:\n    - 6 # utils/crawl_github_files.py\n    - 7 # utils/crawl_local_files.py\n    - 3 # nodes.py\n\n- name: LLM Orchestration (PocketFlow)\n  description: |\n    This abstraction manages the sequence of operations required to generate the codebase tutorial using a directed acyclic graph. It's similar to a workflow management system, where each step (node) performs a specific task, like fetching code or writing chapters, and the flow dictates how data passes between them. It simplifies running large language model based applications.\n  file_indices:\n    - 1 # flow.py\n    - 3 # nodes.py\n\n- name: AI-Powered Code Analysis and Generation\n  description: |\n    This abstraction utilizes large language models (LLMs) to analyze the codebase, identify core components, and generate beginner-friendly tutorial content.  It's similar to an AI tutor that understands complex code and breaks it down into simple explanations, tailored for beginners. LLMs take a prompt and generate code that can be understood by anyone.\n  file_indices:\n    - 3 # nodes.py\n    - 5 # utils/call_llm.py\n\n- name: Main Execution Script\n  description: |\n    This abstraction is the entry point of the application and orchestrates the whole tutorial generation process. It parses command-line arguments, initializes necessary components, and triggers the execution of the workflow to create the final tutorial. It acts like a conductor leading an orchestra.\n  file_indices:\n    - 2 # main.py\n\n- name: Knowledge Base Construction\n  description: |\n    This abstraction organizes extracted code snippets and identified relationships into a structured format for the AI to use.  It's like organizing a messy desk into a well-organized workspace, with labeled folders and clear indexes, so the AI can easily access and understand the information. This could also be viewed as a vector database.\n  file_indices:\n    - 3 # nodes.py\n\n- name: Tutorial Content Combination\n  description: |\n    This abstraction takes the individual chapters generated by the LLM and assembles them into a complete and coherent tutorial, including an introduction, chapter summaries, and a high-level overview diagram. It's similar to an editor compiling individual articles into a polished book.\n  file_indices:\n    - 3 # nodes.py\n```", "\nBased on the following abstractions and relevant code snippets from the project `Tutorial-Codebase-Knowledge`:\n\nList of Abstraction Indices and Names:\n0 # Codebase Crawling\n1 # LLM Orchestration (PocketFlow)\n2 # AI-Powered Code Analysis and Generation\n3 # Main Execution Script\n4 # Knowledge Base Construction\n5 # Tutorial Content Combination\n\nContext (Abstractions, Descriptions, Code):\nIdentified Abstractions:\n- Index 0: Codebase Crawling (Relevant file indices: [3, 6, 7])\n  Description: This abstraction handles fetching and filtering code files from a repository, either a GitHub repository or a local directory. It's like a librarian searching for specific books based on certain criteria (include/exclude patterns) and size restrictions, ensuring only relevant materials are collected for analysis.\n\n- Index 1: LLM Orchestration (PocketFlow) (Relevant file indices: [1, 3])\n  Description: This abstraction manages the sequence of operations required to generate the codebase tutorial using a directed acyclic graph. It's similar to a workflow management system, where each step (node) performs a specific task, like fetching code or writing chapters, and the flow dictates how data passes between them. It simplifies running large language model based applications.\n\n- Index 2: AI-Powered Code Analysis and Generation (Relevant file indices: [3, 5])\n  Description: This abstraction utilizes large language models (LLMs) to analyze the codebase, identify core components, and generate beginner-friendly tutorial content.  It's similar to an AI tutor that understands complex code and breaks it down into simple explanations, tailored for beginners. LLMs take a prompt and generate code that can be understood by anyone.\n\n- Index 3: Main Execution Script (Relevant file indices: [2])\n  Description: This abstraction is the entry point of the application and orchestrates the whole tutorial generation process. It parses command-line arguments, initializes necessary components, and triggers the execution of the workflow to create the final tutorial. It acts like a conductor leading an orchestra.\n\n- Index 4: Knowledge Base Construction (Relevant file indices: [3])\n  Description: This abstraction organizes extracted code snippets and identified relationships into a structured format for the AI to use.  It's like organizing a messy desk into a well-organized workspace, with labeled folders and clear indexes, so the AI can easily access and understand the information. This could also be viewed as a vector database.\n\n- Index 5: Tutorial Content Combination (Relevant file indices: [3])\n  Description: This abstraction takes the individual chapters generated by the LLM and assembles them into a complete and coherent tutorial, including an introduction, chapter summaries, and a high-level overview diagram. It's similar to an editor compiling individual articles into a polished book.\n\n\nRelevant File Snippets (Referenced by Index and Path):\n--- File: 1 # flow.py ---\nfrom pocketflow import Flow\n# Import all node classes from nodes.py\nfrom nodes import (\n    FetchRepo,\n    IdentifyAbstractions,\n    AnalyzeRelationships,\n    OrderChapters,\n    WriteChapters,\n    CombineTutorial\n)\n\ndef create_tutorial_flow():\n    \"\"\"Creates and returns the codebase tutorial generation flow.\"\"\"\n\n    # Instantiate nodes\n    fetch_repo = FetchRepo()\n    identify_abstractions = IdentifyAbstractions(max_retries=3, wait=10)\n    analyze_relationships = AnalyzeRelationships(max_retries=3, wait=10)\n    order_chapters = OrderChapters(max_retries=3, wait=10)\n    write_chapters = WriteChapters(max_retries=3, wait=10) # This is a BatchNode\n    combine_tutorial = CombineTutorial()\n\n    # Connect nodes in sequence based on the design\n    fetch_repo >> identify_abstractions\n    identify_abstractions >> analyze_relationships\n    analyze_relationships >> order_chapters\n    order_chapters >> write_chapters\n    write_chapters >> combine_tutorial\n\n    # Create the flow starting with FetchRepo\n    tutorial_flow = Flow(start=fetch_repo)\n\n    return tutorial_flow\n\n--- File: 2 # main.py ---\nimport dotenv\nimport os\nimport argparse\n# Import the function that creates the flow\nfrom flow import create_tutorial_flow\n\ndotenv.load_dotenv()\n\n# Default file patterns\nDEFAULT_INCLUDE_PATTERNS = {\n    \"*.py\", \"*.js\", \"*.jsx\", \"*.ts\", \"*.tsx\", \"*.go\", \"*.java\", \"*.pyi\", \"*.pyx\", \n    \"*.c\", \"*.cc\", \"*.cpp\", \"*.h\", \"*.md\", \"*.rst\", \"Dockerfile\", \n    \"Makefile\", \"*.yaml\", \"*.yml\",\n}\n\nDEFAULT_EXCLUDE_PATTERNS = {\n    \"*test*\", \"tests/*\", \"docs/*\", \"examples/*\", \"v1/*\", \n    \"dist/*\", \"build/*\", \"experimental/*\", \"deprecated/*\", \n    \"legacy/*\", \".git/*\", \".github/*\", \".next/*\", \".vscode/*\", \"obj/*\", \"bin/*\", \"node_modules/*\", \"*.log\"\n}\n\n# --- Main Function ---\ndef main():\n    parser = argparse.ArgumentParser(description=\"Generate a tutorial for a GitHub codebase or local directory.\")\n    \n    # Create mutually exclusive group for source\n    source_group = parser.add_mutually_exclusive_group(required=True)\n    source_group.add_argument(\"--repo\", help=\"URL of the public GitHub repository.\")\n    source_group.add_argument(\"--dir\", help=\"Path to local directory.\")\n    \n    parser.add_argument(\"-n\", \"--name\", help=\"Project name (optional, derived from repo/directory if omitted).\")\n    parser.add_argument(\"-t\", \"--token\", help=\"GitHub personal access token (optional, reads from GITHUB_TOKEN env var if not provided).\")\n    parser.add_argument(\"-o\", \"--output\", default=\"output\", help=\"Base directory for output (default: ./output).\")\n    parser.add_argument(\"-i\", \"--include\", nargs=\"+\", help=\"Include file patterns (e.g. '*.py' '*.js'). Defaults to common code files if not specified.\")\n    parser.add_argument(\"-e\", \"--exclude\", nargs=\"+\", help=\"Exclude file patterns (e.g. 'tests/*' 'docs/*'). Defaults to test/build directories if not specified.\")\n    parser.add_argument(\"-s\", \"--max-size\", type=int, default=100000, help=\"Maximum file size in bytes (default: 100000, about 100KB).\")\n\n    args = parser.parse_args()\n\n    # Get GitHub token from argument or environment variable if using repo\n    github_token = None\n    if args.repo:\n        github_token = args.token or os.environ.get('GITHUB_TOKEN')\n        if not github_token:\n            print(\"Warning: No GitHub token provided. You might hit rate limits for public repositories.\")\n\n    # Initialize the shared dictionary with inputs\n    shared = {\n        \"repo_url\": args.repo,\n        \"local_dir\": args.dir,\n        \"project_name\": args.name, # Can be None, FetchRepo will derive it\n        \"github_token\": github_token,\n        \"output_dir\": args.output, # Base directory for CombineTutorial output\n\n        # Add include/exclude patterns and max file size\n        \"include_patterns\": set(args.include) if args.include else DEFAULT_INCLUDE_PATTERNS,\n        \"exclude_patterns\": set(args.exclude) if args.exclude else DEFAULT_EXCLUDE_PATTERNS,\n        \"max_file_size\": args.max_size,\n\n        # Outputs will be populated by the nodes\n        \"files\": [],\n        \"abstractions\": [],\n        \"relationships\": {},\n        \"chapter_order\": [],\n        \"chapters\": [],\n        \"final_output_dir\": None\n    }\n\n    print(f\"Starting tutorial generation for: {args.repo or args.dir}\")\n\n    # Create the flow instance\n    tutorial_flow = create_tutorial_flow()\n\n    # Run the flow\n    tutorial_flow.run(shared)\n    \nif __name__ == \"__main__\":\n    main()\n\n--- File: 3 # nodes.py ---\nimport os\nimport yaml\nfrom pocketflow import Node, BatchNode\nfrom utils.crawl_github_files import crawl_github_files\nfrom utils.call_llm import call_llm \nfrom utils.crawl_local_files import crawl_local_files\n\n# Helper to get content for specific file indices\ndef get_content_for_indices(files_data, indices):\n    content_map = {}\n    for i in indices:\n        if 0 <= i < len(files_data):\n            path, content = files_data[i]\n            content_map[f\"{i} # {path}\"] = content # Use index + path as key for context\n    return content_map\n\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"local_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        if not project_name:\n            # Basic name derivation from URL or directory\n            if repo_url:\n                project_name = repo_url.split('/')[-1].replace('.git', '')\n            else:\n                project_name = os.path.basename(os.path.abspath(local_dir))\n            shared[\"project_name\"] = project_name\n\n        # Get file patterns directly from shared\n        include_patterns = shared[\"include_patterns\"]\n        exclude_patterns = shared[\"exclude_patterns\"]\n        max_file_size = shared[\"max_file_size\"]\n\n        return {\n            \"repo_url\": repo_url,\n            \"local_dir\": local_dir,\n            \"token\": shared.get(\"github_token\"),\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns,\n            \"max_file_size\": max_file_size,\n            \"use_relative_paths\": True\n        }\n\n    def exec(self, prep_res):\n        if prep_res[\"repo_url\"]:\n            print(f\"Crawling repository: {prep_res['repo_url']}...\")\n            result = crawl_github_files(\n                repo_url=prep_res[\"repo_url\"],\n                token=prep_res[\"token\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        else:\n            print(f\"Crawling directory: {prep_res['local_dir']}...\")\n            result = crawl_local_files(\n                directory=prep_res[\"local_dir\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n            \n        # Convert dict to list of tuples: [(path, content), ...]\n        files_list = list(result.get(\"files\", {}).items())\n        print(f\"Fetched {len(files_list)} files.\")\n        return files_list\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"files\"] = exec_res # List of (path, content) tuples\n\nclass IdentifyAbstractions(Node):\n    def prep(self, shared):\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        \n        # Helper to create context from files, respecting limits (basic example)\n        def create_llm_context(files_data):\n            context = \"\"\n            file_info = [] # Store tuples of (index, path)\n            for i, (path, content) in enumerate(files_data):\n                entry = f\"--- File Index {i}: {path} ---\\n{content}\\n\\n\"\n                context += entry\n                file_info.append((i, path))\n\n            return context, file_info # file_info is list of (index, path)\n\n        context, file_info = create_llm_context(files_data)\n        # Format file info for the prompt (comment is just a hint for LLM)\n        file_listing_for_prompt = \"\\n\".join([f\"- {idx} # {path}\" for idx, path in file_info])\n        return context, file_listing_for_prompt, len(files_data), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, file_listing_for_prompt, file_count, project_name = prep_res  # Unpack project name\n        print(\"Identifying abstractions using LLM...\")\n        prompt = f\"\"\"\nFor the project `{project_name}`:\n\nCodebase Context:\n{context}\n\nAnalyze the codebase context.\nIdentify the top 5-10 core most important abstractions to help those new to the codebase.\n\nFor each abstraction, provide:\n1. A concise `name`.\n2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\nList of file indices and paths present in the context:\n{file_listing_for_prompt}\n\nFormat the output as a YAML list of dictionaries:\n\n```yaml\n- name: Query Processing\n  description: | \n    Explains what the abstraction does.\n    It's like a central dispatcher routing requests.\n  file_indices:\n    - 0 # path/to/file1.py\n    - 3 # path/to/related.py\n- name: Query Optimization\n  description: |\n    Another core concept, similar to a blueprint for objects.\n  file_indices:\n    - 5 # path/to/another.js\n# ... up to 10 abstractions\n```\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        abstractions = yaml.safe_load(yaml_str)\n\n        if not isinstance(abstractions, list):\n            raise ValueError(\"LLM Output is not a list\")\n\n        validated_abstractions = []\n        for item in abstractions:\n            if not isinstance(item, dict) or not all(k in item for k in [\"name\", \"description\", \"file_indices\"]):\n                raise ValueError(f\"Missing keys in abstraction item: {item}\")\n            if not isinstance(item[\"description\"], str):\n                 raise ValueError(f\"description is not a string in item: {item}\")\n            if not isinstance(item[\"file_indices\"], list):\n                 raise ValueError(f\"file_indices is not a list in item: {item}\")\n\n            # Validate indices\n            validated_indices = []\n            for idx_entry in item[\"file_indices\"]:\n                 try:\n                     if isinstance(idx_entry, int):\n                         idx = idx_entry\n                     elif isinstance(idx_entry, str) and '#' in idx_entry:\n                          idx = int(idx_entry.split('#')[0].strip())\n                     else:\n                          idx = int(str(idx_entry).strip())\n\n                     if not (0 <= idx < file_count):\n                         raise ValueError(f\"Invalid file index {idx} found in item {item['name']}. Max index is {file_count - 1}.\")\n                     validated_indices.append(idx)\n                 except (ValueError, TypeError):\n                      raise ValueError(f\"Could not parse index from entry: {idx_entry} in item {item['name']}\")\n\n            item[\"files\"] = sorted(list(set(validated_indices)))\n            # Store only the required fields\n            validated_abstractions.append({\n                \"name\": item[\"name\"],\n                \"description\": item[\"description\"],\n                \"files\": item[\"files\"]\n            })\n\n        print(f\"Identified {len(validated_abstractions)} abstractions.\")\n        return validated_abstractions\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"abstractions\"] = exec_res # List of {\"name\": str, \"description\": str, \"files\": [int]}\n\nclass AnalyzeRelationships(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"] # Now contains 'files' list of indices\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n\n        # Create context with abstraction names, indices, descriptions, and relevant file snippets\n        context = \"Identified Abstractions:\\n\"\n        all_relevant_indices = set()\n        abstraction_info_for_prompt = []\n        for i, abstr in enumerate(abstractions):\n            # Use 'files' which contains indices directly\n            file_indices_str = \", \".join(map(str, abstr['files']))\n            info_line = f\"- Index {i}: {abstr['name']} (Relevant file indices: [{file_indices_str}])\\n  Description: {abstr['description']}\"\n            context += info_line + \"\\n\"\n            abstraction_info_for_prompt.append(f\"{i} # {abstr['name']}\")\n            all_relevant_indices.update(abstr['files'])\n\n        context += \"\\nRelevant File Snippets (Referenced by Index and Path):\\n\"\n        # Get content for relevant files using helper\n        relevant_files_content_map = get_content_for_indices(\n            files_data,\n            sorted(list(all_relevant_indices))\n        )\n        # Format file content for context\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path} ---\\n{content}\"\n            for idx_path, content in relevant_files_content_map.items()\n        )\n        context += file_context_str\n\n        return context, \"\\n\".join(abstraction_info_for_prompt), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, abstraction_listing, project_name = prep_res  # Unpack project name\n        print(\"Analyzing relationships using LLM...\")\n        prompt = f\"\"\"\nBased on the following abstractions and relevant code snippets from the project `{project_name}`:\n\nList of Abstraction Indices and Names:\n{abstraction_listing}\n\nContext (Abstractions, Descriptions, Code):\n{context}\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        relationships_data = yaml.safe_load(yaml_str)\n\n        if not isinstance(relationships_data, dict) or not all(k in relationships_data for k in [\"summary\", \"relationships\"]):\n            raise ValueError(\"LLM output is not a dict or missing keys ('summary', 'relationships')\")\n        if not isinstance(relationships_data[\"summary\"], str):\n             raise ValueError(\"summary is not a string\")\n        if not isinstance(relationships_data[\"relationships\"], list):\n             raise ValueError(\"relationships is not a list\")\n\n        # Validate relationships structure\n        validated_relationships = []\n        num_abstractions = len(abstraction_listing.split('\\n'))\n        for rel in relationships_data[\"relationships\"]:\n             # Check for 'label' key\n             if not isinstance(rel, dict) or not all(k in rel for k in [\"from_abstraction\", \"to_abstraction\", \"label\"]):\n                  raise ValueError(f\"Missing keys (expected from_abstraction, to_abstraction, label) in relationship item: {rel}\")\n             # Validate 'label' is a string\n             if not isinstance(rel[\"label\"], str):\n                  raise ValueError(f\"Relationship label is not a string: {rel}\")\n\n             # Validate indices\n             try:\n                 from_idx = int(str(rel[\"from_abstraction\"]).split('#')[0].strip())\n                 to_idx = int(str(rel[\"to_abstraction\"]).split('#')[0].strip())\n                 if not (0 <= from_idx < num_abstractions and 0 <= to_idx < num_abstractions):\n                      raise ValueError(f\"Invalid index in relationship: from={from_idx}, to={to_idx}. Max index is {num_abstractions-1}.\")\n                 validated_relationships.append({\n                     \"from\": from_idx,\n                     \"to\": to_idx,\n                     \"label\": rel[\"label\"] \n                 })\n             except (ValueError, TypeError):\n                  raise ValueError(f\"Could not parse indices from relationship: {rel}\")\n\n        print(\"Generated project summary and relationship details.\")\n        return {\n            \"summary\": relationships_data[\"summary\"],\n            \"details\": validated_relationships # Store validated, index-based relationships\n        }\n\n\n    def post(self, shared, prep_res, exec_res):\n        # Structure is now {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        shared[\"relationships\"] = exec_res\n\nclass OrderChapters(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"]\n        relationships = shared[\"relationships\"]\n        project_name = shared[\"project_name\"]  # Get project name\n\n        # Prepare context for the LLM\n        abstraction_info_for_prompt = []\n        for i, a in enumerate(abstractions):\n            abstraction_info_for_prompt.append(f\"- {i} # {a['name']}\")\n        abstraction_listing = \"\\n\".join(abstraction_info_for_prompt)\n\n        context = f\"Project Summary:\\n{relationships['summary']}\\n\\n\"\n        context += \"Relationships (Indices refer to abstractions above):\\n\"\n        for rel in relationships['details']:\n             from_name = abstractions[rel['from']]['name']\n             to_name = abstractions[rel['to']]['name']\n             # Use 'label' instead of 'desc'\n             context += f\"- From {rel['from']} ({from_name}) to {rel['to']} ({to_name}): {rel['label']}\\n\"\n\n        return abstraction_listing, context, len(abstractions), project_name\n\n    def exec(self, prep_res):\n        abstraction_listing, context, num_abstractions, project_name = prep_res\n        print(\"Determining chapter order using LLM...\")\n        prompt = f\"\"\"\nGiven the following project abstractions and their relationships for the project ```` {project_name} ````:\n\nAbstractions (Index # Name):\n{abstraction_listing}\n\nContext about relationships and project summary:\n{context}\n\nIf you are going to make a tutorial for ```` {project_name} ````, what is the best order to explain these abstractions, from first to last?\nIdeally, first explain those that are the most important or foundational, perhaps user-facing concepts or entry points. Then move to more detailed, lower-level implementation details or supporting concepts.\n\nOutput the ordered list of abstraction indices, including the name in a comment for clarity. Use the format `idx # AbstractionName`.\n\n```yaml\n- 2 # FoundationalConcept\n- 0 # CoreClassA\n- 1 # CoreClassB (uses CoreClassA)\n- ...\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        # Rely on Node's built-in retry/fallback\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        ordered_indices_raw = yaml.safe_load(yaml_str)\n\n        if not isinstance(ordered_indices_raw, list):\n            raise ValueError(\"LLM output is not a list\")\n\n        ordered_indices = []\n        seen_indices = set()\n        for entry in ordered_indices_raw:\n            try:\n                 if isinstance(entry, int):\n                     idx = entry\n                 elif isinstance(entry, str) and '#' in entry:\n                      idx = int(entry.split('#')[0].strip())\n                 else:\n                      idx = int(str(entry).strip())\n\n                 if not (0 <= idx < num_abstractions):\n                      raise ValueError(f\"Invalid index {idx} in ordered list. Max index is {num_abstractions-1}.\")\n                 if idx in seen_indices:\n                     raise ValueError(f\"Duplicate index {idx} found in ordered list.\")\n                 ordered_indices.append(idx)\n                 seen_indices.add(idx)\n\n            except (ValueError, TypeError):\n                 raise ValueError(f\"Could not parse index from ordered list entry: {entry}\")\n\n        # Check if all abstractions are included\n        if len(ordered_indices) != num_abstractions:\n             raise ValueError(f\"Ordered list length ({len(ordered_indices)}) does not match number of abstractions ({num_abstractions}). Missing indices: {set(range(num_abstractions)) - seen_indices}\")\n\n        print(f\"Determined chapter order (indices): {ordered_indices}\")\n        return ordered_indices # Return the list of indices\n\n    def post(self, shared, prep_res, exec_res):\n        # exec_res is already the list of ordered indices\n        shared[\"chapter_order\"] = exec_res # List of indices\n\nclass WriteChapters(BatchNode):\n    def prep(self, shared):\n        chapter_order = shared[\"chapter_order\"] # List of indices\n        abstractions = shared[\"abstractions\"]   # List of dicts, now using 'files' with indices\n        files_data = shared[\"files\"]\n        # Get already written chapters to provide context\n        # We store them temporarily during the batch run, not in shared memory yet\n        # The 'previous_chapters_summary' will be built progressively in the exec context\n        self.chapters_written_so_far = [] # Use instance variable for temporary storage across exec calls\n\n        # Create a complete list of all chapters\n        all_chapters = []\n        chapter_filenames = {} # Store chapter filename mapping for linking\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                chapter_num = i + 1\n                chapter_name = abstractions[abstraction_index][\"name\"]\n                # Create safe filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in chapter_name).lower()\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                # Format with link\n                all_chapters.append(f\"{chapter_num}. [{chapter_name}]({filename})\")\n                # Store mapping of chapter index to filename for linking\n                chapter_filenames[abstraction_index] = {\"num\": chapter_num, \"name\": chapter_name, \"filename\": filename}\n        \n        # Create a formatted string with all chapters\n        full_chapter_listing = \"\\n\".join(all_chapters)\n\n        items_to_process = []\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                abstraction_details = abstractions[abstraction_index]\n                # Use 'files' (list of indices) directly\n                related_file_indices = abstraction_details.get(\"files\", [])\n                # Get content using helper, passing indices\n                related_files_content_map = get_content_for_indices(files_data, related_file_indices)\n                \n                # Get previous chapter info for transitions\n                prev_chapter = None\n                if i > 0:\n                    prev_idx = chapter_order[i-1]\n                    prev_chapter = chapter_filenames[prev_idx]\n                \n                # Get next chapter info for transitions\n                next_chapter = None\n                if i < len(chapter_order) - 1:\n                    next_idx = chapter_order[i+1]\n                    next_chapter = chapter_filenames[next_idx]\n\n                items_to_process.append({\n                    \"chapter_num\": i + 1,\n                    \"abstraction_index\": abstraction_index,\n                    \"abstraction_details\": abstraction_details,\n                    \"related_files_content_map\": related_files_content_map,\n                    \"project_name\": shared[\"project_name\"],  # Add project name\n                    \"full_chapter_listing\": full_chapter_listing,  # Add the full chapter listing\n                    \"chapter_filenames\": chapter_filenames,  # Add chapter filenames mapping\n                    \"prev_chapter\": prev_chapter,  # Add previous chapter info\n                    \"next_chapter\": next_chapter,  # Add next chapter info\n                    # previous_chapters_summary will be added dynamically in exec\n                })\n            else:\n                print(f\"Warning: Invalid abstraction index {abstraction_index} in chapter_order. Skipping.\")\n\n        print(f\"Preparing to write {len(items_to_process)} chapters...\")\n        return items_to_process # Iterable for BatchNode\n\n    def exec(self, item):\n        # This runs for each item prepared above\n        abstraction_name = item[\"abstraction_details\"][\"name\"]\n        chapter_num = item[\"chapter_num\"]\n        project_name = item.get(\"project_name\")  # Get from item\n        print(f\"Writing chapter {chapter_num} for: {abstraction_name} using LLM...\")\n\n        # Prepare file context string from the map\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path.split('# ')[1] if '# ' in idx_path else idx_path} ---\\n{content}\"\n            for idx_path, content in item[\"related_files_content_map\"].items()\n        )\n\n        # Get summary of chapters written *before* this one\n        # Use the temporary instance variable\n        previous_chapters_summary = \"\\n---\\n\".join(self.chapters_written_so_far)\n\n\n        prompt = f\"\"\"\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `{project_name}` about the concept: \"{abstraction_name}\". This is Chapter {chapter_num}.\n\nConcept Details:\n- Description:\n{item[\"abstraction_details\"][\"description\"]}\n\nComplete Tutorial Structure:\n{item[\"full_chapter_listing\"]}\n\nContext from previous chapters (summary):\n{previous_chapters_summary if previous_chapters_summary else \"This is the first chapter.\"}\n\nRelevant Code Snippets:\n{file_context_str if file_context_str else \"No specific code snippets provided for this abstraction.\"}\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter {chapter_num}: {abstraction_name}`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n\"\"\"\n        chapter_content = call_llm(prompt)\n        # Basic validation/cleanup\n        actual_heading = f\"# Chapter {chapter_num}: {abstraction_name}\"\n        if not chapter_content.strip().startswith(f\"# Chapter {chapter_num}\"):\n             # Add heading if missing or incorrect, trying to preserve content\n             lines = chapter_content.strip().split('\\n')\n             if lines and lines[0].strip().startswith(\"#\"): # If there's some heading, replace it\n                 lines[0] = actual_heading\n                 chapter_content = \"\\n\".join(lines)\n             else: # Otherwise, prepend it\n                 chapter_content = f\"{actual_heading}\\n\\n{chapter_content}\"\n\n        # Add the generated content to our temporary list for the next iteration's context\n        self.chapters_written_so_far.append(chapter_content)\n\n        return chapter_content # Return the Markdown string\n\n    def post(self, shared, prep_res, exec_res_list):\n        # exec_res_list contains the generated Markdown for each chapter, in order\n        shared[\"chapters\"] = exec_res_list\n        # Clean up the temporary instance variable\n        del self.chapters_written_so_far\n        print(f\"Finished writing {len(exec_res_list)} chapters.\")\n\nclass CombineTutorial(Node):\n    def prep(self, shared):\n        project_name = shared[\"project_name\"]\n        output_base_dir = shared.get(\"output_dir\", \"output\") # Default output dir\n        output_path = os.path.join(output_base_dir, project_name)\n        repo_url = shared[\"repo_url\"]  # Get the repository URL\n\n        # Use 'label' from relationships_data['details']\n        relationships_data = shared[\"relationships\"] # {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        chapter_order = shared[\"chapter_order\"] # indices\n        abstractions = shared[\"abstractions\"]   # list of dicts\n        chapters_content = shared[\"chapters\"]   # list of strings\n\n        # --- Generate Mermaid Diagram ---\n        mermaid_lines = [\"flowchart TD\"]\n        # Add nodes for each abstraction\n        for i, abstr in enumerate(abstractions):\n            # Sanitize name for Mermaid ID and label\n            node_id = f\"A{i}\"\n            sanitized_name = abstr['name'].replace('\"', '')\n            node_label = sanitized_name # Using sanitized name only, no index\n            mermaid_lines.append(f'    {node_id}[\"{node_label}\"]')\n        # Add edges for relationships using 'label'\n        for rel in relationships_data['details']:\n            from_node_id = f\"A{rel['from']}\"\n            to_node_id = f\"A{rel['to']}\"\n            # Sanitize 'label' for edge label\n            edge_label = rel['label'].replace('\"', '').replace('\\n', ' ') # Basic sanitization\n            # Limit edge label length for readability (optional, but good for diagrams)\n            max_label_len = 30 # Make it shorter for labels\n            if len(edge_label) > max_label_len:\n                edge_label = edge_label[:max_label_len-3] + \"...\"\n            mermaid_lines.append(f'    {from_node_id} -- \"{edge_label}\" --> {to_node_id}')\n\n        mermaid_diagram = \"\\n\".join(mermaid_lines)\n        # --- End Mermaid ---\n\n\n        # Prepare index.md content\n        index_content = f\"# Tutorial: {project_name}\\n\\n\"\n        index_content += f\"{relationships_data['summary']}\\n\\n\"\n        index_content += f\"**Source Repository:** [{repo_url}]({repo_url})\\n\\n\"\n\n        # Add Mermaid diagram for relationships\n        index_content += \"```mermaid\\n\"\n        index_content += mermaid_diagram + \"\\n\"\n        index_content += \"```\\n\\n\"\n\n        index_content += \"## Chapters\\n\\n\"\n\n        chapter_files = []\n        # Generate chapter links based on the determined order\n        for i, abstraction_index in enumerate(chapter_order):\n            # Ensure index is valid and we have content for it\n            if 0 <= abstraction_index < len(abstractions) and i < len(chapters_content):\n                abstraction_name = abstractions[abstraction_index][\"name\"]\n                # Sanitize name for filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in abstraction_name).lower()\n                # Use chapter number (i+1) for ordering filename\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                index_content += f\"{i+1}. [{abstraction_name}]({filename})\\n\"\n                \n                # Add attribution to chapter content\n                chapter_content = chapters_content[i]\n                if not chapter_content.endswith(\"\\n\\n\"):\n                    chapter_content += \"\\n\\n\"\n                chapter_content += \"---\\n\\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\"\n                \n                # Store filename and corresponding content\n                chapter_files.append({\"filename\": filename, \"content\": chapter_content})\n            else:\n                 print(f\"Warning: Mismatch between chapter order, abstractions, or content at index {i} (abstraction index {abstraction_index}). Skipping file generation for this entry.\")\n\n        # Add attribution to index content\n        index_content += \"\\n\\n---\\n\\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\"\n\n        return {\n            \"output_path\": output_path,\n            \"index_content\": index_content,\n            \"chapter_files\": chapter_files # List of {\"filename\": str, \"content\": str}\n        }\n\n    def exec(self, prep_res):\n        output_path = prep_res[\"output_path\"]\n        index_content = prep_res[\"index_content\"]\n        chapter_files = prep_res[\"chapter_files\"]\n\n        print(f\"Combining tutorial into directory: {output_path}\")\n        # Rely on Node's built-in retry/fallback\n        os.makedirs(output_path, exist_ok=True)\n\n        # Write index.md\n        index_filepath = os.path.join(output_path, \"index.md\")\n        with open(index_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(index_content)\n        print(f\"  - Wrote {index_filepath}\")\n\n        # Write chapter files\n        for chapter_info in chapter_files:\n            chapter_filepath = os.path.join(output_path, chapter_info[\"filename\"])\n            with open(chapter_filepath, \"w\", encoding=\"utf-8\") as f:\n                f.write(chapter_info[\"content\"])\n            print(f\"  - Wrote {chapter_filepath}\")\n\n        return output_path # Return the final path\n\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"final_output_dir\"] = exec_res # Store the output path\n        print(f\"\\nTutorial generation complete! Files are in: {exec_res}\")\n\n--- File: 5 # utils/call_llm.py ---\nfrom google import genai\nimport os\nimport logging\nimport json\nfrom datetime import datetime\n\n# Configure logging\nlog_directory = os.getenv(\"LOG_DIR\", \"logs\")\nos.makedirs(log_directory, exist_ok=True)\nlog_file = os.path.join(log_directory, f\"llm_calls_{datetime.now().strftime('%Y%m%d')}.log\")\n\n# Set up logger\nlogger = logging.getLogger(\"llm_logger\")\nlogger.setLevel(logging.INFO)\nlogger.propagate = False  # Prevent propagation to root logger\nfile_handler = logging.FileHandler(log_file)\nfile_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\nlogger.addHandler(file_handler)\n\n# Simple cache configuration\ncache_file = \"llm_cache.json\"\n\n# By default, we Google Gemini 2.5 pro, as it shows great performance for code understanding\ndef call_llm(prompt: str, use_cache: bool = True) -> str:\n    # Log the prompt\n    logger.info(f\"PROMPT: {prompt}\")\n    \n    # Check cache if enabled\n    if use_cache:\n        # Load cache from disk\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, 'r') as f:\n                    cache = json.load(f)\n            except:\n                logger.warning(f\"Failed to load cache, starting with empty cache\")\n        \n        # Return from cache if exists\n        if prompt in cache:\n            logger.info(f\"RESPONSE: {cache[prompt]}\")\n            return cache[prompt]\n    \n    # Call the LLM if not in cache or cache disabled\n    client = genai.Client(\n        vertexai=True, \n        # TODO: change to your own project id and location\n        project=os.getenv(\"GEMINI_PROJECT_ID\", \"your-project-id\"),\n        location=os.getenv(\"GEMINI_LOCATION\", \"us-central1\")\n    )\n    # You can comment the previous line and use the AI Studio key instead:\n    # client = genai.Client(\n    #     api_key=os.getenv(\"GEMINI_API_KEY\", \"your-api_key\"),\n    # )\n    model = os.getenv(\"GEMINI_MODEL\", \"gemini-2.5-pro-exp-03-25\")\n    response = client.models.generate_content(\n        model=model,\n        contents=[prompt]\n    )\n    response_text = response.text\n    \n    # Log the response\n    logger.info(f\"RESPONSE: {response_text}\")\n    \n    # Update cache if enabled\n    if use_cache:\n        # Load cache again to avoid overwrites\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, 'r') as f:\n                    cache = json.load(f)\n            except:\n                pass\n        \n        # Add to cache and save\n        cache[prompt] = response_text\n        try:\n            with open(cache_file, 'w') as f:\n                json.dump(cache, f)\n        except Exception as e:\n            logger.error(f\"Failed to save cache: {e}\")\n    \n    return response_text\n\n# # Use Anthropic Claude 3.7 Sonnet Extended Thinking\n# def call_llm(prompt, use_cache: bool = True):\n#     from anthropic import Anthropic\n#     client = Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\", \"your-api-key\"))\n#     response = client.messages.create(\n#         model=\"claude-3-7-sonnet-20250219\",\n#         max_tokens=21000,\n#         thinking={\n#             \"type\": \"enabled\",\n#             \"budget_tokens\": 20000\n#         },\n#         messages=[\n#             {\"role\": \"user\", \"content\": prompt}\n#         ]\n#     )\n#     return response.content[1].text\n\n# # Use OpenAI o1\n# def call_llm(prompt, use_cache: bool = True):    \n#     from openai import OpenAI\n#     client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n#     r = client.chat.completions.create(\n#         model=\"o1\",\n#         messages=[{\"role\": \"user\", \"content\": prompt}],\n#         response_format={\n#             \"type\": \"text\"\n#         },\n#         reasoning_effort=\"medium\",\n#         store=False\n#     )\n#     return r.choices[0].message.content\n\nif __name__ == \"__main__\":\n    test_prompt = \"Hello, how are you?\"\n    \n    # First call - should hit the API\n    print(\"Making call...\")\n    response1 = call_llm(test_prompt, use_cache=False)\n    print(f\"Response: {response1}\")\n    \n\n\n--- File: 6 # utils/crawl_github_files.py ---\nimport requests\nimport base64\nimport os\nimport tempfile\nimport git\nimport time\nimport fnmatch\nfrom typing import Union, Set, List, Dict, Tuple, Any\nfrom urllib.parse import urlparse\n\ndef crawl_github_files(\n    repo_url, \n    token=None, \n    max_file_size: int = 1 * 1024 * 1024,  # 1 MB\n    use_relative_paths: bool = False,\n    include_patterns: Union[str, Set[str]] = None,\n    exclude_patterns: Union[str, Set[str]] = None\n):\n    \"\"\"\n    Crawl files from a specific path in a GitHub repository at a specific commit.\n\n    Args:\n        repo_url (str): URL of the GitHub repository with specific path and commit\n                        (e.g., 'https://github.com/microsoft/autogen/tree/e45a15766746d95f8cfaaa705b0371267bec812e/python/packages/autogen-core/src/autogen_core')\n        token (str, optional): **GitHub personal access token.**\n            - **Required for private repositories.**\n            - **Recommended for public repos to avoid rate limits.**\n            - Can be passed explicitly or set via the `GITHUB_TOKEN` environment variable.\n        max_file_size (int, optional): Maximum file size in bytes to download (default: 1 MB)\n        use_relative_paths (bool, optional): If True, file paths will be relative to the specified subdirectory\n        include_patterns (str or set of str, optional): Pattern or set of patterns specifying which files to include (e.g., \"*.py\", {\"*.md\", \"*.txt\"}).\n                                                       If None, all files are included.\n        exclude_patterns (str or set of str, optional): Pattern or set of patterns specifying which files to exclude.\n                                                       If None, no files are excluded.\n\n    Returns:\n        dict: Dictionary with files and statistics\n    \"\"\"\n    # Convert single pattern to set\n    if include_patterns and isinstance(include_patterns, str):\n        include_patterns = {include_patterns}\n    if exclude_patterns and isinstance(exclude_patterns, str):\n        exclude_patterns = {exclude_patterns}\n\n    def should_include_file(file_path: str, file_name: str) -> bool:\n        \"\"\"Determine if a file should be included based on patterns\"\"\"\n        # If no include patterns are specified, include all files\n        if not include_patterns:\n            include_file = True\n        else:\n            # Check if file matches any include pattern\n            include_file = any(fnmatch.fnmatch(file_name, pattern) for pattern in include_patterns)\n\n        # If exclude patterns are specified, check if file should be excluded\n        if exclude_patterns and include_file:\n            # Exclude if file matches any exclude pattern\n            exclude_file = any(fnmatch.fnmatch(file_path, pattern) for pattern in exclude_patterns)\n            return not exclude_file\n\n        return include_file\n\n    # Detect SSH URL (git@ or .git suffix)\n    is_ssh_url = repo_url.startswith(\"git@\") or repo_url.endswith(\".git\")\n\n    if is_ssh_url:\n        # Clone repo via SSH to temp dir\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            print(f\"Cloning SSH repo {repo_url} to temp dir {tmpdirname} ...\")\n            try:\n                repo = git.Repo.clone_from(repo_url, tmpdirname)\n            except Exception as e:\n                print(f\"Error cloning repo: {e}\")\n                return {\"files\": {}, \"stats\": {\"error\": str(e)}}\n\n            # Attempt to checkout specific commit/branch if in URL\n            # Parse ref and subdir from SSH URL? SSH URLs don't have branch info embedded\n            # So rely on default branch, or user can checkout manually later\n            # Optionally, user can pass ref explicitly in future API\n\n            # Walk directory\n            files = {}\n            skipped_files = []\n\n            for root, dirs, filenames in os.walk(tmpdirname):\n                for filename in filenames:\n                    abs_path = os.path.join(root, filename)\n                    rel_path = os.path.relpath(abs_path, tmpdirname)\n\n                    # Check file size\n                    try:\n                        file_size = os.path.getsize(abs_path)\n                    except OSError:\n                        continue\n\n                    if file_size > max_file_size:\n                        skipped_files.append((rel_path, file_size))\n                        print(f\"Skipping {rel_path}: size {file_size} exceeds limit {max_file_size}\")\n                        continue\n\n                    # Check include/exclude patterns\n                    if not should_include_file(rel_path, filename):\n                        print(f\"Skipping {rel_path}: does not match include/exclude patterns\")\n                        continue\n\n                    # Read content\n                    try:\n                        with open(abs_path, \"r\", encoding=\"utf-8\") as f:\n                            content = f.read()\n                        files[rel_path] = content\n                        print(f\"Added {rel_path} ({file_size} bytes)\")\n                    except Exception as e:\n                        print(f\"Failed to read {rel_path}: {e}\")\n\n            return {\n                \"files\": files,\n                \"stats\": {\n                    \"downloaded_count\": len(files),\n                    \"skipped_count\": len(skipped_files),\n                    \"skipped_files\": skipped_files,\n                    \"base_path\": None,\n                    \"include_patterns\": include_patterns,\n                    \"exclude_patterns\": exclude_patterns,\n                    \"source\": \"ssh_clone\"\n                }\n            }\n\n    # Parse GitHub URL to extract owner, repo, commit/branch, and path\n    parsed_url = urlparse(repo_url)\n    path_parts = parsed_url.path.strip('/').split('/')\n    \n    if len(path_parts) < 2:\n        raise ValueError(f\"Invalid GitHub URL: {repo_url}\")\n    \n    # Extract the basic components\n    owner = path_parts[0]\n    repo = path_parts[1]\n    \n    # Check if URL contains a specific branch/commit\n    if 'tree' in path_parts:\n        tree_index = path_parts.index('tree')\n        ref = path_parts[tree_index + 1]\n        # Combine all parts after the ref as the path\n        path_start = tree_index + 2\n        specific_path = '/'.join(path_parts[path_start:]) if path_start < len(path_parts) else \"\"\n    else:\n        ref = \"main\"  # Default branch\n        specific_path = \"\"\n    \n    # Setup for GitHub API\n    headers = {\"Accept\": \"application/vnd.github.v3+json\"}\n    if token:\n        headers[\"Authorization\"] = f\"token {token}\"\n    \n    # Dictionary to store path -> content mapping\n    files = {}\n    skipped_files = []\n    \n    def should_include_file(file_path: str, file_name: str) -> bool:\n        \"\"\"Determine if a file should be included based on patterns\"\"\"\n        # If no include patterns are specified, include all files\n        if not include_patterns:\n            include_file = True\n        else:\n            # Check if file matches any include pattern\n            include_file = any(fnmatch.fnmatch(file_name, pattern) for pattern in include_patterns)\n        \n        # If exclude patterns are specified, check if file should be excluded\n        if exclude_patterns and include_file:\n            # Exclude if file matches any exclude pattern\n            exclude_file = any(fnmatch.fnmatch(file_path, pattern) for pattern in exclude_patterns)\n            return not exclude_file\n        \n        return include_file\n    \n    def fetch_contents(path):\n        \"\"\"Fetch contents of the repository at a specific path and commit\"\"\"\n        url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{path}\"\n        params = {\"ref\": ref}\n        \n        response = requests.get(url, headers=headers, params=params)\n        \n        if response.status_code == 403 and 'rate limit exceeded' in response.text.lower():\n            reset_time = int(response.headers.get('X-RateLimit-Reset', 0))\n            wait_time = max(reset_time - time.time(), 0) + 1\n            print(f\"Rate limit exceeded. Waiting for {wait_time:.0f} seconds...\")\n            time.sleep(wait_time)\n            return fetch_contents(path)\n            \n        if response.status_code == 404:\n            if not token:\n                print(f\"Error 404: Repository not found or is private.\\n\"\n                      f\"If this is a private repository, please provide a valid GitHub token via the 'token' argument or set the GITHUB_TOKEN environment variable.\")\n            else:\n                print(f\"Error 404: Path '{path}' not found in repository or insufficient permissions with the provided token.\\n\"\n                      f\"Please verify the token has access to this repository and the path exists.\")\n            return\n            \n        if response.status_code != 200:\n            print(f\"Error fetching {path}: {response.status_code} - {response.text}\")\n            return\n        \n        contents = response.json()\n        \n        # Handle both single file and directory responses\n        if not isinstance(contents, list):\n            contents = [contents]\n        \n        for item in contents:\n            item_path = item[\"path\"]\n            \n            # Calculate relative path if requested\n            if use_relative_paths and specific_path:\n                # Make sure the path is relative to the specified subdirectory\n                if item_path.startswith(specific_path):\n                    rel_path = item_path[len(specific_path):].lstrip('/')\n                else:\n                    rel_path = item_path\n            else:\n                rel_path = item_path\n            \n            if item[\"type\"] == \"file\":\n                # Check if file should be included based on patterns\n                if not should_include_file(rel_path, item[\"name\"]):\n                    print(f\"Skipping {rel_path}: Does not match include/exclude patterns\")\n                    continue\n                \n                # Check file size if available\n                file_size = item.get(\"size\", 0)\n                if file_size > max_file_size:\n                    skipped_files.append((item_path, file_size))\n                    print(f\"Skipping {rel_path}: File size ({file_size} bytes) exceeds limit ({max_file_size} bytes)\")\n                    continue\n                \n                # For files, get raw content\n                if \"download_url\" in item and item[\"download_url\"]:\n                    file_url = item[\"download_url\"]\n                    file_response = requests.get(file_url, headers=headers)\n                    \n                    # Final size check in case content-length header is available but differs from metadata\n                    content_length = int(file_response.headers.get('content-length', 0))\n                    if content_length > max_file_size:\n                        skipped_files.append((item_path, content_length))\n                        print(f\"Skipping {rel_path}: Content length ({content_length} bytes) exceeds limit ({max_file_size} bytes)\")\n                        continue\n                        \n                    if file_response.status_code == 200:\n                        files[rel_path] = file_response.text\n                        print(f\"Downloaded: {rel_path} ({file_size} bytes) \")\n                    else:\n                        print(f\"Failed to download {rel_path}: {file_response.status_code}\")\n                else:\n                    # Alternative method if download_url is not available\n                    content_response = requests.get(item[\"url\"], headers=headers)\n                    if content_response.status_code == 200:\n                        content_data = content_response.json()\n                        if content_data.get(\"encoding\") == \"base64\" and \"content\" in content_data:\n                            # Check size of base64 content before decoding\n                            if len(content_data[\"content\"]) * 0.75 > max_file_size:  # Approximate size calculation\n                                estimated_size = int(len(content_data[\"content\"]) * 0.75)\n                                skipped_files.append((item_path, estimated_size))\n                                print(f\"Skipping {rel_path}: Encoded content exceeds size limit\")\n                                continue\n                                \n                            file_content = base64.b64decode(content_data[\"content\"]).decode('utf-8')\n                            files[rel_path] = file_content\n                            print(f\"Downloaded: {rel_path} ({file_size} bytes)\")\n                        else:\n                            print(f\"Unexpected content format for {rel_path}\")\n                    else:\n                        print(f\"Failed to get content for {rel_path}: {content_response.status_code}\")\n            \n            elif item[\"type\"] == \"dir\":\n                # Recursively process subdirectories\n                fetch_contents(item_path)\n    \n    # Start crawling from the specified path\n    fetch_contents(specific_path)\n    \n    return {\n        \"files\": files,\n        \"stats\": {\n            \"downloaded_count\": len(files),\n            \"skipped_count\": len(skipped_files),\n            \"skipped_files\": skipped_files,\n            \"base_path\": specific_path if use_relative_paths else None,\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns\n        }\n    }\n\n# Example usage\nif __name__ == \"__main__\":\n    # Get token from environment variable (recommended for private repos)\n    github_token = os.environ.get(\"GITHUB_TOKEN\")\n    if not github_token:\n        print(\"Warning: No GitHub token found in environment variable 'GITHUB_TOKEN'.\\n\"\n              \"Private repositories will not be accessible without a token.\\n\"\n              \"To access private repos, set the environment variable or pass the token explicitly.\")\n    \n    repo_url = \"https://github.com/pydantic/pydantic/tree/6c38dc93f40a47f4d1350adca9ec0d72502e223f/pydantic\"\n    \n    # Example: Get Python and Markdown files, but exclude test files\n    result = crawl_github_files(\n        repo_url, \n        token=github_token,\n        max_file_size=1 * 1024 * 1024,  # 1 MB in bytes\n        use_relative_paths=True,  # Enable relative paths\n        include_patterns={\"*.py\", \"*.md\"},  # Include Python and Markdown files\n    )\n    \n    files = result[\"files\"]\n    stats = result[\"stats\"]\n    \n    print(f\"\\nDownloaded {stats['downloaded_count']} files.\")\n    print(f\"Skipped {stats['skipped_count']} files due to size limits or patterns.\")\n    print(f\"Base path for relative paths: {stats['base_path']}\")\n    print(f\"Include patterns: {stats['include_patterns']}\")\n    print(f\"Exclude patterns: {stats['exclude_patterns']}\")\n    \n    # Display all file paths in the dictionary\n    print(\"\\nFiles in dictionary:\")\n    for file_path in sorted(files.keys()):\n        print(f\"  {file_path}\")\n    \n    # Example: accessing content of a specific file\n    if files:\n        sample_file = next(iter(files))\n        print(f\"\\nSample file: {sample_file}\")\n        print(f\"Content preview: {files[sample_file][:200]}...\")\n\n--- File: 7 # utils/crawl_local_files.py ---\nimport os\nimport fnmatch\n\ndef crawl_local_files(directory, include_patterns=None, exclude_patterns=None, max_file_size=None, use_relative_paths=True):\n    \"\"\"\n    Crawl files in a local directory with similar interface as crawl_github_files.\n    \n    Args:\n        directory (str): Path to local directory\n        include_patterns (set): File patterns to include (e.g. {\"*.py\", \"*.js\"})\n        exclude_patterns (set): File patterns to exclude (e.g. {\"tests/*\"})\n        max_file_size (int): Maximum file size in bytes\n        use_relative_paths (bool): Whether to use paths relative to directory\n        \n    Returns:\n        dict: {\"files\": {filepath: content}}\n    \"\"\"\n    if not os.path.isdir(directory):\n        raise ValueError(f\"Directory does not exist: {directory}\")\n        \n    files_dict = {}\n    \n    for root, _, files in os.walk(directory):\n        for filename in files:\n            filepath = os.path.join(root, filename)\n            \n            # Get path relative to directory if requested\n            if use_relative_paths:\n                relpath = os.path.relpath(filepath, directory)\n            else:\n                relpath = filepath\n                \n            # Check if file matches any include pattern\n            included = False\n            if include_patterns:\n                for pattern in include_patterns:\n                    if fnmatch.fnmatch(relpath, pattern):\n                        included = True\n                        break\n            else:\n                included = True\n                \n            # Check if file matches any exclude pattern\n            excluded = False\n            if exclude_patterns:\n                for pattern in exclude_patterns:\n                    if fnmatch.fnmatch(relpath, pattern):\n                        excluded = True\n                        break\n                        \n            if not included or excluded:\n                continue\n                \n            # Check file size\n            if max_file_size and os.path.getsize(filepath) > max_file_size:\n                continue\n                \n            try:\n                with open(filepath, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                files_dict[relpath] = content\n            except Exception as e:\n                print(f\"Warning: Could not read file {filepath}: {e}\")\n                \n    return {\"files\": files_dict}\n\nif __name__ == \"__main__\":\n    print(\"--- Crawling parent directory ('..') ---\")\n    files_data = crawl_local_files(\"..\", exclude_patterns={\"*.pyc\", \"__pycache__/*\", \".git/*\", \"output/*\"})\n    print(f\"Found {len(files_data['files'])} files:\")\n    for path in files_data[\"files\"]:\n        print(f\"  {path}\")\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n": "```yaml\nsummary: |\n  This project, Tutorial-Codebase-Knowledge, automates the creation of beginner-friendly tutorials for any codebase, like a **smart teaching assistant**. It *crawls* code, *analyzes* it with AI to identify key components, and then *generates* chapters explaining each part in simple terms. The final product is a complete tutorial, designed to quickly onboard new developers.\nrelationships:\n  - from_abstraction: 0 # Codebase Crawling\n    to_abstraction: 4 # Knowledge Base Construction\n    label: \"Provides data\"\n  - from_abstraction: 4 # Knowledge Base Construction\n    to_abstraction: 2 # AI-Powered Code Analysis and Generation\n    label: \"Supplies context\"\n  - from_abstraction: 2 # AI-Powered Code Analysis and Generation\n    to_abstraction: 5 # Tutorial Content Combination\n    label: \"Generates chapters\"\n  - from_abstraction: 5 # Tutorial Content Combination\n    to_abstraction: 1 # LLM Orchestration (PocketFlow)\n    label: \"Triggers final assembly\"\n  - from_abstraction: 1 # LLM Orchestration (PocketFlow)\n    to_abstraction: 0 # Codebase Crawling\n    label: \"Orchestrates initial data fetch\"\n  - from_abstraction: 3 # Main Execution Script\n    to_abstraction: 1 # LLM Orchestration (PocketFlow)\n    label: \"Initiates tutorial generation\"\n```", "\nGiven the following project abstractions and their relationships for the project ```` Tutorial-Codebase-Knowledge ````:\n\nAbstractions (Index # Name):\n- 0 # Codebase Crawling\n- 1 # LLM Orchestration (PocketFlow)\n- 2 # AI-Powered Code Analysis and Generation\n- 3 # Main Execution Script\n- 4 # Knowledge Base Construction\n- 5 # Tutorial Content Combination\n\nContext about relationships and project summary:\nProject Summary:\nThis project, Tutorial-Codebase-Knowledge, automates the creation of beginner-friendly tutorials for any codebase, like a **smart teaching assistant**. It *crawls* code, *analyzes* it with AI to identify key components, and then *generates* chapters explaining each part in simple terms. The final product is a complete tutorial, designed to quickly onboard new developers.\n\n\nRelationships (Indices refer to abstractions above):\n- From 0 (Codebase Crawling) to 4 (Knowledge Base Construction): Provides data\n- From 4 (Knowledge Base Construction) to 2 (AI-Powered Code Analysis and Generation): Supplies context\n- From 2 (AI-Powered Code Analysis and Generation) to 5 (Tutorial Content Combination): Generates chapters\n- From 5 (Tutorial Content Combination) to 1 (LLM Orchestration (PocketFlow)): Triggers final assembly\n- From 1 (LLM Orchestration (PocketFlow)) to 0 (Codebase Crawling): Orchestrates initial data fetch\n- From 3 (Main Execution Script) to 1 (LLM Orchestration (PocketFlow)): Initiates tutorial generation\n\n\nIf you are going to make a tutorial for ```` Tutorial-Codebase-Knowledge ````, what is the best order to explain these abstractions, from first to last?\nIdeally, first explain those that are the most important or foundational, perhaps user-facing concepts or entry points. Then move to more detailed, lower-level implementation details or supporting concepts.\n\nOutput the ordered list of abstraction indices, including the name in a comment for clarity. Use the format `idx # AbstractionName`.\n\n```yaml\n- 2 # FoundationalConcept\n- 0 # CoreClassA\n- 1 # CoreClassB (uses CoreClassA)\n- ...\n```\n\nNow, provide the YAML output:\n": "```yaml\n- 3 # Main Execution Script\n- 2 # AI-Powered Code Analysis and Generation\n- 0 # Codebase Crawling\n- 4 # Knowledge Base Construction\n- 5 # Tutorial Content Combination\n- 1 # LLM Orchestration (PocketFlow)\n```", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `Tutorial-Codebase-Knowledge` about the concept: \"Main Execution Script\". This is Chapter 1.\n\nConcept Details:\n- Description:\nThis abstraction is the entry point of the application and orchestrates the whole tutorial generation process. It parses command-line arguments, initializes necessary components, and triggers the execution of the workflow to create the final tutorial. It acts like a conductor leading an orchestra.\n\n\nComplete Tutorial Structure:\n1. [Main Execution Script](01_main_execution_script.md)\n2. [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md)\n3. [Codebase Crawling](03_codebase_crawling.md)\n4. [Knowledge Base Construction](04_knowledge_base_construction.md)\n5. [Tutorial Content Combination](05_tutorial_content_combination.md)\n6. [LLM Orchestration (PocketFlow)](06_llm_orchestration__pocketflow_.md)\n\nContext from previous chapters (summary):\nThis is the first chapter.\n\nRelevant Code Snippets:\n--- File: main.py ---\nimport dotenv\nimport os\nimport argparse\n# Import the function that creates the flow\nfrom flow import create_tutorial_flow\n\ndotenv.load_dotenv()\n\n# Default file patterns\nDEFAULT_INCLUDE_PATTERNS = {\n    \"*.py\", \"*.js\", \"*.jsx\", \"*.ts\", \"*.tsx\", \"*.go\", \"*.java\", \"*.pyi\", \"*.pyx\", \n    \"*.c\", \"*.cc\", \"*.cpp\", \"*.h\", \"*.md\", \"*.rst\", \"Dockerfile\", \n    \"Makefile\", \"*.yaml\", \"*.yml\",\n}\n\nDEFAULT_EXCLUDE_PATTERNS = {\n    \"*test*\", \"tests/*\", \"docs/*\", \"examples/*\", \"v1/*\", \n    \"dist/*\", \"build/*\", \"experimental/*\", \"deprecated/*\", \n    \"legacy/*\", \".git/*\", \".github/*\", \".next/*\", \".vscode/*\", \"obj/*\", \"bin/*\", \"node_modules/*\", \"*.log\"\n}\n\n# --- Main Function ---\ndef main():\n    parser = argparse.ArgumentParser(description=\"Generate a tutorial for a GitHub codebase or local directory.\")\n    \n    # Create mutually exclusive group for source\n    source_group = parser.add_mutually_exclusive_group(required=True)\n    source_group.add_argument(\"--repo\", help=\"URL of the public GitHub repository.\")\n    source_group.add_argument(\"--dir\", help=\"Path to local directory.\")\n    \n    parser.add_argument(\"-n\", \"--name\", help=\"Project name (optional, derived from repo/directory if omitted).\")\n    parser.add_argument(\"-t\", \"--token\", help=\"GitHub personal access token (optional, reads from GITHUB_TOKEN env var if not provided).\")\n    parser.add_argument(\"-o\", \"--output\", default=\"output\", help=\"Base directory for output (default: ./output).\")\n    parser.add_argument(\"-i\", \"--include\", nargs=\"+\", help=\"Include file patterns (e.g. '*.py' '*.js'). Defaults to common code files if not specified.\")\n    parser.add_argument(\"-e\", \"--exclude\", nargs=\"+\", help=\"Exclude file patterns (e.g. 'tests/*' 'docs/*'). Defaults to test/build directories if not specified.\")\n    parser.add_argument(\"-s\", \"--max-size\", type=int, default=100000, help=\"Maximum file size in bytes (default: 100000, about 100KB).\")\n\n    args = parser.parse_args()\n\n    # Get GitHub token from argument or environment variable if using repo\n    github_token = None\n    if args.repo:\n        github_token = args.token or os.environ.get('GITHUB_TOKEN')\n        if not github_token:\n            print(\"Warning: No GitHub token provided. You might hit rate limits for public repositories.\")\n\n    # Initialize the shared dictionary with inputs\n    shared = {\n        \"repo_url\": args.repo,\n        \"local_dir\": args.dir,\n        \"project_name\": args.name, # Can be None, FetchRepo will derive it\n        \"github_token\": github_token,\n        \"output_dir\": args.output, # Base directory for CombineTutorial output\n\n        # Add include/exclude patterns and max file size\n        \"include_patterns\": set(args.include) if args.include else DEFAULT_INCLUDE_PATTERNS,\n        \"exclude_patterns\": set(args.exclude) if args.exclude else DEFAULT_EXCLUDE_PATTERNS,\n        \"max_file_size\": args.max_size,\n\n        # Outputs will be populated by the nodes\n        \"files\": [],\n        \"abstractions\": [],\n        \"relationships\": {},\n        \"chapter_order\": [],\n        \"chapters\": [],\n        \"final_output_dir\": None\n    }\n\n    print(f\"Starting tutorial generation for: {args.repo or args.dir}\")\n\n    # Create the flow instance\n    tutorial_flow = create_tutorial_flow()\n\n    # Run the flow\n    tutorial_flow.run(shared)\n    \nif __name__ == \"__main__\":\n    main()\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 1: Main Execution Script`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "# Chapter 1: Main Execution Script\n\nWelcome to the first chapter of our tutorial on building a tutorial generation system! This chapter focuses on the \"Main Execution Script,\" the conductor of our orchestra that makes everything happen.\n\nImagine you want to create a tutorial for a cool new Python library you found on GitHub.  Where do you even begin?  That's where the Main Execution Script comes in. It's the starting point of our whole tutorial-making process. It takes your instructions (like the GitHub link) and kicks off all the other parts of the system to create your tutorial.\n\n**What Problem Does It Solve?**\n\nWithout a central entry point, we'd have a bunch of disconnected pieces. The Main Execution Script brings order to chaos. It:\n\n*   **Takes your input:** Where is the code? What do you want to name the tutorial?\n*   **Sets the stage:** Gets everything ready for the tutorial generation.\n*   **Kicks off the process:** Starts all the other parts of the system in the right order.\n\n**Key Concepts**\n\nLet's break down the key concepts of the Main Execution Script:\n\n1.  **Command-Line Arguments:** These are the instructions you give the script when you run it.  Think of them as telling the script *what* to do.\n\n2.  **Configuration:**  This is setting up all the necessary components, like where to save the output.\n\n3.  **Orchestration:** This is the script's main job: making sure all the other parts of the system run in the correct sequence, like a conductor leading an orchestra. We use [LLM Orchestration (PocketFlow)](06_llm_orchestration__pocketflow_.md) for this.\n\n**Using the Main Execution Script**\n\nLet's see how you'd use the Main Execution Script to generate a tutorial for a GitHub repository.\n\n**Example Input:**\n\nLet's say you want to create a tutorial for a repository named \"my-cool-project\" on GitHub, located at `https://github.com/user/my-cool-project`. You'd run the script from your terminal like this:\n\n```bash\npython main.py --repo https://github.com/user/my-cool-project -n MyCoolProjectTutorial\n```\n\nLet's break down that command:\n\n*   `python main.py`:  This tells your computer to run the `main.py` file using Python.\n*   `--repo https://github.com/user/my-cool-project`:  This tells the script the URL of the GitHub repository.\n*   `-n MyCoolProjectTutorial`: This tells the script to name the project \"MyCoolProjectTutorial\".  If you leave this out, the script will try to figure out the name from the repository URL.\n\n**Example Output:**\n\nAfter running the command, the script will:\n\n1.  Download the code from the GitHub repository.\n2.  Analyze the code.\n3.  Create a tutorial based on the code.\n4.  Save the tutorial in a directory named \"output\" (unless you specify a different output directory).\n\nYou'll find a structured tutorial with different chapters covering the codebase's essential aspects in the `output` directory!\n\n**Code Walkthrough**\n\nLet's look at some parts of the `main.py` script:\n\n```python\nimport argparse\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Generate a tutorial...\")\n    parser.add_argument(\"--repo\", help=\"URL of the public GitHub repository.\")\n    parser.add_argument(\"-n\", \"--name\", help=\"Project name (optional...\")\n    args = parser.parse_args()\n\n    # --- Rest of the Code ---\n```\n\nThis code sets up the command-line argument parsing.  It defines the arguments we can use (like `--repo` and `-n`) and provides helpful descriptions. The `argparse` module makes it easy to get the values you provide when you run the script.\n\n```python\n    shared = {\n        \"repo_url\": args.repo,\n        \"project_name\": args.name,\n        \"output_dir\": \"output\",  # Simplification\n    }\n\n    print(f\"Starting tutorial generation for: {args.repo}\")\n\n    # --- Rest of the Code ---\n```\n\nThis code creates a `shared` dictionary, a central place to store information that all parts of the system need.  We store the repository URL, the project name, and the output directory here.  Think of it as a shared whiteboard where all the different parts of the tutorial generation system can read and write information.\n\n```python\n    # Create the flow instance\n    tutorial_flow = create_tutorial_flow()\n\n    # Run the flow\n    tutorial_flow.run(shared)\n```\n\nThis is where the magic happens! We create a `tutorial_flow` (which we will discuss more in [LLM Orchestration (PocketFlow)](06_llm_orchestration__pocketflow_.md)) and tell it to run, passing in the `shared` dictionary. The `tutorial_flow` orchestrates all the other steps to generate the tutorial.\n\n**Internal Implementation: A Step-by-Step View**\n\nHere's a simplified view of what happens inside the Main Execution Script when you run it:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant MainScript as Main Execution Script\n    participant ArgumentParser as Argument Parser\n    participant Flow as Tutorial Flow\n    \n    User->>MainScript: Run script with arguments\n    MainScript->>ArgumentParser: Parse arguments\n    ArgumentParser-->>MainScript: Return arguments\n    MainScript->>Flow: Create tutorial flow\n    MainScript->>Flow: Run flow with arguments\n    Flow-->>MainScript: Tutorial generated\n    MainScript->>User: Completion Message\n```\n\n1.  **User:** You run the `main.py` script with command-line arguments (e.g., the GitHub repository URL).\n2.  **Main Execution Script:** The script receives your command and uses `ArgumentParser` to understand your instructions.\n3.  **Argument Parser:** The argument parser takes the command-line arguments and organizes them into a usable format.\n4.  **Main Execution Script:**  The script then creates an instance of the `Tutorial Flow`. The script then runs the `Tutorial Flow`.\n5.  **Tutorial Flow:** The tutorial flow coordinates with other components like the [Codebase Crawling](03_codebase_crawling.md), [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md), and [Knowledge Base Construction](04_knowledge_base_construction.md) components to create the tutorial.\n6.  **Main Execution Script:** The script prints a completion message, indicating that the tutorial has been generated.\n\n**Implementation Details**\n\nThe `create_tutorial_flow()` function, defined in `flow.py` (not shown in the code snippets above), is responsible for creating the `tutorial_flow` instance.\n\n**Conclusion**\n\nIn this chapter, we explored the Main Execution Script, the entry point of our tutorial generation system. We learned how it takes user input, configures the system, and orchestrates the tutorial generation process.  It's the conductor that makes sure all the instruments play in harmony!\n\nIn the next chapter, we'll dive into the crucial component of [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md), where we'll see how AI helps us understand the codebase and generate meaningful tutorial content.\n", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `Tutorial-Codebase-Knowledge` about the concept: \"AI-Powered Code Analysis and Generation\". This is Chapter 2.\n\nConcept Details:\n- Description:\nThis abstraction utilizes large language models (LLMs) to analyze the codebase, identify core components, and generate beginner-friendly tutorial content.  It's similar to an AI tutor that understands complex code and breaks it down into simple explanations, tailored for beginners. LLMs take a prompt and generate code that can be understood by anyone.\n\n\nComplete Tutorial Structure:\n1. [Main Execution Script](01_main_execution_script.md)\n2. [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md)\n3. [Codebase Crawling](03_codebase_crawling.md)\n4. [Knowledge Base Construction](04_knowledge_base_construction.md)\n5. [Tutorial Content Combination](05_tutorial_content_combination.md)\n6. [LLM Orchestration (PocketFlow)](06_llm_orchestration__pocketflow_.md)\n\nContext from previous chapters (summary):\n# Chapter 1: Main Execution Script\n\nWelcome to the first chapter of our tutorial on building a tutorial generation system! This chapter focuses on the \"Main Execution Script,\" the conductor of our orchestra that makes everything happen.\n\nImagine you want to create a tutorial for a cool new Python library you found on GitHub.  Where do you even begin?  That's where the Main Execution Script comes in. It's the starting point of our whole tutorial-making process. It takes your instructions (like the GitHub link) and kicks off all the other parts of the system to create your tutorial.\n\n**What Problem Does It Solve?**\n\nWithout a central entry point, we'd have a bunch of disconnected pieces. The Main Execution Script brings order to chaos. It:\n\n*   **Takes your input:** Where is the code? What do you want to name the tutorial?\n*   **Sets the stage:** Gets everything ready for the tutorial generation.\n*   **Kicks off the process:** Starts all the other parts of the system in the right order.\n\n**Key Concepts**\n\nLet's break down the key concepts of the Main Execution Script:\n\n1.  **Command-Line Arguments:** These are the instructions you give the script when you run it.  Think of them as telling the script *what* to do.\n\n2.  **Configuration:**  This is setting up all the necessary components, like where to save the output.\n\n3.  **Orchestration:** This is the script's main job: making sure all the other parts of the system run in the correct sequence, like a conductor leading an orchestra. We use [LLM Orchestration (PocketFlow)](06_llm_orchestration__pocketflow_.md) for this.\n\n**Using the Main Execution Script**\n\nLet's see how you'd use the Main Execution Script to generate a tutorial for a GitHub repository.\n\n**Example Input:**\n\nLet's say you want to create a tutorial for a repository named \"my-cool-project\" on GitHub, located at `https://github.com/user/my-cool-project`. You'd run the script from your terminal like this:\n\n```bash\npython main.py --repo https://github.com/user/my-cool-project -n MyCoolProjectTutorial\n```\n\nLet's break down that command:\n\n*   `python main.py`:  This tells your computer to run the `main.py` file using Python.\n*   `--repo https://github.com/user/my-cool-project`:  This tells the script the URL of the GitHub repository.\n*   `-n MyCoolProjectTutorial`: This tells the script to name the project \"MyCoolProjectTutorial\".  If you leave this out, the script will try to figure out the name from the repository URL.\n\n**Example Output:**\n\nAfter running the command, the script will:\n\n1.  Download the code from the GitHub repository.\n2.  Analyze the code.\n3.  Create a tutorial based on the code.\n4.  Save the tutorial in a directory named \"output\" (unless you specify a different output directory).\n\nYou'll find a structured tutorial with different chapters covering the codebase's essential aspects in the `output` directory!\n\n**Code Walkthrough**\n\nLet's look at some parts of the `main.py` script:\n\n```python\nimport argparse\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Generate a tutorial...\")\n    parser.add_argument(\"--repo\", help=\"URL of the public GitHub repository.\")\n    parser.add_argument(\"-n\", \"--name\", help=\"Project name (optional...\")\n    args = parser.parse_args()\n\n    # --- Rest of the Code ---\n```\n\nThis code sets up the command-line argument parsing.  It defines the arguments we can use (like `--repo` and `-n`) and provides helpful descriptions. The `argparse` module makes it easy to get the values you provide when you run the script.\n\n```python\n    shared = {\n        \"repo_url\": args.repo,\n        \"project_name\": args.name,\n        \"output_dir\": \"output\",  # Simplification\n    }\n\n    print(f\"Starting tutorial generation for: {args.repo}\")\n\n    # --- Rest of the Code ---\n```\n\nThis code creates a `shared` dictionary, a central place to store information that all parts of the system need.  We store the repository URL, the project name, and the output directory here.  Think of it as a shared whiteboard where all the different parts of the tutorial generation system can read and write information.\n\n```python\n    # Create the flow instance\n    tutorial_flow = create_tutorial_flow()\n\n    # Run the flow\n    tutorial_flow.run(shared)\n```\n\nThis is where the magic happens! We create a `tutorial_flow` (which we will discuss more in [LLM Orchestration (PocketFlow)](06_llm_orchestration__pocketflow_.md)) and tell it to run, passing in the `shared` dictionary. The `tutorial_flow` orchestrates all the other steps to generate the tutorial.\n\n**Internal Implementation: A Step-by-Step View**\n\nHere's a simplified view of what happens inside the Main Execution Script when you run it:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant MainScript as Main Execution Script\n    participant ArgumentParser as Argument Parser\n    participant Flow as Tutorial Flow\n    \n    User->>MainScript: Run script with arguments\n    MainScript->>ArgumentParser: Parse arguments\n    ArgumentParser-->>MainScript: Return arguments\n    MainScript->>Flow: Create tutorial flow\n    MainScript->>Flow: Run flow with arguments\n    Flow-->>MainScript: Tutorial generated\n    MainScript->>User: Completion Message\n```\n\n1.  **User:** You run the `main.py` script with command-line arguments (e.g., the GitHub repository URL).\n2.  **Main Execution Script:** The script receives your command and uses `ArgumentParser` to understand your instructions.\n3.  **Argument Parser:** The argument parser takes the command-line arguments and organizes them into a usable format.\n4.  **Main Execution Script:**  The script then creates an instance of the `Tutorial Flow`. The script then runs the `Tutorial Flow`.\n5.  **Tutorial Flow:** The tutorial flow coordinates with other components like the [Codebase Crawling](03_codebase_crawling.md), [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md), and [Knowledge Base Construction](04_knowledge_base_construction.md) components to create the tutorial.\n6.  **Main Execution Script:** The script prints a completion message, indicating that the tutorial has been generated.\n\n**Implementation Details**\n\nThe `create_tutorial_flow()` function, defined in `flow.py` (not shown in the code snippets above), is responsible for creating the `tutorial_flow` instance.\n\n**Conclusion**\n\nIn this chapter, we explored the Main Execution Script, the entry point of our tutorial generation system. We learned how it takes user input, configures the system, and orchestrates the tutorial generation process.  It's the conductor that makes sure all the instruments play in harmony!\n\nIn the next chapter, we'll dive into the crucial component of [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md), where we'll see how AI helps us understand the codebase and generate meaningful tutorial content.\n\n\nRelevant Code Snippets:\n--- File: nodes.py ---\nimport os\nimport yaml\nfrom pocketflow import Node, BatchNode\nfrom utils.crawl_github_files import crawl_github_files\nfrom utils.call_llm import call_llm \nfrom utils.crawl_local_files import crawl_local_files\n\n# Helper to get content for specific file indices\ndef get_content_for_indices(files_data, indices):\n    content_map = {}\n    for i in indices:\n        if 0 <= i < len(files_data):\n            path, content = files_data[i]\n            content_map[f\"{i} # {path}\"] = content # Use index + path as key for context\n    return content_map\n\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"local_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        if not project_name:\n            # Basic name derivation from URL or directory\n            if repo_url:\n                project_name = repo_url.split('/')[-1].replace('.git', '')\n            else:\n                project_name = os.path.basename(os.path.abspath(local_dir))\n            shared[\"project_name\"] = project_name\n\n        # Get file patterns directly from shared\n        include_patterns = shared[\"include_patterns\"]\n        exclude_patterns = shared[\"exclude_patterns\"]\n        max_file_size = shared[\"max_file_size\"]\n\n        return {\n            \"repo_url\": repo_url,\n            \"local_dir\": local_dir,\n            \"token\": shared.get(\"github_token\"),\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns,\n            \"max_file_size\": max_file_size,\n            \"use_relative_paths\": True\n        }\n\n    def exec(self, prep_res):\n        if prep_res[\"repo_url\"]:\n            print(f\"Crawling repository: {prep_res['repo_url']}...\")\n            result = crawl_github_files(\n                repo_url=prep_res[\"repo_url\"],\n                token=prep_res[\"token\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        else:\n            print(f\"Crawling directory: {prep_res['local_dir']}...\")\n            result = crawl_local_files(\n                directory=prep_res[\"local_dir\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n            \n        # Convert dict to list of tuples: [(path, content), ...]\n        files_list = list(result.get(\"files\", {}).items())\n        print(f\"Fetched {len(files_list)} files.\")\n        return files_list\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"files\"] = exec_res # List of (path, content) tuples\n\nclass IdentifyAbstractions(Node):\n    def prep(self, shared):\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        \n        # Helper to create context from files, respecting limits (basic example)\n        def create_llm_context(files_data):\n            context = \"\"\n            file_info = [] # Store tuples of (index, path)\n            for i, (path, content) in enumerate(files_data):\n                entry = f\"--- File Index {i}: {path} ---\\n{content}\\n\\n\"\n                context += entry\n                file_info.append((i, path))\n\n            return context, file_info # file_info is list of (index, path)\n\n        context, file_info = create_llm_context(files_data)\n        # Format file info for the prompt (comment is just a hint for LLM)\n        file_listing_for_prompt = \"\\n\".join([f\"- {idx} # {path}\" for idx, path in file_info])\n        return context, file_listing_for_prompt, len(files_data), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, file_listing_for_prompt, file_count, project_name = prep_res  # Unpack project name\n        print(\"Identifying abstractions using LLM...\")\n        prompt = f\"\"\"\nFor the project `{project_name}`:\n\nCodebase Context:\n{context}\n\nAnalyze the codebase context.\nIdentify the top 5-10 core most important abstractions to help those new to the codebase.\n\nFor each abstraction, provide:\n1. A concise `name`.\n2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\nList of file indices and paths present in the context:\n{file_listing_for_prompt}\n\nFormat the output as a YAML list of dictionaries:\n\n```yaml\n- name: Query Processing\n  description: | \n    Explains what the abstraction does.\n    It's like a central dispatcher routing requests.\n  file_indices:\n    - 0 # path/to/file1.py\n    - 3 # path/to/related.py\n- name: Query Optimization\n  description: |\n    Another core concept, similar to a blueprint for objects.\n  file_indices:\n    - 5 # path/to/another.js\n# ... up to 10 abstractions\n```\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        abstractions = yaml.safe_load(yaml_str)\n\n        if not isinstance(abstractions, list):\n            raise ValueError(\"LLM Output is not a list\")\n\n        validated_abstractions = []\n        for item in abstractions:\n            if not isinstance(item, dict) or not all(k in item for k in [\"name\", \"description\", \"file_indices\"]):\n                raise ValueError(f\"Missing keys in abstraction item: {item}\")\n            if not isinstance(item[\"description\"], str):\n                 raise ValueError(f\"description is not a string in item: {item}\")\n            if not isinstance(item[\"file_indices\"], list):\n                 raise ValueError(f\"file_indices is not a list in item: {item}\")\n\n            # Validate indices\n            validated_indices = []\n            for idx_entry in item[\"file_indices\"]:\n                 try:\n                     if isinstance(idx_entry, int):\n                         idx = idx_entry\n                     elif isinstance(idx_entry, str) and '#' in idx_entry:\n                          idx = int(idx_entry.split('#')[0].strip())\n                     else:\n                          idx = int(str(idx_entry).strip())\n\n                     if not (0 <= idx < file_count):\n                         raise ValueError(f\"Invalid file index {idx} found in item {item['name']}. Max index is {file_count - 1}.\")\n                     validated_indices.append(idx)\n                 except (ValueError, TypeError):\n                      raise ValueError(f\"Could not parse index from entry: {idx_entry} in item {item['name']}\")\n\n            item[\"files\"] = sorted(list(set(validated_indices)))\n            # Store only the required fields\n            validated_abstractions.append({\n                \"name\": item[\"name\"],\n                \"description\": item[\"description\"],\n                \"files\": item[\"files\"]\n            })\n\n        print(f\"Identified {len(validated_abstractions)} abstractions.\")\n        return validated_abstractions\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"abstractions\"] = exec_res # List of {\"name\": str, \"description\": str, \"files\": [int]}\n\nclass AnalyzeRelationships(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"] # Now contains 'files' list of indices\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n\n        # Create context with abstraction names, indices, descriptions, and relevant file snippets\n        context = \"Identified Abstractions:\\n\"\n        all_relevant_indices = set()\n        abstraction_info_for_prompt = []\n        for i, abstr in enumerate(abstractions):\n            # Use 'files' which contains indices directly\n            file_indices_str = \", \".join(map(str, abstr['files']))\n            info_line = f\"- Index {i}: {abstr['name']} (Relevant file indices: [{file_indices_str}])\\n  Description: {abstr['description']}\"\n            context += info_line + \"\\n\"\n            abstraction_info_for_prompt.append(f\"{i} # {abstr['name']}\")\n            all_relevant_indices.update(abstr['files'])\n\n        context += \"\\nRelevant File Snippets (Referenced by Index and Path):\\n\"\n        # Get content for relevant files using helper\n        relevant_files_content_map = get_content_for_indices(\n            files_data,\n            sorted(list(all_relevant_indices))\n        )\n        # Format file content for context\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path} ---\\n{content}\"\n            for idx_path, content in relevant_files_content_map.items()\n        )\n        context += file_context_str\n\n        return context, \"\\n\".join(abstraction_info_for_prompt), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, abstraction_listing, project_name = prep_res  # Unpack project name\n        print(\"Analyzing relationships using LLM...\")\n        prompt = f\"\"\"\nBased on the following abstractions and relevant code snippets from the project `{project_name}`:\n\nList of Abstraction Indices and Names:\n{abstraction_listing}\n\nContext (Abstractions, Descriptions, Code):\n{context}\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        relationships_data = yaml.safe_load(yaml_str)\n\n        if not isinstance(relationships_data, dict) or not all(k in relationships_data for k in [\"summary\", \"relationships\"]):\n            raise ValueError(\"LLM output is not a dict or missing keys ('summary', 'relationships')\")\n        if not isinstance(relationships_data[\"summary\"], str):\n             raise ValueError(\"summary is not a string\")\n        if not isinstance(relationships_data[\"relationships\"], list):\n             raise ValueError(\"relationships is not a list\")\n\n        # Validate relationships structure\n        validated_relationships = []\n        num_abstractions = len(abstraction_listing.split('\\n'))\n        for rel in relationships_data[\"relationships\"]:\n             # Check for 'label' key\n             if not isinstance(rel, dict) or not all(k in rel for k in [\"from_abstraction\", \"to_abstraction\", \"label\"]):\n                  raise ValueError(f\"Missing keys (expected from_abstraction, to_abstraction, label) in relationship item: {rel}\")\n             # Validate 'label' is a string\n             if not isinstance(rel[\"label\"], str):\n                  raise ValueError(f\"Relationship label is not a string: {rel}\")\n\n             # Validate indices\n             try:\n                 from_idx = int(str(rel[\"from_abstraction\"]).split('#')[0].strip())\n                 to_idx = int(str(rel[\"to_abstraction\"]).split('#')[0].strip())\n                 if not (0 <= from_idx < num_abstractions and 0 <= to_idx < num_abstractions):\n                      raise ValueError(f\"Invalid index in relationship: from={from_idx}, to={to_idx}. Max index is {num_abstractions-1}.\")\n                 validated_relationships.append({\n                     \"from\": from_idx,\n                     \"to\": to_idx,\n                     \"label\": rel[\"label\"] \n                 })\n             except (ValueError, TypeError):\n                  raise ValueError(f\"Could not parse indices from relationship: {rel}\")\n\n        print(\"Generated project summary and relationship details.\")\n        return {\n            \"summary\": relationships_data[\"summary\"],\n            \"details\": validated_relationships # Store validated, index-based relationships\n        }\n\n\n    def post(self, shared, prep_res, exec_res):\n        # Structure is now {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        shared[\"relationships\"] = exec_res\n\nclass OrderChapters(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"]\n        relationships = shared[\"relationships\"]\n        project_name = shared[\"project_name\"]  # Get project name\n\n        # Prepare context for the LLM\n        abstraction_info_for_prompt = []\n        for i, a in enumerate(abstractions):\n            abstraction_info_for_prompt.append(f\"- {i} # {a['name']}\")\n        abstraction_listing = \"\\n\".join(abstraction_info_for_prompt)\n\n        context = f\"Project Summary:\\n{relationships['summary']}\\n\\n\"\n        context += \"Relationships (Indices refer to abstractions above):\\n\"\n        for rel in relationships['details']:\n             from_name = abstractions[rel['from']]['name']\n             to_name = abstractions[rel['to']]['name']\n             # Use 'label' instead of 'desc'\n             context += f\"- From {rel['from']} ({from_name}) to {rel['to']} ({to_name}): {rel['label']}\\n\"\n\n        return abstraction_listing, context, len(abstractions), project_name\n\n    def exec(self, prep_res):\n        abstraction_listing, context, num_abstractions, project_name = prep_res\n        print(\"Determining chapter order using LLM...\")\n        prompt = f\"\"\"\nGiven the following project abstractions and their relationships for the project ```` {project_name} ````:\n\nAbstractions (Index # Name):\n{abstraction_listing}\n\nContext about relationships and project summary:\n{context}\n\nIf you are going to make a tutorial for ```` {project_name} ````, what is the best order to explain these abstractions, from first to last?\nIdeally, first explain those that are the most important or foundational, perhaps user-facing concepts or entry points. Then move to more detailed, lower-level implementation details or supporting concepts.\n\nOutput the ordered list of abstraction indices, including the name in a comment for clarity. Use the format `idx # AbstractionName`.\n\n```yaml\n- 2 # FoundationalConcept\n- 0 # CoreClassA\n- 1 # CoreClassB (uses CoreClassA)\n- ...\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        # Rely on Node's built-in retry/fallback\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        ordered_indices_raw = yaml.safe_load(yaml_str)\n\n        if not isinstance(ordered_indices_raw, list):\n            raise ValueError(\"LLM output is not a list\")\n\n        ordered_indices = []\n        seen_indices = set()\n        for entry in ordered_indices_raw:\n            try:\n                 if isinstance(entry, int):\n                     idx = entry\n                 elif isinstance(entry, str) and '#' in entry:\n                      idx = int(entry.split('#')[0].strip())\n                 else:\n                      idx = int(str(entry).strip())\n\n                 if not (0 <= idx < num_abstractions):\n                      raise ValueError(f\"Invalid index {idx} in ordered list. Max index is {num_abstractions-1}.\")\n                 if idx in seen_indices:\n                     raise ValueError(f\"Duplicate index {idx} found in ordered list.\")\n                 ordered_indices.append(idx)\n                 seen_indices.add(idx)\n\n            except (ValueError, TypeError):\n                 raise ValueError(f\"Could not parse index from ordered list entry: {entry}\")\n\n        # Check if all abstractions are included\n        if len(ordered_indices) != num_abstractions:\n             raise ValueError(f\"Ordered list length ({len(ordered_indices)}) does not match number of abstractions ({num_abstractions}). Missing indices: {set(range(num_abstractions)) - seen_indices}\")\n\n        print(f\"Determined chapter order (indices): {ordered_indices}\")\n        return ordered_indices # Return the list of indices\n\n    def post(self, shared, prep_res, exec_res):\n        # exec_res is already the list of ordered indices\n        shared[\"chapter_order\"] = exec_res # List of indices\n\nclass WriteChapters(BatchNode):\n    def prep(self, shared):\n        chapter_order = shared[\"chapter_order\"] # List of indices\n        abstractions = shared[\"abstractions\"]   # List of dicts, now using 'files' with indices\n        files_data = shared[\"files\"]\n        # Get already written chapters to provide context\n        # We store them temporarily during the batch run, not in shared memory yet\n        # The 'previous_chapters_summary' will be built progressively in the exec context\n        self.chapters_written_so_far = [] # Use instance variable for temporary storage across exec calls\n\n        # Create a complete list of all chapters\n        all_chapters = []\n        chapter_filenames = {} # Store chapter filename mapping for linking\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                chapter_num = i + 1\n                chapter_name = abstractions[abstraction_index][\"name\"]\n                # Create safe filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in chapter_name).lower()\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                # Format with link\n                all_chapters.append(f\"{chapter_num}. [{chapter_name}]({filename})\")\n                # Store mapping of chapter index to filename for linking\n                chapter_filenames[abstraction_index] = {\"num\": chapter_num, \"name\": chapter_name, \"filename\": filename}\n        \n        # Create a formatted string with all chapters\n        full_chapter_listing = \"\\n\".join(all_chapters)\n\n        items_to_process = []\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                abstraction_details = abstractions[abstraction_index]\n                # Use 'files' (list of indices) directly\n                related_file_indices = abstraction_details.get(\"files\", [])\n                # Get content using helper, passing indices\n                related_files_content_map = get_content_for_indices(files_data, related_file_indices)\n                \n                # Get previous chapter info for transitions\n                prev_chapter = None\n                if i > 0:\n                    prev_idx = chapter_order[i-1]\n                    prev_chapter = chapter_filenames[prev_idx]\n                \n                # Get next chapter info for transitions\n                next_chapter = None\n                if i < len(chapter_order) - 1:\n                    next_idx = chapter_order[i+1]\n                    next_chapter = chapter_filenames[next_idx]\n\n                items_to_process.append({\n                    \"chapter_num\": i + 1,\n                    \"abstraction_index\": abstraction_index,\n                    \"abstraction_details\": abstraction_details,\n                    \"related_files_content_map\": related_files_content_map,\n                    \"project_name\": shared[\"project_name\"],  # Add project name\n                    \"full_chapter_listing\": full_chapter_listing,  # Add the full chapter listing\n                    \"chapter_filenames\": chapter_filenames,  # Add chapter filenames mapping\n                    \"prev_chapter\": prev_chapter,  # Add previous chapter info\n                    \"next_chapter\": next_chapter,  # Add next chapter info\n                    # previous_chapters_summary will be added dynamically in exec\n                })\n            else:\n                print(f\"Warning: Invalid abstraction index {abstraction_index} in chapter_order. Skipping.\")\n\n        print(f\"Preparing to write {len(items_to_process)} chapters...\")\n        return items_to_process # Iterable for BatchNode\n\n    def exec(self, item):\n        # This runs for each item prepared above\n        abstraction_name = item[\"abstraction_details\"][\"name\"]\n        chapter_num = item[\"chapter_num\"]\n        project_name = item.get(\"project_name\")  # Get from item\n        print(f\"Writing chapter {chapter_num} for: {abstraction_name} using LLM...\")\n\n        # Prepare file context string from the map\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path.split('# ')[1] if '# ' in idx_path else idx_path} ---\\n{content}\"\n            for idx_path, content in item[\"related_files_content_map\"].items()\n        )\n\n        # Get summary of chapters written *before* this one\n        # Use the temporary instance variable\n        previous_chapters_summary = \"\\n---\\n\".join(self.chapters_written_so_far)\n\n\n        prompt = f\"\"\"\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `{project_name}` about the concept: \"{abstraction_name}\". This is Chapter {chapter_num}.\n\nConcept Details:\n- Description:\n{item[\"abstraction_details\"][\"description\"]}\n\nComplete Tutorial Structure:\n{item[\"full_chapter_listing\"]}\n\nContext from previous chapters (summary):\n{previous_chapters_summary if previous_chapters_summary else \"This is the first chapter.\"}\n\nRelevant Code Snippets:\n{file_context_str if file_context_str else \"No specific code snippets provided for this abstraction.\"}\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter {chapter_num}: {abstraction_name}`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n\"\"\"\n        chapter_content = call_llm(prompt)\n        # Basic validation/cleanup\n        actual_heading = f\"# Chapter {chapter_num}: {abstraction_name}\"\n        if not chapter_content.strip().startswith(f\"# Chapter {chapter_num}\"):\n             # Add heading if missing or incorrect, trying to preserve content\n             lines = chapter_content.strip().split('\\n')\n             if lines and lines[0].strip().startswith(\"#\"): # If there's some heading, replace it\n                 lines[0] = actual_heading\n                 chapter_content = \"\\n\".join(lines)\n             else: # Otherwise, prepend it\n                 chapter_content = f\"{actual_heading}\\n\\n{chapter_content}\"\n\n        # Add the generated content to our temporary list for the next iteration's context\n        self.chapters_written_so_far.append(chapter_content)\n\n        return chapter_content # Return the Markdown string\n\n    def post(self, shared, prep_res, exec_res_list):\n        # exec_res_list contains the generated Markdown for each chapter, in order\n        shared[\"chapters\"] = exec_res_list\n        # Clean up the temporary instance variable\n        del self.chapters_written_so_far\n        print(f\"Finished writing {len(exec_res_list)} chapters.\")\n\nclass CombineTutorial(Node):\n    def prep(self, shared):\n        project_name = shared[\"project_name\"]\n        output_base_dir = shared.get(\"output_dir\", \"output\") # Default output dir\n        output_path = os.path.join(output_base_dir, project_name)\n        repo_url = shared[\"repo_url\"]  # Get the repository URL\n\n        # Use 'label' from relationships_data['details']\n        relationships_data = shared[\"relationships\"] # {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        chapter_order = shared[\"chapter_order\"] # indices\n        abstractions = shared[\"abstractions\"]   # list of dicts\n        chapters_content = shared[\"chapters\"]   # list of strings\n\n        # --- Generate Mermaid Diagram ---\n        mermaid_lines = [\"flowchart TD\"]\n        # Add nodes for each abstraction\n        for i, abstr in enumerate(abstractions):\n            # Sanitize name for Mermaid ID and label\n            node_id = f\"A{i}\"\n            sanitized_name = abstr['name'].replace('\"', '')\n            node_label = sanitized_name # Using sanitized name only, no index\n            mermaid_lines.append(f'    {node_id}[\"{node_label}\"]')\n        # Add edges for relationships using 'label'\n        for rel in relationships_data['details']:\n            from_node_id = f\"A{rel['from']}\"\n            to_node_id = f\"A{rel['to']}\"\n            # Sanitize 'label' for edge label\n            edge_label = rel['label'].replace('\"', '').replace('\\n', ' ') # Basic sanitization\n            # Limit edge label length for readability (optional, but good for diagrams)\n            max_label_len = 30 # Make it shorter for labels\n            if len(edge_label) > max_label_len:\n                edge_label = edge_label[:max_label_len-3] + \"...\"\n            mermaid_lines.append(f'    {from_node_id} -- \"{edge_label}\" --> {to_node_id}')\n\n        mermaid_diagram = \"\\n\".join(mermaid_lines)\n        # --- End Mermaid ---\n\n\n        # Prepare index.md content\n        index_content = f\"# Tutorial: {project_name}\\n\\n\"\n        index_content += f\"{relationships_data['summary']}\\n\\n\"\n        index_content += f\"**Source Repository:** [{repo_url}]({repo_url})\\n\\n\"\n\n        # Add Mermaid diagram for relationships\n        index_content += \"```mermaid\\n\"\n        index_content += mermaid_diagram + \"\\n\"\n        index_content += \"```\\n\\n\"\n\n        index_content += \"## Chapters\\n\\n\"\n\n        chapter_files = []\n        # Generate chapter links based on the determined order\n        for i, abstraction_index in enumerate(chapter_order):\n            # Ensure index is valid and we have content for it\n            if 0 <= abstraction_index < len(abstractions) and i < len(chapters_content):\n                abstraction_name = abstractions[abstraction_index][\"name\"]\n                # Sanitize name for filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in abstraction_name).lower()\n                # Use chapter number (i+1) for ordering filename\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                index_content += f\"{i+1}. [{abstraction_name}]({filename})\\n\"\n                \n                # Add attribution to chapter content\n                chapter_content = chapters_content[i]\n                if not chapter_content.endswith(\"\\n\\n\"):\n                    chapter_content += \"\\n\\n\"\n                chapter_content += \"---\\n\\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\"\n                \n                # Store filename and corresponding content\n                chapter_files.append({\"filename\": filename, \"content\": chapter_content})\n            else:\n                 print(f\"Warning: Mismatch between chapter order, abstractions, or content at index {i} (abstraction index {abstraction_index}). Skipping file generation for this entry.\")\n\n        # Add attribution to index content\n        index_content += \"\\n\\n---\\n\\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\"\n\n        return {\n            \"output_path\": output_path,\n            \"index_content\": index_content,\n            \"chapter_files\": chapter_files # List of {\"filename\": str, \"content\": str}\n        }\n\n    def exec(self, prep_res):\n        output_path = prep_res[\"output_path\"]\n        index_content = prep_res[\"index_content\"]\n        chapter_files = prep_res[\"chapter_files\"]\n\n        print(f\"Combining tutorial into directory: {output_path}\")\n        # Rely on Node's built-in retry/fallback\n        os.makedirs(output_path, exist_ok=True)\n\n        # Write index.md\n        index_filepath = os.path.join(output_path, \"index.md\")\n        with open(index_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(index_content)\n        print(f\"  - Wrote {index_filepath}\")\n\n        # Write chapter files\n        for chapter_info in chapter_files:\n            chapter_filepath = os.path.join(output_path, chapter_info[\"filename\"])\n            with open(chapter_filepath, \"w\", encoding=\"utf-8\") as f:\n                f.write(chapter_info[\"content\"])\n            print(f\"  - Wrote {chapter_filepath}\")\n\n        return output_path # Return the final path\n\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"final_output_dir\"] = exec_res # Store the output path\n        print(f\"\\nTutorial generation complete! Files are in: {exec_res}\")\n\n--- File: utils/call_llm.py ---\nfrom google import genai\nimport os\nimport logging\nimport json\nfrom datetime import datetime\n\n# Configure logging\nlog_directory = os.getenv(\"LOG_DIR\", \"logs\")\nos.makedirs(log_directory, exist_ok=True)\nlog_file = os.path.join(log_directory, f\"llm_calls_{datetime.now().strftime('%Y%m%d')}.log\")\n\n# Set up logger\nlogger = logging.getLogger(\"llm_logger\")\nlogger.setLevel(logging.INFO)\nlogger.propagate = False  # Prevent propagation to root logger\nfile_handler = logging.FileHandler(log_file)\nfile_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\nlogger.addHandler(file_handler)\n\n# Simple cache configuration\ncache_file = \"llm_cache.json\"\n\n# By default, we Google Gemini 2.5 pro, as it shows great performance for code understanding\ndef call_llm(prompt: str, use_cache: bool = True) -> str:\n    # Log the prompt\n    logger.info(f\"PROMPT: {prompt}\")\n    \n    # Check cache if enabled\n    if use_cache:\n        # Load cache from disk\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, 'r') as f:\n                    cache = json.load(f)\n            except:\n                logger.warning(f\"Failed to load cache, starting with empty cache\")\n        \n        # Return from cache if exists\n        if prompt in cache:\n            logger.info(f\"RESPONSE: {cache[prompt]}\")\n            return cache[prompt]\n    \n    # Call the LLM if not in cache or cache disabled\n    client = genai.Client(\n        vertexai=True, \n        # TODO: change to your own project id and location\n        project=os.getenv(\"GEMINI_PROJECT_ID\", \"your-project-id\"),\n        location=os.getenv(\"GEMINI_LOCATION\", \"us-central1\")\n    )\n    # You can comment the previous line and use the AI Studio key instead:\n    # client = genai.Client(\n    #     api_key=os.getenv(\"GEMINI_API_KEY\", \"your-api_key\"),\n    # )\n    model = os.getenv(\"GEMINI_MODEL\", \"gemini-2.5-pro-exp-03-25\")\n    response = client.models.generate_content(\n        model=model,\n        contents=[prompt]\n    )\n    response_text = response.text\n    \n    # Log the response\n    logger.info(f\"RESPONSE: {response_text}\")\n    \n    # Update cache if enabled\n    if use_cache:\n        # Load cache again to avoid overwrites\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, 'r') as f:\n                    cache = json.load(f)\n            except:\n                pass\n        \n        # Add to cache and save\n        cache[prompt] = response_text\n        try:\n            with open(cache_file, 'w') as f:\n                json.dump(cache, f)\n        except Exception as e:\n            logger.error(f\"Failed to save cache: {e}\")\n    \n    return response_text\n\n# # Use Anthropic Claude 3.7 Sonnet Extended Thinking\n# def call_llm(prompt, use_cache: bool = True):\n#     from anthropic import Anthropic\n#     client = Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\", \"your-api-key\"))\n#     response = client.messages.create(\n#         model=\"claude-3-7-sonnet-20250219\",\n#         max_tokens=21000,\n#         thinking={\n#             \"type\": \"enabled\",\n#             \"budget_tokens\": 20000\n#         },\n#         messages=[\n#             {\"role\": \"user\", \"content\": prompt}\n#         ]\n#     )\n#     return response.content[1].text\n\n# # Use OpenAI o1\n# def call_llm(prompt, use_cache: bool = True):    \n#     from openai import OpenAI\n#     client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n#     r = client.chat.completions.create(\n#         model=\"o1\",\n#         messages=[{\"role\": \"user\", \"content\": prompt}],\n#         response_format={\n#             \"type\": \"text\"\n#         },\n#         reasoning_effort=\"medium\",\n#         store=False\n#     )\n#     return r.choices[0].message.content\n\nif __name__ == \"__main__\":\n    test_prompt = \"Hello, how are you?\"\n    \n    # First call - should hit the API\n    print(\"Making call...\")\n    response1 = call_llm(test_prompt, use_cache=False)\n    print(f\"Response: {response1}\")\n    \n\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 2: AI-Powered Code Analysis and Generation`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "# Chapter 2: AI-Powered Code Analysis and Generation\n\nIn the previous chapter, [Main Execution Script](01_main_execution_script.md), we saw how the script acts as the conductor of our tutorial-generation process, taking your input and orchestrating the workflow. Now, we'll dive into a crucial part of that workflow: using AI to understand the code and generate content!\n\n**Why AI for Code Analysis and Generation?**\n\nImagine you're given a huge, complex Lego set with thousands of pieces and no instructions. That's what it's like trying to understand a large codebase!  It's overwhelming.  The goal of this chapter is to learn how to use AI to understand any codebase and describe it so that others can also understand it.\n\nThat's where AI comes in! Our `AI-Powered Code Analysis and Generation` component acts like an experienced tutor. It uses Large Language Models (LLMs) to:\n\n*   **Analyze the code:**  Like carefully examining each Lego brick.\n*   **Identify key components:**  Finding the essential parts of the set.\n*   **Generate beginner-friendly explanations:**  Writing clear, step-by-step instructions.\n\n**The Big Picture: Turning Code into Tutorials**\n\nThis component sits in the middle of our tutorial generation process. It takes raw code (from crawling the codebase) and turns it into structured, understandable tutorial content. We start by downloading the source code, then we'll use this component to create a tutorial.\n\n**Key Concepts**\n\nLet's break down the core ideas behind our AI-powered approach:\n\n1.  **Large Language Models (LLMs):** These are powerful AI models trained on vast amounts of text data. Think of them as having read millions of books and articles on programming.  We use them to understand code and write explanations. We pass the prompts to the LLM through the `call_llm` function, which is defined in `utils/call_llm.py`. This allows us to easily switch models by changing the environment variables for the model.\n\n2.  **Code Abstraction Identification:** The AI identifies key concepts or modules within the codebase. This is like finding the most important functions or classes.\n\n3.  **Relationship Analysis:** The AI determines how these components interact with each other.  This is like understanding how the Lego bricks fit together to build the final model.\n\n4.  **Beginner-Friendly Content Generation:** The AI translates complex code into simple, easy-to-understand explanations, tailored for beginners. It generates markdown content that is later combined to build a chapter.\n\n**How it Works: A Simplified Walkthrough**\n\nLet's walk through a simplified example using pieces of the code, starting from what it receives from the main script.\n\n```python\nshared = {\n  \"repo_url\": \"https://github.com/user/my-cool-project\",\n  \"project_name\": \"MyCoolProject\",\n  \"files\": [(\"file1.py\", \"def add(a, b): return a + b\"), (\"file2.py\", \"def multiply(a, b): return a * b\")],\n}\n```\n\nThis `shared` dictionary is passed between different components, as we saw in the [Main Execution Script](01_main_execution_script.md) chapter. We will focus on how the `IdentifyAbstractions` node processes the `shared` data.\n\n1.  **Fetching Code:** We already have the source code (simulated as `shared[\"files\"]` here).  In reality, this would come from the [Codebase Crawling](03_codebase_crawling.md) stage.\n\n2.  **Identifying Abstractions (IdentifyAbstractions Node):**\n\n   *   This node takes the codebase's content and asks the LLM to find the most important concepts.\n\n   *   It prepares a prompt for the LLM that includes the entire codebase context.\n\n   *   Here's a simplified snippet of the `IdentifyAbstractions` node's `prep` and `exec` methods (from `nodes.py`):\n\n     ```python\n     class IdentifyAbstractions(Node):\n         def prep(self, shared):\n             files_data = shared[\"files\"]\n             project_name = shared[\"project_name\"]\n             # simplified context creation\n             context = \"\"\n             file_info = []\n             for i, (path, content) in enumerate(files_data):\n                 context += f\"--- File Index {i}: {path} ---\\n{content}\\n\\n\"\n                 file_info.append((i, path))\n\n             file_listing_for_prompt = \"\\n\".join([f\"- {idx} # {path}\" for idx, path in file_info])\n             return context, file_listing_for_prompt, len(files_data), project_name\n\n         def exec(self, prep_res):\n             context, file_listing_for_prompt, file_count, project_name = prep_res\n\n             prompt = f\"\"\"\n     For the project `{project_name}`:\n\n     Codebase Context:\n     {context}\n\n     Analyze the codebase context.\n     Identify the top 2 core most important abstractions to help those new to the codebase.\n\n     For each abstraction, provide:\n     1. A concise `name`.\n     2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n     3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\n     List of file indices and paths present in the context:\n     {file_listing_for_prompt}\n\n     Format the output as a YAML list of dictionaries:\n\n     ```yaml\n     - name: Addition\n       description: |\n         This abstraction performs addition.\n         It's like adding two numbers together.\n       file_indices:\n         - 0 # file1.py\n     - name: Multiplication\n       description: |\n         This abstraction performs multiplication.\n         It's like multiplying two numbers together.\n       file_indices:\n         - 1 # file2.py\n     ```\"\"\"\n             response = call_llm(prompt) #call llm\n\n             yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n             abstractions = yaml.safe_load(yaml_str)\n\n             validated_abstractions = []\n             for item in abstractions:\n                  item[\"files\"] = [int(idx_entry.split('#')[0].strip()) for idx_entry in item[\"file_indices\"]]\n                  validated_abstractions.append({\n                       \"name\": item[\"name\"],\n                       \"description\": item[\"description\"],\n                       \"files\": item[\"files\"]\n                  })\n\n             return validated_abstractions\n\n         def post(self, shared, prep_res, exec_res):\n             shared[\"abstractions\"] = exec_res\n     ```\n\n   *   The `prep` method prepares the context and project name for the LLM by listing all files and their code in the files_data list.\n\n   *   The `exec` method prompts the LLM to analyze the code and identify the important abstractions, generating a YAML formatted string with the abstraction name, description, and corresponding file indices. After receiving this string, the LLM output is parsed and the file indices are added to the `shared` dictionary, specifically at `shared[\"abstractions\"]`.\n\n   *   The LLM might respond with something like this (the LLM will come up with a description):\n\n     ```yaml\n     - name: Addition\n       description: |\n         This abstraction performs addition.\n         It's like adding two numbers together.\n       file_indices:\n         - 0 # file1.py\n     - name: Multiplication\n       description: |\n         This abstraction performs multiplication.\n         It's like multiplying two numbers together.\n       file_indices:\n         - 1 # file2.py\n     ```\n\n3.  **Analyzing Relationships (AnalyzeRelationships Node):**\n\n   *   The `AnalyzeRelationships` node determines how the identified abstractions relate to each other, and creates a summary for the whole project.\n\n   *   Here's a simplified snippet of the `AnalyzeRelationships` node's `prep` and `exec` methods (from `nodes.py`):\n\n     ```python\n     class AnalyzeRelationships(Node):\n         def prep(self, shared):\n             abstractions = shared[\"abstractions\"]\n             files_data = shared[\"files\"]\n             context = \"Identified Abstractions:\\n\"\n             for i, abstr in enumerate(abstractions):\n                 file_indices_str = \", \".join(map(str, abstr['files']))\n                 info_line = f\"- Index {i}: {abstr['name']} (Relevant file indices: [{file_indices_str}])\\n  Description: {abstr['description']}\"\n                 context += info_line + \"\\n\"\n\n             all_relevant_indices = set()\n             for abstr in abstractions:\n                  all_relevant_indices.update(abstr['files'])\n\n             relevant_files_content_map = {}\n             for i in sorted(list(all_relevant_indices)):\n                  path, content = files_data[i]\n                  relevant_files_content_map[f\"{i} # {path}\"] = content\n\n             context += \"\\nRelevant File Snippets (Referenced by Index and Path):\\n\"\n             file_context_str = \"\\n\\n\".join(\n                  f\"--- File: {idx_path} ---\\n{content}\"\n                  for idx_path, content in relevant_files_content_map.items()\n             )\n             context += file_context_str\n             abstraction_info_for_prompt = []\n             for i, abstr in enumerate(abstractions):\n                  abstraction_info_for_prompt.append(f\"{i} # {abstr['name']}\")\n             return context, \"\\n\".join(abstraction_info_for_prompt), shared[\"project_name\"]\n\n         def exec(self, prep_res):\n             context, abstraction_listing, project_name = prep_res\n\n             prompt = f\"\"\"\n     Based on the following abstractions and relevant code snippets from the project `{project_name}`:\n\n     List of Abstraction Indices and Names:\n     {abstraction_listing}\n\n     Context (Abstractions, Descriptions, Code):\n     {context}\n\n     Please provide:\n     1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n     2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n         - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n         - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n         - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n         Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n         Simplify the relationship and exclude those non-important ones.\n\n     IMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\n     Format the output as YAML:\n\n     ```yaml\n     summary: |\n       A brief, simple explanation of the project.\n       Can span multiple lines with **bold** and *italic* for emphasis.\n     relationships:\n       - from_abstraction: 0 # AbstractionName1\n         to_abstraction: 1 # AbstractionName2\n         label: \"Manages\"\n     ```\n\n     Now, provide the YAML output:\n     \"\"\"\n             response = call_llm(prompt) #call llm\n\n             yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n             relationships_data = yaml.safe_load(yaml_str)\n\n             validated_relationships = []\n             num_abstractions = len(abstraction_listing.split('\\n'))\n             for rel in relationships_data[\"relationships\"]:\n                  from_idx = int(str(rel[\"from_abstraction\"]).split('#')[0].strip())\n                  to_idx = int(str(rel[\"to_abstraction\"]).split('#')[0].strip())\n                  validated_relationships.append({\n                       \"from\": from_idx,\n                       \"to\": to_idx,\n                       \"label\": rel[\"label\"]\n                  })\n\n             return {\n                  \"summary\": relationships_data[\"summary\"],\n                  \"details\": validated_relationships\n             }\n\n         def post(self, shared, prep_res, exec_res):\n             shared[\"relationships\"] = exec_res\n     ```\n\n   *   The `prep` method formats the context and project name for the LLM by listing the abstraction descriptions and file content.\n\n   *   The `exec` method prompts the LLM to analyze the context and provide a summary and relationships. It parses the LLM output and adds a project summary and details (including what abstraction links to what) to the `shared` dictionary at `shared[\"relationships\"]`.\n\n   *   The LLM might respond with something like this (LLM generated):\n\n     ```yaml\n     summary: |\n       This project provides basic arithmetic operations: **addition** and **multiplication**.\n       It allows users to perform simple calculations.\n     relationships:\n       - from_abstraction: 0 # Addition\n         to_abstraction: 1 # Multiplication\n         label: \"None\"\n     ```\n\n4.  **Ordering Chapters (OrderChapters Node)**\n\n   *   The `OrderChapters` node asks the LLM what order to present the abstractions in.\n   *   Here's a simplified snippet of the `OrderChapters` node's `prep` and `exec` methods (from `nodes.py`):\n\n     ```python\n     class OrderChapters(Node):\n         def prep(self, shared):\n             abstractions = shared[\"abstractions\"]\n             relationships = shared[\"relationships\"]\n\n             abstraction_info_for_prompt = []\n             for i, a in enumerate(abstractions):\n                 abstraction_info_for_prompt.append(f\"- {i} # {a['name']}\")\n             abstraction_listing = \"\\n\".join(abstraction_info_for_prompt)\n\n             context = f\"Project Summary:\\n{relationships['summary']}\\n\\n\"\n             context += \"Relationships (Indices refer to abstractions above):\\n\"\n             for rel in relationships['details']:\n                  from_name = abstractions[rel['from']]['name']\n                  to_name = abstractions[rel['to']]['name']\n                  context += f\"- From {rel['from']} ({from_name}) to {rel['to']} ({to_name}): {rel['label']}\\n\"\n\n             return abstraction_listing, context, len(abstractions), shared[\"project_name\"]\n\n         def exec(self, prep_res):\n             abstraction_listing, context, num_abstractions, project_name = prep_res\n             prompt = f\"\"\"\n     Given the following project abstractions and their relationships for the project ```` {project_name} ````:\n\n     Abstractions (Index # Name):\n     {abstraction_listing}\n\n     Context about relationships and project summary:\n     {context}\n\n     If you are going to make a tutorial for ```` {project_name} ````, what is the best order to explain these abstractions, from first to last?\n     Ideally, first explain those that are the most important or foundational, perhaps user-facing concepts or entry points. Then move to more detailed, lower-level implementation details or supporting concepts.\n\n     Output the ordered list of abstraction indices, including the name in a comment for clarity. Use the format `idx # AbstractionName`.\n\n     ```yaml\n     - 0 # Addition\n     - 1 # Multiplication\n     ```\n     \"\"\"\n             response = call_llm(prompt) #call llm\n\n             yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n             ordered_indices_raw = yaml.safe_load(yaml_str)\n\n             ordered_indices = []\n             for entry in ordered_indices_raw:\n                  idx = int(entry.split('#')[0].strip())\n                  ordered_indices.append(idx)\n\n             return ordered_indices\n\n         def post(self, shared, prep_res, exec_res):\n             shared[\"chapter_order\"] = exec_res\n     ```\n\n   *   The `prep` method prepares the context and the project name for the LLM by listing all abstractions, relationship information, and project summary.\n\n   *   The `exec` method calls the LLM to ask what the best order to explain the abstractions are. Then the method adds the ordered list to the `shared` dictionary at `shared[\"chapter_order\"]`.\n\n   *   The LLM might respond with something like this (LLM generated):\n\n     ```yaml\n     - 0 # Addition\n     - 1 # Multiplication\n     ```\n\n5.  **Writing Chapters (WriteChapters Node)**\n\n   *   The `WriteChapters` node creates the markdown chapters for the final tutorial.\n   *   Here's a simplified snippet of the `WriteChapters` node's `prep` and `exec` methods (from `nodes.py`):\n\n     ```python\n     class WriteChapters(BatchNode):\n         def prep(self, shared):\n             chapter_order = shared[\"chapter_order\"]\n             abstractions = shared[\"abstractions\"]\n             files_data = shared[\"files\"]\n\n             all_chapters = []\n             chapter_filenames = {}\n             for i, abstraction_index in enumerate(chapter_order):\n                  chapter_num = i + 1\n                  chapter_name = abstractions[abstraction_index][\"name\"]\n                  safe_name = \"\".join(c if c.isalnum() else '_' for c in chapter_name).lower()\n                  filename = f\"{i+1:02d}_{safe_name}.md\"\n                  all_chapters.append(f\"{chapter_num}. [{chapter_name}]({filename})\")\n                  chapter_filenames[abstraction_index] = {\"num\": chapter_num, \"name\": chapter_name, \"filename\": filename}\n\n             full_chapter_listing = \"\\n\".join(all_chapters)\n             items_to_process = []\n             for i, abstraction_index in enumerate(chapter_order):\n                  abstraction_details = abstractions[abstraction_index]\n                  related_file_indices = abstraction_details.get(\"files\", [])\n\n                  related_files_content_map = {}\n                  for i in related_file_indices:\n                       path, content = files_data[i]\n                       related_files_content_map[f\"{i} # {path}\"] = content\n\n                  items_to_process.append({\n                       \"chapter_num\": i + 1,\n                       \"abstraction_index\": abstraction_index,\n                       \"abstraction_details\": abstraction_details,\n                       \"related_files_content_map\": related_files_content_map,\n                       \"project_name\": shared[\"project_name\"],\n                       \"full_chapter_listing\": full_chapter_listing,\n                       \"chapter_filenames\": chapter_filenames,\n                  })\n             print(f\"Preparing to write {len(items_to_process)} chapters...\")\n             return items_to_process\n\n         def exec(self, item):\n             abstraction_name = item[\"abstraction_details\"][\"name\"]\n             chapter_num = item[\"chapter_num\"]\n             project_name = item.get(\"project_name\")\n             print(f\"Writing chapter {chapter_num} for: {abstraction_name} using LLM...\")\n\n             file_context_str = \"\\n\\n\".join(\n                  f\"--- File: {idx_path.split('# ')[1] if '# ' in idx_path else idx_path} ---\\n{content}\"\n                  for idx_path, content in item[\"related_files_content_map\"].items()\n             )\n\n             prompt = f\"\"\"\n     Write a very beginner-friendly tutorial chapter (in Markdown format) for the project `{project_name}` about the concept: \"{abstraction_name}\". This is Chapter {chapter_num}.\n\n     Concept Details:\n     - Description:\n     {item[\"abstraction_details\"][\"description\"]}\n\n     Complete Tutorial Structure:\n     {item[\"full_chapter_listing\"]}\n\n     Relevant Code Snippets:\n     {file_context_str if file_context_str else \"No specific code snippets provided for this abstraction.\"}\n\n     Instructions for the chapter:\n     - Start with a clear heading (e.g., `# Chapter {chapter_num}: {abstraction_name}`).\n     - Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example.\n     - If the abstraction is complex, break it down into key concepts.\n     - Explain how to use this abstraction. Give example inputs and outputs.\n     - Describe the internal implementation to help understand what's under the hood.\n     - Heavily use analogies and examples throughout to help beginners understand.\n     - End the chapter with a brief conclusion.\n     - Ensure the tone is welcoming and easy for a newcomer to understand.\n\n     Now, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n     \"\"\"\n             chapter_content = call_llm(prompt) #call llm\n\n             actual_heading = f\"# Chapter {chapter_num}: {abstraction_name}\"\n             if not chapter_content.strip().startswith(f\"# Chapter {chapter_num}\"):\n                  lines = chapter_content.strip().split('\\n')\n                  if lines and lines[0].strip().startswith(\"#\"):\n                       lines[0] = actual_heading\n                       chapter_content = \"\\n\".join(lines)\n                  else:\n                       chapter_content = f\"{actual_heading}\\n\\n{chapter_content}\"\n\n             return chapter_content\n\n         def post(self, shared, prep_res, exec_res_list):\n             shared[\"chapters\"] = exec_res_list\n             print(f\"Finished writing {len(exec_res_list)} chapters.\")\n     ```\n\n   *   The `prep` method formats the project and file content and abstraction details for the LLM by listing all file content that corresponds to an abstraction. It generates filenames for each chapter, and adds each chapter to the `items_to_process` list.\n\n   *   The `exec` method prompts the LLM to generate a chapter based on the abstraction details and corresponding files. It then adds the chapters to the `shared` dictionary at `shared[\"chapters\"]`.\n\n   *   An example of the output generated by the LLM would be:\n\n     ```markdown\n     # Chapter 1: Addition\n\n     Addition is the process of combining two or more numbers to find their total, or sum. It's like combining two piles of toys to see how many you have in all!\n\n     ## How to Use Addition\n\n     You can use the `add` function in `file1.py` to perform addition:\n\n     ```python\n     def add(a, b):\n         return a + b\n     ```\n\n     This code simply takes two numbers, `a` and `b`, and returns their sum. For example:\n\n     ```python\n     result = add(5, 3) # result will be 8\n     print(result)\n     ```\n\n     In this example, `add(5, 3)` returns `8`, because 5 + 3 = 8. It's that simple!\n\n     ## Internal Implementation\n\n     The function `add(a, b)` simply uses the `+` operator in Python to perform the addition. This is a built-in feature of Python, so it's very efficient.\n\n     ## Conclusion\n\n     In this chapter, we learned about addition and how to use the `add` function to perform it in Python. In the next chapter, we will talk about [Multiplication](02_multiplication.md).\n     ```\n\n**Under the Hood: Implementation Details**\n\n*   **Nodes.py:** The core logic for this component resides in the `nodes.py` file, which contains the `IdentifyAbstractions`, `AnalyzeRelationships`, `OrderChapters`, and `WriteChapters` classes. Each of these classes inherits from the `Node` or `BatchNode` class from the `pocketflow` library.\n*   **LLM Integration:** The `call_llm` function (in `utils/call_llm.py`) handles communication with the LLM. It takes a prompt as input and returns the LLM's response. By default, we use the Google Gemini 2.5 Pro model.\n*   **Prompt Engineering:**  A key part of this component is crafting effective prompts for the LLM.  These prompts guide the LLM to analyze the code, identify key concepts, and generate beginner-friendly explanations. Examples of prompts are shown in the `exec` method of the nodes shown above.\n\n**Benefits and Considerations**\n\n*   **Benefits:**\n    *   **Automation:**  Automates the process of understanding and explaining complex codebases.\n    *   **Beginner-Friendly:**  Generates content tailored for beginners, making it easier to learn new codebases.\n    *   **Scalability:**  Can handle large codebases that would be difficult for a human to analyze manually.\n*   **Considerations:**\n    *   **LLM Limitations:** The quality of the generated content depends on the LLM's capabilities and the effectiveness of the prompts.\n    *   **Accuracy:** The AI might not always perfectly understand the code, so it's important to review the generated content for accuracy.\n\n**Conclusion**\n\nIn this chapter, we explored how AI can be used to analyze codebases and generate beginner-friendly tutorial content. This `AI-Powered Code Analysis and Generation` component is a key part of our tutorial generation system, enabling us to automate the process of understanding and explaining complex code.  In the next chapter, [Codebase Crawling](03_codebase_crawling.md), we'll see how we actually get the source code that this component analyzes.\n", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `Tutorial-Codebase-Knowledge` about the concept: \"Codebase Crawling\". This is Chapter 3.\n\nConcept Details:\n- Description:\nThis abstraction handles fetching and filtering code files from a repository, either a GitHub repository or a local directory. It's like a librarian searching for specific books based on certain criteria (include/exclude patterns) and size restrictions, ensuring only relevant materials are collected for analysis.\n\n\nComplete Tutorial Structure:\n1. [Main Execution Script](01_main_execution_script.md)\n2. [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md)\n3. [Codebase Crawling](03_codebase_crawling.md)\n4. [Knowledge Base Construction](04_knowledge_base_construction.md)\n5. [Tutorial Content Combination](05_tutorial_content_combination.md)\n6. [LLM Orchestration (PocketFlow)](06_llm_orchestration__pocketflow_.md)\n\nContext from previous chapters (summary):\n# Chapter 1: Main Execution Script\n\nWelcome to the first chapter of our tutorial on building a tutorial generation system! This chapter focuses on the \"Main Execution Script,\" the conductor of our orchestra that makes everything happen.\n\nImagine you want to create a tutorial for a cool new Python library you found on GitHub.  Where do you even begin?  That's where the Main Execution Script comes in. It's the starting point of our whole tutorial-making process. It takes your instructions (like the GitHub link) and kicks off all the other parts of the system to create your tutorial.\n\n**What Problem Does It Solve?**\n\nWithout a central entry point, we'd have a bunch of disconnected pieces. The Main Execution Script brings order to chaos. It:\n\n*   **Takes your input:** Where is the code? What do you want to name the tutorial?\n*   **Sets the stage:** Gets everything ready for the tutorial generation.\n*   **Kicks off the process:** Starts all the other parts of the system in the right order.\n\n**Key Concepts**\n\nLet's break down the key concepts of the Main Execution Script:\n\n1.  **Command-Line Arguments:** These are the instructions you give the script when you run it.  Think of them as telling the script *what* to do.\n\n2.  **Configuration:**  This is setting up all the necessary components, like where to save the output.\n\n3.  **Orchestration:** This is the script's main job: making sure all the other parts of the system run in the correct sequence, like a conductor leading an orchestra. We use [LLM Orchestration (PocketFlow)](06_llm_orchestration__pocketflow_.md) for this.\n\n**Using the Main Execution Script**\n\nLet's see how you'd use the Main Execution Script to generate a tutorial for a GitHub repository.\n\n**Example Input:**\n\nLet's say you want to create a tutorial for a repository named \"my-cool-project\" on GitHub, located at `https://github.com/user/my-cool-project`. You'd run the script from your terminal like this:\n\n```bash\npython main.py --repo https://github.com/user/my-cool-project -n MyCoolProjectTutorial\n```\n\nLet's break down that command:\n\n*   `python main.py`:  This tells your computer to run the `main.py` file using Python.\n*   `--repo https://github.com/user/my-cool-project`:  This tells the script the URL of the GitHub repository.\n*   `-n MyCoolProjectTutorial`: This tells the script to name the project \"MyCoolProjectTutorial\".  If you leave this out, the script will try to figure out the name from the repository URL.\n\n**Example Output:**\n\nAfter running the command, the script will:\n\n1.  Download the code from the GitHub repository.\n2.  Analyze the code.\n3.  Create a tutorial based on the code.\n4.  Save the tutorial in a directory named \"output\" (unless you specify a different output directory).\n\nYou'll find a structured tutorial with different chapters covering the codebase's essential aspects in the `output` directory!\n\n**Code Walkthrough**\n\nLet's look at some parts of the `main.py` script:\n\n```python\nimport argparse\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Generate a tutorial...\")\n    parser.add_argument(\"--repo\", help=\"URL of the public GitHub repository.\")\n    parser.add_argument(\"-n\", \"--name\", help=\"Project name (optional...\")\n    args = parser.parse_args()\n\n    # --- Rest of the Code ---\n```\n\nThis code sets up the command-line argument parsing.  It defines the arguments we can use (like `--repo` and `-n`) and provides helpful descriptions. The `argparse` module makes it easy to get the values you provide when you run the script.\n\n```python\n    shared = {\n        \"repo_url\": args.repo,\n        \"project_name\": args.name,\n        \"output_dir\": \"output\",  # Simplification\n    }\n\n    print(f\"Starting tutorial generation for: {args.repo}\")\n\n    # --- Rest of the Code ---\n```\n\nThis code creates a `shared` dictionary, a central place to store information that all parts of the system need.  We store the repository URL, the project name, and the output directory here.  Think of it as a shared whiteboard where all the different parts of the tutorial generation system can read and write information.\n\n```python\n    # Create the flow instance\n    tutorial_flow = create_tutorial_flow()\n\n    # Run the flow\n    tutorial_flow.run(shared)\n```\n\nThis is where the magic happens! We create a `tutorial_flow` (which we will discuss more in [LLM Orchestration (PocketFlow)](06_llm_orchestration__pocketflow_.md)) and tell it to run, passing in the `shared` dictionary. The `tutorial_flow` orchestrates all the other steps to generate the tutorial.\n\n**Internal Implementation: A Step-by-Step View**\n\nHere's a simplified view of what happens inside the Main Execution Script when you run it:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant MainScript as Main Execution Script\n    participant ArgumentParser as Argument Parser\n    participant Flow as Tutorial Flow\n    \n    User->>MainScript: Run script with arguments\n    MainScript->>ArgumentParser: Parse arguments\n    ArgumentParser-->>MainScript: Return arguments\n    MainScript->>Flow: Create tutorial flow\n    MainScript->>Flow: Run flow with arguments\n    Flow-->>MainScript: Tutorial generated\n    MainScript->>User: Completion Message\n```\n\n1.  **User:** You run the `main.py` script with command-line arguments (e.g., the GitHub repository URL).\n2.  **Main Execution Script:** The script receives your command and uses `ArgumentParser` to understand your instructions.\n3.  **Argument Parser:** The argument parser takes the command-line arguments and organizes them into a usable format.\n4.  **Main Execution Script:**  The script then creates an instance of the `Tutorial Flow`. The script then runs the `Tutorial Flow`.\n5.  **Tutorial Flow:** The tutorial flow coordinates with other components like the [Codebase Crawling](03_codebase_crawling.md), [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md), and [Knowledge Base Construction](04_knowledge_base_construction.md) components to create the tutorial.\n6.  **Main Execution Script:** The script prints a completion message, indicating that the tutorial has been generated.\n\n**Implementation Details**\n\nThe `create_tutorial_flow()` function, defined in `flow.py` (not shown in the code snippets above), is responsible for creating the `tutorial_flow` instance.\n\n**Conclusion**\n\nIn this chapter, we explored the Main Execution Script, the entry point of our tutorial generation system. We learned how it takes user input, configures the system, and orchestrates the tutorial generation process.  It's the conductor that makes sure all the instruments play in harmony!\n\nIn the next chapter, we'll dive into the crucial component of [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md), where we'll see how AI helps us understand the codebase and generate meaningful tutorial content.\n\n---\n# Chapter 2: AI-Powered Code Analysis and Generation\n\nIn the previous chapter, [Main Execution Script](01_main_execution_script.md), we saw how the script acts as the conductor of our tutorial-generation process, taking your input and orchestrating the workflow. Now, we'll dive into a crucial part of that workflow: using AI to understand the code and generate content!\n\n**Why AI for Code Analysis and Generation?**\n\nImagine you're given a huge, complex Lego set with thousands of pieces and no instructions. That's what it's like trying to understand a large codebase!  It's overwhelming.  The goal of this chapter is to learn how to use AI to understand any codebase and describe it so that others can also understand it.\n\nThat's where AI comes in! Our `AI-Powered Code Analysis and Generation` component acts like an experienced tutor. It uses Large Language Models (LLMs) to:\n\n*   **Analyze the code:**  Like carefully examining each Lego brick.\n*   **Identify key components:**  Finding the essential parts of the set.\n*   **Generate beginner-friendly explanations:**  Writing clear, step-by-step instructions.\n\n**The Big Picture: Turning Code into Tutorials**\n\nThis component sits in the middle of our tutorial generation process. It takes raw code (from crawling the codebase) and turns it into structured, understandable tutorial content. We start by downloading the source code, then we'll use this component to create a tutorial.\n\n**Key Concepts**\n\nLet's break down the core ideas behind our AI-powered approach:\n\n1.  **Large Language Models (LLMs):** These are powerful AI models trained on vast amounts of text data. Think of them as having read millions of books and articles on programming.  We use them to understand code and write explanations. We pass the prompts to the LLM through the `call_llm` function, which is defined in `utils/call_llm.py`. This allows us to easily switch models by changing the environment variables for the model.\n\n2.  **Code Abstraction Identification:** The AI identifies key concepts or modules within the codebase. This is like finding the most important functions or classes.\n\n3.  **Relationship Analysis:** The AI determines how these components interact with each other.  This is like understanding how the Lego bricks fit together to build the final model.\n\n4.  **Beginner-Friendly Content Generation:** The AI translates complex code into simple, easy-to-understand explanations, tailored for beginners. It generates markdown content that is later combined to build a chapter.\n\n**How it Works: A Simplified Walkthrough**\n\nLet's walk through a simplified example using pieces of the code, starting from what it receives from the main script.\n\n```python\nshared = {\n  \"repo_url\": \"https://github.com/user/my-cool-project\",\n  \"project_name\": \"MyCoolProject\",\n  \"files\": [(\"file1.py\", \"def add(a, b): return a + b\"), (\"file2.py\", \"def multiply(a, b): return a * b\")],\n}\n```\n\nThis `shared` dictionary is passed between different components, as we saw in the [Main Execution Script](01_main_execution_script.md) chapter. We will focus on how the `IdentifyAbstractions` node processes the `shared` data.\n\n1.  **Fetching Code:** We already have the source code (simulated as `shared[\"files\"]` here).  In reality, this would come from the [Codebase Crawling](03_codebase_crawling.md) stage.\n\n2.  **Identifying Abstractions (IdentifyAbstractions Node):**\n\n   *   This node takes the codebase's content and asks the LLM to find the most important concepts.\n\n   *   It prepares a prompt for the LLM that includes the entire codebase context.\n\n   *   Here's a simplified snippet of the `IdentifyAbstractions` node's `prep` and `exec` methods (from `nodes.py`):\n\n     ```python\n     class IdentifyAbstractions(Node):\n         def prep(self, shared):\n             files_data = shared[\"files\"]\n             project_name = shared[\"project_name\"]\n             # simplified context creation\n             context = \"\"\n             file_info = []\n             for i, (path, content) in enumerate(files_data):\n                 context += f\"--- File Index {i}: {path} ---\\n{content}\\n\\n\"\n                 file_info.append((i, path))\n\n             file_listing_for_prompt = \"\\n\".join([f\"- {idx} # {path}\" for idx, path in file_info])\n             return context, file_listing_for_prompt, len(files_data), project_name\n\n         def exec(self, prep_res):\n             context, file_listing_for_prompt, file_count, project_name = prep_res\n\n             prompt = f\"\"\"\n     For the project `{project_name}`:\n\n     Codebase Context:\n     {context}\n\n     Analyze the codebase context.\n     Identify the top 2 core most important abstractions to help those new to the codebase.\n\n     For each abstraction, provide:\n     1. A concise `name`.\n     2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n     3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\n     List of file indices and paths present in the context:\n     {file_listing_for_prompt}\n\n     Format the output as a YAML list of dictionaries:\n\n     ```yaml\n     - name: Addition\n       description: |\n         This abstraction performs addition.\n         It's like adding two numbers together.\n       file_indices:\n         - 0 # file1.py\n     - name: Multiplication\n       description: |\n         This abstraction performs multiplication.\n         It's like multiplying two numbers together.\n       file_indices:\n         - 1 # file2.py\n     ```\"\"\"\n             response = call_llm(prompt) #call llm\n\n             yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n             abstractions = yaml.safe_load(yaml_str)\n\n             validated_abstractions = []\n             for item in abstractions:\n                  item[\"files\"] = [int(idx_entry.split('#')[0].strip()) for idx_entry in item[\"file_indices\"]]\n                  validated_abstractions.append({\n                       \"name\": item[\"name\"],\n                       \"description\": item[\"description\"],\n                       \"files\": item[\"files\"]\n                  })\n\n             return validated_abstractions\n\n         def post(self, shared, prep_res, exec_res):\n             shared[\"abstractions\"] = exec_res\n     ```\n\n   *   The `prep` method prepares the context and project name for the LLM by listing all files and their code in the files_data list.\n\n   *   The `exec` method prompts the LLM to analyze the code and identify the important abstractions, generating a YAML formatted string with the abstraction name, description, and corresponding file indices. After receiving this string, the LLM output is parsed and the file indices are added to the `shared` dictionary, specifically at `shared[\"abstractions\"]`.\n\n   *   The LLM might respond with something like this (the LLM will come up with a description):\n\n     ```yaml\n     - name: Addition\n       description: |\n         This abstraction performs addition.\n         It's like adding two numbers together.\n       file_indices:\n         - 0 # file1.py\n     - name: Multiplication\n       description: |\n         This abstraction performs multiplication.\n         It's like multiplying two numbers together.\n       file_indices:\n         - 1 # file2.py\n     ```\n\n3.  **Analyzing Relationships (AnalyzeRelationships Node):**\n\n   *   The `AnalyzeRelationships` node determines how the identified abstractions relate to each other, and creates a summary for the whole project.\n\n   *   Here's a simplified snippet of the `AnalyzeRelationships` node's `prep` and `exec` methods (from `nodes.py`):\n\n     ```python\n     class AnalyzeRelationships(Node):\n         def prep(self, shared):\n             abstractions = shared[\"abstractions\"]\n             files_data = shared[\"files\"]\n             context = \"Identified Abstractions:\\n\"\n             for i, abstr in enumerate(abstractions):\n                 file_indices_str = \", \".join(map(str, abstr['files']))\n                 info_line = f\"- Index {i}: {abstr['name']} (Relevant file indices: [{file_indices_str}])\\n  Description: {abstr['description']}\"\n                 context += info_line + \"\\n\"\n\n             all_relevant_indices = set()\n             for abstr in abstractions:\n                  all_relevant_indices.update(abstr['files'])\n\n             relevant_files_content_map = {}\n             for i in sorted(list(all_relevant_indices)):\n                  path, content = files_data[i]\n                  relevant_files_content_map[f\"{i} # {path}\"] = content\n\n             context += \"\\nRelevant File Snippets (Referenced by Index and Path):\\n\"\n             file_context_str = \"\\n\\n\".join(\n                  f\"--- File: {idx_path} ---\\n{content}\"\n                  for idx_path, content in relevant_files_content_map.items()\n             )\n             context += file_context_str\n             abstraction_info_for_prompt = []\n             for i, abstr in enumerate(abstractions):\n                  abstraction_info_for_prompt.append(f\"{i} # {abstr['name']}\")\n             return context, \"\\n\".join(abstraction_info_for_prompt), shared[\"project_name\"]\n\n         def exec(self, prep_res):\n             context, abstraction_listing, project_name = prep_res\n\n             prompt = f\"\"\"\n     Based on the following abstractions and relevant code snippets from the project `{project_name}`:\n\n     List of Abstraction Indices and Names:\n     {abstraction_listing}\n\n     Context (Abstractions, Descriptions, Code):\n     {context}\n\n     Please provide:\n     1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n     2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n         - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n         - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n         - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n         Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n         Simplify the relationship and exclude those non-important ones.\n\n     IMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\n     Format the output as YAML:\n\n     ```yaml\n     summary: |\n       A brief, simple explanation of the project.\n       Can span multiple lines with **bold** and *italic* for emphasis.\n     relationships:\n       - from_abstraction: 0 # AbstractionName1\n         to_abstraction: 1 # AbstractionName2\n         label: \"Manages\"\n     ```\n\n     Now, provide the YAML output:\n     \"\"\"\n             response = call_llm(prompt) #call llm\n\n             yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n             relationships_data = yaml.safe_load(yaml_str)\n\n             validated_relationships = []\n             num_abstractions = len(abstraction_listing.split('\\n'))\n             for rel in relationships_data[\"relationships\"]:\n                  from_idx = int(str(rel[\"from_abstraction\"]).split('#')[0].strip())\n                  to_idx = int(str(rel[\"to_abstraction\"]).split('#')[0].strip())\n                  validated_relationships.append({\n                       \"from\": from_idx,\n                       \"to\": to_idx,\n                       \"label\": rel[\"label\"]\n                  })\n\n             return {\n                  \"summary\": relationships_data[\"summary\"],\n                  \"details\": validated_relationships\n             }\n\n         def post(self, shared, prep_res, exec_res):\n             shared[\"relationships\"] = exec_res\n     ```\n\n   *   The `prep` method formats the context and project name for the LLM by listing the abstraction descriptions and file content.\n\n   *   The `exec` method prompts the LLM to analyze the context and provide a summary and relationships. It parses the LLM output and adds a project summary and details (including what abstraction links to what) to the `shared` dictionary at `shared[\"relationships\"]`.\n\n   *   The LLM might respond with something like this (LLM generated):\n\n     ```yaml\n     summary: |\n       This project provides basic arithmetic operations: **addition** and **multiplication**.\n       It allows users to perform simple calculations.\n     relationships:\n       - from_abstraction: 0 # Addition\n         to_abstraction: 1 # Multiplication\n         label: \"None\"\n     ```\n\n4.  **Ordering Chapters (OrderChapters Node)**\n\n   *   The `OrderChapters` node asks the LLM what order to present the abstractions in.\n   *   Here's a simplified snippet of the `OrderChapters` node's `prep` and `exec` methods (from `nodes.py`):\n\n     ```python\n     class OrderChapters(Node):\n         def prep(self, shared):\n             abstractions = shared[\"abstractions\"]\n             relationships = shared[\"relationships\"]\n\n             abstraction_info_for_prompt = []\n             for i, a in enumerate(abstractions):\n                 abstraction_info_for_prompt.append(f\"- {i} # {a['name']}\")\n             abstraction_listing = \"\\n\".join(abstraction_info_for_prompt)\n\n             context = f\"Project Summary:\\n{relationships['summary']}\\n\\n\"\n             context += \"Relationships (Indices refer to abstractions above):\\n\"\n             for rel in relationships['details']:\n                  from_name = abstractions[rel['from']]['name']\n                  to_name = abstractions[rel['to']]['name']\n                  context += f\"- From {rel['from']} ({from_name}) to {rel['to']} ({to_name}): {rel['label']}\\n\"\n\n             return abstraction_listing, context, len(abstractions), shared[\"project_name\"]\n\n         def exec(self, prep_res):\n             abstraction_listing, context, num_abstractions, project_name = prep_res\n             prompt = f\"\"\"\n     Given the following project abstractions and their relationships for the project ```` {project_name} ````:\n\n     Abstractions (Index # Name):\n     {abstraction_listing}\n\n     Context about relationships and project summary:\n     {context}\n\n     If you are going to make a tutorial for ```` {project_name} ````, what is the best order to explain these abstractions, from first to last?\n     Ideally, first explain those that are the most important or foundational, perhaps user-facing concepts or entry points. Then move to more detailed, lower-level implementation details or supporting concepts.\n\n     Output the ordered list of abstraction indices, including the name in a comment for clarity. Use the format `idx # AbstractionName`.\n\n     ```yaml\n     - 0 # Addition\n     - 1 # Multiplication\n     ```\n     \"\"\"\n             response = call_llm(prompt) #call llm\n\n             yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n             ordered_indices_raw = yaml.safe_load(yaml_str)\n\n             ordered_indices = []\n             for entry in ordered_indices_raw:\n                  idx = int(entry.split('#')[0].strip())\n                  ordered_indices.append(idx)\n\n             return ordered_indices\n\n         def post(self, shared, prep_res, exec_res):\n             shared[\"chapter_order\"] = exec_res\n     ```\n\n   *   The `prep` method prepares the context and the project name for the LLM by listing all abstractions, relationship information, and project summary.\n\n   *   The `exec` method calls the LLM to ask what the best order to explain the abstractions are. Then the method adds the ordered list to the `shared` dictionary at `shared[\"chapter_order\"]`.\n\n   *   The LLM might respond with something like this (LLM generated):\n\n     ```yaml\n     - 0 # Addition\n     - 1 # Multiplication\n     ```\n\n5.  **Writing Chapters (WriteChapters Node)**\n\n   *   The `WriteChapters` node creates the markdown chapters for the final tutorial.\n   *   Here's a simplified snippet of the `WriteChapters` node's `prep` and `exec` methods (from `nodes.py`):\n\n     ```python\n     class WriteChapters(BatchNode):\n         def prep(self, shared):\n             chapter_order = shared[\"chapter_order\"]\n             abstractions = shared[\"abstractions\"]\n             files_data = shared[\"files\"]\n\n             all_chapters = []\n             chapter_filenames = {}\n             for i, abstraction_index in enumerate(chapter_order):\n                  chapter_num = i + 1\n                  chapter_name = abstractions[abstraction_index][\"name\"]\n                  safe_name = \"\".join(c if c.isalnum() else '_' for c in chapter_name).lower()\n                  filename = f\"{i+1:02d}_{safe_name}.md\"\n                  all_chapters.append(f\"{chapter_num}. [{chapter_name}]({filename})\")\n                  chapter_filenames[abstraction_index] = {\"num\": chapter_num, \"name\": chapter_name, \"filename\": filename}\n\n             full_chapter_listing = \"\\n\".join(all_chapters)\n             items_to_process = []\n             for i, abstraction_index in enumerate(chapter_order):\n                  abstraction_details = abstractions[abstraction_index]\n                  related_file_indices = abstraction_details.get(\"files\", [])\n\n                  related_files_content_map = {}\n                  for i in related_file_indices:\n                       path, content = files_data[i]\n                       related_files_content_map[f\"{i} # {path}\"] = content\n\n                  items_to_process.append({\n                       \"chapter_num\": i + 1,\n                       \"abstraction_index\": abstraction_index,\n                       \"abstraction_details\": abstraction_details,\n                       \"related_files_content_map\": related_files_content_map,\n                       \"project_name\": shared[\"project_name\"],\n                       \"full_chapter_listing\": full_chapter_listing,\n                       \"chapter_filenames\": chapter_filenames,\n                  })\n             print(f\"Preparing to write {len(items_to_process)} chapters...\")\n             return items_to_process\n\n         def exec(self, item):\n             abstraction_name = item[\"abstraction_details\"][\"name\"]\n             chapter_num = item[\"chapter_num\"]\n             project_name = item.get(\"project_name\")\n             print(f\"Writing chapter {chapter_num} for: {abstraction_name} using LLM...\")\n\n             file_context_str = \"\\n\\n\".join(\n                  f\"--- File: {idx_path.split('# ')[1] if '# ' in idx_path else idx_path} ---\\n{content}\"\n                  for idx_path, content in item[\"related_files_content_map\"].items()\n             )\n\n             prompt = f\"\"\"\n     Write a very beginner-friendly tutorial chapter (in Markdown format) for the project `{project_name}` about the concept: \"{abstraction_name}\". This is Chapter {chapter_num}.\n\n     Concept Details:\n     - Description:\n     {item[\"abstraction_details\"][\"description\"]}\n\n     Complete Tutorial Structure:\n     {item[\"full_chapter_listing\"]}\n\n     Relevant Code Snippets:\n     {file_context_str if file_context_str else \"No specific code snippets provided for this abstraction.\"}\n\n     Instructions for the chapter:\n     - Start with a clear heading (e.g., `# Chapter {chapter_num}: {abstraction_name}`).\n     - Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example.\n     - If the abstraction is complex, break it down into key concepts.\n     - Explain how to use this abstraction. Give example inputs and outputs.\n     - Describe the internal implementation to help understand what's under the hood.\n     - Heavily use analogies and examples throughout to help beginners understand.\n     - End the chapter with a brief conclusion.\n     - Ensure the tone is welcoming and easy for a newcomer to understand.\n\n     Now, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n     \"\"\"\n             chapter_content = call_llm(prompt) #call llm\n\n             actual_heading = f\"# Chapter {chapter_num}: {abstraction_name}\"\n             if not chapter_content.strip().startswith(f\"# Chapter {chapter_num}\"):\n                  lines = chapter_content.strip().split('\\n')\n                  if lines and lines[0].strip().startswith(\"#\"):\n                       lines[0] = actual_heading\n                       chapter_content = \"\\n\".join(lines)\n                  else:\n                       chapter_content = f\"{actual_heading}\\n\\n{chapter_content}\"\n\n             return chapter_content\n\n         def post(self, shared, prep_res, exec_res_list):\n             shared[\"chapters\"] = exec_res_list\n             print(f\"Finished writing {len(exec_res_list)} chapters.\")\n     ```\n\n   *   The `prep` method formats the project and file content and abstraction details for the LLM by listing all file content that corresponds to an abstraction. It generates filenames for each chapter, and adds each chapter to the `items_to_process` list.\n\n   *   The `exec` method prompts the LLM to generate a chapter based on the abstraction details and corresponding files. It then adds the chapters to the `shared` dictionary at `shared[\"chapters\"]`.\n\n   *   An example of the output generated by the LLM would be:\n\n     ```markdown\n     # Chapter 1: Addition\n\n     Addition is the process of combining two or more numbers to find their total, or sum. It's like combining two piles of toys to see how many you have in all!\n\n     ## How to Use Addition\n\n     You can use the `add` function in `file1.py` to perform addition:\n\n     ```python\n     def add(a, b):\n         return a + b\n     ```\n\n     This code simply takes two numbers, `a` and `b`, and returns their sum. For example:\n\n     ```python\n     result = add(5, 3) # result will be 8\n     print(result)\n     ```\n\n     In this example, `add(5, 3)` returns `8`, because 5 + 3 = 8. It's that simple!\n\n     ## Internal Implementation\n\n     The function `add(a, b)` simply uses the `+` operator in Python to perform the addition. This is a built-in feature of Python, so it's very efficient.\n\n     ## Conclusion\n\n     In this chapter, we learned about addition and how to use the `add` function to perform it in Python. In the next chapter, we will talk about [Multiplication](02_multiplication.md).\n     ```\n\n**Under the Hood: Implementation Details**\n\n*   **Nodes.py:** The core logic for this component resides in the `nodes.py` file, which contains the `IdentifyAbstractions`, `AnalyzeRelationships`, `OrderChapters`, and `WriteChapters` classes. Each of these classes inherits from the `Node` or `BatchNode` class from the `pocketflow` library.\n*   **LLM Integration:** The `call_llm` function (in `utils/call_llm.py`) handles communication with the LLM. It takes a prompt as input and returns the LLM's response. By default, we use the Google Gemini 2.5 Pro model.\n*   **Prompt Engineering:**  A key part of this component is crafting effective prompts for the LLM.  These prompts guide the LLM to analyze the code, identify key concepts, and generate beginner-friendly explanations. Examples of prompts are shown in the `exec` method of the nodes shown above.\n\n**Benefits and Considerations**\n\n*   **Benefits:**\n    *   **Automation:**  Automates the process of understanding and explaining complex codebases.\n    *   **Beginner-Friendly:**  Generates content tailored for beginners, making it easier to learn new codebases.\n    *   **Scalability:**  Can handle large codebases that would be difficult for a human to analyze manually.\n*   **Considerations:**\n    *   **LLM Limitations:** The quality of the generated content depends on the LLM's capabilities and the effectiveness of the prompts.\n    *   **Accuracy:** The AI might not always perfectly understand the code, so it's important to review the generated content for accuracy.\n\n**Conclusion**\n\nIn this chapter, we explored how AI can be used to analyze codebases and generate beginner-friendly tutorial content. This `AI-Powered Code Analysis and Generation` component is a key part of our tutorial generation system, enabling us to automate the process of understanding and explaining complex code.  In the next chapter, [Codebase Crawling](03_codebase_crawling.md), we'll see how we actually get the source code that this component analyzes.\n\n\nRelevant Code Snippets:\n--- File: nodes.py ---\nimport os\nimport yaml\nfrom pocketflow import Node, BatchNode\nfrom utils.crawl_github_files import crawl_github_files\nfrom utils.call_llm import call_llm \nfrom utils.crawl_local_files import crawl_local_files\n\n# Helper to get content for specific file indices\ndef get_content_for_indices(files_data, indices):\n    content_map = {}\n    for i in indices:\n        if 0 <= i < len(files_data):\n            path, content = files_data[i]\n            content_map[f\"{i} # {path}\"] = content # Use index + path as key for context\n    return content_map\n\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"local_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        if not project_name:\n            # Basic name derivation from URL or directory\n            if repo_url:\n                project_name = repo_url.split('/')[-1].replace('.git', '')\n            else:\n                project_name = os.path.basename(os.path.abspath(local_dir))\n            shared[\"project_name\"] = project_name\n\n        # Get file patterns directly from shared\n        include_patterns = shared[\"include_patterns\"]\n        exclude_patterns = shared[\"exclude_patterns\"]\n        max_file_size = shared[\"max_file_size\"]\n\n        return {\n            \"repo_url\": repo_url,\n            \"local_dir\": local_dir,\n            \"token\": shared.get(\"github_token\"),\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns,\n            \"max_file_size\": max_file_size,\n            \"use_relative_paths\": True\n        }\n\n    def exec(self, prep_res):\n        if prep_res[\"repo_url\"]:\n            print(f\"Crawling repository: {prep_res['repo_url']}...\")\n            result = crawl_github_files(\n                repo_url=prep_res[\"repo_url\"],\n                token=prep_res[\"token\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        else:\n            print(f\"Crawling directory: {prep_res['local_dir']}...\")\n            result = crawl_local_files(\n                directory=prep_res[\"local_dir\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n            \n        # Convert dict to list of tuples: [(path, content), ...]\n        files_list = list(result.get(\"files\", {}).items())\n        print(f\"Fetched {len(files_list)} files.\")\n        return files_list\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"files\"] = exec_res # List of (path, content) tuples\n\nclass IdentifyAbstractions(Node):\n    def prep(self, shared):\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        \n        # Helper to create context from files, respecting limits (basic example)\n        def create_llm_context(files_data):\n            context = \"\"\n            file_info = [] # Store tuples of (index, path)\n            for i, (path, content) in enumerate(files_data):\n                entry = f\"--- File Index {i}: {path} ---\\n{content}\\n\\n\"\n                context += entry\n                file_info.append((i, path))\n\n            return context, file_info # file_info is list of (index, path)\n\n        context, file_info = create_llm_context(files_data)\n        # Format file info for the prompt (comment is just a hint for LLM)\n        file_listing_for_prompt = \"\\n\".join([f\"- {idx} # {path}\" for idx, path in file_info])\n        return context, file_listing_for_prompt, len(files_data), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, file_listing_for_prompt, file_count, project_name = prep_res  # Unpack project name\n        print(\"Identifying abstractions using LLM...\")\n        prompt = f\"\"\"\nFor the project `{project_name}`:\n\nCodebase Context:\n{context}\n\nAnalyze the codebase context.\nIdentify the top 5-10 core most important abstractions to help those new to the codebase.\n\nFor each abstraction, provide:\n1. A concise `name`.\n2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\nList of file indices and paths present in the context:\n{file_listing_for_prompt}\n\nFormat the output as a YAML list of dictionaries:\n\n```yaml\n- name: Query Processing\n  description: | \n    Explains what the abstraction does.\n    It's like a central dispatcher routing requests.\n  file_indices:\n    - 0 # path/to/file1.py\n    - 3 # path/to/related.py\n- name: Query Optimization\n  description: |\n    Another core concept, similar to a blueprint for objects.\n  file_indices:\n    - 5 # path/to/another.js\n# ... up to 10 abstractions\n```\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        abstractions = yaml.safe_load(yaml_str)\n\n        if not isinstance(abstractions, list):\n            raise ValueError(\"LLM Output is not a list\")\n\n        validated_abstractions = []\n        for item in abstractions:\n            if not isinstance(item, dict) or not all(k in item for k in [\"name\", \"description\", \"file_indices\"]):\n                raise ValueError(f\"Missing keys in abstraction item: {item}\")\n            if not isinstance(item[\"description\"], str):\n                 raise ValueError(f\"description is not a string in item: {item}\")\n            if not isinstance(item[\"file_indices\"], list):\n                 raise ValueError(f\"file_indices is not a list in item: {item}\")\n\n            # Validate indices\n            validated_indices = []\n            for idx_entry in item[\"file_indices\"]:\n                 try:\n                     if isinstance(idx_entry, int):\n                         idx = idx_entry\n                     elif isinstance(idx_entry, str) and '#' in idx_entry:\n                          idx = int(idx_entry.split('#')[0].strip())\n                     else:\n                          idx = int(str(idx_entry).strip())\n\n                     if not (0 <= idx < file_count):\n                         raise ValueError(f\"Invalid file index {idx} found in item {item['name']}. Max index is {file_count - 1}.\")\n                     validated_indices.append(idx)\n                 except (ValueError, TypeError):\n                      raise ValueError(f\"Could not parse index from entry: {idx_entry} in item {item['name']}\")\n\n            item[\"files\"] = sorted(list(set(validated_indices)))\n            # Store only the required fields\n            validated_abstractions.append({\n                \"name\": item[\"name\"],\n                \"description\": item[\"description\"],\n                \"files\": item[\"files\"]\n            })\n\n        print(f\"Identified {len(validated_abstractions)} abstractions.\")\n        return validated_abstractions\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"abstractions\"] = exec_res # List of {\"name\": str, \"description\": str, \"files\": [int]}\n\nclass AnalyzeRelationships(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"] # Now contains 'files' list of indices\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n\n        # Create context with abstraction names, indices, descriptions, and relevant file snippets\n        context = \"Identified Abstractions:\\n\"\n        all_relevant_indices = set()\n        abstraction_info_for_prompt = []\n        for i, abstr in enumerate(abstractions):\n            # Use 'files' which contains indices directly\n            file_indices_str = \", \".join(map(str, abstr['files']))\n            info_line = f\"- Index {i}: {abstr['name']} (Relevant file indices: [{file_indices_str}])\\n  Description: {abstr['description']}\"\n            context += info_line + \"\\n\"\n            abstraction_info_for_prompt.append(f\"{i} # {abstr['name']}\")\n            all_relevant_indices.update(abstr['files'])\n\n        context += \"\\nRelevant File Snippets (Referenced by Index and Path):\\n\"\n        # Get content for relevant files using helper\n        relevant_files_content_map = get_content_for_indices(\n            files_data,\n            sorted(list(all_relevant_indices))\n        )\n        # Format file content for context\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path} ---\\n{content}\"\n            for idx_path, content in relevant_files_content_map.items()\n        )\n        context += file_context_str\n\n        return context, \"\\n\".join(abstraction_info_for_prompt), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, abstraction_listing, project_name = prep_res  # Unpack project name\n        print(\"Analyzing relationships using LLM...\")\n        prompt = f\"\"\"\nBased on the following abstractions and relevant code snippets from the project `{project_name}`:\n\nList of Abstraction Indices and Names:\n{abstraction_listing}\n\nContext (Abstractions, Descriptions, Code):\n{context}\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        relationships_data = yaml.safe_load(yaml_str)\n\n        if not isinstance(relationships_data, dict) or not all(k in relationships_data for k in [\"summary\", \"relationships\"]):\n            raise ValueError(\"LLM output is not a dict or missing keys ('summary', 'relationships')\")\n        if not isinstance(relationships_data[\"summary\"], str):\n             raise ValueError(\"summary is not a string\")\n        if not isinstance(relationships_data[\"relationships\"], list):\n             raise ValueError(\"relationships is not a list\")\n\n        # Validate relationships structure\n        validated_relationships = []\n        num_abstractions = len(abstraction_listing.split('\\n'))\n        for rel in relationships_data[\"relationships\"]:\n             # Check for 'label' key\n             if not isinstance(rel, dict) or not all(k in rel for k in [\"from_abstraction\", \"to_abstraction\", \"label\"]):\n                  raise ValueError(f\"Missing keys (expected from_abstraction, to_abstraction, label) in relationship item: {rel}\")\n             # Validate 'label' is a string\n             if not isinstance(rel[\"label\"], str):\n                  raise ValueError(f\"Relationship label is not a string: {rel}\")\n\n             # Validate indices\n             try:\n                 from_idx = int(str(rel[\"from_abstraction\"]).split('#')[0].strip())\n                 to_idx = int(str(rel[\"to_abstraction\"]).split('#')[0].strip())\n                 if not (0 <= from_idx < num_abstractions and 0 <= to_idx < num_abstractions):\n                      raise ValueError(f\"Invalid index in relationship: from={from_idx}, to={to_idx}. Max index is {num_abstractions-1}.\")\n                 validated_relationships.append({\n                     \"from\": from_idx,\n                     \"to\": to_idx,\n                     \"label\": rel[\"label\"] \n                 })\n             except (ValueError, TypeError):\n                  raise ValueError(f\"Could not parse indices from relationship: {rel}\")\n\n        print(\"Generated project summary and relationship details.\")\n        return {\n            \"summary\": relationships_data[\"summary\"],\n            \"details\": validated_relationships # Store validated, index-based relationships\n        }\n\n\n    def post(self, shared, prep_res, exec_res):\n        # Structure is now {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        shared[\"relationships\"] = exec_res\n\nclass OrderChapters(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"]\n        relationships = shared[\"relationships\"]\n        project_name = shared[\"project_name\"]  # Get project name\n\n        # Prepare context for the LLM\n        abstraction_info_for_prompt = []\n        for i, a in enumerate(abstractions):\n            abstraction_info_for_prompt.append(f\"- {i} # {a['name']}\")\n        abstraction_listing = \"\\n\".join(abstraction_info_for_prompt)\n\n        context = f\"Project Summary:\\n{relationships['summary']}\\n\\n\"\n        context += \"Relationships (Indices refer to abstractions above):\\n\"\n        for rel in relationships['details']:\n             from_name = abstractions[rel['from']]['name']\n             to_name = abstractions[rel['to']]['name']\n             # Use 'label' instead of 'desc'\n             context += f\"- From {rel['from']} ({from_name}) to {rel['to']} ({to_name}): {rel['label']}\\n\"\n\n        return abstraction_listing, context, len(abstractions), project_name\n\n    def exec(self, prep_res):\n        abstraction_listing, context, num_abstractions, project_name = prep_res\n        print(\"Determining chapter order using LLM...\")\n        prompt = f\"\"\"\nGiven the following project abstractions and their relationships for the project ```` {project_name} ````:\n\nAbstractions (Index # Name):\n{abstraction_listing}\n\nContext about relationships and project summary:\n{context}\n\nIf you are going to make a tutorial for ```` {project_name} ````, what is the best order to explain these abstractions, from first to last?\nIdeally, first explain those that are the most important or foundational, perhaps user-facing concepts or entry points. Then move to more detailed, lower-level implementation details or supporting concepts.\n\nOutput the ordered list of abstraction indices, including the name in a comment for clarity. Use the format `idx # AbstractionName`.\n\n```yaml\n- 2 # FoundationalConcept\n- 0 # CoreClassA\n- 1 # CoreClassB (uses CoreClassA)\n- ...\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        # Rely on Node's built-in retry/fallback\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        ordered_indices_raw = yaml.safe_load(yaml_str)\n\n        if not isinstance(ordered_indices_raw, list):\n            raise ValueError(\"LLM output is not a list\")\n\n        ordered_indices = []\n        seen_indices = set()\n        for entry in ordered_indices_raw:\n            try:\n                 if isinstance(entry, int):\n                     idx = entry\n                 elif isinstance(entry, str) and '#' in entry:\n                      idx = int(entry.split('#')[0].strip())\n                 else:\n                      idx = int(str(entry).strip())\n\n                 if not (0 <= idx < num_abstractions):\n                      raise ValueError(f\"Invalid index {idx} in ordered list. Max index is {num_abstractions-1}.\")\n                 if idx in seen_indices:\n                     raise ValueError(f\"Duplicate index {idx} found in ordered list.\")\n                 ordered_indices.append(idx)\n                 seen_indices.add(idx)\n\n            except (ValueError, TypeError):\n                 raise ValueError(f\"Could not parse index from ordered list entry: {entry}\")\n\n        # Check if all abstractions are included\n        if len(ordered_indices) != num_abstractions:\n             raise ValueError(f\"Ordered list length ({len(ordered_indices)}) does not match number of abstractions ({num_abstractions}). Missing indices: {set(range(num_abstractions)) - seen_indices}\")\n\n        print(f\"Determined chapter order (indices): {ordered_indices}\")\n        return ordered_indices # Return the list of indices\n\n    def post(self, shared, prep_res, exec_res):\n        # exec_res is already the list of ordered indices\n        shared[\"chapter_order\"] = exec_res # List of indices\n\nclass WriteChapters(BatchNode):\n    def prep(self, shared):\n        chapter_order = shared[\"chapter_order\"] # List of indices\n        abstractions = shared[\"abstractions\"]   # List of dicts, now using 'files' with indices\n        files_data = shared[\"files\"]\n        # Get already written chapters to provide context\n        # We store them temporarily during the batch run, not in shared memory yet\n        # The 'previous_chapters_summary' will be built progressively in the exec context\n        self.chapters_written_so_far = [] # Use instance variable for temporary storage across exec calls\n\n        # Create a complete list of all chapters\n        all_chapters = []\n        chapter_filenames = {} # Store chapter filename mapping for linking\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                chapter_num = i + 1\n                chapter_name = abstractions[abstraction_index][\"name\"]\n                # Create safe filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in chapter_name).lower()\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                # Format with link\n                all_chapters.append(f\"{chapter_num}. [{chapter_name}]({filename})\")\n                # Store mapping of chapter index to filename for linking\n                chapter_filenames[abstraction_index] = {\"num\": chapter_num, \"name\": chapter_name, \"filename\": filename}\n        \n        # Create a formatted string with all chapters\n        full_chapter_listing = \"\\n\".join(all_chapters)\n\n        items_to_process = []\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                abstraction_details = abstractions[abstraction_index]\n                # Use 'files' (list of indices) directly\n                related_file_indices = abstraction_details.get(\"files\", [])\n                # Get content using helper, passing indices\n                related_files_content_map = get_content_for_indices(files_data, related_file_indices)\n                \n                # Get previous chapter info for transitions\n                prev_chapter = None\n                if i > 0:\n                    prev_idx = chapter_order[i-1]\n                    prev_chapter = chapter_filenames[prev_idx]\n                \n                # Get next chapter info for transitions\n                next_chapter = None\n                if i < len(chapter_order) - 1:\n                    next_idx = chapter_order[i+1]\n                    next_chapter = chapter_filenames[next_idx]\n\n                items_to_process.append({\n                    \"chapter_num\": i + 1,\n                    \"abstraction_index\": abstraction_index,\n                    \"abstraction_details\": abstraction_details,\n                    \"related_files_content_map\": related_files_content_map,\n                    \"project_name\": shared[\"project_name\"],  # Add project name\n                    \"full_chapter_listing\": full_chapter_listing,  # Add the full chapter listing\n                    \"chapter_filenames\": chapter_filenames,  # Add chapter filenames mapping\n                    \"prev_chapter\": prev_chapter,  # Add previous chapter info\n                    \"next_chapter\": next_chapter,  # Add next chapter info\n                    # previous_chapters_summary will be added dynamically in exec\n                })\n            else:\n                print(f\"Warning: Invalid abstraction index {abstraction_index} in chapter_order. Skipping.\")\n\n        print(f\"Preparing to write {len(items_to_process)} chapters...\")\n        return items_to_process # Iterable for BatchNode\n\n    def exec(self, item):\n        # This runs for each item prepared above\n        abstraction_name = item[\"abstraction_details\"][\"name\"]\n        chapter_num = item[\"chapter_num\"]\n        project_name = item.get(\"project_name\")  # Get from item\n        print(f\"Writing chapter {chapter_num} for: {abstraction_name} using LLM...\")\n\n        # Prepare file context string from the map\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path.split('# ')[1] if '# ' in idx_path else idx_path} ---\\n{content}\"\n            for idx_path, content in item[\"related_files_content_map\"].items()\n        )\n\n        # Get summary of chapters written *before* this one\n        # Use the temporary instance variable\n        previous_chapters_summary = \"\\n---\\n\".join(self.chapters_written_so_far)\n\n\n        prompt = f\"\"\"\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `{project_name}` about the concept: \"{abstraction_name}\". This is Chapter {chapter_num}.\n\nConcept Details:\n- Description:\n{item[\"abstraction_details\"][\"description\"]}\n\nComplete Tutorial Structure:\n{item[\"full_chapter_listing\"]}\n\nContext from previous chapters (summary):\n{previous_chapters_summary if previous_chapters_summary else \"This is the first chapter.\"}\n\nRelevant Code Snippets:\n{file_context_str if file_context_str else \"No specific code snippets provided for this abstraction.\"}\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter {chapter_num}: {abstraction_name}`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n\"\"\"\n        chapter_content = call_llm(prompt)\n        # Basic validation/cleanup\n        actual_heading = f\"# Chapter {chapter_num}: {abstraction_name}\"\n        if not chapter_content.strip().startswith(f\"# Chapter {chapter_num}\"):\n             # Add heading if missing or incorrect, trying to preserve content\n             lines = chapter_content.strip().split('\\n')\n             if lines and lines[0].strip().startswith(\"#\"): # If there's some heading, replace it\n                 lines[0] = actual_heading\n                 chapter_content = \"\\n\".join(lines)\n             else: # Otherwise, prepend it\n                 chapter_content = f\"{actual_heading}\\n\\n{chapter_content}\"\n\n        # Add the generated content to our temporary list for the next iteration's context\n        self.chapters_written_so_far.append(chapter_content)\n\n        return chapter_content # Return the Markdown string\n\n    def post(self, shared, prep_res, exec_res_list):\n        # exec_res_list contains the generated Markdown for each chapter, in order\n        shared[\"chapters\"] = exec_res_list\n        # Clean up the temporary instance variable\n        del self.chapters_written_so_far\n        print(f\"Finished writing {len(exec_res_list)} chapters.\")\n\nclass CombineTutorial(Node):\n    def prep(self, shared):\n        project_name = shared[\"project_name\"]\n        output_base_dir = shared.get(\"output_dir\", \"output\") # Default output dir\n        output_path = os.path.join(output_base_dir, project_name)\n        repo_url = shared[\"repo_url\"]  # Get the repository URL\n\n        # Use 'label' from relationships_data['details']\n        relationships_data = shared[\"relationships\"] # {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        chapter_order = shared[\"chapter_order\"] # indices\n        abstractions = shared[\"abstractions\"]   # list of dicts\n        chapters_content = shared[\"chapters\"]   # list of strings\n\n        # --- Generate Mermaid Diagram ---\n        mermaid_lines = [\"flowchart TD\"]\n        # Add nodes for each abstraction\n        for i, abstr in enumerate(abstractions):\n            # Sanitize name for Mermaid ID and label\n            node_id = f\"A{i}\"\n            sanitized_name = abstr['name'].replace('\"', '')\n            node_label = sanitized_name # Using sanitized name only, no index\n            mermaid_lines.append(f'    {node_id}[\"{node_label}\"]')\n        # Add edges for relationships using 'label'\n        for rel in relationships_data['details']:\n            from_node_id = f\"A{rel['from']}\"\n            to_node_id = f\"A{rel['to']}\"\n            # Sanitize 'label' for edge label\n            edge_label = rel['label'].replace('\"', '').replace('\\n', ' ') # Basic sanitization\n            # Limit edge label length for readability (optional, but good for diagrams)\n            max_label_len = 30 # Make it shorter for labels\n            if len(edge_label) > max_label_len:\n                edge_label = edge_label[:max_label_len-3] + \"...\"\n            mermaid_lines.append(f'    {from_node_id} -- \"{edge_label}\" --> {to_node_id}')\n\n        mermaid_diagram = \"\\n\".join(mermaid_lines)\n        # --- End Mermaid ---\n\n\n        # Prepare index.md content\n        index_content = f\"# Tutorial: {project_name}\\n\\n\"\n        index_content += f\"{relationships_data['summary']}\\n\\n\"\n        index_content += f\"**Source Repository:** [{repo_url}]({repo_url})\\n\\n\"\n\n        # Add Mermaid diagram for relationships\n        index_content += \"```mermaid\\n\"\n        index_content += mermaid_diagram + \"\\n\"\n        index_content += \"```\\n\\n\"\n\n        index_content += \"## Chapters\\n\\n\"\n\n        chapter_files = []\n        # Generate chapter links based on the determined order\n        for i, abstraction_index in enumerate(chapter_order):\n            # Ensure index is valid and we have content for it\n            if 0 <= abstraction_index < len(abstractions) and i < len(chapters_content):\n                abstraction_name = abstractions[abstraction_index][\"name\"]\n                # Sanitize name for filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in abstraction_name).lower()\n                # Use chapter number (i+1) for ordering filename\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                index_content += f\"{i+1}. [{abstraction_name}]({filename})\\n\"\n                \n                # Add attribution to chapter content\n                chapter_content = chapters_content[i]\n                if not chapter_content.endswith(\"\\n\\n\"):\n                    chapter_content += \"\\n\\n\"\n                chapter_content += \"---\\n\\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\"\n                \n                # Store filename and corresponding content\n                chapter_files.append({\"filename\": filename, \"content\": chapter_content})\n            else:\n                 print(f\"Warning: Mismatch between chapter order, abstractions, or content at index {i} (abstraction index {abstraction_index}). Skipping file generation for this entry.\")\n\n        # Add attribution to index content\n        index_content += \"\\n\\n---\\n\\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\"\n\n        return {\n            \"output_path\": output_path,\n            \"index_content\": index_content,\n            \"chapter_files\": chapter_files # List of {\"filename\": str, \"content\": str}\n        }\n\n    def exec(self, prep_res):\n        output_path = prep_res[\"output_path\"]\n        index_content = prep_res[\"index_content\"]\n        chapter_files = prep_res[\"chapter_files\"]\n\n        print(f\"Combining tutorial into directory: {output_path}\")\n        # Rely on Node's built-in retry/fallback\n        os.makedirs(output_path, exist_ok=True)\n\n        # Write index.md\n        index_filepath = os.path.join(output_path, \"index.md\")\n        with open(index_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(index_content)\n        print(f\"  - Wrote {index_filepath}\")\n\n        # Write chapter files\n        for chapter_info in chapter_files:\n            chapter_filepath = os.path.join(output_path, chapter_info[\"filename\"])\n            with open(chapter_filepath, \"w\", encoding=\"utf-8\") as f:\n                f.write(chapter_info[\"content\"])\n            print(f\"  - Wrote {chapter_filepath}\")\n\n        return output_path # Return the final path\n\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"final_output_dir\"] = exec_res # Store the output path\n        print(f\"\\nTutorial generation complete! Files are in: {exec_res}\")\n\n--- File: utils/crawl_github_files.py ---\nimport requests\nimport base64\nimport os\nimport tempfile\nimport git\nimport time\nimport fnmatch\nfrom typing import Union, Set, List, Dict, Tuple, Any\nfrom urllib.parse import urlparse\n\ndef crawl_github_files(\n    repo_url, \n    token=None, \n    max_file_size: int = 1 * 1024 * 1024,  # 1 MB\n    use_relative_paths: bool = False,\n    include_patterns: Union[str, Set[str]] = None,\n    exclude_patterns: Union[str, Set[str]] = None\n):\n    \"\"\"\n    Crawl files from a specific path in a GitHub repository at a specific commit.\n\n    Args:\n        repo_url (str): URL of the GitHub repository with specific path and commit\n                        (e.g., 'https://github.com/microsoft/autogen/tree/e45a15766746d95f8cfaaa705b0371267bec812e/python/packages/autogen-core/src/autogen_core')\n        token (str, optional): **GitHub personal access token.**\n            - **Required for private repositories.**\n            - **Recommended for public repos to avoid rate limits.**\n            - Can be passed explicitly or set via the `GITHUB_TOKEN` environment variable.\n        max_file_size (int, optional): Maximum file size in bytes to download (default: 1 MB)\n        use_relative_paths (bool, optional): If True, file paths will be relative to the specified subdirectory\n        include_patterns (str or set of str, optional): Pattern or set of patterns specifying which files to include (e.g., \"*.py\", {\"*.md\", \"*.txt\"}).\n                                                       If None, all files are included.\n        exclude_patterns (str or set of str, optional): Pattern or set of patterns specifying which files to exclude.\n                                                       If None, no files are excluded.\n\n    Returns:\n        dict: Dictionary with files and statistics\n    \"\"\"\n    # Convert single pattern to set\n    if include_patterns and isinstance(include_patterns, str):\n        include_patterns = {include_patterns}\n    if exclude_patterns and isinstance(exclude_patterns, str):\n        exclude_patterns = {exclude_patterns}\n\n    def should_include_file(file_path: str, file_name: str) -> bool:\n        \"\"\"Determine if a file should be included based on patterns\"\"\"\n        # If no include patterns are specified, include all files\n        if not include_patterns:\n            include_file = True\n        else:\n            # Check if file matches any include pattern\n            include_file = any(fnmatch.fnmatch(file_name, pattern) for pattern in include_patterns)\n\n        # If exclude patterns are specified, check if file should be excluded\n        if exclude_patterns and include_file:\n            # Exclude if file matches any exclude pattern\n            exclude_file = any(fnmatch.fnmatch(file_path, pattern) for pattern in exclude_patterns)\n            return not exclude_file\n\n        return include_file\n\n    # Detect SSH URL (git@ or .git suffix)\n    is_ssh_url = repo_url.startswith(\"git@\") or repo_url.endswith(\".git\")\n\n    if is_ssh_url:\n        # Clone repo via SSH to temp dir\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            print(f\"Cloning SSH repo {repo_url} to temp dir {tmpdirname} ...\")\n            try:\n                repo = git.Repo.clone_from(repo_url, tmpdirname)\n            except Exception as e:\n                print(f\"Error cloning repo: {e}\")\n                return {\"files\": {}, \"stats\": {\"error\": str(e)}}\n\n            # Attempt to checkout specific commit/branch if in URL\n            # Parse ref and subdir from SSH URL? SSH URLs don't have branch info embedded\n            # So rely on default branch, or user can checkout manually later\n            # Optionally, user can pass ref explicitly in future API\n\n            # Walk directory\n            files = {}\n            skipped_files = []\n\n            for root, dirs, filenames in os.walk(tmpdirname):\n                for filename in filenames:\n                    abs_path = os.path.join(root, filename)\n                    rel_path = os.path.relpath(abs_path, tmpdirname)\n\n                    # Check file size\n                    try:\n                        file_size = os.path.getsize(abs_path)\n                    except OSError:\n                        continue\n\n                    if file_size > max_file_size:\n                        skipped_files.append((rel_path, file_size))\n                        print(f\"Skipping {rel_path}: size {file_size} exceeds limit {max_file_size}\")\n                        continue\n\n                    # Check include/exclude patterns\n                    if not should_include_file(rel_path, filename):\n                        print(f\"Skipping {rel_path}: does not match include/exclude patterns\")\n                        continue\n\n                    # Read content\n                    try:\n                        with open(abs_path, \"r\", encoding=\"utf-8\") as f:\n                            content = f.read()\n                        files[rel_path] = content\n                        print(f\"Added {rel_path} ({file_size} bytes)\")\n                    except Exception as e:\n                        print(f\"Failed to read {rel_path}: {e}\")\n\n            return {\n                \"files\": files,\n                \"stats\": {\n                    \"downloaded_count\": len(files),\n                    \"skipped_count\": len(skipped_files),\n                    \"skipped_files\": skipped_files,\n                    \"base_path\": None,\n                    \"include_patterns\": include_patterns,\n                    \"exclude_patterns\": exclude_patterns,\n                    \"source\": \"ssh_clone\"\n                }\n            }\n\n    # Parse GitHub URL to extract owner, repo, commit/branch, and path\n    parsed_url = urlparse(repo_url)\n    path_parts = parsed_url.path.strip('/').split('/')\n    \n    if len(path_parts) < 2:\n        raise ValueError(f\"Invalid GitHub URL: {repo_url}\")\n    \n    # Extract the basic components\n    owner = path_parts[0]\n    repo = path_parts[1]\n    \n    # Check if URL contains a specific branch/commit\n    if 'tree' in path_parts:\n        tree_index = path_parts.index('tree')\n        ref = path_parts[tree_index + 1]\n        # Combine all parts after the ref as the path\n        path_start = tree_index + 2\n        specific_path = '/'.join(path_parts[path_start:]) if path_start < len(path_parts) else \"\"\n    else:\n        ref = \"main\"  # Default branch\n        specific_path = \"\"\n    \n    # Setup for GitHub API\n    headers = {\"Accept\": \"application/vnd.github.v3+json\"}\n    if token:\n        headers[\"Authorization\"] = f\"token {token}\"\n    \n    # Dictionary to store path -> content mapping\n    files = {}\n    skipped_files = []\n    \n    def should_include_file(file_path: str, file_name: str) -> bool:\n        \"\"\"Determine if a file should be included based on patterns\"\"\"\n        # If no include patterns are specified, include all files\n        if not include_patterns:\n            include_file = True\n        else:\n            # Check if file matches any include pattern\n            include_file = any(fnmatch.fnmatch(file_name, pattern) for pattern in include_patterns)\n        \n        # If exclude patterns are specified, check if file should be excluded\n        if exclude_patterns and include_file:\n            # Exclude if file matches any exclude pattern\n            exclude_file = any(fnmatch.fnmatch(file_path, pattern) for pattern in exclude_patterns)\n            return not exclude_file\n        \n        return include_file\n    \n    def fetch_contents(path):\n        \"\"\"Fetch contents of the repository at a specific path and commit\"\"\"\n        url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{path}\"\n        params = {\"ref\": ref}\n        \n        response = requests.get(url, headers=headers, params=params)\n        \n        if response.status_code == 403 and 'rate limit exceeded' in response.text.lower():\n            reset_time = int(response.headers.get('X-RateLimit-Reset', 0))\n            wait_time = max(reset_time - time.time(), 0) + 1\n            print(f\"Rate limit exceeded. Waiting for {wait_time:.0f} seconds...\")\n            time.sleep(wait_time)\n            return fetch_contents(path)\n            \n        if response.status_code == 404:\n            if not token:\n                print(f\"Error 404: Repository not found or is private.\\n\"\n                      f\"If this is a private repository, please provide a valid GitHub token via the 'token' argument or set the GITHUB_TOKEN environment variable.\")\n            else:\n                print(f\"Error 404: Path '{path}' not found in repository or insufficient permissions with the provided token.\\n\"\n                      f\"Please verify the token has access to this repository and the path exists.\")\n            return\n            \n        if response.status_code != 200:\n            print(f\"Error fetching {path}: {response.status_code} - {response.text}\")\n            return\n        \n        contents = response.json()\n        \n        # Handle both single file and directory responses\n        if not isinstance(contents, list):\n            contents = [contents]\n        \n        for item in contents:\n            item_path = item[\"path\"]\n            \n            # Calculate relative path if requested\n            if use_relative_paths and specific_path:\n                # Make sure the path is relative to the specified subdirectory\n                if item_path.startswith(specific_path):\n                    rel_path = item_path[len(specific_path):].lstrip('/')\n                else:\n                    rel_path = item_path\n            else:\n                rel_path = item_path\n            \n            if item[\"type\"] == \"file\":\n                # Check if file should be included based on patterns\n                if not should_include_file(rel_path, item[\"name\"]):\n                    print(f\"Skipping {rel_path}: Does not match include/exclude patterns\")\n                    continue\n                \n                # Check file size if available\n                file_size = item.get(\"size\", 0)\n                if file_size > max_file_size:\n                    skipped_files.append((item_path, file_size))\n                    print(f\"Skipping {rel_path}: File size ({file_size} bytes) exceeds limit ({max_file_size} bytes)\")\n                    continue\n                \n                # For files, get raw content\n                if \"download_url\" in item and item[\"download_url\"]:\n                    file_url = item[\"download_url\"]\n                    file_response = requests.get(file_url, headers=headers)\n                    \n                    # Final size check in case content-length header is available but differs from metadata\n                    content_length = int(file_response.headers.get('content-length', 0))\n                    if content_length > max_file_size:\n                        skipped_files.append((item_path, content_length))\n                        print(f\"Skipping {rel_path}: Content length ({content_length} bytes) exceeds limit ({max_file_size} bytes)\")\n                        continue\n                        \n                    if file_response.status_code == 200:\n                        files[rel_path] = file_response.text\n                        print(f\"Downloaded: {rel_path} ({file_size} bytes) \")\n                    else:\n                        print(f\"Failed to download {rel_path}: {file_response.status_code}\")\n                else:\n                    # Alternative method if download_url is not available\n                    content_response = requests.get(item[\"url\"], headers=headers)\n                    if content_response.status_code == 200:\n                        content_data = content_response.json()\n                        if content_data.get(\"encoding\") == \"base64\" and \"content\" in content_data:\n                            # Check size of base64 content before decoding\n                            if len(content_data[\"content\"]) * 0.75 > max_file_size:  # Approximate size calculation\n                                estimated_size = int(len(content_data[\"content\"]) * 0.75)\n                                skipped_files.append((item_path, estimated_size))\n                                print(f\"Skipping {rel_path}: Encoded content exceeds size limit\")\n                                continue\n                                \n                            file_content = base64.b64decode(content_data[\"content\"]).decode('utf-8')\n                            files[rel_path] = file_content\n                            print(f\"Downloaded: {rel_path} ({file_size} bytes)\")\n                        else:\n                            print(f\"Unexpected content format for {rel_path}\")\n                    else:\n                        print(f\"Failed to get content for {rel_path}: {content_response.status_code}\")\n            \n            elif item[\"type\"] == \"dir\":\n                # Recursively process subdirectories\n                fetch_contents(item_path)\n    \n    # Start crawling from the specified path\n    fetch_contents(specific_path)\n    \n    return {\n        \"files\": files,\n        \"stats\": {\n            \"downloaded_count\": len(files),\n            \"skipped_count\": len(skipped_files),\n            \"skipped_files\": skipped_files,\n            \"base_path\": specific_path if use_relative_paths else None,\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns\n        }\n    }\n\n# Example usage\nif __name__ == \"__main__\":\n    # Get token from environment variable (recommended for private repos)\n    github_token = os.environ.get(\"GITHUB_TOKEN\")\n    if not github_token:\n        print(\"Warning: No GitHub token found in environment variable 'GITHUB_TOKEN'.\\n\"\n              \"Private repositories will not be accessible without a token.\\n\"\n              \"To access private repos, set the environment variable or pass the token explicitly.\")\n    \n    repo_url = \"https://github.com/pydantic/pydantic/tree/6c38dc93f40a47f4d1350adca9ec0d72502e223f/pydantic\"\n    \n    # Example: Get Python and Markdown files, but exclude test files\n    result = crawl_github_files(\n        repo_url, \n        token=github_token,\n        max_file_size=1 * 1024 * 1024,  # 1 MB in bytes\n        use_relative_paths=True,  # Enable relative paths\n        include_patterns={\"*.py\", \"*.md\"},  # Include Python and Markdown files\n    )\n    \n    files = result[\"files\"]\n    stats = result[\"stats\"]\n    \n    print(f\"\\nDownloaded {stats['downloaded_count']} files.\")\n    print(f\"Skipped {stats['skipped_count']} files due to size limits or patterns.\")\n    print(f\"Base path for relative paths: {stats['base_path']}\")\n    print(f\"Include patterns: {stats['include_patterns']}\")\n    print(f\"Exclude patterns: {stats['exclude_patterns']}\")\n    \n    # Display all file paths in the dictionary\n    print(\"\\nFiles in dictionary:\")\n    for file_path in sorted(files.keys()):\n        print(f\"  {file_path}\")\n    \n    # Example: accessing content of a specific file\n    if files:\n        sample_file = next(iter(files))\n        print(f\"\\nSample file: {sample_file}\")\n        print(f\"Content preview: {files[sample_file][:200]}...\")\n\n--- File: utils/crawl_local_files.py ---\nimport os\nimport fnmatch\n\ndef crawl_local_files(directory, include_patterns=None, exclude_patterns=None, max_file_size=None, use_relative_paths=True):\n    \"\"\"\n    Crawl files in a local directory with similar interface as crawl_github_files.\n    \n    Args:\n        directory (str): Path to local directory\n        include_patterns (set): File patterns to include (e.g. {\"*.py\", \"*.js\"})\n        exclude_patterns (set): File patterns to exclude (e.g. {\"tests/*\"})\n        max_file_size (int): Maximum file size in bytes\n        use_relative_paths (bool): Whether to use paths relative to directory\n        \n    Returns:\n        dict: {\"files\": {filepath: content}}\n    \"\"\"\n    if not os.path.isdir(directory):\n        raise ValueError(f\"Directory does not exist: {directory}\")\n        \n    files_dict = {}\n    \n    for root, _, files in os.walk(directory):\n        for filename in files:\n            filepath = os.path.join(root, filename)\n            \n            # Get path relative to directory if requested\n            if use_relative_paths:\n                relpath = os.path.relpath(filepath, directory)\n            else:\n                relpath = filepath\n                \n            # Check if file matches any include pattern\n            included = False\n            if include_patterns:\n                for pattern in include_patterns:\n                    if fnmatch.fnmatch(relpath, pattern):\n                        included = True\n                        break\n            else:\n                included = True\n                \n            # Check if file matches any exclude pattern\n            excluded = False\n            if exclude_patterns:\n                for pattern in exclude_patterns:\n                    if fnmatch.fnmatch(relpath, pattern):\n                        excluded = True\n                        break\n                        \n            if not included or excluded:\n                continue\n                \n            # Check file size\n            if max_file_size and os.path.getsize(filepath) > max_file_size:\n                continue\n                \n            try:\n                with open(filepath, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                files_dict[relpath] = content\n            except Exception as e:\n                print(f\"Warning: Could not read file {filepath}: {e}\")\n                \n    return {\"files\": files_dict}\n\nif __name__ == \"__main__\":\n    print(\"--- Crawling parent directory ('..') ---\")\n    files_data = crawl_local_files(\"..\", exclude_patterns={\"*.pyc\", \"__pycache__/*\", \".git/*\", \"output/*\"})\n    print(f\"Found {len(files_data['files'])} files:\")\n    for path in files_data[\"files\"]:\n        print(f\"  {path}\")\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 3: Codebase Crawling`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "# Chapter 3: Codebase Crawling\n\nIn the previous chapter, [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md), we learned how AI can analyze code and generate beginner-friendly explanations. But where does that code come from? That's where codebase crawling comes in!\n\nImagine you want to bake a cake. The AI-Powered analysis is like having a master chef who can explain every ingredient and baking step in detail. But first, you need to *get* the ingredients! Codebase crawling is like going to the grocery store and carefully selecting all the necessary ingredients (code files) for our AI chef to work with.\n\n**Why Codebase Crawling?**\n\nBefore we can analyze a project's code and create a tutorial, we need to first gather the relevant files. This can involve:\n\n*   **Fetching from a remote repository:**  Downloading code from platforms like GitHub.\n*   **Exploring a local directory:**  Reading code from your computer.\n*   **Filtering out irrelevant files:**  Ignoring tests, documentation, or other non-essential code.\n\n**The Goal: Getting the Right Code, Efficiently**\n\nOur `Codebase Crawling` component aims to provide a flexible and efficient way to fetch the necessary code files for analysis. It's like a librarian searching for specific books based on certain criteria (include/exclude patterns) and size restrictions, ensuring only relevant materials are collected.\n\n**Key Concepts**\n\nLet's break down the core ideas behind our codebase crawling:\n\n1.  **Fetching Source Code (FetchRepo Node):**\n    *   This node takes the Github repo URL or a local directory path and downloads source code using the `crawl_github_files` or `crawl_local_files` functions.\n\n2.  **Repository URL:** The address of the code repository (e.g., a GitHub link).\n\n3.  **Local Directory:** The path to a directory on your computer containing the code.\n\n4.  **Include/Exclude Patterns:**  Rules for selecting specific files based on their names or paths (e.g., including only `.py` files, excluding `test` directories). This is done using `fnmatch`, which supports wildcard matching similar to bash.\n\n5.  **File Size Limits:**  Maximum size for individual files to prevent processing large, irrelevant files.\n\n**How it Works: A Step-by-Step View**\n\nLet's walk through a simple scenario: You want to create a tutorial for a small Python library on GitHub.\n\n1.  **The `FetchRepo` Node:** This node is responsible for retrieving the code files. It uses either `crawl_github_files` (for GitHub repos) or `crawl_local_files` (for local directories).\n\n    *   Here's a simplified snippet of the `FetchRepo` node's `prep` and `exec` methods (from `nodes.py`):\n\n     ```python\n     class FetchRepo(Node):\n         def prep(self, shared):\n             repo_url = shared.get(\"repo_url\")\n             local_dir = shared.get(\"local_dir\")\n             project_name = shared.get(\"project_name\")\n             \n             if not project_name:\n                 # Basic name derivation from URL or directory\n                 if repo_url:\n                     project_name = repo_url.split('/')[-1].replace('.git', '')\n                 else:\n                     project_name = os.path.basename(os.path.abspath(local_dir))\n                 shared[\"project_name\"] = project_name\n\n             # Get file patterns directly from shared\n             include_patterns = shared[\"include_patterns\"]\n             exclude_patterns = shared[\"exclude_patterns\"]\n             max_file_size = shared[\"max_file_size\"]\n\n             return {\n                 \"repo_url\": repo_url,\n                 \"local_dir\": local_dir,\n                 \"token\": shared.get(\"github_token\"),\n                 \"include_patterns\": include_patterns,\n                 \"exclude_patterns\": exclude_patterns,\n                 \"max_file_size\": max_file_size,\n                 \"use_relative_paths\": True\n             }\n\n         def exec(self, prep_res):\n             if prep_res[\"repo_url\"]:\n                 print(f\"Crawling repository: {prep_res['repo_url']}...\")\n                 result = crawl_github_files(\n                     repo_url=prep_res[\"repo_url\"],\n                     token=prep_res[\"token\"],\n                     include_patterns=prep_res[\"include_patterns\"],\n                     exclude_patterns=prep_res[\"exclude_patterns\"],\n                     max_file_size=prep_res[\"max_file_size\"],\n                     use_relative_paths=prep_res[\"use_relative_paths\"]\n                 )\n             else:\n                 print(f\"Crawling directory: {prep_res['local_dir']}...\")\n                 result = crawl_local_files(\n                     directory=prep_res[\"local_dir\"],\n                     include_patterns=prep_res[\"include_patterns\"],\n                     exclude_patterns=prep_res[\"exclude_patterns\"],\n                     max_file_size=prep_res[\"max_file_size\"],\n                     use_relative_paths=prep_res[\"use_relative_paths\"]\n                 )\n                 \n             # Convert dict to list of tuples: [(path, content), ...]\n             files_list = list(result.get(\"files\", {}).items())\n             print(f\"Fetched {len(files_list)} files.\")\n             return files_list\n\n         def post(self, shared, prep_res, exec_res):\n             shared[\"files\"] = exec_res # List of (path, content) tuples\n     ```\n\n    *   The `prep` method retrieves all config options (such as Github repo URL, local directory path, file inclusion/exclusion patterns, max file size), and sets them to the `prep_res` to be used by the `exec` method.\n    *   The `exec` method determines if source code is fetched from Github or local directory based on the presence of a Github repo URL. It then downloads the source code, and stores the path and code as a tuple into a list.\n    *   The `post` method adds the source code to the `shared` dictionary, specifically at `shared[\"files\"]`.\n\n2.  **`crawl_github_files` Function:** This function is defined in `utils/crawl_github_files.py`.\n\n    *   Here's a simplified view of what happens inside the `crawl_github_files` function:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant CrawlGithubFiles as crawl_github_files\n    participant GithubAPI as Github API\n\n    User->>CrawlGithubFiles: Call crawl_github_files(repo_url, token, include_patterns, exclude_patterns, max_file_size, use_relative_paths)\n    CrawlGithubFiles->>GithubAPI: GET contents from Github API for the repo URL\n    GithubAPI-->>CrawlGithubFiles: Return contents from Github API\n    CrawlGithubFiles->>CrawlGithubFiles: Apply include/exclude patterns\n    CrawlGithubFiles->>CrawlGithubFiles: Apply file size limits\n    CrawlGithubFiles-->>User: Return list of (path, code)\n```\n\n    *   The `crawl_github_files` function downloads the source code for the repo URL by calling the Github API.\n    *   The `crawl_github_files` function then filters all source code based on include/exclude patterns, and file size limits.\n    *   The function returns a list of (path, code).\n\n3.  **`crawl_local_files` Function:** This function is defined in `utils/crawl_local_files.py`.\n\n    *   Here's a simplified view of what happens inside the `crawl_local_files` function:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant CrawlLocalFiles as crawl_local_files\n    participant OS as OS\n\n    User->>CrawlLocalFiles: Call crawl_local_files(local_dir, include_patterns, exclude_patterns, max_file_size, use_relative_paths)\n    CrawlLocalFiles->>OS: Walk directory for the local directory path\n    OS-->>CrawlLocalFiles: Return files\n    CrawlLocalFiles->>CrawlLocalFiles: Apply include/exclude patterns\n    CrawlLocalFiles->>CrawlLocalFiles: Apply file size limits\n    CrawlLocalFiles-->>User: Return list of (path, code)\n```\n\n    *   The `crawl_local_files` function walks the directory structure to find all files in the directory path.\n    *   The `crawl_local_files` function then filters all source code based on include/exclude patterns, and file size limits.\n    *   The function returns a list of (path, code).\n\n4.  **Example:**\n\n    ```python\n    shared = {\n        \"repo_url\": \"https://github.com/user/my-cool-project\",\n        \"include_patterns\": {\"*.py\"},\n        \"exclude_patterns\": {\"tests/*\"},\n        \"max_file_size\": 100000  # 100KB\n    }\n    ```\n\n    *   In this example, the `FetchRepo` node would:\n\n        *   Download the code from the specified GitHub repository.\n        *   Only include files ending with `.py`.\n        *   Exclude any files in directories named `tests`.\n        *   Skip any files larger than 100KB.\n\n5.  **Output:** The `FetchRepo` node outputs a list of tuples, where each tuple contains the file path and its contents. This list is then stored in the `shared` dictionary for use by subsequent nodes, such as [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md).\n\n**Code Example**\n\nLet's look at a snippet from `utils/crawl_github_files.py` to understand how file filtering works:\n\n```python\nimport fnmatch\n\ndef should_include_file(file_path: str, file_name: str) -> bool:\n    \"\"\"Determine if a file should be included based on patterns\"\"\"\n    # If no include patterns are specified, include all files\n    if not include_patterns:\n        include_file = True\n    else:\n        # Check if file matches any include pattern\n        include_file = any(fnmatch.fnmatch(file_name, pattern) for pattern in include_patterns)\n\n    # If exclude patterns are specified, check if file should be excluded\n    if exclude_patterns and include_file:\n        # Exclude if file matches any exclude pattern\n        exclude_file = any(fnmatch.fnmatch(file_path, pattern) for pattern in exclude_patterns)\n        return not exclude_file\n\n    return include_file\n```\n\nThis code snippet demonstrates how `fnmatch` is used to match file paths against include and exclude patterns.  It's a simple but powerful way to control which files are included in the analysis.\n\n**Benefits and Considerations**\n\n*   **Benefits:**\n    *   **Flexibility:**  Supports both remote repositories and local directories.\n    *   **Efficiency:**  Filters out irrelevant files to reduce processing time.\n    *   **Customization:**  Allows specifying include/exclude patterns and file size limits.\n*   **Considerations:**\n    *   **Rate Limits:**  When crawling GitHub repositories, be aware of API rate limits. Using a GitHub token is highly recommended.\n    *   **Complexity:**  Complex include/exclude patterns can be difficult to manage.\n\n**Conclusion**\n\nIn this chapter, we learned about codebase crawling, a crucial step in our tutorial generation process. By fetching and filtering code files, we ensure that our AI-powered analysis focuses on the most relevant parts of the codebase.  In the next chapter, [Knowledge Base Construction](04_knowledge_base_construction.md), we'll see how we use the crawled code to build a knowledge base for generating our tutorials.\n", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `Tutorial-Codebase-Knowledge` about the concept: \"Knowledge Base Construction\". This is Chapter 4.\n\nConcept Details:\n- Description:\nThis abstraction organizes extracted code snippets and identified relationships into a structured format for the AI to use.  It's like organizing a messy desk into a well-organized workspace, with labeled folders and clear indexes, so the AI can easily access and understand the information. This could also be viewed as a vector database.\n\n\nComplete Tutorial Structure:\n1. [Main Execution Script](01_main_execution_script.md)\n2. [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md)\n3. [Codebase Crawling](03_codebase_crawling.md)\n4. [Knowledge Base Construction](04_knowledge_base_construction.md)\n5. [Tutorial Content Combination](05_tutorial_content_combination.md)\n6. [LLM Orchestration (PocketFlow)](06_llm_orchestration__pocketflow_.md)\n\nContext from previous chapters (summary):\n# Chapter 1: Main Execution Script\n\nWelcome to the first chapter of our tutorial on building a tutorial generation system! This chapter focuses on the \"Main Execution Script,\" the conductor of our orchestra that makes everything happen.\n\nImagine you want to create a tutorial for a cool new Python library you found on GitHub.  Where do you even begin?  That's where the Main Execution Script comes in. It's the starting point of our whole tutorial-making process. It takes your instructions (like the GitHub link) and kicks off all the other parts of the system to create your tutorial.\n\n**What Problem Does It Solve?**\n\nWithout a central entry point, we'd have a bunch of disconnected pieces. The Main Execution Script brings order to chaos. It:\n\n*   **Takes your input:** Where is the code? What do you want to name the tutorial?\n*   **Sets the stage:** Gets everything ready for the tutorial generation.\n*   **Kicks off the process:** Starts all the other parts of the system in the right order.\n\n**Key Concepts**\n\nLet's break down the key concepts of the Main Execution Script:\n\n1.  **Command-Line Arguments:** These are the instructions you give the script when you run it.  Think of them as telling the script *what* to do.\n\n2.  **Configuration:**  This is setting up all the necessary components, like where to save the output.\n\n3.  **Orchestration:** This is the script's main job: making sure all the other parts of the system run in the correct sequence, like a conductor leading an orchestra. We use [LLM Orchestration (PocketFlow)](06_llm_orchestration__pocketflow_.md) for this.\n\n**Using the Main Execution Script**\n\nLet's see how you'd use the Main Execution Script to generate a tutorial for a GitHub repository.\n\n**Example Input:**\n\nLet's say you want to create a tutorial for a repository named \"my-cool-project\" on GitHub, located at `https://github.com/user/my-cool-project`. You'd run the script from your terminal like this:\n\n```bash\npython main.py --repo https://github.com/user/my-cool-project -n MyCoolProjectTutorial\n```\n\nLet's break down that command:\n\n*   `python main.py`:  This tells your computer to run the `main.py` file using Python.\n*   `--repo https://github.com/user/my-cool-project`:  This tells the script the URL of the GitHub repository.\n*   `-n MyCoolProjectTutorial`: This tells the script to name the project \"MyCoolProjectTutorial\".  If you leave this out, the script will try to figure out the name from the repository URL.\n\n**Example Output:**\n\nAfter running the command, the script will:\n\n1.  Download the code from the GitHub repository.\n2.  Analyze the code.\n3.  Create a tutorial based on the code.\n4.  Save the tutorial in a directory named \"output\" (unless you specify a different output directory).\n\nYou'll find a structured tutorial with different chapters covering the codebase's essential aspects in the `output` directory!\n\n**Code Walkthrough**\n\nLet's look at some parts of the `main.py` script:\n\n```python\nimport argparse\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Generate a tutorial...\")\n    parser.add_argument(\"--repo\", help=\"URL of the public GitHub repository.\")\n    parser.add_argument(\"-n\", \"--name\", help=\"Project name (optional...\")\n    args = parser.parse_args()\n\n    # --- Rest of the Code ---\n```\n\nThis code sets up the command-line argument parsing.  It defines the arguments we can use (like `--repo` and `-n`) and provides helpful descriptions. The `argparse` module makes it easy to get the values you provide when you run the script.\n\n```python\n    shared = {\n        \"repo_url\": args.repo,\n        \"project_name\": args.name,\n        \"output_dir\": \"output\",  # Simplification\n    }\n\n    print(f\"Starting tutorial generation for: {args.repo}\")\n\n    # --- Rest of the Code ---\n```\n\nThis code creates a `shared` dictionary, a central place to store information that all parts of the system need.  We store the repository URL, the project name, and the output directory here.  Think of it as a shared whiteboard where all the different parts of the tutorial generation system can read and write information.\n\n```python\n    # Create the flow instance\n    tutorial_flow = create_tutorial_flow()\n\n    # Run the flow\n    tutorial_flow.run(shared)\n```\n\nThis is where the magic happens! We create a `tutorial_flow` (which we will discuss more in [LLM Orchestration (PocketFlow)](06_llm_orchestration__pocketflow_.md)) and tell it to run, passing in the `shared` dictionary. The `tutorial_flow` orchestrates all the other steps to generate the tutorial.\n\n**Internal Implementation: A Step-by-Step View**\n\nHere's a simplified view of what happens inside the Main Execution Script when you run it:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant MainScript as Main Execution Script\n    participant ArgumentParser as Argument Parser\n    participant Flow as Tutorial Flow\n    \n    User->>MainScript: Run script with arguments\n    MainScript->>ArgumentParser: Parse arguments\n    ArgumentParser-->>MainScript: Return arguments\n    MainScript->>Flow: Create tutorial flow\n    MainScript->>Flow: Run flow with arguments\n    Flow-->>MainScript: Tutorial generated\n    MainScript->>User: Completion Message\n```\n\n1.  **User:** You run the `main.py` script with command-line arguments (e.g., the GitHub repository URL).\n2.  **Main Execution Script:** The script receives your command and uses `ArgumentParser` to understand your instructions.\n3.  **Argument Parser:** The argument parser takes the command-line arguments and organizes them into a usable format.\n4.  **Main Execution Script:**  The script then creates an instance of the `Tutorial Flow`. The script then runs the `Tutorial Flow`.\n5.  **Tutorial Flow:** The tutorial flow coordinates with other components like the [Codebase Crawling](03_codebase_crawling.md), [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md), and [Knowledge Base Construction](04_knowledge_base_construction.md) components to create the tutorial.\n6.  **Main Execution Script:** The script prints a completion message, indicating that the tutorial has been generated.\n\n**Implementation Details**\n\nThe `create_tutorial_flow()` function, defined in `flow.py` (not shown in the code snippets above), is responsible for creating the `tutorial_flow` instance.\n\n**Conclusion**\n\nIn this chapter, we explored the Main Execution Script, the entry point of our tutorial generation system. We learned how it takes user input, configures the system, and orchestrates the tutorial generation process.  It's the conductor that makes sure all the instruments play in harmony!\n\nIn the next chapter, we'll dive into the crucial component of [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md), where we'll see how AI helps us understand the codebase and generate meaningful tutorial content.\n\n---\n# Chapter 2: AI-Powered Code Analysis and Generation\n\nIn the previous chapter, [Main Execution Script](01_main_execution_script.md), we saw how the script acts as the conductor of our tutorial-generation process, taking your input and orchestrating the workflow. Now, we'll dive into a crucial part of that workflow: using AI to understand the code and generate content!\n\n**Why AI for Code Analysis and Generation?**\n\nImagine you're given a huge, complex Lego set with thousands of pieces and no instructions. That's what it's like trying to understand a large codebase!  It's overwhelming.  The goal of this chapter is to learn how to use AI to understand any codebase and describe it so that others can also understand it.\n\nThat's where AI comes in! Our `AI-Powered Code Analysis and Generation` component acts like an experienced tutor. It uses Large Language Models (LLMs) to:\n\n*   **Analyze the code:**  Like carefully examining each Lego brick.\n*   **Identify key components:**  Finding the essential parts of the set.\n*   **Generate beginner-friendly explanations:**  Writing clear, step-by-step instructions.\n\n**The Big Picture: Turning Code into Tutorials**\n\nThis component sits in the middle of our tutorial generation process. It takes raw code (from crawling the codebase) and turns it into structured, understandable tutorial content. We start by downloading the source code, then we'll use this component to create a tutorial.\n\n**Key Concepts**\n\nLet's break down the core ideas behind our AI-powered approach:\n\n1.  **Large Language Models (LLMs):** These are powerful AI models trained on vast amounts of text data. Think of them as having read millions of books and articles on programming.  We use them to understand code and write explanations. We pass the prompts to the LLM through the `call_llm` function, which is defined in `utils/call_llm.py`. This allows us to easily switch models by changing the environment variables for the model.\n\n2.  **Code Abstraction Identification:** The AI identifies key concepts or modules within the codebase. This is like finding the most important functions or classes.\n\n3.  **Relationship Analysis:** The AI determines how these components interact with each other.  This is like understanding how the Lego bricks fit together to build the final model.\n\n4.  **Beginner-Friendly Content Generation:** The AI translates complex code into simple, easy-to-understand explanations, tailored for beginners. It generates markdown content that is later combined to build a chapter.\n\n**How it Works: A Simplified Walkthrough**\n\nLet's walk through a simplified example using pieces of the code, starting from what it receives from the main script.\n\n```python\nshared = {\n  \"repo_url\": \"https://github.com/user/my-cool-project\",\n  \"project_name\": \"MyCoolProject\",\n  \"files\": [(\"file1.py\", \"def add(a, b): return a + b\"), (\"file2.py\", \"def multiply(a, b): return a * b\")],\n}\n```\n\nThis `shared` dictionary is passed between different components, as we saw in the [Main Execution Script](01_main_execution_script.md) chapter. We will focus on how the `IdentifyAbstractions` node processes the `shared` data.\n\n1.  **Fetching Code:** We already have the source code (simulated as `shared[\"files\"]` here).  In reality, this would come from the [Codebase Crawling](03_codebase_crawling.md) stage.\n\n2.  **Identifying Abstractions (IdentifyAbstractions Node):**\n\n   *   This node takes the codebase's content and asks the LLM to find the most important concepts.\n\n   *   It prepares a prompt for the LLM that includes the entire codebase context.\n\n   *   Here's a simplified snippet of the `IdentifyAbstractions` node's `prep` and `exec` methods (from `nodes.py`):\n\n     ```python\n     class IdentifyAbstractions(Node):\n         def prep(self, shared):\n             files_data = shared[\"files\"]\n             project_name = shared[\"project_name\"]\n             # simplified context creation\n             context = \"\"\n             file_info = []\n             for i, (path, content) in enumerate(files_data):\n                 context += f\"--- File Index {i}: {path} ---\\n{content}\\n\\n\"\n                 file_info.append((i, path))\n\n             file_listing_for_prompt = \"\\n\".join([f\"- {idx} # {path}\" for idx, path in file_info])\n             return context, file_listing_for_prompt, len(files_data), project_name\n\n         def exec(self, prep_res):\n             context, file_listing_for_prompt, file_count, project_name = prep_res\n\n             prompt = f\"\"\"\n     For the project `{project_name}`:\n\n     Codebase Context:\n     {context}\n\n     Analyze the codebase context.\n     Identify the top 2 core most important abstractions to help those new to the codebase.\n\n     For each abstraction, provide:\n     1. A concise `name`.\n     2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n     3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\n     List of file indices and paths present in the context:\n     {file_listing_for_prompt}\n\n     Format the output as a YAML list of dictionaries:\n\n     ```yaml\n     - name: Addition\n       description: |\n         This abstraction performs addition.\n         It's like adding two numbers together.\n       file_indices:\n         - 0 # file1.py\n     - name: Multiplication\n       description: |\n         This abstraction performs multiplication.\n         It's like multiplying two numbers together.\n       file_indices:\n         - 1 # file2.py\n     ```\"\"\"\n             response = call_llm(prompt) #call llm\n\n             yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n             abstractions = yaml.safe_load(yaml_str)\n\n             validated_abstractions = []\n             for item in abstractions:\n                  item[\"files\"] = [int(idx_entry.split('#')[0].strip()) for idx_entry in item[\"file_indices\"]]\n                  validated_abstractions.append({\n                       \"name\": item[\"name\"],\n                       \"description\": item[\"description\"],\n                       \"files\": item[\"files\"]\n                  })\n\n             return validated_abstractions\n\n         def post(self, shared, prep_res, exec_res):\n             shared[\"abstractions\"] = exec_res\n     ```\n\n   *   The `prep` method prepares the context and project name for the LLM by listing all files and their code in the files_data list.\n\n   *   The `exec` method prompts the LLM to analyze the code and identify the important abstractions, generating a YAML formatted string with the abstraction name, description, and corresponding file indices. After receiving this string, the LLM output is parsed and the file indices are added to the `shared` dictionary, specifically at `shared[\"abstractions\"]`.\n\n   *   The LLM might respond with something like this (the LLM will come up with a description):\n\n     ```yaml\n     - name: Addition\n       description: |\n         This abstraction performs addition.\n         It's like adding two numbers together.\n       file_indices:\n         - 0 # file1.py\n     - name: Multiplication\n       description: |\n         This abstraction performs multiplication.\n         It's like multiplying two numbers together.\n       file_indices:\n         - 1 # file2.py\n     ```\n\n3.  **Analyzing Relationships (AnalyzeRelationships Node):**\n\n   *   The `AnalyzeRelationships` node determines how the identified abstractions relate to each other, and creates a summary for the whole project.\n\n   *   Here's a simplified snippet of the `AnalyzeRelationships` node's `prep` and `exec` methods (from `nodes.py`):\n\n     ```python\n     class AnalyzeRelationships(Node):\n         def prep(self, shared):\n             abstractions = shared[\"abstractions\"]\n             files_data = shared[\"files\"]\n             context = \"Identified Abstractions:\\n\"\n             for i, abstr in enumerate(abstractions):\n                 file_indices_str = \", \".join(map(str, abstr['files']))\n                 info_line = f\"- Index {i}: {abstr['name']} (Relevant file indices: [{file_indices_str}])\\n  Description: {abstr['description']}\"\n                 context += info_line + \"\\n\"\n\n             all_relevant_indices = set()\n             for abstr in abstractions:\n                  all_relevant_indices.update(abstr['files'])\n\n             relevant_files_content_map = {}\n             for i in sorted(list(all_relevant_indices)):\n                  path, content = files_data[i]\n                  relevant_files_content_map[f\"{i} # {path}\"] = content\n\n             context += \"\\nRelevant File Snippets (Referenced by Index and Path):\\n\"\n             file_context_str = \"\\n\\n\".join(\n                  f\"--- File: {idx_path} ---\\n{content}\"\n                  for idx_path, content in relevant_files_content_map.items()\n             )\n             context += file_context_str\n             abstraction_info_for_prompt = []\n             for i, abstr in enumerate(abstractions):\n                  abstraction_info_for_prompt.append(f\"{i} # {abstr['name']}\")\n             return context, \"\\n\".join(abstraction_info_for_prompt), shared[\"project_name\"]\n\n         def exec(self, prep_res):\n             context, abstraction_listing, project_name = prep_res\n\n             prompt = f\"\"\"\n     Based on the following abstractions and relevant code snippets from the project `{project_name}`:\n\n     List of Abstraction Indices and Names:\n     {abstraction_listing}\n\n     Context (Abstractions, Descriptions, Code):\n     {context}\n\n     Please provide:\n     1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n     2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n         - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n         - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n         - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n         Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n         Simplify the relationship and exclude those non-important ones.\n\n     IMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\n     Format the output as YAML:\n\n     ```yaml\n     summary: |\n       A brief, simple explanation of the project.\n       Can span multiple lines with **bold** and *italic* for emphasis.\n     relationships:\n       - from_abstraction: 0 # AbstractionName1\n         to_abstraction: 1 # AbstractionName2\n         label: \"Manages\"\n     ```\n\n     Now, provide the YAML output:\n     \"\"\"\n             response = call_llm(prompt) #call llm\n\n             yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n             relationships_data = yaml.safe_load(yaml_str)\n\n             validated_relationships = []\n             num_abstractions = len(abstraction_listing.split('\\n'))\n             for rel in relationships_data[\"relationships\"]:\n                  from_idx = int(str(rel[\"from_abstraction\"]).split('#')[0].strip())\n                  to_idx = int(str(rel[\"to_abstraction\"]).split('#')[0].strip())\n                  validated_relationships.append({\n                       \"from\": from_idx,\n                       \"to\": to_idx,\n                       \"label\": rel[\"label\"]\n                  })\n\n             return {\n                  \"summary\": relationships_data[\"summary\"],\n                  \"details\": validated_relationships\n             }\n\n         def post(self, shared, prep_res, exec_res):\n             shared[\"relationships\"] = exec_res\n     ```\n\n   *   The `prep` method formats the context and project name for the LLM by listing the abstraction descriptions and file content.\n\n   *   The `exec` method prompts the LLM to analyze the context and provide a summary and relationships. It parses the LLM output and adds a project summary and details (including what abstraction links to what) to the `shared` dictionary at `shared[\"relationships\"]`.\n\n   *   The LLM might respond with something like this (LLM generated):\n\n     ```yaml\n     summary: |\n       This project provides basic arithmetic operations: **addition** and **multiplication**.\n       It allows users to perform simple calculations.\n     relationships:\n       - from_abstraction: 0 # Addition\n         to_abstraction: 1 # Multiplication\n         label: \"None\"\n     ```\n\n4.  **Ordering Chapters (OrderChapters Node)**\n\n   *   The `OrderChapters` node asks the LLM what order to present the abstractions in.\n   *   Here's a simplified snippet of the `OrderChapters` node's `prep` and `exec` methods (from `nodes.py`):\n\n     ```python\n     class OrderChapters(Node):\n         def prep(self, shared):\n             abstractions = shared[\"abstractions\"]\n             relationships = shared[\"relationships\"]\n\n             abstraction_info_for_prompt = []\n             for i, a in enumerate(abstractions):\n                 abstraction_info_for_prompt.append(f\"- {i} # {a['name']}\")\n             abstraction_listing = \"\\n\".join(abstraction_info_for_prompt)\n\n             context = f\"Project Summary:\\n{relationships['summary']}\\n\\n\"\n             context += \"Relationships (Indices refer to abstractions above):\\n\"\n             for rel in relationships['details']:\n                  from_name = abstractions[rel['from']]['name']\n                  to_name = abstractions[rel['to']]['name']\n                  context += f\"- From {rel['from']} ({from_name}) to {rel['to']} ({to_name}): {rel['label']}\\n\"\n\n             return abstraction_listing, context, len(abstractions), shared[\"project_name\"]\n\n         def exec(self, prep_res):\n             abstraction_listing, context, num_abstractions, project_name = prep_res\n             prompt = f\"\"\"\n     Given the following project abstractions and their relationships for the project ```` {project_name} ````:\n\n     Abstractions (Index # Name):\n     {abstraction_listing}\n\n     Context about relationships and project summary:\n     {context}\n\n     If you are going to make a tutorial for ```` {project_name} ````, what is the best order to explain these abstractions, from first to last?\n     Ideally, first explain those that are the most important or foundational, perhaps user-facing concepts or entry points. Then move to more detailed, lower-level implementation details or supporting concepts.\n\n     Output the ordered list of abstraction indices, including the name in a comment for clarity. Use the format `idx # AbstractionName`.\n\n     ```yaml\n     - 0 # Addition\n     - 1 # Multiplication\n     ```\n     \"\"\"\n             response = call_llm(prompt) #call llm\n\n             yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n             ordered_indices_raw = yaml.safe_load(yaml_str)\n\n             ordered_indices = []\n             for entry in ordered_indices_raw:\n                  idx = int(entry.split('#')[0].strip())\n                  ordered_indices.append(idx)\n\n             return ordered_indices\n\n         def post(self, shared, prep_res, exec_res):\n             shared[\"chapter_order\"] = exec_res\n     ```\n\n   *   The `prep` method prepares the context and the project name for the LLM by listing all abstractions, relationship information, and project summary.\n\n   *   The `exec` method calls the LLM to ask what the best order to explain the abstractions are. Then the method adds the ordered list to the `shared` dictionary at `shared[\"chapter_order\"]`.\n\n   *   The LLM might respond with something like this (LLM generated):\n\n     ```yaml\n     - 0 # Addition\n     - 1 # Multiplication\n     ```\n\n5.  **Writing Chapters (WriteChapters Node)**\n\n   *   The `WriteChapters` node creates the markdown chapters for the final tutorial.\n   *   Here's a simplified snippet of the `WriteChapters` node's `prep` and `exec` methods (from `nodes.py`):\n\n     ```python\n     class WriteChapters(BatchNode):\n         def prep(self, shared):\n             chapter_order = shared[\"chapter_order\"]\n             abstractions = shared[\"abstractions\"]\n             files_data = shared[\"files\"]\n\n             all_chapters = []\n             chapter_filenames = {}\n             for i, abstraction_index in enumerate(chapter_order):\n                  chapter_num = i + 1\n                  chapter_name = abstractions[abstraction_index][\"name\"]\n                  safe_name = \"\".join(c if c.isalnum() else '_' for c in chapter_name).lower()\n                  filename = f\"{i+1:02d}_{safe_name}.md\"\n                  all_chapters.append(f\"{chapter_num}. [{chapter_name}]({filename})\")\n                  chapter_filenames[abstraction_index] = {\"num\": chapter_num, \"name\": chapter_name, \"filename\": filename}\n\n             full_chapter_listing = \"\\n\".join(all_chapters)\n             items_to_process = []\n             for i, abstraction_index in enumerate(chapter_order):\n                  abstraction_details = abstractions[abstraction_index]\n                  related_file_indices = abstraction_details.get(\"files\", [])\n\n                  related_files_content_map = {}\n                  for i in related_file_indices:\n                       path, content = files_data[i]\n                       related_files_content_map[f\"{i} # {path}\"] = content\n\n                  items_to_process.append({\n                       \"chapter_num\": i + 1,\n                       \"abstraction_index\": abstraction_index,\n                       \"abstraction_details\": abstraction_details,\n                       \"related_files_content_map\": related_files_content_map,\n                       \"project_name\": shared[\"project_name\"],\n                       \"full_chapter_listing\": full_chapter_listing,\n                       \"chapter_filenames\": chapter_filenames,\n                  })\n             print(f\"Preparing to write {len(items_to_process)} chapters...\")\n             return items_to_process\n\n         def exec(self, item):\n             abstraction_name = item[\"abstraction_details\"][\"name\"]\n             chapter_num = item[\"chapter_num\"]\n             project_name = item.get(\"project_name\")\n             print(f\"Writing chapter {chapter_num} for: {abstraction_name} using LLM...\")\n\n             file_context_str = \"\\n\\n\".join(\n                  f\"--- File: {idx_path.split('# ')[1] if '# ' in idx_path else idx_path} ---\\n{content}\"\n                  for idx_path, content in item[\"related_files_content_map\"].items()\n             )\n\n             prompt = f\"\"\"\n     Write a very beginner-friendly tutorial chapter (in Markdown format) for the project `{project_name}` about the concept: \"{abstraction_name}\". This is Chapter {chapter_num}.\n\n     Concept Details:\n     - Description:\n     {item[\"abstraction_details\"][\"description\"]}\n\n     Complete Tutorial Structure:\n     {item[\"full_chapter_listing\"]}\n\n     Relevant Code Snippets:\n     {file_context_str if file_context_str else \"No specific code snippets provided for this abstraction.\"}\n\n     Instructions for the chapter:\n     - Start with a clear heading (e.g., `# Chapter {chapter_num}: {abstraction_name}`).\n     - Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example.\n     - If the abstraction is complex, break it down into key concepts.\n     - Explain how to use this abstraction. Give example inputs and outputs.\n     - Describe the internal implementation to help understand what's under the hood.\n     - Heavily use analogies and examples throughout to help beginners understand.\n     - End the chapter with a brief conclusion.\n     - Ensure the tone is welcoming and easy for a newcomer to understand.\n\n     Now, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n     \"\"\"\n             chapter_content = call_llm(prompt) #call llm\n\n             actual_heading = f\"# Chapter {chapter_num}: {abstraction_name}\"\n             if not chapter_content.strip().startswith(f\"# Chapter {chapter_num}\"):\n                  lines = chapter_content.strip().split('\\n')\n                  if lines and lines[0].strip().startswith(\"#\"):\n                       lines[0] = actual_heading\n                       chapter_content = \"\\n\".join(lines)\n                  else:\n                       chapter_content = f\"{actual_heading}\\n\\n{chapter_content}\"\n\n             return chapter_content\n\n         def post(self, shared, prep_res, exec_res_list):\n             shared[\"chapters\"] = exec_res_list\n             print(f\"Finished writing {len(exec_res_list)} chapters.\")\n     ```\n\n   *   The `prep` method formats the project and file content and abstraction details for the LLM by listing all file content that corresponds to an abstraction. It generates filenames for each chapter, and adds each chapter to the `items_to_process` list.\n\n   *   The `exec` method prompts the LLM to generate a chapter based on the abstraction details and corresponding files. It then adds the chapters to the `shared` dictionary at `shared[\"chapters\"]`.\n\n   *   An example of the output generated by the LLM would be:\n\n     ```markdown\n     # Chapter 1: Addition\n\n     Addition is the process of combining two or more numbers to find their total, or sum. It's like combining two piles of toys to see how many you have in all!\n\n     ## How to Use Addition\n\n     You can use the `add` function in `file1.py` to perform addition:\n\n     ```python\n     def add(a, b):\n         return a + b\n     ```\n\n     This code simply takes two numbers, `a` and `b`, and returns their sum. For example:\n\n     ```python\n     result = add(5, 3) # result will be 8\n     print(result)\n     ```\n\n     In this example, `add(5, 3)` returns `8`, because 5 + 3 = 8. It's that simple!\n\n     ## Internal Implementation\n\n     The function `add(a, b)` simply uses the `+` operator in Python to perform the addition. This is a built-in feature of Python, so it's very efficient.\n\n     ## Conclusion\n\n     In this chapter, we learned about addition and how to use the `add` function to perform it in Python. In the next chapter, we will talk about [Multiplication](02_multiplication.md).\n     ```\n\n**Under the Hood: Implementation Details**\n\n*   **Nodes.py:** The core logic for this component resides in the `nodes.py` file, which contains the `IdentifyAbstractions`, `AnalyzeRelationships`, `OrderChapters`, and `WriteChapters` classes. Each of these classes inherits from the `Node` or `BatchNode` class from the `pocketflow` library.\n*   **LLM Integration:** The `call_llm` function (in `utils/call_llm.py`) handles communication with the LLM. It takes a prompt as input and returns the LLM's response. By default, we use the Google Gemini 2.5 Pro model.\n*   **Prompt Engineering:**  A key part of this component is crafting effective prompts for the LLM.  These prompts guide the LLM to analyze the code, identify key concepts, and generate beginner-friendly explanations. Examples of prompts are shown in the `exec` method of the nodes shown above.\n\n**Benefits and Considerations**\n\n*   **Benefits:**\n    *   **Automation:**  Automates the process of understanding and explaining complex codebases.\n    *   **Beginner-Friendly:**  Generates content tailored for beginners, making it easier to learn new codebases.\n    *   **Scalability:**  Can handle large codebases that would be difficult for a human to analyze manually.\n*   **Considerations:**\n    *   **LLM Limitations:** The quality of the generated content depends on the LLM's capabilities and the effectiveness of the prompts.\n    *   **Accuracy:** The AI might not always perfectly understand the code, so it's important to review the generated content for accuracy.\n\n**Conclusion**\n\nIn this chapter, we explored how AI can be used to analyze codebases and generate beginner-friendly tutorial content. This `AI-Powered Code Analysis and Generation` component is a key part of our tutorial generation system, enabling us to automate the process of understanding and explaining complex code.  In the next chapter, [Codebase Crawling](03_codebase_crawling.md), we'll see how we actually get the source code that this component analyzes.\n\n---\n# Chapter 3: Codebase Crawling\n\nIn the previous chapter, [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md), we learned how AI can analyze code and generate beginner-friendly explanations. But where does that code come from? That's where codebase crawling comes in!\n\nImagine you want to bake a cake. The AI-Powered analysis is like having a master chef who can explain every ingredient and baking step in detail. But first, you need to *get* the ingredients! Codebase crawling is like going to the grocery store and carefully selecting all the necessary ingredients (code files) for our AI chef to work with.\n\n**Why Codebase Crawling?**\n\nBefore we can analyze a project's code and create a tutorial, we need to first gather the relevant files. This can involve:\n\n*   **Fetching from a remote repository:**  Downloading code from platforms like GitHub.\n*   **Exploring a local directory:**  Reading code from your computer.\n*   **Filtering out irrelevant files:**  Ignoring tests, documentation, or other non-essential code.\n\n**The Goal: Getting the Right Code, Efficiently**\n\nOur `Codebase Crawling` component aims to provide a flexible and efficient way to fetch the necessary code files for analysis. It's like a librarian searching for specific books based on certain criteria (include/exclude patterns) and size restrictions, ensuring only relevant materials are collected.\n\n**Key Concepts**\n\nLet's break down the core ideas behind our codebase crawling:\n\n1.  **Fetching Source Code (FetchRepo Node):**\n    *   This node takes the Github repo URL or a local directory path and downloads source code using the `crawl_github_files` or `crawl_local_files` functions.\n\n2.  **Repository URL:** The address of the code repository (e.g., a GitHub link).\n\n3.  **Local Directory:** The path to a directory on your computer containing the code.\n\n4.  **Include/Exclude Patterns:**  Rules for selecting specific files based on their names or paths (e.g., including only `.py` files, excluding `test` directories). This is done using `fnmatch`, which supports wildcard matching similar to bash.\n\n5.  **File Size Limits:**  Maximum size for individual files to prevent processing large, irrelevant files.\n\n**How it Works: A Step-by-Step View**\n\nLet's walk through a simple scenario: You want to create a tutorial for a small Python library on GitHub.\n\n1.  **The `FetchRepo` Node:** This node is responsible for retrieving the code files. It uses either `crawl_github_files` (for GitHub repos) or `crawl_local_files` (for local directories).\n\n    *   Here's a simplified snippet of the `FetchRepo` node's `prep` and `exec` methods (from `nodes.py`):\n\n     ```python\n     class FetchRepo(Node):\n         def prep(self, shared):\n             repo_url = shared.get(\"repo_url\")\n             local_dir = shared.get(\"local_dir\")\n             project_name = shared.get(\"project_name\")\n             \n             if not project_name:\n                 # Basic name derivation from URL or directory\n                 if repo_url:\n                     project_name = repo_url.split('/')[-1].replace('.git', '')\n                 else:\n                     project_name = os.path.basename(os.path.abspath(local_dir))\n                 shared[\"project_name\"] = project_name\n\n             # Get file patterns directly from shared\n             include_patterns = shared[\"include_patterns\"]\n             exclude_patterns = shared[\"exclude_patterns\"]\n             max_file_size = shared[\"max_file_size\"]\n\n             return {\n                 \"repo_url\": repo_url,\n                 \"local_dir\": local_dir,\n                 \"token\": shared.get(\"github_token\"),\n                 \"include_patterns\": include_patterns,\n                 \"exclude_patterns\": exclude_patterns,\n                 \"max_file_size\": max_file_size,\n                 \"use_relative_paths\": True\n             }\n\n         def exec(self, prep_res):\n             if prep_res[\"repo_url\"]:\n                 print(f\"Crawling repository: {prep_res['repo_url']}...\")\n                 result = crawl_github_files(\n                     repo_url=prep_res[\"repo_url\"],\n                     token=prep_res[\"token\"],\n                     include_patterns=prep_res[\"include_patterns\"],\n                     exclude_patterns=prep_res[\"exclude_patterns\"],\n                     max_file_size=prep_res[\"max_file_size\"],\n                     use_relative_paths=prep_res[\"use_relative_paths\"]\n                 )\n             else:\n                 print(f\"Crawling directory: {prep_res['local_dir']}...\")\n                 result = crawl_local_files(\n                     directory=prep_res[\"local_dir\"],\n                     include_patterns=prep_res[\"include_patterns\"],\n                     exclude_patterns=prep_res[\"exclude_patterns\"],\n                     max_file_size=prep_res[\"max_file_size\"],\n                     use_relative_paths=prep_res[\"use_relative_paths\"]\n                 )\n                 \n             # Convert dict to list of tuples: [(path, content), ...]\n             files_list = list(result.get(\"files\", {}).items())\n             print(f\"Fetched {len(files_list)} files.\")\n             return files_list\n\n         def post(self, shared, prep_res, exec_res):\n             shared[\"files\"] = exec_res # List of (path, content) tuples\n     ```\n\n    *   The `prep` method retrieves all config options (such as Github repo URL, local directory path, file inclusion/exclusion patterns, max file size), and sets them to the `prep_res` to be used by the `exec` method.\n    *   The `exec` method determines if source code is fetched from Github or local directory based on the presence of a Github repo URL. It then downloads the source code, and stores the path and code as a tuple into a list.\n    *   The `post` method adds the source code to the `shared` dictionary, specifically at `shared[\"files\"]`.\n\n2.  **`crawl_github_files` Function:** This function is defined in `utils/crawl_github_files.py`.\n\n    *   Here's a simplified view of what happens inside the `crawl_github_files` function:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant CrawlGithubFiles as crawl_github_files\n    participant GithubAPI as Github API\n\n    User->>CrawlGithubFiles: Call crawl_github_files(repo_url, token, include_patterns, exclude_patterns, max_file_size, use_relative_paths)\n    CrawlGithubFiles->>GithubAPI: GET contents from Github API for the repo URL\n    GithubAPI-->>CrawlGithubFiles: Return contents from Github API\n    CrawlGithubFiles->>CrawlGithubFiles: Apply include/exclude patterns\n    CrawlGithubFiles->>CrawlGithubFiles: Apply file size limits\n    CrawlGithubFiles-->>User: Return list of (path, code)\n```\n\n    *   The `crawl_github_files` function downloads the source code for the repo URL by calling the Github API.\n    *   The `crawl_github_files` function then filters all source code based on include/exclude patterns, and file size limits.\n    *   The function returns a list of (path, code).\n\n3.  **`crawl_local_files` Function:** This function is defined in `utils/crawl_local_files.py`.\n\n    *   Here's a simplified view of what happens inside the `crawl_local_files` function:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant CrawlLocalFiles as crawl_local_files\n    participant OS as OS\n\n    User->>CrawlLocalFiles: Call crawl_local_files(local_dir, include_patterns, exclude_patterns, max_file_size, use_relative_paths)\n    CrawlLocalFiles->>OS: Walk directory for the local directory path\n    OS-->>CrawlLocalFiles: Return files\n    CrawlLocalFiles->>CrawlLocalFiles: Apply include/exclude patterns\n    CrawlLocalFiles->>CrawlLocalFiles: Apply file size limits\n    CrawlLocalFiles-->>User: Return list of (path, code)\n```\n\n    *   The `crawl_local_files` function walks the directory structure to find all files in the directory path.\n    *   The `crawl_local_files` function then filters all source code based on include/exclude patterns, and file size limits.\n    *   The function returns a list of (path, code).\n\n4.  **Example:**\n\n    ```python\n    shared = {\n        \"repo_url\": \"https://github.com/user/my-cool-project\",\n        \"include_patterns\": {\"*.py\"},\n        \"exclude_patterns\": {\"tests/*\"},\n        \"max_file_size\": 100000  # 100KB\n    }\n    ```\n\n    *   In this example, the `FetchRepo` node would:\n\n        *   Download the code from the specified GitHub repository.\n        *   Only include files ending with `.py`.\n        *   Exclude any files in directories named `tests`.\n        *   Skip any files larger than 100KB.\n\n5.  **Output:** The `FetchRepo` node outputs a list of tuples, where each tuple contains the file path and its contents. This list is then stored in the `shared` dictionary for use by subsequent nodes, such as [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md).\n\n**Code Example**\n\nLet's look at a snippet from `utils/crawl_github_files.py` to understand how file filtering works:\n\n```python\nimport fnmatch\n\ndef should_include_file(file_path: str, file_name: str) -> bool:\n    \"\"\"Determine if a file should be included based on patterns\"\"\"\n    # If no include patterns are specified, include all files\n    if not include_patterns:\n        include_file = True\n    else:\n        # Check if file matches any include pattern\n        include_file = any(fnmatch.fnmatch(file_name, pattern) for pattern in include_patterns)\n\n    # If exclude patterns are specified, check if file should be excluded\n    if exclude_patterns and include_file:\n        # Exclude if file matches any exclude pattern\n        exclude_file = any(fnmatch.fnmatch(file_path, pattern) for pattern in exclude_patterns)\n        return not exclude_file\n\n    return include_file\n```\n\nThis code snippet demonstrates how `fnmatch` is used to match file paths against include and exclude patterns.  It's a simple but powerful way to control which files are included in the analysis.\n\n**Benefits and Considerations**\n\n*   **Benefits:**\n    *   **Flexibility:**  Supports both remote repositories and local directories.\n    *   **Efficiency:**  Filters out irrelevant files to reduce processing time.\n    *   **Customization:**  Allows specifying include/exclude patterns and file size limits.\n*   **Considerations:**\n    *   **Rate Limits:**  When crawling GitHub repositories, be aware of API rate limits. Using a GitHub token is highly recommended.\n    *   **Complexity:**  Complex include/exclude patterns can be difficult to manage.\n\n**Conclusion**\n\nIn this chapter, we learned about codebase crawling, a crucial step in our tutorial generation process. By fetching and filtering code files, we ensure that our AI-powered analysis focuses on the most relevant parts of the codebase.  In the next chapter, [Knowledge Base Construction](04_knowledge_base_construction.md), we'll see how we use the crawled code to build a knowledge base for generating our tutorials.\n\n\nRelevant Code Snippets:\n--- File: nodes.py ---\nimport os\nimport yaml\nfrom pocketflow import Node, BatchNode\nfrom utils.crawl_github_files import crawl_github_files\nfrom utils.call_llm import call_llm \nfrom utils.crawl_local_files import crawl_local_files\n\n# Helper to get content for specific file indices\ndef get_content_for_indices(files_data, indices):\n    content_map = {}\n    for i in indices:\n        if 0 <= i < len(files_data):\n            path, content = files_data[i]\n            content_map[f\"{i} # {path}\"] = content # Use index + path as key for context\n    return content_map\n\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"local_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        if not project_name:\n            # Basic name derivation from URL or directory\n            if repo_url:\n                project_name = repo_url.split('/')[-1].replace('.git', '')\n            else:\n                project_name = os.path.basename(os.path.abspath(local_dir))\n            shared[\"project_name\"] = project_name\n\n        # Get file patterns directly from shared\n        include_patterns = shared[\"include_patterns\"]\n        exclude_patterns = shared[\"exclude_patterns\"]\n        max_file_size = shared[\"max_file_size\"]\n\n        return {\n            \"repo_url\": repo_url,\n            \"local_dir\": local_dir,\n            \"token\": shared.get(\"github_token\"),\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns,\n            \"max_file_size\": max_file_size,\n            \"use_relative_paths\": True\n        }\n\n    def exec(self, prep_res):\n        if prep_res[\"repo_url\"]:\n            print(f\"Crawling repository: {prep_res['repo_url']}...\")\n            result = crawl_github_files(\n                repo_url=prep_res[\"repo_url\"],\n                token=prep_res[\"token\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        else:\n            print(f\"Crawling directory: {prep_res['local_dir']}...\")\n            result = crawl_local_files(\n                directory=prep_res[\"local_dir\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n            \n        # Convert dict to list of tuples: [(path, content), ...]\n        files_list = list(result.get(\"files\", {}).items())\n        print(f\"Fetched {len(files_list)} files.\")\n        return files_list\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"files\"] = exec_res # List of (path, content) tuples\n\nclass IdentifyAbstractions(Node):\n    def prep(self, shared):\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        \n        # Helper to create context from files, respecting limits (basic example)\n        def create_llm_context(files_data):\n            context = \"\"\n            file_info = [] # Store tuples of (index, path)\n            for i, (path, content) in enumerate(files_data):\n                entry = f\"--- File Index {i}: {path} ---\\n{content}\\n\\n\"\n                context += entry\n                file_info.append((i, path))\n\n            return context, file_info # file_info is list of (index, path)\n\n        context, file_info = create_llm_context(files_data)\n        # Format file info for the prompt (comment is just a hint for LLM)\n        file_listing_for_prompt = \"\\n\".join([f\"- {idx} # {path}\" for idx, path in file_info])\n        return context, file_listing_for_prompt, len(files_data), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, file_listing_for_prompt, file_count, project_name = prep_res  # Unpack project name\n        print(\"Identifying abstractions using LLM...\")\n        prompt = f\"\"\"\nFor the project `{project_name}`:\n\nCodebase Context:\n{context}\n\nAnalyze the codebase context.\nIdentify the top 5-10 core most important abstractions to help those new to the codebase.\n\nFor each abstraction, provide:\n1. A concise `name`.\n2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\nList of file indices and paths present in the context:\n{file_listing_for_prompt}\n\nFormat the output as a YAML list of dictionaries:\n\n```yaml\n- name: Query Processing\n  description: | \n    Explains what the abstraction does.\n    It's like a central dispatcher routing requests.\n  file_indices:\n    - 0 # path/to/file1.py\n    - 3 # path/to/related.py\n- name: Query Optimization\n  description: |\n    Another core concept, similar to a blueprint for objects.\n  file_indices:\n    - 5 # path/to/another.js\n# ... up to 10 abstractions\n```\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        abstractions = yaml.safe_load(yaml_str)\n\n        if not isinstance(abstractions, list):\n            raise ValueError(\"LLM Output is not a list\")\n\n        validated_abstractions = []\n        for item in abstractions:\n            if not isinstance(item, dict) or not all(k in item for k in [\"name\", \"description\", \"file_indices\"]):\n                raise ValueError(f\"Missing keys in abstraction item: {item}\")\n            if not isinstance(item[\"description\"], str):\n                 raise ValueError(f\"description is not a string in item: {item}\")\n            if not isinstance(item[\"file_indices\"], list):\n                 raise ValueError(f\"file_indices is not a list in item: {item}\")\n\n            # Validate indices\n            validated_indices = []\n            for idx_entry in item[\"file_indices\"]:\n                 try:\n                     if isinstance(idx_entry, int):\n                         idx = idx_entry\n                     elif isinstance(idx_entry, str) and '#' in idx_entry:\n                          idx = int(idx_entry.split('#')[0].strip())\n                     else:\n                          idx = int(str(idx_entry).strip())\n\n                     if not (0 <= idx < file_count):\n                         raise ValueError(f\"Invalid file index {idx} found in item {item['name']}. Max index is {file_count - 1}.\")\n                     validated_indices.append(idx)\n                 except (ValueError, TypeError):\n                      raise ValueError(f\"Could not parse index from entry: {idx_entry} in item {item['name']}\")\n\n            item[\"files\"] = sorted(list(set(validated_indices)))\n            # Store only the required fields\n            validated_abstractions.append({\n                \"name\": item[\"name\"],\n                \"description\": item[\"description\"],\n                \"files\": item[\"files\"]\n            })\n\n        print(f\"Identified {len(validated_abstractions)} abstractions.\")\n        return validated_abstractions\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"abstractions\"] = exec_res # List of {\"name\": str, \"description\": str, \"files\": [int]}\n\nclass AnalyzeRelationships(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"] # Now contains 'files' list of indices\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n\n        # Create context with abstraction names, indices, descriptions, and relevant file snippets\n        context = \"Identified Abstractions:\\n\"\n        all_relevant_indices = set()\n        abstraction_info_for_prompt = []\n        for i, abstr in enumerate(abstractions):\n            # Use 'files' which contains indices directly\n            file_indices_str = \", \".join(map(str, abstr['files']))\n            info_line = f\"- Index {i}: {abstr['name']} (Relevant file indices: [{file_indices_str}])\\n  Description: {abstr['description']}\"\n            context += info_line + \"\\n\"\n            abstraction_info_for_prompt.append(f\"{i} # {abstr['name']}\")\n            all_relevant_indices.update(abstr['files'])\n\n        context += \"\\nRelevant File Snippets (Referenced by Index and Path):\\n\"\n        # Get content for relevant files using helper\n        relevant_files_content_map = get_content_for_indices(\n            files_data,\n            sorted(list(all_relevant_indices))\n        )\n        # Format file content for context\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path} ---\\n{content}\"\n            for idx_path, content in relevant_files_content_map.items()\n        )\n        context += file_context_str\n\n        return context, \"\\n\".join(abstraction_info_for_prompt), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, abstraction_listing, project_name = prep_res  # Unpack project name\n        print(\"Analyzing relationships using LLM...\")\n        prompt = f\"\"\"\nBased on the following abstractions and relevant code snippets from the project `{project_name}`:\n\nList of Abstraction Indices and Names:\n{abstraction_listing}\n\nContext (Abstractions, Descriptions, Code):\n{context}\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        relationships_data = yaml.safe_load(yaml_str)\n\n        if not isinstance(relationships_data, dict) or not all(k in relationships_data for k in [\"summary\", \"relationships\"]):\n            raise ValueError(\"LLM output is not a dict or missing keys ('summary', 'relationships')\")\n        if not isinstance(relationships_data[\"summary\"], str):\n             raise ValueError(\"summary is not a string\")\n        if not isinstance(relationships_data[\"relationships\"], list):\n             raise ValueError(\"relationships is not a list\")\n\n        # Validate relationships structure\n        validated_relationships = []\n        num_abstractions = len(abstraction_listing.split('\\n'))\n        for rel in relationships_data[\"relationships\"]:\n             # Check for 'label' key\n             if not isinstance(rel, dict) or not all(k in rel for k in [\"from_abstraction\", \"to_abstraction\", \"label\"]):\n                  raise ValueError(f\"Missing keys (expected from_abstraction, to_abstraction, label) in relationship item: {rel}\")\n             # Validate 'label' is a string\n             if not isinstance(rel[\"label\"], str):\n                  raise ValueError(f\"Relationship label is not a string: {rel}\")\n\n             # Validate indices\n             try:\n                 from_idx = int(str(rel[\"from_abstraction\"]).split('#')[0].strip())\n                 to_idx = int(str(rel[\"to_abstraction\"]).split('#')[0].strip())\n                 if not (0 <= from_idx < num_abstractions and 0 <= to_idx < num_abstractions):\n                      raise ValueError(f\"Invalid index in relationship: from={from_idx}, to={to_idx}. Max index is {num_abstractions-1}.\")\n                 validated_relationships.append({\n                     \"from\": from_idx,\n                     \"to\": to_idx,\n                     \"label\": rel[\"label\"] \n                 })\n             except (ValueError, TypeError):\n                  raise ValueError(f\"Could not parse indices from relationship: {rel}\")\n\n        print(\"Generated project summary and relationship details.\")\n        return {\n            \"summary\": relationships_data[\"summary\"],\n            \"details\": validated_relationships # Store validated, index-based relationships\n        }\n\n\n    def post(self, shared, prep_res, exec_res):\n        # Structure is now {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        shared[\"relationships\"] = exec_res\n\nclass OrderChapters(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"]\n        relationships = shared[\"relationships\"]\n        project_name = shared[\"project_name\"]  # Get project name\n\n        # Prepare context for the LLM\n        abstraction_info_for_prompt = []\n        for i, a in enumerate(abstractions):\n            abstraction_info_for_prompt.append(f\"- {i} # {a['name']}\")\n        abstraction_listing = \"\\n\".join(abstraction_info_for_prompt)\n\n        context = f\"Project Summary:\\n{relationships['summary']}\\n\\n\"\n        context += \"Relationships (Indices refer to abstractions above):\\n\"\n        for rel in relationships['details']:\n             from_name = abstractions[rel['from']]['name']\n             to_name = abstractions[rel['to']]['name']\n             # Use 'label' instead of 'desc'\n             context += f\"- From {rel['from']} ({from_name}) to {rel['to']} ({to_name}): {rel['label']}\\n\"\n\n        return abstraction_listing, context, len(abstractions), project_name\n\n    def exec(self, prep_res):\n        abstraction_listing, context, num_abstractions, project_name = prep_res\n        print(\"Determining chapter order using LLM...\")\n        prompt = f\"\"\"\nGiven the following project abstractions and their relationships for the project ```` {project_name} ````:\n\nAbstractions (Index # Name):\n{abstraction_listing}\n\nContext about relationships and project summary:\n{context}\n\nIf you are going to make a tutorial for ```` {project_name} ````, what is the best order to explain these abstractions, from first to last?\nIdeally, first explain those that are the most important or foundational, perhaps user-facing concepts or entry points. Then move to more detailed, lower-level implementation details or supporting concepts.\n\nOutput the ordered list of abstraction indices, including the name in a comment for clarity. Use the format `idx # AbstractionName`.\n\n```yaml\n- 2 # FoundationalConcept\n- 0 # CoreClassA\n- 1 # CoreClassB (uses CoreClassA)\n- ...\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        # Rely on Node's built-in retry/fallback\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        ordered_indices_raw = yaml.safe_load(yaml_str)\n\n        if not isinstance(ordered_indices_raw, list):\n            raise ValueError(\"LLM output is not a list\")\n\n        ordered_indices = []\n        seen_indices = set()\n        for entry in ordered_indices_raw:\n            try:\n                 if isinstance(entry, int):\n                     idx = entry\n                 elif isinstance(entry, str) and '#' in entry:\n                      idx = int(entry.split('#')[0].strip())\n                 else:\n                      idx = int(str(entry).strip())\n\n                 if not (0 <= idx < num_abstractions):\n                      raise ValueError(f\"Invalid index {idx} in ordered list. Max index is {num_abstractions-1}.\")\n                 if idx in seen_indices:\n                     raise ValueError(f\"Duplicate index {idx} found in ordered list.\")\n                 ordered_indices.append(idx)\n                 seen_indices.add(idx)\n\n            except (ValueError, TypeError):\n                 raise ValueError(f\"Could not parse index from ordered list entry: {entry}\")\n\n        # Check if all abstractions are included\n        if len(ordered_indices) != num_abstractions:\n             raise ValueError(f\"Ordered list length ({len(ordered_indices)}) does not match number of abstractions ({num_abstractions}). Missing indices: {set(range(num_abstractions)) - seen_indices}\")\n\n        print(f\"Determined chapter order (indices): {ordered_indices}\")\n        return ordered_indices # Return the list of indices\n\n    def post(self, shared, prep_res, exec_res):\n        # exec_res is already the list of ordered indices\n        shared[\"chapter_order\"] = exec_res # List of indices\n\nclass WriteChapters(BatchNode):\n    def prep(self, shared):\n        chapter_order = shared[\"chapter_order\"] # List of indices\n        abstractions = shared[\"abstractions\"]   # List of dicts, now using 'files' with indices\n        files_data = shared[\"files\"]\n        # Get already written chapters to provide context\n        # We store them temporarily during the batch run, not in shared memory yet\n        # The 'previous_chapters_summary' will be built progressively in the exec context\n        self.chapters_written_so_far = [] # Use instance variable for temporary storage across exec calls\n\n        # Create a complete list of all chapters\n        all_chapters = []\n        chapter_filenames = {} # Store chapter filename mapping for linking\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                chapter_num = i + 1\n                chapter_name = abstractions[abstraction_index][\"name\"]\n                # Create safe filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in chapter_name).lower()\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                # Format with link\n                all_chapters.append(f\"{chapter_num}. [{chapter_name}]({filename})\")\n                # Store mapping of chapter index to filename for linking\n                chapter_filenames[abstraction_index] = {\"num\": chapter_num, \"name\": chapter_name, \"filename\": filename}\n        \n        # Create a formatted string with all chapters\n        full_chapter_listing = \"\\n\".join(all_chapters)\n\n        items_to_process = []\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                abstraction_details = abstractions[abstraction_index]\n                # Use 'files' (list of indices) directly\n                related_file_indices = abstraction_details.get(\"files\", [])\n                # Get content using helper, passing indices\n                related_files_content_map = get_content_for_indices(files_data, related_file_indices)\n                \n                # Get previous chapter info for transitions\n                prev_chapter = None\n                if i > 0:\n                    prev_idx = chapter_order[i-1]\n                    prev_chapter = chapter_filenames[prev_idx]\n                \n                # Get next chapter info for transitions\n                next_chapter = None\n                if i < len(chapter_order) - 1:\n                    next_idx = chapter_order[i+1]\n                    next_chapter = chapter_filenames[next_idx]\n\n                items_to_process.append({\n                    \"chapter_num\": i + 1,\n                    \"abstraction_index\": abstraction_index,\n                    \"abstraction_details\": abstraction_details,\n                    \"related_files_content_map\": related_files_content_map,\n                    \"project_name\": shared[\"project_name\"],  # Add project name\n                    \"full_chapter_listing\": full_chapter_listing,  # Add the full chapter listing\n                    \"chapter_filenames\": chapter_filenames,  # Add chapter filenames mapping\n                    \"prev_chapter\": prev_chapter,  # Add previous chapter info\n                    \"next_chapter\": next_chapter,  # Add next chapter info\n                    # previous_chapters_summary will be added dynamically in exec\n                })\n            else:\n                print(f\"Warning: Invalid abstraction index {abstraction_index} in chapter_order. Skipping.\")\n\n        print(f\"Preparing to write {len(items_to_process)} chapters...\")\n        return items_to_process # Iterable for BatchNode\n\n    def exec(self, item):\n        # This runs for each item prepared above\n        abstraction_name = item[\"abstraction_details\"][\"name\"]\n        chapter_num = item[\"chapter_num\"]\n        project_name = item.get(\"project_name\")  # Get from item\n        print(f\"Writing chapter {chapter_num} for: {abstraction_name} using LLM...\")\n\n        # Prepare file context string from the map\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path.split('# ')[1] if '# ' in idx_path else idx_path} ---\\n{content}\"\n            for idx_path, content in item[\"related_files_content_map\"].items()\n        )\n\n        # Get summary of chapters written *before* this one\n        # Use the temporary instance variable\n        previous_chapters_summary = \"\\n---\\n\".join(self.chapters_written_so_far)\n\n\n        prompt = f\"\"\"\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `{project_name}` about the concept: \"{abstraction_name}\". This is Chapter {chapter_num}.\n\nConcept Details:\n- Description:\n{item[\"abstraction_details\"][\"description\"]}\n\nComplete Tutorial Structure:\n{item[\"full_chapter_listing\"]}\n\nContext from previous chapters (summary):\n{previous_chapters_summary if previous_chapters_summary else \"This is the first chapter.\"}\n\nRelevant Code Snippets:\n{file_context_str if file_context_str else \"No specific code snippets provided for this abstraction.\"}\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter {chapter_num}: {abstraction_name}`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n\"\"\"\n        chapter_content = call_llm(prompt)\n        # Basic validation/cleanup\n        actual_heading = f\"# Chapter {chapter_num}: {abstraction_name}\"\n        if not chapter_content.strip().startswith(f\"# Chapter {chapter_num}\"):\n             # Add heading if missing or incorrect, trying to preserve content\n             lines = chapter_content.strip().split('\\n')\n             if lines and lines[0].strip().startswith(\"#\"): # If there's some heading, replace it\n                 lines[0] = actual_heading\n                 chapter_content = \"\\n\".join(lines)\n             else: # Otherwise, prepend it\n                 chapter_content = f\"{actual_heading}\\n\\n{chapter_content}\"\n\n        # Add the generated content to our temporary list for the next iteration's context\n        self.chapters_written_so_far.append(chapter_content)\n\n        return chapter_content # Return the Markdown string\n\n    def post(self, shared, prep_res, exec_res_list):\n        # exec_res_list contains the generated Markdown for each chapter, in order\n        shared[\"chapters\"] = exec_res_list\n        # Clean up the temporary instance variable\n        del self.chapters_written_so_far\n        print(f\"Finished writing {len(exec_res_list)} chapters.\")\n\nclass CombineTutorial(Node):\n    def prep(self, shared):\n        project_name = shared[\"project_name\"]\n        output_base_dir = shared.get(\"output_dir\", \"output\") # Default output dir\n        output_path = os.path.join(output_base_dir, project_name)\n        repo_url = shared[\"repo_url\"]  # Get the repository URL\n\n        # Use 'label' from relationships_data['details']\n        relationships_data = shared[\"relationships\"] # {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        chapter_order = shared[\"chapter_order\"] # indices\n        abstractions = shared[\"abstractions\"]   # list of dicts\n        chapters_content = shared[\"chapters\"]   # list of strings\n\n        # --- Generate Mermaid Diagram ---\n        mermaid_lines = [\"flowchart TD\"]\n        # Add nodes for each abstraction\n        for i, abstr in enumerate(abstractions):\n            # Sanitize name for Mermaid ID and label\n            node_id = f\"A{i}\"\n            sanitized_name = abstr['name'].replace('\"', '')\n            node_label = sanitized_name # Using sanitized name only, no index\n            mermaid_lines.append(f'    {node_id}[\"{node_label}\"]')\n        # Add edges for relationships using 'label'\n        for rel in relationships_data['details']:\n            from_node_id = f\"A{rel['from']}\"\n            to_node_id = f\"A{rel['to']}\"\n            # Sanitize 'label' for edge label\n            edge_label = rel['label'].replace('\"', '').replace('\\n', ' ') # Basic sanitization\n            # Limit edge label length for readability (optional, but good for diagrams)\n            max_label_len = 30 # Make it shorter for labels\n            if len(edge_label) > max_label_len:\n                edge_label = edge_label[:max_label_len-3] + \"...\"\n            mermaid_lines.append(f'    {from_node_id} -- \"{edge_label}\" --> {to_node_id}')\n\n        mermaid_diagram = \"\\n\".join(mermaid_lines)\n        # --- End Mermaid ---\n\n\n        # Prepare index.md content\n        index_content = f\"# Tutorial: {project_name}\\n\\n\"\n        index_content += f\"{relationships_data['summary']}\\n\\n\"\n        index_content += f\"**Source Repository:** [{repo_url}]({repo_url})\\n\\n\"\n\n        # Add Mermaid diagram for relationships\n        index_content += \"```mermaid\\n\"\n        index_content += mermaid_diagram + \"\\n\"\n        index_content += \"```\\n\\n\"\n\n        index_content += \"## Chapters\\n\\n\"\n\n        chapter_files = []\n        # Generate chapter links based on the determined order\n        for i, abstraction_index in enumerate(chapter_order):\n            # Ensure index is valid and we have content for it\n            if 0 <= abstraction_index < len(abstractions) and i < len(chapters_content):\n                abstraction_name = abstractions[abstraction_index][\"name\"]\n                # Sanitize name for filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in abstraction_name).lower()\n                # Use chapter number (i+1) for ordering filename\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                index_content += f\"{i+1}. [{abstraction_name}]({filename})\\n\"\n                \n                # Add attribution to chapter content\n                chapter_content = chapters_content[i]\n                if not chapter_content.endswith(\"\\n\\n\"):\n                    chapter_content += \"\\n\\n\"\n                chapter_content += \"---\\n\\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\"\n                \n                # Store filename and corresponding content\n                chapter_files.append({\"filename\": filename, \"content\": chapter_content})\n            else:\n                 print(f\"Warning: Mismatch between chapter order, abstractions, or content at index {i} (abstraction index {abstraction_index}). Skipping file generation for this entry.\")\n\n        # Add attribution to index content\n        index_content += \"\\n\\n---\\n\\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\"\n\n        return {\n            \"output_path\": output_path,\n            \"index_content\": index_content,\n            \"chapter_files\": chapter_files # List of {\"filename\": str, \"content\": str}\n        }\n\n    def exec(self, prep_res):\n        output_path = prep_res[\"output_path\"]\n        index_content = prep_res[\"index_content\"]\n        chapter_files = prep_res[\"chapter_files\"]\n\n        print(f\"Combining tutorial into directory: {output_path}\")\n        # Rely on Node's built-in retry/fallback\n        os.makedirs(output_path, exist_ok=True)\n\n        # Write index.md\n        index_filepath = os.path.join(output_path, \"index.md\")\n        with open(index_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(index_content)\n        print(f\"  - Wrote {index_filepath}\")\n\n        # Write chapter files\n        for chapter_info in chapter_files:\n            chapter_filepath = os.path.join(output_path, chapter_info[\"filename\"])\n            with open(chapter_filepath, \"w\", encoding=\"utf-8\") as f:\n                f.write(chapter_info[\"content\"])\n            print(f\"  - Wrote {chapter_filepath}\")\n\n        return output_path # Return the final path\n\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"final_output_dir\"] = exec_res # Store the output path\n        print(f\"\\nTutorial generation complete! Files are in: {exec_res}\")\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 4: Knowledge Base Construction`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "```markdown\n# Chapter 4: Knowledge Base Construction\n\nIn the previous chapter, [Codebase Crawling](03_codebase_crawling.md), we learned how to fetch the necessary code files from a repository or local directory. Now that we have these files, what do we do with them? This is where Knowledge Base Construction comes in!\n\nImagine you have a giant pile of Lego bricks (the code files).  Codebase Crawling gave us those bricks.  Now, Knowledge Base Construction is like organizing those bricks into labeled containers, grouping similar pieces together, and creating an instruction manual that explains how the pieces connect and interact. This organized structure is what we call a \"knowledge base.\"\n\n**Why is Knowledge Base Construction Important?**\n\nThe code itself is raw data.  The AI can't use it effectively for generating a tutorial without some organization.  A knowledge base helps the AI:\n\n*   **Understand the code:** By extracting key information and relationships.\n*   **Access information quickly:** By providing a structured format for retrieval.\n*   **Generate better tutorials:** By having a clear understanding of the codebase's structure and purpose.\n\nThink of it like organizing a messy desk. Without organization, it's hard to find anything. But with a well-organized desk, with labeled folders and a clear index, you can quickly access the information you need.\n\n**Key Concepts**\n\n*   **Extraction:** Taking important pieces of information from the code (like functions, classes, and their relationships).  The [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md) module handles the underlying extraction of information.\n*   **Organization:** Arranging the extracted information into a structured format, like a database or a graph.  In our case, this is mostly in the form of Python dictionaries and lists, which are structured.\n*   **Relationships:** Identifying how different parts of the code relate to each other. This is what the AnalyzeRelationships node does!\n*   **Vector Database:**  While we don't *explicitly* create a vector database in this tutorial, the concept is related. A vector database stores code snippets and other data as numerical vectors, allowing for efficient similarity searches. The knowledge base we create serves a similar purpose, allowing the AI to quickly find relevant code snippets and information.\n\n**How it Works: Building Our Knowledge Base**\n\nWhile the whole system contributes to the knowledge base, let's focus on where the main knowledge is stored after the [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md) stage. The shared dictionary contains all the information that could be viewed as a knowledge base.\n\n1.  **The `shared` Dictionary:** This dictionary acts as our central knowledge base. It's like a shared whiteboard where different parts of the system can store and access information.\n\n2.  **Key components in `shared` that comprise the knowledge base:**\n\n    *   **`shared[\"files\"]`:** A list of tuples, where each tuple contains the file path and its contents.  This is the raw code we crawled in [Codebase Crawling](03_codebase_crawling.md). Example: `[(\"file1.py\", \"def add(a, b): return a + b\"), (\"file2.py\", \"def multiply(a, b): return a * b\")]`\n\n    *   **`shared[\"abstractions\"]`:** A list of dictionaries, where each dictionary represents an abstraction (a key concept in the codebase). This list is created by the `IdentifyAbstractions` node in [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md). It contains the name, description, and relevant file indices for each abstraction. Example:\n\n        ```python\n        [\n            {\n                \"name\": \"Addition\",\n                \"description\": \"This abstraction performs addition. It's like combining two piles of toys to see how many you have in all!\",\n                \"files\": [0]  # Index of file1.py\n            },\n            {\n                \"name\": \"Multiplication\",\n                \"description\": \"This abstraction performs multiplication. It's like repeated addition!\",\n                \"files\": [1]  # Index of file2.py\n            }\n        ]\n        ```\n\n    *   **`shared[\"relationships\"]`:** A dictionary containing a summary of the project and a list of relationships between abstractions. This dictionary is created by the `AnalyzeRelationships` node in [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md). The relationships describe how the abstractions interact with each other. Example:\n\n        ```python\n        {\n            \"summary\": \"This project provides basic arithmetic operations: addition and multiplication.  It allows users to perform simple calculations.\",\n            \"details\": [\n                {\n                    \"from\": 0,  # Index of the \"Addition\" abstraction\n                    \"to\": 1,  # Index of the \"Multiplication\" abstraction\n                    \"label\": \"None\"\n                }\n            ]\n        }\n        ```\n\n3.  **Putting it All Together:** The AI uses this structured information to understand the codebase and generate the tutorial content. For example, when writing a chapter about \"Addition,\" the AI can:\n\n    *   Look up the description of \"Addition\" in `shared[\"abstractions\"]`.\n    *   Find the relevant code file (`file1.py`) using the file index in `shared[\"abstractions\"]`.\n    *   Explain the relationship between \"Addition\" and \"Multiplication\" using the information in `shared[\"relationships\"]`.\n\n**Code Example: Accessing the Knowledge Base**\n\nLet's say you want to access the description of the \"Addition\" abstraction. Here's how you would do it:\n\n```python\n# Assuming 'shared' dictionary is already populated\nabstractions = shared[\"abstractions\"]\nfor abstraction in abstractions:\n    if abstraction[\"name\"] == \"Addition\":\n        print(abstraction[\"description\"])\n        break\n```\n\nThis code snippet iterates through the `shared[\"abstractions\"]` list and prints the description of the \"Addition\" abstraction.\n\n**Benefits and Considerations**\n\n*   **Benefits:**\n    *   **Structured Information:** Provides the AI with a clear and organized view of the codebase.\n    *   **Efficient Retrieval:** Allows the AI to quickly access relevant information.\n    *   **Improved Tutorial Quality:** Enables the AI to generate more accurate and informative tutorials.\n*   **Considerations:**\n    *   **Complexity:** Building a knowledge base for a large and complex codebase can be challenging.\n    *   **Maintenance:** The knowledge base needs to be updated as the codebase evolves.\n\n**Analogy: The Library of Code**\n\nImagine a vast library containing all the code in the world.\n\n*   **Codebase Crawling:**  Collecting the books (code files) for our library.\n*   **Knowledge Base Construction:**  Cataloging those books, creating a card catalog (the `shared` dictionary), and writing summaries of each book (abstractions and relationships).\n*   **AI-Powered Content Generation:**  Using the library's resources to write a new book (the tutorial) about a specific topic.\n\n**Conclusion**\n\nIn this chapter, we explored the concept of Knowledge Base Construction, which is crucial for enabling the AI to understand the codebase and generate high-quality tutorials. By extracting, organizing, and relating information, we create a structured knowledge base that the AI can use to access and retrieve relevant information, ultimately improving the tutorial generation process. In the next chapter, [Tutorial Content Combination](05_tutorial_content_combination.md), we'll see how the generated content is combined to create the final tutorial.\n```", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `Tutorial-Codebase-Knowledge` about the concept: \"Tutorial Content Combination\". This is Chapter 5.\n\nConcept Details:\n- Description:\nThis abstraction takes the individual chapters generated by the LLM and assembles them into a complete and coherent tutorial, including an introduction, chapter summaries, and a high-level overview diagram. It's similar to an editor compiling individual articles into a polished book.\n\n\nComplete Tutorial Structure:\n1. [Main Execution Script](01_main_execution_script.md)\n2. [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md)\n3. [Codebase Crawling](03_codebase_crawling.md)\n4. [Knowledge Base Construction](04_knowledge_base_construction.md)\n5. [Tutorial Content Combination](05_tutorial_content_combination.md)\n6. [LLM Orchestration (PocketFlow)](06_llm_orchestration__pocketflow_.md)\n\nContext from previous chapters (summary):\n# Chapter 1: Main Execution Script\n\nWelcome to the first chapter of our tutorial on building a tutorial generation system! This chapter focuses on the \"Main Execution Script,\" the conductor of our orchestra that makes everything happen.\n\nImagine you want to create a tutorial for a cool new Python library you found on GitHub.  Where do you even begin?  That's where the Main Execution Script comes in. It's the starting point of our whole tutorial-making process. It takes your instructions (like the GitHub link) and kicks off all the other parts of the system to create your tutorial.\n\n**What Problem Does It Solve?**\n\nWithout a central entry point, we'd have a bunch of disconnected pieces. The Main Execution Script brings order to chaos. It:\n\n*   **Takes your input:** Where is the code? What do you want to name the tutorial?\n*   **Sets the stage:** Gets everything ready for the tutorial generation.\n*   **Kicks off the process:** Starts all the other parts of the system in the right order.\n\n**Key Concepts**\n\nLet's break down the key concepts of the Main Execution Script:\n\n1.  **Command-Line Arguments:** These are the instructions you give the script when you run it.  Think of them as telling the script *what* to do.\n\n2.  **Configuration:**  This is setting up all the necessary components, like where to save the output.\n\n3.  **Orchestration:** This is the script's main job: making sure all the other parts of the system run in the correct sequence, like a conductor leading an orchestra. We use [LLM Orchestration (PocketFlow)](06_llm_orchestration__pocketflow_.md) for this.\n\n**Using the Main Execution Script**\n\nLet's see how you'd use the Main Execution Script to generate a tutorial for a GitHub repository.\n\n**Example Input:**\n\nLet's say you want to create a tutorial for a repository named \"my-cool-project\" on GitHub, located at `https://github.com/user/my-cool-project`. You'd run the script from your terminal like this:\n\n```bash\npython main.py --repo https://github.com/user/my-cool-project -n MyCoolProjectTutorial\n```\n\nLet's break down that command:\n\n*   `python main.py`:  This tells your computer to run the `main.py` file using Python.\n*   `--repo https://github.com/user/my-cool-project`:  This tells the script the URL of the GitHub repository.\n*   `-n MyCoolProjectTutorial`: This tells the script to name the project \"MyCoolProjectTutorial\".  If you leave this out, the script will try to figure out the name from the repository URL.\n\n**Example Output:**\n\nAfter running the command, the script will:\n\n1.  Download the code from the GitHub repository.\n2.  Analyze the code.\n3.  Create a tutorial based on the code.\n4.  Save the tutorial in a directory named \"output\" (unless you specify a different output directory).\n\nYou'll find a structured tutorial with different chapters covering the codebase's essential aspects in the `output` directory!\n\n**Code Walkthrough**\n\nLet's look at some parts of the `main.py` script:\n\n```python\nimport argparse\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Generate a tutorial...\")\n    parser.add_argument(\"--repo\", help=\"URL of the public GitHub repository.\")\n    parser.add_argument(\"-n\", \"--name\", help=\"Project name (optional...\")\n    args = parser.parse_args()\n\n    # --- Rest of the Code ---\n```\n\nThis code sets up the command-line argument parsing.  It defines the arguments we can use (like `--repo` and `-n`) and provides helpful descriptions. The `argparse` module makes it easy to get the values you provide when you run the script.\n\n```python\n    shared = {\n        \"repo_url\": args.repo,\n        \"project_name\": args.name,\n        \"output_dir\": \"output\",  # Simplification\n    }\n\n    print(f\"Starting tutorial generation for: {args.repo}\")\n\n    # --- Rest of the Code ---\n```\n\nThis code creates a `shared` dictionary, a central place to store information that all parts of the system need.  We store the repository URL, the project name, and the output directory here.  Think of it as a shared whiteboard where all the different parts of the tutorial generation system can read and write information.\n\n```python\n    # Create the flow instance\n    tutorial_flow = create_tutorial_flow()\n\n    # Run the flow\n    tutorial_flow.run(shared)\n```\n\nThis is where the magic happens! We create a `tutorial_flow` (which we will discuss more in [LLM Orchestration (PocketFlow)](06_llm_orchestration__pocketflow_.md)) and tell it to run, passing in the `shared` dictionary. The `tutorial_flow` orchestrates all the other steps to generate the tutorial.\n\n**Internal Implementation: A Step-by-Step View**\n\nHere's a simplified view of what happens inside the Main Execution Script when you run it:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant MainScript as Main Execution Script\n    participant ArgumentParser as Argument Parser\n    participant Flow as Tutorial Flow\n    \n    User->>MainScript: Run script with arguments\n    MainScript->>ArgumentParser: Parse arguments\n    ArgumentParser-->>MainScript: Return arguments\n    MainScript->>Flow: Create tutorial flow\n    MainScript->>Flow: Run flow with arguments\n    Flow-->>MainScript: Tutorial generated\n    MainScript->>User: Completion Message\n```\n\n1.  **User:** You run the `main.py` script with command-line arguments (e.g., the GitHub repository URL).\n2.  **Main Execution Script:** The script receives your command and uses `ArgumentParser` to understand your instructions.\n3.  **Argument Parser:** The argument parser takes the command-line arguments and organizes them into a usable format.\n4.  **Main Execution Script:**  The script then creates an instance of the `Tutorial Flow`. The script then runs the `Tutorial Flow`.\n5.  **Tutorial Flow:** The tutorial flow coordinates with other components like the [Codebase Crawling](03_codebase_crawling.md), [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md), and [Knowledge Base Construction](04_knowledge_base_construction.md) components to create the tutorial.\n6.  **Main Execution Script:** The script prints a completion message, indicating that the tutorial has been generated.\n\n**Implementation Details**\n\nThe `create_tutorial_flow()` function, defined in `flow.py` (not shown in the code snippets above), is responsible for creating the `tutorial_flow` instance.\n\n**Conclusion**\n\nIn this chapter, we explored the Main Execution Script, the entry point of our tutorial generation system. We learned how it takes user input, configures the system, and orchestrates the tutorial generation process.  It's the conductor that makes sure all the instruments play in harmony!\n\nIn the next chapter, we'll dive into the crucial component of [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md), where we'll see how AI helps us understand the codebase and generate meaningful tutorial content.\n\n---\n# Chapter 2: AI-Powered Code Analysis and Generation\n\nIn the previous chapter, [Main Execution Script](01_main_execution_script.md), we saw how the script acts as the conductor of our tutorial-generation process, taking your input and orchestrating the workflow. Now, we'll dive into a crucial part of that workflow: using AI to understand the code and generate content!\n\n**Why AI for Code Analysis and Generation?**\n\nImagine you're given a huge, complex Lego set with thousands of pieces and no instructions. That's what it's like trying to understand a large codebase!  It's overwhelming.  The goal of this chapter is to learn how to use AI to understand any codebase and describe it so that others can also understand it.\n\nThat's where AI comes in! Our `AI-Powered Code Analysis and Generation` component acts like an experienced tutor. It uses Large Language Models (LLMs) to:\n\n*   **Analyze the code:**  Like carefully examining each Lego brick.\n*   **Identify key components:**  Finding the essential parts of the set.\n*   **Generate beginner-friendly explanations:**  Writing clear, step-by-step instructions.\n\n**The Big Picture: Turning Code into Tutorials**\n\nThis component sits in the middle of our tutorial generation process. It takes raw code (from crawling the codebase) and turns it into structured, understandable tutorial content. We start by downloading the source code, then we'll use this component to create a tutorial.\n\n**Key Concepts**\n\nLet's break down the core ideas behind our AI-powered approach:\n\n1.  **Large Language Models (LLMs):** These are powerful AI models trained on vast amounts of text data. Think of them as having read millions of books and articles on programming.  We use them to understand code and write explanations. We pass the prompts to the LLM through the `call_llm` function, which is defined in `utils/call_llm.py`. This allows us to easily switch models by changing the environment variables for the model.\n\n2.  **Code Abstraction Identification:** The AI identifies key concepts or modules within the codebase. This is like finding the most important functions or classes.\n\n3.  **Relationship Analysis:** The AI determines how these components interact with each other.  This is like understanding how the Lego bricks fit together to build the final model.\n\n4.  **Beginner-Friendly Content Generation:** The AI translates complex code into simple, easy-to-understand explanations, tailored for beginners. It generates markdown content that is later combined to build a chapter.\n\n**How it Works: A Simplified Walkthrough**\n\nLet's walk through a simplified example using pieces of the code, starting from what it receives from the main script.\n\n```python\nshared = {\n  \"repo_url\": \"https://github.com/user/my-cool-project\",\n  \"project_name\": \"MyCoolProject\",\n  \"files\": [(\"file1.py\", \"def add(a, b): return a + b\"), (\"file2.py\", \"def multiply(a, b): return a * b\")],\n}\n```\n\nThis `shared` dictionary is passed between different components, as we saw in the [Main Execution Script](01_main_execution_script.md) chapter. We will focus on how the `IdentifyAbstractions` node processes the `shared` data.\n\n1.  **Fetching Code:** We already have the source code (simulated as `shared[\"files\"]` here).  In reality, this would come from the [Codebase Crawling](03_codebase_crawling.md) stage.\n\n2.  **Identifying Abstractions (IdentifyAbstractions Node):**\n\n   *   This node takes the codebase's content and asks the LLM to find the most important concepts.\n\n   *   It prepares a prompt for the LLM that includes the entire codebase context.\n\n   *   Here's a simplified snippet of the `IdentifyAbstractions` node's `prep` and `exec` methods (from `nodes.py`):\n\n     ```python\n     class IdentifyAbstractions(Node):\n         def prep(self, shared):\n             files_data = shared[\"files\"]\n             project_name = shared[\"project_name\"]\n             # simplified context creation\n             context = \"\"\n             file_info = []\n             for i, (path, content) in enumerate(files_data):\n                 context += f\"--- File Index {i}: {path} ---\\n{content}\\n\\n\"\n                 file_info.append((i, path))\n\n             file_listing_for_prompt = \"\\n\".join([f\"- {idx} # {path}\" for idx, path in file_info])\n             return context, file_listing_for_prompt, len(files_data), project_name\n\n         def exec(self, prep_res):\n             context, file_listing_for_prompt, file_count, project_name = prep_res\n\n             prompt = f\"\"\"\n     For the project `{project_name}`:\n\n     Codebase Context:\n     {context}\n\n     Analyze the codebase context.\n     Identify the top 2 core most important abstractions to help those new to the codebase.\n\n     For each abstraction, provide:\n     1. A concise `name`.\n     2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n     3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\n     List of file indices and paths present in the context:\n     {file_listing_for_prompt}\n\n     Format the output as a YAML list of dictionaries:\n\n     ```yaml\n     - name: Addition\n       description: |\n         This abstraction performs addition.\n         It's like adding two numbers together.\n       file_indices:\n         - 0 # file1.py\n     - name: Multiplication\n       description: |\n         This abstraction performs multiplication.\n         It's like multiplying two numbers together.\n       file_indices:\n         - 1 # file2.py\n     ```\"\"\"\n             response = call_llm(prompt) #call llm\n\n             yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n             abstractions = yaml.safe_load(yaml_str)\n\n             validated_abstractions = []\n             for item in abstractions:\n                  item[\"files\"] = [int(idx_entry.split('#')[0].strip()) for idx_entry in item[\"file_indices\"]]\n                  validated_abstractions.append({\n                       \"name\": item[\"name\"],\n                       \"description\": item[\"description\"],\n                       \"files\": item[\"files\"]\n                  })\n\n             return validated_abstractions\n\n         def post(self, shared, prep_res, exec_res):\n             shared[\"abstractions\"] = exec_res\n     ```\n\n   *   The `prep` method prepares the context and project name for the LLM by listing all files and their code in the files_data list.\n\n   *   The `exec` method prompts the LLM to analyze the code and identify the important abstractions, generating a YAML formatted string with the abstraction name, description, and corresponding file indices. After receiving this string, the LLM output is parsed and the file indices are added to the `shared` dictionary, specifically at `shared[\"abstractions\"]`.\n\n   *   The LLM might respond with something like this (the LLM will come up with a description):\n\n     ```yaml\n     - name: Addition\n       description: |\n         This abstraction performs addition.\n         It's like adding two numbers together.\n       file_indices:\n         - 0 # file1.py\n     - name: Multiplication\n       description: |\n         This abstraction performs multiplication.\n         It's like multiplying two numbers together.\n       file_indices:\n         - 1 # file2.py\n     ```\n\n3.  **Analyzing Relationships (AnalyzeRelationships Node):**\n\n   *   The `AnalyzeRelationships` node determines how the identified abstractions relate to each other, and creates a summary for the whole project.\n\n   *   Here's a simplified snippet of the `AnalyzeRelationships` node's `prep` and `exec` methods (from `nodes.py`):\n\n     ```python\n     class AnalyzeRelationships(Node):\n         def prep(self, shared):\n             abstractions = shared[\"abstractions\"]\n             files_data = shared[\"files\"]\n             context = \"Identified Abstractions:\\n\"\n             for i, abstr in enumerate(abstractions):\n                 file_indices_str = \", \".join(map(str, abstr['files']))\n                 info_line = f\"- Index {i}: {abstr['name']} (Relevant file indices: [{file_indices_str}])\\n  Description: {abstr['description']}\"\n                 context += info_line + \"\\n\"\n\n             all_relevant_indices = set()\n             for abstr in abstractions:\n                  all_relevant_indices.update(abstr['files'])\n\n             relevant_files_content_map = {}\n             for i in sorted(list(all_relevant_indices)):\n                  path, content = files_data[i]\n                  relevant_files_content_map[f\"{i} # {path}\"] = content\n\n             context += \"\\nRelevant File Snippets (Referenced by Index and Path):\\n\"\n             file_context_str = \"\\n\\n\".join(\n                  f\"--- File: {idx_path} ---\\n{content}\"\n                  for idx_path, content in relevant_files_content_map.items()\n             )\n             context += file_context_str\n             abstraction_info_for_prompt = []\n             for i, abstr in enumerate(abstractions):\n                  abstraction_info_for_prompt.append(f\"{i} # {abstr['name']}\")\n             return context, \"\\n\".join(abstraction_info_for_prompt), shared[\"project_name\"]\n\n         def exec(self, prep_res):\n             context, abstraction_listing, project_name = prep_res\n\n             prompt = f\"\"\"\n     Based on the following abstractions and relevant code snippets from the project `{project_name}`:\n\n     List of Abstraction Indices and Names:\n     {abstraction_listing}\n\n     Context (Abstractions, Descriptions, Code):\n     {context}\n\n     Please provide:\n     1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n     2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n         - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n         - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n         - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n         Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n         Simplify the relationship and exclude those non-important ones.\n\n     IMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\n     Format the output as YAML:\n\n     ```yaml\n     summary: |\n       A brief, simple explanation of the project.\n       Can span multiple lines with **bold** and *italic* for emphasis.\n     relationships:\n       - from_abstraction: 0 # AbstractionName1\n         to_abstraction: 1 # AbstractionName2\n         label: \"Manages\"\n     ```\n\n     Now, provide the YAML output:\n     \"\"\"\n             response = call_llm(prompt) #call llm\n\n             yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n             relationships_data = yaml.safe_load(yaml_str)\n\n             validated_relationships = []\n             num_abstractions = len(abstraction_listing.split('\\n'))\n             for rel in relationships_data[\"relationships\"]:\n                  from_idx = int(str(rel[\"from_abstraction\"]).split('#')[0].strip())\n                  to_idx = int(str(rel[\"to_abstraction\"]).split('#')[0].strip())\n                  validated_relationships.append({\n                       \"from\": from_idx,\n                       \"to\": to_idx,\n                       \"label\": rel[\"label\"]\n                  })\n\n             return {\n                  \"summary\": relationships_data[\"summary\"],\n                  \"details\": validated_relationships\n             }\n\n         def post(self, shared, prep_res, exec_res):\n             shared[\"relationships\"] = exec_res\n     ```\n\n   *   The `prep` method formats the context and project name for the LLM by listing the abstraction descriptions and file content.\n\n   *   The `exec` method prompts the LLM to analyze the context and provide a summary and relationships. It parses the LLM output and adds a project summary and details (including what abstraction links to what) to the `shared` dictionary at `shared[\"relationships\"]`.\n\n   *   The LLM might respond with something like this (LLM generated):\n\n     ```yaml\n     summary: |\n       This project provides basic arithmetic operations: **addition** and **multiplication**.\n       It allows users to perform simple calculations.\n     relationships:\n       - from_abstraction: 0 # Addition\n         to_abstraction: 1 # Multiplication\n         label: \"None\"\n     ```\n\n4.  **Ordering Chapters (OrderChapters Node)**\n\n   *   The `OrderChapters` node asks the LLM what order to present the abstractions in.\n   *   Here's a simplified snippet of the `OrderChapters` node's `prep` and `exec` methods (from `nodes.py`):\n\n     ```python\n     class OrderChapters(Node):\n         def prep(self, shared):\n             abstractions = shared[\"abstractions\"]\n             relationships = shared[\"relationships\"]\n\n             abstraction_info_for_prompt = []\n             for i, a in enumerate(abstractions):\n                 abstraction_info_for_prompt.append(f\"- {i} # {a['name']}\")\n             abstraction_listing = \"\\n\".join(abstraction_info_for_prompt)\n\n             context = f\"Project Summary:\\n{relationships['summary']}\\n\\n\"\n             context += \"Relationships (Indices refer to abstractions above):\\n\"\n             for rel in relationships['details']:\n                  from_name = abstractions[rel['from']]['name']\n                  to_name = abstractions[rel['to']]['name']\n                  context += f\"- From {rel['from']} ({from_name}) to {rel['to']} ({to_name}): {rel['label']}\\n\"\n\n             return abstraction_listing, context, len(abstractions), shared[\"project_name\"]\n\n         def exec(self, prep_res):\n             abstraction_listing, context, num_abstractions, project_name = prep_res\n             prompt = f\"\"\"\n     Given the following project abstractions and their relationships for the project ```` {project_name} ````:\n\n     Abstractions (Index # Name):\n     {abstraction_listing}\n\n     Context about relationships and project summary:\n     {context}\n\n     If you are going to make a tutorial for ```` {project_name} ````, what is the best order to explain these abstractions, from first to last?\n     Ideally, first explain those that are the most important or foundational, perhaps user-facing concepts or entry points. Then move to more detailed, lower-level implementation details or supporting concepts.\n\n     Output the ordered list of abstraction indices, including the name in a comment for clarity. Use the format `idx # AbstractionName`.\n\n     ```yaml\n     - 0 # Addition\n     - 1 # Multiplication\n     ```\n     \"\"\"\n             response = call_llm(prompt) #call llm\n\n             yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n             ordered_indices_raw = yaml.safe_load(yaml_str)\n\n             ordered_indices = []\n             for entry in ordered_indices_raw:\n                  idx = int(entry.split('#')[0].strip())\n                  ordered_indices.append(idx)\n\n             return ordered_indices\n\n         def post(self, shared, prep_res, exec_res):\n             shared[\"chapter_order\"] = exec_res\n     ```\n\n   *   The `prep` method prepares the context and the project name for the LLM by listing all abstractions, relationship information, and project summary.\n\n   *   The `exec` method calls the LLM to ask what the best order to explain the abstractions are. Then the method adds the ordered list to the `shared` dictionary at `shared[\"chapter_order\"]`.\n\n   *   The LLM might respond with something like this (LLM generated):\n\n     ```yaml\n     - 0 # Addition\n     - 1 # Multiplication\n     ```\n\n5.  **Writing Chapters (WriteChapters Node)**\n\n   *   The `WriteChapters` node creates the markdown chapters for the final tutorial.\n   *   Here's a simplified snippet of the `WriteChapters` node's `prep` and `exec` methods (from `nodes.py`):\n\n     ```python\n     class WriteChapters(BatchNode):\n         def prep(self, shared):\n             chapter_order = shared[\"chapter_order\"]\n             abstractions = shared[\"abstractions\"]\n             files_data = shared[\"files\"]\n\n             all_chapters = []\n             chapter_filenames = {}\n             for i, abstraction_index in enumerate(chapter_order):\n                  chapter_num = i + 1\n                  chapter_name = abstractions[abstraction_index][\"name\"]\n                  safe_name = \"\".join(c if c.isalnum() else '_' for c in chapter_name).lower()\n                  filename = f\"{i+1:02d}_{safe_name}.md\"\n                  all_chapters.append(f\"{chapter_num}. [{chapter_name}]({filename})\")\n                  chapter_filenames[abstraction_index] = {\"num\": chapter_num, \"name\": chapter_name, \"filename\": filename}\n\n             full_chapter_listing = \"\\n\".join(all_chapters)\n             items_to_process = []\n             for i, abstraction_index in enumerate(chapter_order):\n                  abstraction_details = abstractions[abstraction_index]\n                  related_file_indices = abstraction_details.get(\"files\", [])\n\n                  related_files_content_map = {}\n                  for i in related_file_indices:\n                       path, content = files_data[i]\n                       related_files_content_map[f\"{i} # {path}\"] = content\n\n                  items_to_process.append({\n                       \"chapter_num\": i + 1,\n                       \"abstraction_index\": abstraction_index,\n                       \"abstraction_details\": abstraction_details,\n                       \"related_files_content_map\": related_files_content_map,\n                       \"project_name\": shared[\"project_name\"],\n                       \"full_chapter_listing\": full_chapter_listing,\n                       \"chapter_filenames\": chapter_filenames,\n                  })\n             print(f\"Preparing to write {len(items_to_process)} chapters...\")\n             return items_to_process\n\n         def exec(self, item):\n             abstraction_name = item[\"abstraction_details\"][\"name\"]\n             chapter_num = item[\"chapter_num\"]\n             project_name = item.get(\"project_name\")\n             print(f\"Writing chapter {chapter_num} for: {abstraction_name} using LLM...\")\n\n             file_context_str = \"\\n\\n\".join(\n                  f\"--- File: {idx_path.split('# ')[1] if '# ' in idx_path else idx_path} ---\\n{content}\"\n                  for idx_path, content in item[\"related_files_content_map\"].items()\n             )\n\n             prompt = f\"\"\"\n     Write a very beginner-friendly tutorial chapter (in Markdown format) for the project `{project_name}` about the concept: \"{abstraction_name}\". This is Chapter {chapter_num}.\n\n     Concept Details:\n     - Description:\n     {item[\"abstraction_details\"][\"description\"]}\n\n     Complete Tutorial Structure:\n     {item[\"full_chapter_listing\"]}\n\n     Relevant Code Snippets:\n     {file_context_str if file_context_str else \"No specific code snippets provided for this abstraction.\"}\n\n     Instructions for the chapter:\n     - Start with a clear heading (e.g., `# Chapter {chapter_num}: {abstraction_name}`).\n     - Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example.\n     - If the abstraction is complex, break it down into key concepts.\n     - Explain how to use this abstraction. Give example inputs and outputs.\n     - Describe the internal implementation to help understand what's under the hood.\n     - Heavily use analogies and examples throughout to help beginners understand.\n     - End the chapter with a brief conclusion.\n     - Ensure the tone is welcoming and easy for a newcomer to understand.\n\n     Now, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n     \"\"\"\n             chapter_content = call_llm(prompt) #call llm\n\n             actual_heading = f\"# Chapter {chapter_num}: {abstraction_name}\"\n             if not chapter_content.strip().startswith(f\"# Chapter {chapter_num}\"):\n                  lines = chapter_content.strip().split('\\n')\n                  if lines and lines[0].strip().startswith(\"#\"):\n                       lines[0] = actual_heading\n                       chapter_content = \"\\n\".join(lines)\n                  else:\n                       chapter_content = f\"{actual_heading}\\n\\n{chapter_content}\"\n\n             return chapter_content\n\n         def post(self, shared, prep_res, exec_res_list):\n             shared[\"chapters\"] = exec_res_list\n             print(f\"Finished writing {len(exec_res_list)} chapters.\")\n     ```\n\n   *   The `prep` method formats the project and file content and abstraction details for the LLM by listing all file content that corresponds to an abstraction. It generates filenames for each chapter, and adds each chapter to the `items_to_process` list.\n\n   *   The `exec` method prompts the LLM to generate a chapter based on the abstraction details and corresponding files. It then adds the chapters to the `shared` dictionary at `shared[\"chapters\"]`.\n\n   *   An example of the output generated by the LLM would be:\n\n     ```markdown\n     # Chapter 1: Addition\n\n     Addition is the process of combining two or more numbers to find their total, or sum. It's like combining two piles of toys to see how many you have in all!\n\n     ## How to Use Addition\n\n     You can use the `add` function in `file1.py` to perform addition:\n\n     ```python\n     def add(a, b):\n         return a + b\n     ```\n\n     This code simply takes two numbers, `a` and `b`, and returns their sum. For example:\n\n     ```python\n     result = add(5, 3) # result will be 8\n     print(result)\n     ```\n\n     In this example, `add(5, 3)` returns `8`, because 5 + 3 = 8. It's that simple!\n\n     ## Internal Implementation\n\n     The function `add(a, b)` simply uses the `+` operator in Python to perform the addition. This is a built-in feature of Python, so it's very efficient.\n\n     ## Conclusion\n\n     In this chapter, we learned about addition and how to use the `add` function to perform it in Python. In the next chapter, we will talk about [Multiplication](02_multiplication.md).\n     ```\n\n**Under the Hood: Implementation Details**\n\n*   **Nodes.py:** The core logic for this component resides in the `nodes.py` file, which contains the `IdentifyAbstractions`, `AnalyzeRelationships`, `OrderChapters`, and `WriteChapters` classes. Each of these classes inherits from the `Node` or `BatchNode` class from the `pocketflow` library.\n*   **LLM Integration:** The `call_llm` function (in `utils/call_llm.py`) handles communication with the LLM. It takes a prompt as input and returns the LLM's response. By default, we use the Google Gemini 2.5 Pro model.\n*   **Prompt Engineering:**  A key part of this component is crafting effective prompts for the LLM.  These prompts guide the LLM to analyze the code, identify key concepts, and generate beginner-friendly explanations. Examples of prompts are shown in the `exec` method of the nodes shown above.\n\n**Benefits and Considerations**\n\n*   **Benefits:**\n    *   **Automation:**  Automates the process of understanding and explaining complex codebases.\n    *   **Beginner-Friendly:**  Generates content tailored for beginners, making it easier to learn new codebases.\n    *   **Scalability:**  Can handle large codebases that would be difficult for a human to analyze manually.\n*   **Considerations:**\n    *   **LLM Limitations:** The quality of the generated content depends on the LLM's capabilities and the effectiveness of the prompts.\n    *   **Accuracy:** The AI might not always perfectly understand the code, so it's important to review the generated content for accuracy.\n\n**Conclusion**\n\nIn this chapter, we explored how AI can be used to analyze codebases and generate beginner-friendly tutorial content. This `AI-Powered Code Analysis and Generation` component is a key part of our tutorial generation system, enabling us to automate the process of understanding and explaining complex code.  In the next chapter, [Codebase Crawling](03_codebase_crawling.md), we'll see how we actually get the source code that this component analyzes.\n\n---\n# Chapter 3: Codebase Crawling\n\nIn the previous chapter, [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md), we learned how AI can analyze code and generate beginner-friendly explanations. But where does that code come from? That's where codebase crawling comes in!\n\nImagine you want to bake a cake. The AI-Powered analysis is like having a master chef who can explain every ingredient and baking step in detail. But first, you need to *get* the ingredients! Codebase crawling is like going to the grocery store and carefully selecting all the necessary ingredients (code files) for our AI chef to work with.\n\n**Why Codebase Crawling?**\n\nBefore we can analyze a project's code and create a tutorial, we need to first gather the relevant files. This can involve:\n\n*   **Fetching from a remote repository:**  Downloading code from platforms like GitHub.\n*   **Exploring a local directory:**  Reading code from your computer.\n*   **Filtering out irrelevant files:**  Ignoring tests, documentation, or other non-essential code.\n\n**The Goal: Getting the Right Code, Efficiently**\n\nOur `Codebase Crawling` component aims to provide a flexible and efficient way to fetch the necessary code files for analysis. It's like a librarian searching for specific books based on certain criteria (include/exclude patterns) and size restrictions, ensuring only relevant materials are collected.\n\n**Key Concepts**\n\nLet's break down the core ideas behind our codebase crawling:\n\n1.  **Fetching Source Code (FetchRepo Node):**\n    *   This node takes the Github repo URL or a local directory path and downloads source code using the `crawl_github_files` or `crawl_local_files` functions.\n\n2.  **Repository URL:** The address of the code repository (e.g., a GitHub link).\n\n3.  **Local Directory:** The path to a directory on your computer containing the code.\n\n4.  **Include/Exclude Patterns:**  Rules for selecting specific files based on their names or paths (e.g., including only `.py` files, excluding `test` directories). This is done using `fnmatch`, which supports wildcard matching similar to bash.\n\n5.  **File Size Limits:**  Maximum size for individual files to prevent processing large, irrelevant files.\n\n**How it Works: A Step-by-Step View**\n\nLet's walk through a simple scenario: You want to create a tutorial for a small Python library on GitHub.\n\n1.  **The `FetchRepo` Node:** This node is responsible for retrieving the code files. It uses either `crawl_github_files` (for GitHub repos) or `crawl_local_files` (for local directories).\n\n    *   Here's a simplified snippet of the `FetchRepo` node's `prep` and `exec` methods (from `nodes.py`):\n\n     ```python\n     class FetchRepo(Node):\n         def prep(self, shared):\n             repo_url = shared.get(\"repo_url\")\n             local_dir = shared.get(\"local_dir\")\n             project_name = shared.get(\"project_name\")\n             \n             if not project_name:\n                 # Basic name derivation from URL or directory\n                 if repo_url:\n                     project_name = repo_url.split('/')[-1].replace('.git', '')\n                 else:\n                     project_name = os.path.basename(os.path.abspath(local_dir))\n                 shared[\"project_name\"] = project_name\n\n             # Get file patterns directly from shared\n             include_patterns = shared[\"include_patterns\"]\n             exclude_patterns = shared[\"exclude_patterns\"]\n             max_file_size = shared[\"max_file_size\"]\n\n             return {\n                 \"repo_url\": repo_url,\n                 \"local_dir\": local_dir,\n                 \"token\": shared.get(\"github_token\"),\n                 \"include_patterns\": include_patterns,\n                 \"exclude_patterns\": exclude_patterns,\n                 \"max_file_size\": max_file_size,\n                 \"use_relative_paths\": True\n             }\n\n         def exec(self, prep_res):\n             if prep_res[\"repo_url\"]:\n                 print(f\"Crawling repository: {prep_res['repo_url']}...\")\n                 result = crawl_github_files(\n                     repo_url=prep_res[\"repo_url\"],\n                     token=prep_res[\"token\"],\n                     include_patterns=prep_res[\"include_patterns\"],\n                     exclude_patterns=prep_res[\"exclude_patterns\"],\n                     max_file_size=prep_res[\"max_file_size\"],\n                     use_relative_paths=prep_res[\"use_relative_paths\"]\n                 )\n             else:\n                 print(f\"Crawling directory: {prep_res['local_dir']}...\")\n                 result = crawl_local_files(\n                     directory=prep_res[\"local_dir\"],\n                     include_patterns=prep_res[\"include_patterns\"],\n                     exclude_patterns=prep_res[\"exclude_patterns\"],\n                     max_file_size=prep_res[\"max_file_size\"],\n                     use_relative_paths=prep_res[\"use_relative_paths\"]\n                 )\n                 \n             # Convert dict to list of tuples: [(path, content), ...]\n             files_list = list(result.get(\"files\", {}).items())\n             print(f\"Fetched {len(files_list)} files.\")\n             return files_list\n\n         def post(self, shared, prep_res, exec_res):\n             shared[\"files\"] = exec_res # List of (path, content) tuples\n     ```\n\n    *   The `prep` method retrieves all config options (such as Github repo URL, local directory path, file inclusion/exclusion patterns, max file size), and sets them to the `prep_res` to be used by the `exec` method.\n    *   The `exec` method determines if source code is fetched from Github or local directory based on the presence of a Github repo URL. It then downloads the source code, and stores the path and code as a tuple into a list.\n    *   The `post` method adds the source code to the `shared` dictionary, specifically at `shared[\"files\"]`.\n\n2.  **`crawl_github_files` Function:** This function is defined in `utils/crawl_github_files.py`.\n\n    *   Here's a simplified view of what happens inside the `crawl_github_files` function:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant CrawlGithubFiles as crawl_github_files\n    participant GithubAPI as Github API\n\n    User->>CrawlGithubFiles: Call crawl_github_files(repo_url, token, include_patterns, exclude_patterns, max_file_size, use_relative_paths)\n    CrawlGithubFiles->>GithubAPI: GET contents from Github API for the repo URL\n    GithubAPI-->>CrawlGithubFiles: Return contents from Github API\n    CrawlGithubFiles->>CrawlGithubFiles: Apply include/exclude patterns\n    CrawlGithubFiles->>CrawlGithubFiles: Apply file size limits\n    CrawlGithubFiles-->>User: Return list of (path, code)\n```\n\n    *   The `crawl_github_files` function downloads the source code for the repo URL by calling the Github API.\n    *   The `crawl_github_files` function then filters all source code based on include/exclude patterns, and file size limits.\n    *   The function returns a list of (path, code).\n\n3.  **`crawl_local_files` Function:** This function is defined in `utils/crawl_local_files.py`.\n\n    *   Here's a simplified view of what happens inside the `crawl_local_files` function:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant CrawlLocalFiles as crawl_local_files\n    participant OS as OS\n\n    User->>CrawlLocalFiles: Call crawl_local_files(local_dir, include_patterns, exclude_patterns, max_file_size, use_relative_paths)\n    CrawlLocalFiles->>OS: Walk directory for the local directory path\n    OS-->>CrawlLocalFiles: Return files\n    CrawlLocalFiles->>CrawlLocalFiles: Apply include/exclude patterns\n    CrawlLocalFiles->>CrawlLocalFiles: Apply file size limits\n    CrawlLocalFiles-->>User: Return list of (path, code)\n```\n\n    *   The `crawl_local_files` function walks the directory structure to find all files in the directory path.\n    *   The `crawl_local_files` function then filters all source code based on include/exclude patterns, and file size limits.\n    *   The function returns a list of (path, code).\n\n4.  **Example:**\n\n    ```python\n    shared = {\n        \"repo_url\": \"https://github.com/user/my-cool-project\",\n        \"include_patterns\": {\"*.py\"},\n        \"exclude_patterns\": {\"tests/*\"},\n        \"max_file_size\": 100000  # 100KB\n    }\n    ```\n\n    *   In this example, the `FetchRepo` node would:\n\n        *   Download the code from the specified GitHub repository.\n        *   Only include files ending with `.py`.\n        *   Exclude any files in directories named `tests`.\n        *   Skip any files larger than 100KB.\n\n5.  **Output:** The `FetchRepo` node outputs a list of tuples, where each tuple contains the file path and its contents. This list is then stored in the `shared` dictionary for use by subsequent nodes, such as [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md).\n\n**Code Example**\n\nLet's look at a snippet from `utils/crawl_github_files.py` to understand how file filtering works:\n\n```python\nimport fnmatch\n\ndef should_include_file(file_path: str, file_name: str) -> bool:\n    \"\"\"Determine if a file should be included based on patterns\"\"\"\n    # If no include patterns are specified, include all files\n    if not include_patterns:\n        include_file = True\n    else:\n        # Check if file matches any include pattern\n        include_file = any(fnmatch.fnmatch(file_name, pattern) for pattern in include_patterns)\n\n    # If exclude patterns are specified, check if file should be excluded\n    if exclude_patterns and include_file:\n        # Exclude if file matches any exclude pattern\n        exclude_file = any(fnmatch.fnmatch(file_path, pattern) for pattern in exclude_patterns)\n        return not exclude_file\n\n    return include_file\n```\n\nThis code snippet demonstrates how `fnmatch` is used to match file paths against include and exclude patterns.  It's a simple but powerful way to control which files are included in the analysis.\n\n**Benefits and Considerations**\n\n*   **Benefits:**\n    *   **Flexibility:**  Supports both remote repositories and local directories.\n    *   **Efficiency:**  Filters out irrelevant files to reduce processing time.\n    *   **Customization:**  Allows specifying include/exclude patterns and file size limits.\n*   **Considerations:**\n    *   **Rate Limits:**  When crawling GitHub repositories, be aware of API rate limits. Using a GitHub token is highly recommended.\n    *   **Complexity:**  Complex include/exclude patterns can be difficult to manage.\n\n**Conclusion**\n\nIn this chapter, we learned about codebase crawling, a crucial step in our tutorial generation process. By fetching and filtering code files, we ensure that our AI-powered analysis focuses on the most relevant parts of the codebase.  In the next chapter, [Knowledge Base Construction](04_knowledge_base_construction.md), we'll see how we use the crawled code to build a knowledge base for generating our tutorials.\n\n---\n# Chapter 4: Knowledge Base Construction\n\n```markdown\n# Chapter 4: Knowledge Base Construction\n\nIn the previous chapter, [Codebase Crawling](03_codebase_crawling.md), we learned how to fetch the necessary code files from a repository or local directory. Now that we have these files, what do we do with them? This is where Knowledge Base Construction comes in!\n\nImagine you have a giant pile of Lego bricks (the code files).  Codebase Crawling gave us those bricks.  Now, Knowledge Base Construction is like organizing those bricks into labeled containers, grouping similar pieces together, and creating an instruction manual that explains how the pieces connect and interact. This organized structure is what we call a \"knowledge base.\"\n\n**Why is Knowledge Base Construction Important?**\n\nThe code itself is raw data.  The AI can't use it effectively for generating a tutorial without some organization.  A knowledge base helps the AI:\n\n*   **Understand the code:** By extracting key information and relationships.\n*   **Access information quickly:** By providing a structured format for retrieval.\n*   **Generate better tutorials:** By having a clear understanding of the codebase's structure and purpose.\n\nThink of it like organizing a messy desk. Without organization, it's hard to find anything. But with a well-organized desk, with labeled folders and a clear index, you can quickly access the information you need.\n\n**Key Concepts**\n\n*   **Extraction:** Taking important pieces of information from the code (like functions, classes, and their relationships).  The [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md) module handles the underlying extraction of information.\n*   **Organization:** Arranging the extracted information into a structured format, like a database or a graph.  In our case, this is mostly in the form of Python dictionaries and lists, which are structured.\n*   **Relationships:** Identifying how different parts of the code relate to each other. This is what the AnalyzeRelationships node does!\n*   **Vector Database:**  While we don't *explicitly* create a vector database in this tutorial, the concept is related. A vector database stores code snippets and other data as numerical vectors, allowing for efficient similarity searches. The knowledge base we create serves a similar purpose, allowing the AI to quickly find relevant code snippets and information.\n\n**How it Works: Building Our Knowledge Base**\n\nWhile the whole system contributes to the knowledge base, let's focus on where the main knowledge is stored after the [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md) stage. The shared dictionary contains all the information that could be viewed as a knowledge base.\n\n1.  **The `shared` Dictionary:** This dictionary acts as our central knowledge base. It's like a shared whiteboard where different parts of the system can store and access information.\n\n2.  **Key components in `shared` that comprise the knowledge base:**\n\n    *   **`shared[\"files\"]`:** A list of tuples, where each tuple contains the file path and its contents.  This is the raw code we crawled in [Codebase Crawling](03_codebase_crawling.md). Example: `[(\"file1.py\", \"def add(a, b): return a + b\"), (\"file2.py\", \"def multiply(a, b): return a * b\")]`\n\n    *   **`shared[\"abstractions\"]`:** A list of dictionaries, where each dictionary represents an abstraction (a key concept in the codebase). This list is created by the `IdentifyAbstractions` node in [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md). It contains the name, description, and relevant file indices for each abstraction. Example:\n\n        ```python\n        [\n            {\n                \"name\": \"Addition\",\n                \"description\": \"This abstraction performs addition. It's like combining two piles of toys to see how many you have in all!\",\n                \"files\": [0]  # Index of file1.py\n            },\n            {\n                \"name\": \"Multiplication\",\n                \"description\": \"This abstraction performs multiplication. It's like repeated addition!\",\n                \"files\": [1]  # Index of file2.py\n            }\n        ]\n        ```\n\n    *   **`shared[\"relationships\"]`:** A dictionary containing a summary of the project and a list of relationships between abstractions. This dictionary is created by the `AnalyzeRelationships` node in [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md). The relationships describe how the abstractions interact with each other. Example:\n\n        ```python\n        {\n            \"summary\": \"This project provides basic arithmetic operations: addition and multiplication.  It allows users to perform simple calculations.\",\n            \"details\": [\n                {\n                    \"from\": 0,  # Index of the \"Addition\" abstraction\n                    \"to\": 1,  # Index of the \"Multiplication\" abstraction\n                    \"label\": \"None\"\n                }\n            ]\n        }\n        ```\n\n3.  **Putting it All Together:** The AI uses this structured information to understand the codebase and generate the tutorial content. For example, when writing a chapter about \"Addition,\" the AI can:\n\n    *   Look up the description of \"Addition\" in `shared[\"abstractions\"]`.\n    *   Find the relevant code file (`file1.py`) using the file index in `shared[\"abstractions\"]`.\n    *   Explain the relationship between \"Addition\" and \"Multiplication\" using the information in `shared[\"relationships\"]`.\n\n**Code Example: Accessing the Knowledge Base**\n\nLet's say you want to access the description of the \"Addition\" abstraction. Here's how you would do it:\n\n```python\n# Assuming 'shared' dictionary is already populated\nabstractions = shared[\"abstractions\"]\nfor abstraction in abstractions:\n    if abstraction[\"name\"] == \"Addition\":\n        print(abstraction[\"description\"])\n        break\n```\n\nThis code snippet iterates through the `shared[\"abstractions\"]` list and prints the description of the \"Addition\" abstraction.\n\n**Benefits and Considerations**\n\n*   **Benefits:**\n    *   **Structured Information:** Provides the AI with a clear and organized view of the codebase.\n    *   **Efficient Retrieval:** Allows the AI to quickly access relevant information.\n    *   **Improved Tutorial Quality:** Enables the AI to generate more accurate and informative tutorials.\n*   **Considerations:**\n    *   **Complexity:** Building a knowledge base for a large and complex codebase can be challenging.\n    *   **Maintenance:** The knowledge base needs to be updated as the codebase evolves.\n\n**Analogy: The Library of Code**\n\nImagine a vast library containing all the code in the world.\n\n*   **Codebase Crawling:**  Collecting the books (code files) for our library.\n*   **Knowledge Base Construction:**  Cataloging those books, creating a card catalog (the `shared` dictionary), and writing summaries of each book (abstractions and relationships).\n*   **AI-Powered Content Generation:**  Using the library's resources to write a new book (the tutorial) about a specific topic.\n\n**Conclusion**\n\nIn this chapter, we explored the concept of Knowledge Base Construction, which is crucial for enabling the AI to understand the codebase and generate high-quality tutorials. By extracting, organizing, and relating information, we create a structured knowledge base that the AI can use to access and retrieve relevant information, ultimately improving the tutorial generation process. In the next chapter, [Tutorial Content Combination](05_tutorial_content_combination.md), we'll see how the generated content is combined to create the final tutorial.\n```\n\nRelevant Code Snippets:\n--- File: nodes.py ---\nimport os\nimport yaml\nfrom pocketflow import Node, BatchNode\nfrom utils.crawl_github_files import crawl_github_files\nfrom utils.call_llm import call_llm \nfrom utils.crawl_local_files import crawl_local_files\n\n# Helper to get content for specific file indices\ndef get_content_for_indices(files_data, indices):\n    content_map = {}\n    for i in indices:\n        if 0 <= i < len(files_data):\n            path, content = files_data[i]\n            content_map[f\"{i} # {path}\"] = content # Use index + path as key for context\n    return content_map\n\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"local_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        if not project_name:\n            # Basic name derivation from URL or directory\n            if repo_url:\n                project_name = repo_url.split('/')[-1].replace('.git', '')\n            else:\n                project_name = os.path.basename(os.path.abspath(local_dir))\n            shared[\"project_name\"] = project_name\n\n        # Get file patterns directly from shared\n        include_patterns = shared[\"include_patterns\"]\n        exclude_patterns = shared[\"exclude_patterns\"]\n        max_file_size = shared[\"max_file_size\"]\n\n        return {\n            \"repo_url\": repo_url,\n            \"local_dir\": local_dir,\n            \"token\": shared.get(\"github_token\"),\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns,\n            \"max_file_size\": max_file_size,\n            \"use_relative_paths\": True\n        }\n\n    def exec(self, prep_res):\n        if prep_res[\"repo_url\"]:\n            print(f\"Crawling repository: {prep_res['repo_url']}...\")\n            result = crawl_github_files(\n                repo_url=prep_res[\"repo_url\"],\n                token=prep_res[\"token\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        else:\n            print(f\"Crawling directory: {prep_res['local_dir']}...\")\n            result = crawl_local_files(\n                directory=prep_res[\"local_dir\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n            \n        # Convert dict to list of tuples: [(path, content), ...]\n        files_list = list(result.get(\"files\", {}).items())\n        print(f\"Fetched {len(files_list)} files.\")\n        return files_list\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"files\"] = exec_res # List of (path, content) tuples\n\nclass IdentifyAbstractions(Node):\n    def prep(self, shared):\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        \n        # Helper to create context from files, respecting limits (basic example)\n        def create_llm_context(files_data):\n            context = \"\"\n            file_info = [] # Store tuples of (index, path)\n            for i, (path, content) in enumerate(files_data):\n                entry = f\"--- File Index {i}: {path} ---\\n{content}\\n\\n\"\n                context += entry\n                file_info.append((i, path))\n\n            return context, file_info # file_info is list of (index, path)\n\n        context, file_info = create_llm_context(files_data)\n        # Format file info for the prompt (comment is just a hint for LLM)\n        file_listing_for_prompt = \"\\n\".join([f\"- {idx} # {path}\" for idx, path in file_info])\n        return context, file_listing_for_prompt, len(files_data), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, file_listing_for_prompt, file_count, project_name = prep_res  # Unpack project name\n        print(\"Identifying abstractions using LLM...\")\n        prompt = f\"\"\"\nFor the project `{project_name}`:\n\nCodebase Context:\n{context}\n\nAnalyze the codebase context.\nIdentify the top 5-10 core most important abstractions to help those new to the codebase.\n\nFor each abstraction, provide:\n1. A concise `name`.\n2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\nList of file indices and paths present in the context:\n{file_listing_for_prompt}\n\nFormat the output as a YAML list of dictionaries:\n\n```yaml\n- name: Query Processing\n  description: | \n    Explains what the abstraction does.\n    It's like a central dispatcher routing requests.\n  file_indices:\n    - 0 # path/to/file1.py\n    - 3 # path/to/related.py\n- name: Query Optimization\n  description: |\n    Another core concept, similar to a blueprint for objects.\n  file_indices:\n    - 5 # path/to/another.js\n# ... up to 10 abstractions\n```\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        abstractions = yaml.safe_load(yaml_str)\n\n        if not isinstance(abstractions, list):\n            raise ValueError(\"LLM Output is not a list\")\n\n        validated_abstractions = []\n        for item in abstractions:\n            if not isinstance(item, dict) or not all(k in item for k in [\"name\", \"description\", \"file_indices\"]):\n                raise ValueError(f\"Missing keys in abstraction item: {item}\")\n            if not isinstance(item[\"description\"], str):\n                 raise ValueError(f\"description is not a string in item: {item}\")\n            if not isinstance(item[\"file_indices\"], list):\n                 raise ValueError(f\"file_indices is not a list in item: {item}\")\n\n            # Validate indices\n            validated_indices = []\n            for idx_entry in item[\"file_indices\"]:\n                 try:\n                     if isinstance(idx_entry, int):\n                         idx = idx_entry\n                     elif isinstance(idx_entry, str) and '#' in idx_entry:\n                          idx = int(idx_entry.split('#')[0].strip())\n                     else:\n                          idx = int(str(idx_entry).strip())\n\n                     if not (0 <= idx < file_count):\n                         raise ValueError(f\"Invalid file index {idx} found in item {item['name']}. Max index is {file_count - 1}.\")\n                     validated_indices.append(idx)\n                 except (ValueError, TypeError):\n                      raise ValueError(f\"Could not parse index from entry: {idx_entry} in item {item['name']}\")\n\n            item[\"files\"] = sorted(list(set(validated_indices)))\n            # Store only the required fields\n            validated_abstractions.append({\n                \"name\": item[\"name\"],\n                \"description\": item[\"description\"],\n                \"files\": item[\"files\"]\n            })\n\n        print(f\"Identified {len(validated_abstractions)} abstractions.\")\n        return validated_abstractions\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"abstractions\"] = exec_res # List of {\"name\": str, \"description\": str, \"files\": [int]}\n\nclass AnalyzeRelationships(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"] # Now contains 'files' list of indices\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n\n        # Create context with abstraction names, indices, descriptions, and relevant file snippets\n        context = \"Identified Abstractions:\\n\"\n        all_relevant_indices = set()\n        abstraction_info_for_prompt = []\n        for i, abstr in enumerate(abstractions):\n            # Use 'files' which contains indices directly\n            file_indices_str = \", \".join(map(str, abstr['files']))\n            info_line = f\"- Index {i}: {abstr['name']} (Relevant file indices: [{file_indices_str}])\\n  Description: {abstr['description']}\"\n            context += info_line + \"\\n\"\n            abstraction_info_for_prompt.append(f\"{i} # {abstr['name']}\")\n            all_relevant_indices.update(abstr['files'])\n\n        context += \"\\nRelevant File Snippets (Referenced by Index and Path):\\n\"\n        # Get content for relevant files using helper\n        relevant_files_content_map = get_content_for_indices(\n            files_data,\n            sorted(list(all_relevant_indices))\n        )\n        # Format file content for context\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path} ---\\n{content}\"\n            for idx_path, content in relevant_files_content_map.items()\n        )\n        context += file_context_str\n\n        return context, \"\\n\".join(abstraction_info_for_prompt), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, abstraction_listing, project_name = prep_res  # Unpack project name\n        print(\"Analyzing relationships using LLM...\")\n        prompt = f\"\"\"\nBased on the following abstractions and relevant code snippets from the project `{project_name}`:\n\nList of Abstraction Indices and Names:\n{abstraction_listing}\n\nContext (Abstractions, Descriptions, Code):\n{context}\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        relationships_data = yaml.safe_load(yaml_str)\n\n        if not isinstance(relationships_data, dict) or not all(k in relationships_data for k in [\"summary\", \"relationships\"]):\n            raise ValueError(\"LLM output is not a dict or missing keys ('summary', 'relationships')\")\n        if not isinstance(relationships_data[\"summary\"], str):\n             raise ValueError(\"summary is not a string\")\n        if not isinstance(relationships_data[\"relationships\"], list):\n             raise ValueError(\"relationships is not a list\")\n\n        # Validate relationships structure\n        validated_relationships = []\n        num_abstractions = len(abstraction_listing.split('\\n'))\n        for rel in relationships_data[\"relationships\"]:\n             # Check for 'label' key\n             if not isinstance(rel, dict) or not all(k in rel for k in [\"from_abstraction\", \"to_abstraction\", \"label\"]):\n                  raise ValueError(f\"Missing keys (expected from_abstraction, to_abstraction, label) in relationship item: {rel}\")\n             # Validate 'label' is a string\n             if not isinstance(rel[\"label\"], str):\n                  raise ValueError(f\"Relationship label is not a string: {rel}\")\n\n             # Validate indices\n             try:\n                 from_idx = int(str(rel[\"from_abstraction\"]).split('#')[0].strip())\n                 to_idx = int(str(rel[\"to_abstraction\"]).split('#')[0].strip())\n                 if not (0 <= from_idx < num_abstractions and 0 <= to_idx < num_abstractions):\n                      raise ValueError(f\"Invalid index in relationship: from={from_idx}, to={to_idx}. Max index is {num_abstractions-1}.\")\n                 validated_relationships.append({\n                     \"from\": from_idx,\n                     \"to\": to_idx,\n                     \"label\": rel[\"label\"] \n                 })\n             except (ValueError, TypeError):\n                  raise ValueError(f\"Could not parse indices from relationship: {rel}\")\n\n        print(\"Generated project summary and relationship details.\")\n        return {\n            \"summary\": relationships_data[\"summary\"],\n            \"details\": validated_relationships # Store validated, index-based relationships\n        }\n\n\n    def post(self, shared, prep_res, exec_res):\n        # Structure is now {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        shared[\"relationships\"] = exec_res\n\nclass OrderChapters(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"]\n        relationships = shared[\"relationships\"]\n        project_name = shared[\"project_name\"]  # Get project name\n\n        # Prepare context for the LLM\n        abstraction_info_for_prompt = []\n        for i, a in enumerate(abstractions):\n            abstraction_info_for_prompt.append(f\"- {i} # {a['name']}\")\n        abstraction_listing = \"\\n\".join(abstraction_info_for_prompt)\n\n        context = f\"Project Summary:\\n{relationships['summary']}\\n\\n\"\n        context += \"Relationships (Indices refer to abstractions above):\\n\"\n        for rel in relationships['details']:\n             from_name = abstractions[rel['from']]['name']\n             to_name = abstractions[rel['to']]['name']\n             # Use 'label' instead of 'desc'\n             context += f\"- From {rel['from']} ({from_name}) to {rel['to']} ({to_name}): {rel['label']}\\n\"\n\n        return abstraction_listing, context, len(abstractions), project_name\n\n    def exec(self, prep_res):\n        abstraction_listing, context, num_abstractions, project_name = prep_res\n        print(\"Determining chapter order using LLM...\")\n        prompt = f\"\"\"\nGiven the following project abstractions and their relationships for the project ```` {project_name} ````:\n\nAbstractions (Index # Name):\n{abstraction_listing}\n\nContext about relationships and project summary:\n{context}\n\nIf you are going to make a tutorial for ```` {project_name} ````, what is the best order to explain these abstractions, from first to last?\nIdeally, first explain those that are the most important or foundational, perhaps user-facing concepts or entry points. Then move to more detailed, lower-level implementation details or supporting concepts.\n\nOutput the ordered list of abstraction indices, including the name in a comment for clarity. Use the format `idx # AbstractionName`.\n\n```yaml\n- 2 # FoundationalConcept\n- 0 # CoreClassA\n- 1 # CoreClassB (uses CoreClassA)\n- ...\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        # Rely on Node's built-in retry/fallback\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        ordered_indices_raw = yaml.safe_load(yaml_str)\n\n        if not isinstance(ordered_indices_raw, list):\n            raise ValueError(\"LLM output is not a list\")\n\n        ordered_indices = []\n        seen_indices = set()\n        for entry in ordered_indices_raw:\n            try:\n                 if isinstance(entry, int):\n                     idx = entry\n                 elif isinstance(entry, str) and '#' in entry:\n                      idx = int(entry.split('#')[0].strip())\n                 else:\n                      idx = int(str(entry).strip())\n\n                 if not (0 <= idx < num_abstractions):\n                      raise ValueError(f\"Invalid index {idx} in ordered list. Max index is {num_abstractions-1}.\")\n                 if idx in seen_indices:\n                     raise ValueError(f\"Duplicate index {idx} found in ordered list.\")\n                 ordered_indices.append(idx)\n                 seen_indices.add(idx)\n\n            except (ValueError, TypeError):\n                 raise ValueError(f\"Could not parse index from ordered list entry: {entry}\")\n\n        # Check if all abstractions are included\n        if len(ordered_indices) != num_abstractions:\n             raise ValueError(f\"Ordered list length ({len(ordered_indices)}) does not match number of abstractions ({num_abstractions}). Missing indices: {set(range(num_abstractions)) - seen_indices}\")\n\n        print(f\"Determined chapter order (indices): {ordered_indices}\")\n        return ordered_indices # Return the list of indices\n\n    def post(self, shared, prep_res, exec_res):\n        # exec_res is already the list of ordered indices\n        shared[\"chapter_order\"] = exec_res # List of indices\n\nclass WriteChapters(BatchNode):\n    def prep(self, shared):\n        chapter_order = shared[\"chapter_order\"] # List of indices\n        abstractions = shared[\"abstractions\"]   # List of dicts, now using 'files' with indices\n        files_data = shared[\"files\"]\n        # Get already written chapters to provide context\n        # We store them temporarily during the batch run, not in shared memory yet\n        # The 'previous_chapters_summary' will be built progressively in the exec context\n        self.chapters_written_so_far = [] # Use instance variable for temporary storage across exec calls\n\n        # Create a complete list of all chapters\n        all_chapters = []\n        chapter_filenames = {} # Store chapter filename mapping for linking\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                chapter_num = i + 1\n                chapter_name = abstractions[abstraction_index][\"name\"]\n                # Create safe filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in chapter_name).lower()\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                # Format with link\n                all_chapters.append(f\"{chapter_num}. [{chapter_name}]({filename})\")\n                # Store mapping of chapter index to filename for linking\n                chapter_filenames[abstraction_index] = {\"num\": chapter_num, \"name\": chapter_name, \"filename\": filename}\n        \n        # Create a formatted string with all chapters\n        full_chapter_listing = \"\\n\".join(all_chapters)\n\n        items_to_process = []\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                abstraction_details = abstractions[abstraction_index]\n                # Use 'files' (list of indices) directly\n                related_file_indices = abstraction_details.get(\"files\", [])\n                # Get content using helper, passing indices\n                related_files_content_map = get_content_for_indices(files_data, related_file_indices)\n                \n                # Get previous chapter info for transitions\n                prev_chapter = None\n                if i > 0:\n                    prev_idx = chapter_order[i-1]\n                    prev_chapter = chapter_filenames[prev_idx]\n                \n                # Get next chapter info for transitions\n                next_chapter = None\n                if i < len(chapter_order) - 1:\n                    next_idx = chapter_order[i+1]\n                    next_chapter = chapter_filenames[next_idx]\n\n                items_to_process.append({\n                    \"chapter_num\": i + 1,\n                    \"abstraction_index\": abstraction_index,\n                    \"abstraction_details\": abstraction_details,\n                    \"related_files_content_map\": related_files_content_map,\n                    \"project_name\": shared[\"project_name\"],  # Add project name\n                    \"full_chapter_listing\": full_chapter_listing,  # Add the full chapter listing\n                    \"chapter_filenames\": chapter_filenames,  # Add chapter filenames mapping\n                    \"prev_chapter\": prev_chapter,  # Add previous chapter info\n                    \"next_chapter\": next_chapter,  # Add next chapter info\n                    # previous_chapters_summary will be added dynamically in exec\n                })\n            else:\n                print(f\"Warning: Invalid abstraction index {abstraction_index} in chapter_order. Skipping.\")\n\n        print(f\"Preparing to write {len(items_to_process)} chapters...\")\n        return items_to_process # Iterable for BatchNode\n\n    def exec(self, item):\n        # This runs for each item prepared above\n        abstraction_name = item[\"abstraction_details\"][\"name\"]\n        chapter_num = item[\"chapter_num\"]\n        project_name = item.get(\"project_name\")  # Get from item\n        print(f\"Writing chapter {chapter_num} for: {abstraction_name} using LLM...\")\n\n        # Prepare file context string from the map\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path.split('# ')[1] if '# ' in idx_path else idx_path} ---\\n{content}\"\n            for idx_path, content in item[\"related_files_content_map\"].items()\n        )\n\n        # Get summary of chapters written *before* this one\n        # Use the temporary instance variable\n        previous_chapters_summary = \"\\n---\\n\".join(self.chapters_written_so_far)\n\n\n        prompt = f\"\"\"\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `{project_name}` about the concept: \"{abstraction_name}\". This is Chapter {chapter_num}.\n\nConcept Details:\n- Description:\n{item[\"abstraction_details\"][\"description\"]}\n\nComplete Tutorial Structure:\n{item[\"full_chapter_listing\"]}\n\nContext from previous chapters (summary):\n{previous_chapters_summary if previous_chapters_summary else \"This is the first chapter.\"}\n\nRelevant Code Snippets:\n{file_context_str if file_context_str else \"No specific code snippets provided for this abstraction.\"}\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter {chapter_num}: {abstraction_name}`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n\"\"\"\n        chapter_content = call_llm(prompt)\n        # Basic validation/cleanup\n        actual_heading = f\"# Chapter {chapter_num}: {abstraction_name}\"\n        if not chapter_content.strip().startswith(f\"# Chapter {chapter_num}\"):\n             # Add heading if missing or incorrect, trying to preserve content\n             lines = chapter_content.strip().split('\\n')\n             if lines and lines[0].strip().startswith(\"#\"): # If there's some heading, replace it\n                 lines[0] = actual_heading\n                 chapter_content = \"\\n\".join(lines)\n             else: # Otherwise, prepend it\n                 chapter_content = f\"{actual_heading}\\n\\n{chapter_content}\"\n\n        # Add the generated content to our temporary list for the next iteration's context\n        self.chapters_written_so_far.append(chapter_content)\n\n        return chapter_content # Return the Markdown string\n\n    def post(self, shared, prep_res, exec_res_list):\n        # exec_res_list contains the generated Markdown for each chapter, in order\n        shared[\"chapters\"] = exec_res_list\n        # Clean up the temporary instance variable\n        del self.chapters_written_so_far\n        print(f\"Finished writing {len(exec_res_list)} chapters.\")\n\nclass CombineTutorial(Node):\n    def prep(self, shared):\n        project_name = shared[\"project_name\"]\n        output_base_dir = shared.get(\"output_dir\", \"output\") # Default output dir\n        output_path = os.path.join(output_base_dir, project_name)\n        repo_url = shared[\"repo_url\"]  # Get the repository URL\n\n        # Use 'label' from relationships_data['details']\n        relationships_data = shared[\"relationships\"] # {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        chapter_order = shared[\"chapter_order\"] # indices\n        abstractions = shared[\"abstractions\"]   # list of dicts\n        chapters_content = shared[\"chapters\"]   # list of strings\n\n        # --- Generate Mermaid Diagram ---\n        mermaid_lines = [\"flowchart TD\"]\n        # Add nodes for each abstraction\n        for i, abstr in enumerate(abstractions):\n            # Sanitize name for Mermaid ID and label\n            node_id = f\"A{i}\"\n            sanitized_name = abstr['name'].replace('\"', '')\n            node_label = sanitized_name # Using sanitized name only, no index\n            mermaid_lines.append(f'    {node_id}[\"{node_label}\"]')\n        # Add edges for relationships using 'label'\n        for rel in relationships_data['details']:\n            from_node_id = f\"A{rel['from']}\"\n            to_node_id = f\"A{rel['to']}\"\n            # Sanitize 'label' for edge label\n            edge_label = rel['label'].replace('\"', '').replace('\\n', ' ') # Basic sanitization\n            # Limit edge label length for readability (optional, but good for diagrams)\n            max_label_len = 30 # Make it shorter for labels\n            if len(edge_label) > max_label_len:\n                edge_label = edge_label[:max_label_len-3] + \"...\"\n            mermaid_lines.append(f'    {from_node_id} -- \"{edge_label}\" --> {to_node_id}')\n\n        mermaid_diagram = \"\\n\".join(mermaid_lines)\n        # --- End Mermaid ---\n\n\n        # Prepare index.md content\n        index_content = f\"# Tutorial: {project_name}\\n\\n\"\n        index_content += f\"{relationships_data['summary']}\\n\\n\"\n        index_content += f\"**Source Repository:** [{repo_url}]({repo_url})\\n\\n\"\n\n        # Add Mermaid diagram for relationships\n        index_content += \"```mermaid\\n\"\n        index_content += mermaid_diagram + \"\\n\"\n        index_content += \"```\\n\\n\"\n\n        index_content += \"## Chapters\\n\\n\"\n\n        chapter_files = []\n        # Generate chapter links based on the determined order\n        for i, abstraction_index in enumerate(chapter_order):\n            # Ensure index is valid and we have content for it\n            if 0 <= abstraction_index < len(abstractions) and i < len(chapters_content):\n                abstraction_name = abstractions[abstraction_index][\"name\"]\n                # Sanitize name for filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in abstraction_name).lower()\n                # Use chapter number (i+1) for ordering filename\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                index_content += f\"{i+1}. [{abstraction_name}]({filename})\\n\"\n                \n                # Add attribution to chapter content\n                chapter_content = chapters_content[i]\n                if not chapter_content.endswith(\"\\n\\n\"):\n                    chapter_content += \"\\n\\n\"\n                chapter_content += \"---\\n\\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\"\n                \n                # Store filename and corresponding content\n                chapter_files.append({\"filename\": filename, \"content\": chapter_content})\n            else:\n                 print(f\"Warning: Mismatch between chapter order, abstractions, or content at index {i} (abstraction index {abstraction_index}). Skipping file generation for this entry.\")\n\n        # Add attribution to index content\n        index_content += \"\\n\\n---\\n\\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\"\n\n        return {\n            \"output_path\": output_path,\n            \"index_content\": index_content,\n            \"chapter_files\": chapter_files # List of {\"filename\": str, \"content\": str}\n        }\n\n    def exec(self, prep_res):\n        output_path = prep_res[\"output_path\"]\n        index_content = prep_res[\"index_content\"]\n        chapter_files = prep_res[\"chapter_files\"]\n\n        print(f\"Combining tutorial into directory: {output_path}\")\n        # Rely on Node's built-in retry/fallback\n        os.makedirs(output_path, exist_ok=True)\n\n        # Write index.md\n        index_filepath = os.path.join(output_path, \"index.md\")\n        with open(index_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(index_content)\n        print(f\"  - Wrote {index_filepath}\")\n\n        # Write chapter files\n        for chapter_info in chapter_files:\n            chapter_filepath = os.path.join(output_path, chapter_info[\"filename\"])\n            with open(chapter_filepath, \"w\", encoding=\"utf-8\") as f:\n                f.write(chapter_info[\"content\"])\n            print(f\"  - Wrote {chapter_filepath}\")\n\n        return output_path # Return the final path\n\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"final_output_dir\"] = exec_res # Store the output path\n        print(f\"\\nTutorial generation complete! Files are in: {exec_res}\")\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 5: Tutorial Content Combination`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "```markdown\n# Chapter 5: Tutorial Content Combination\n\nIn the previous chapter, [Knowledge Base Construction](04_knowledge_base_construction.md), we learned how to organize the extracted information into a structured format, acting as a foundation for the AI to build upon. Now, we'll see how all the individual pieces of content generated thus far (the chapters, the introduction, the overview diagram) are brought together to form a cohesive and complete tutorial!\n\nImagine you've baked all the layers of a cake, made the frosting, and have a beautiful decoration ready. Tutorial Content Combination is like the final step of assembling the cake, frosting it, and adding the decorations to create a finished product ready to be enjoyed.\n\n**Why Combine the Content?**\n\nWhile the AI has generated individual chapters, they are just pieces of a larger puzzle. Tutorial Content Combination ensures:\n\n*   **Coherence:**  The chapters flow logically and smoothly from one to the next.\n*   **Completeness:**  The tutorial includes an introduction, table of contents, and a high-level overview, providing a complete learning experience.\n*   **Readability:**  The final product is well-formatted and easy to read.\n\n**The Goal: Polishing the Tutorial**\n\nOur `Tutorial Content Combination` component takes the generated chapters, table of contents, relationship diagram, and project summary to weave them into a polished, beginner-friendly tutorial. It's like an editor taking individual articles and assembling them into a coherent book.\n\n**Key Concepts**\n\nLet's break down the core ideas behind our content combination:\n\n1.  **`CombineTutorial` Node:** The heart of this process. This node is responsible for:\n    *   Generating an `index.md` file that serves as the tutorial's landing page.\n    *   Creating a table of contents that links to each chapter.\n    *   Including a high-level project summary from [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md).\n    *   Generating a Mermaid diagram visualizing the relationships between key concepts.\n    *   Writing each chapter to a separate `.md` file.\n\n2.  **`index.md`:**  The main entry point to the tutorial, containing the project summary, table of contents, and a diagram representing the project's structure. Think of it as the book's cover and introduction.\n\n3.  **Mermaid Diagram:** A visual representation of the relationships between the core abstractions in the codebase, created using the Mermaid syntax.  This provides a high-level overview of the project's architecture.\n\n4.  **Chapter Files (`*.md`):** Each chapter is saved as a separate Markdown file, allowing for easy navigation and editing.\n\n**How it Works: Assembling the Tutorial**\n\nHere's a step-by-step look at how the `CombineTutorial` node assembles the tutorial:\n\n1.  **The `CombineTutorial` Node:** This node orchestrates the entire content combination process.\n\n    *   Here's a simplified snippet of the `CombineTutorial` node's `prep` and `exec` methods (from `nodes.py`):\n\n     ```python\n     class CombineTutorial(Node):\n         def prep(self, shared):\n             project_name = shared[\"project_name\"]\n             output_base_dir = shared.get(\"output_dir\", \"output\") # Default output dir\n             output_path = os.path.join(output_base_dir, project_name)\n             repo_url = shared[\"repo_url\"]  # Get the repository URL\n\n             # Use 'label' from relationships_data['details']\n             relationships_data = shared[\"relationships\"] # {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n             chapter_order = shared[\"chapter_order\"] # indices\n             abstractions = shared[\"abstractions\"]   # list of dicts\n             chapters_content = shared[\"chapters\"]   # list of strings\n\n             # --- Generate Mermaid Diagram ---\n             mermaid_lines = [\"flowchart TD\"]\n             # Add nodes for each abstraction\n             for i, abstr in enumerate(abstractions):\n                 # Sanitize name for Mermaid ID and label\n                 node_id = f\"A{i}\"\n                 sanitized_name = abstr['name'].replace('\"', '')\n                 node_label = sanitized_name # Using sanitized name only, no index\n                 mermaid_lines.append(f'    {node_id}[\"{node_label}\"]')\n             # Add edges for relationships using 'label'\n             for rel in relationships_data['details']:\n                 from_node_id = f\"A{rel['from']}\"\n                 to_node_id = f\"A{rel['to']}\"\n                 # Sanitize 'label' for edge label\n                 edge_label = rel['label'].replace('\"', '').replace('\\n', ' ') # Basic sanitization\n                 # Limit edge label length for readability (optional, but good for diagrams)\n                 max_label_len = 30 # Make it shorter for labels\n                 if len(edge_label) > max_label_len:\n                     edge_label = edge_label[:max_label_len-3] + \"...\"\n                 mermaid_lines.append(f'    {from_node_id} -- \"{edge_label}\" --> {to_node_id}')\n\n             mermaid_diagram = \"\\n\".join(mermaid_lines)\n             # --- End Mermaid ---\n\n\n             # Prepare index.md content\n             index_content = f\"# Tutorial: {project_name}\\n\\n\"\n             index_content += f\"{relationships_data['summary']}\\n\\n\"\n             index_content += f\"**Source Repository:** [{repo_url}]({repo_url})\\n\\n\"\n\n             # Add Mermaid diagram for relationships\n             index_content += \"```mermaid\\n\"\n             index_content += mermaid_diagram + \"\\n\"\n             index_content += \"```\\n\\n\"\n\n             index_content += \"## Chapters\\n\\n\"\n\n             chapter_files = []\n             # Generate chapter links based on the determined order\n             for i, abstraction_index in enumerate(chapter_order):\n                 # Ensure index is valid and we have content for it\n                 if 0 <= abstraction_index < len(abstractions) and i < len(chapters_content):\n                     abstraction_name = abstractions[abstraction_index][\"name\"]\n                     # Sanitize name for filename\n                     safe_name = \"\".join(c if c.isalnum() else '_' for c in abstraction_name).lower()\n                     # Use chapter number (i+1) for ordering filename\n                     filename = f\"{i+1:02d}_{safe_name}.md\"\n                     index_content += f\"{i+1}. [{abstraction_name}]({filename})\\n\"\n                     \n                     # Add attribution to chapter content\n                     chapter_content = chapters_content[i]\n                     if not chapter_content.endswith(\"\\n\\n\"):\n                         chapter_content += \"\\n\\n\"\n                     chapter_content += \"---\\n\\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\"\n                     \n                     # Store filename and corresponding content\n                     chapter_files.append({\"filename\": filename, \"content\": chapter_content})\n                 else:\n                      print(f\"Warning: Mismatch between chapter order, abstractions, or content at index {i} (abstraction index {abstraction_index}). Skipping file generation for this entry.\")\n\n             # Add attribution to index content\n             index_content += \"\\n\\n---\\n\\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\"\n\n             return {\n                 \"output_path\": output_path,\n                 \"index_content\": index_content,\n                 \"chapter_files\": chapter_files # List of {\"filename\": str, \"content\": str}\n             }\n\n         def exec(self, prep_res):\n             output_path = prep_res[\"output_path\"]\n             index_content = prep_res[\"index_content\"]\n             chapter_files = prep_res[\"chapter_files\"]\n\n             print(f\"Combining tutorial into directory: {output_path}\")\n             # Rely on Node's built-in retry/fallback\n             os.makedirs(output_path, exist_ok=True)\n\n             # Write index.md\n             index_filepath = os.path.join(output_path, \"index.md\")\n             with open(index_filepath, \"w\", encoding=\"utf-8\") as f:\n                 f.write(index_content)\n             print(f\"  - Wrote {index_filepath}\")\n\n             # Write chapter files\n             for chapter_info in chapter_files:\n                 chapter_filepath = os.path.join(output_path, chapter_info[\"filename\"])\n                 with open(chapter_filepath, \"w\", encoding=\"utf-8\") as f:\n                     f.write(chapter_info[\"content\"])\n                 print(f\"  - Wrote {chapter_filepath}\")\n\n             return output_path # Return the final path\n\n         def post(self, shared, prep_res, exec_res):\n             shared[\"final_output_dir\"] = exec_res # Store the output path\n             print(f\"\\nTutorial generation complete! Files are in: {exec_res}\")\n     ```\n\n    *   The `prep` method retrieves the project name, output directory, repository URL, relationships data, chapter order, abstractions, and chapters content from the `shared` dictionary.  It then generates the Mermaid diagram representing the relationships between core abstractions and prepares the content for the `index.md` file, including the tutorial's table of contents.  Finally, it formats the chapter content to include proper attribution.\n\n    *   The `exec` method takes the prepared data and writes the `index.md` file and each chapter file to the specified output directory.\n\n2.  **Generating `index.md`:** The `CombineTutorial` node creates the `index.md` file in the output directory. This file includes:\n    *   A title (e.g., `# Tutorial: MyCoolProject`).\n    *   The project summary generated by [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md).\n    *   A link to the source repository.\n    *   The Mermaid diagram illustrating the relationships between core concepts.\n    *   A table of contents, listing each chapter with a link to its corresponding `.md` file.\n\n3.  **Creating the Mermaid Diagram:**\n\n    *   The node generates a Mermaid diagram based on the relationships between the abstractions in the codebase.\n\n    *   Example of a Mermaid diagram:\n\n        ```mermaid\n        flowchart TD\n            A0[\"Addition\"]\n            A1[\"Multiplication\"]\n            A0 -- \"None\" --> A1\n        ```\n\n        *   In this example, `A0` and `A1` represent the \"Addition\" and \"Multiplication\" abstractions, respectively. The `-->` arrow indicates a relationship between them.\n\n4.  **Writing Chapter Files:** The `CombineTutorial` node saves each chapter as a separate Markdown file in the output directory. The filenames are numbered to reflect the chapter order (e.g., `01_addition.md`, `02_multiplication.md`).  Each chapter includes proper attribution to the `AI Codebase Knowledge Builder`.\n\n**Benefits and Considerations**\n\n*   **Benefits:**\n    *   **Automated Assembly:** Automates the process of combining individual pieces of content into a complete tutorial.\n    *   **Improved Coherence:** Ensures that the tutorial flows logically and smoothly.\n    *   **Enhanced Readability:** Provides a well-formatted and easy-to-read final product.\n*   **Considerations:**\n    *   **Customization:** The generated `index.md` and chapter files can be further customized to match specific formatting preferences or branding guidelines.\n    *   **Diagram Complexity:** For very large and complex codebases, the Mermaid diagram may become difficult to read.\n\n**Code Example: Generating the Table of Contents**\n\nLet's look at the code that generates the table of contents in the `index.md` file:\n\n```python\n# Generate chapter links based on the determined order\nfor i, abstraction_index in enumerate(chapter_order):\n    # Ensure index is valid and we have content for it\n    if 0 <= abstraction_index < len(abstractions) and i < len(chapters_content):\n        abstraction_name = abstractions[abstraction_index][\"name\"]\n        # Sanitize name for filename\n        safe_name = \"\".join(c if c.isalnum() else '_' for c in abstraction_name).lower()\n        # Use chapter number (i+1) for ordering filename\n        filename = f\"{i+1:02d}_{safe_name}.md\"\n        index_content += f\"{i+1}. [{abstraction_name}]({filename})\\n\"\n```\n\nThis code snippet iterates through the `chapter_order` list and generates a Markdown link for each chapter in the correct order.  It also creates a safe filename for each chapter by replacing non-alphanumeric characters with underscores.\n\n**Final Result**\n\nAfter running the `CombineTutorial` node, you will have a complete tutorial in the specified output directory. The tutorial will include:\n\n*   An `index.md` file with a project summary, table of contents, and diagram.\n*   A set of `.md` files, each containing a chapter of the tutorial.\n\n**Conclusion**\n\nIn this chapter, we explored the concept of Tutorial Content Combination, the final step in our tutorial generation process. By assembling the generated content into a cohesive and complete tutorial, we ensure that the final product is well-organized, easy to read, and provides a valuable learning experience for beginners.\n\nCongratulations! You have now seen all the stages from [Main Execution Script](01_main_execution_script.md), [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md), [Codebase Crawling](03_codebase_crawling.md), [Knowledge Base Construction](04_knowledge_base_construction.md), and [Tutorial Content Combination](05_tutorial_content_combination.md).\n\nIn the next chapter, we will briefly discuss [LLM Orchestration (PocketFlow)](06_llm_orchestration__pocketflow_.md) used in this project, and then discuss how to use the entire system to generate a tutorial.\n```\n", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `Tutorial-Codebase-Knowledge` about the concept: \"LLM Orchestration (PocketFlow)\". This is Chapter 6.\n\nConcept Details:\n- Description:\nThis abstraction manages the sequence of operations required to generate the codebase tutorial using a directed acyclic graph. It's similar to a workflow management system, where each step (node) performs a specific task, like fetching code or writing chapters, and the flow dictates how data passes between them. It simplifies running large language model based applications.\n\n\nComplete Tutorial Structure:\n1. [Main Execution Script](01_main_execution_script.md)\n2. [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md)\n3. [Codebase Crawling](03_codebase_crawling.md)\n4. [Knowledge Base Construction](04_knowledge_base_construction.md)\n5. [Tutorial Content Combination](05_tutorial_content_combination.md)\n6. [LLM Orchestration (PocketFlow)](06_llm_orchestration__pocketflow_.md)\n\nContext from previous chapters (summary):\n# Chapter 1: Main Execution Script\n\nWelcome to the first chapter of our tutorial on building a tutorial generation system! This chapter focuses on the \"Main Execution Script,\" the conductor of our orchestra that makes everything happen.\n\nImagine you want to create a tutorial for a cool new Python library you found on GitHub.  Where do you even begin?  That's where the Main Execution Script comes in. It's the starting point of our whole tutorial-making process. It takes your instructions (like the GitHub link) and kicks off all the other parts of the system to create your tutorial.\n\n**What Problem Does It Solve?**\n\nWithout a central entry point, we'd have a bunch of disconnected pieces. The Main Execution Script brings order to chaos. It:\n\n*   **Takes your input:** Where is the code? What do you want to name the tutorial?\n*   **Sets the stage:** Gets everything ready for the tutorial generation.\n*   **Kicks off the process:** Starts all the other parts of the system in the right order.\n\n**Key Concepts**\n\nLet's break down the key concepts of the Main Execution Script:\n\n1.  **Command-Line Arguments:** These are the instructions you give the script when you run it.  Think of them as telling the script *what* to do.\n\n2.  **Configuration:**  This is setting up all the necessary components, like where to save the output.\n\n3.  **Orchestration:** This is the script's main job: making sure all the other parts of the system run in the correct sequence, like a conductor leading an orchestra. We use [LLM Orchestration (PocketFlow)](06_llm_orchestration__pocketflow_.md) for this.\n\n**Using the Main Execution Script**\n\nLet's see how you'd use the Main Execution Script to generate a tutorial for a GitHub repository.\n\n**Example Input:**\n\nLet's say you want to create a tutorial for a repository named \"my-cool-project\" on GitHub, located at `https://github.com/user/my-cool-project`. You'd run the script from your terminal like this:\n\n```bash\npython main.py --repo https://github.com/user/my-cool-project -n MyCoolProjectTutorial\n```\n\nLet's break down that command:\n\n*   `python main.py`:  This tells your computer to run the `main.py` file using Python.\n*   `--repo https://github.com/user/my-cool-project`:  This tells the script the URL of the GitHub repository.\n*   `-n MyCoolProjectTutorial`: This tells the script to name the project \"MyCoolProjectTutorial\".  If you leave this out, the script will try to figure out the name from the repository URL.\n\n**Example Output:**\n\nAfter running the command, the script will:\n\n1.  Download the code from the GitHub repository.\n2.  Analyze the code.\n3.  Create a tutorial based on the code.\n4.  Save the tutorial in a directory named \"output\" (unless you specify a different output directory).\n\nYou'll find a structured tutorial with different chapters covering the codebase's essential aspects in the `output` directory!\n\n**Code Walkthrough**\n\nLet's look at some parts of the `main.py` script:\n\n```python\nimport argparse\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Generate a tutorial...\")\n    parser.add_argument(\"--repo\", help=\"URL of the public GitHub repository.\")\n    parser.add_argument(\"-n\", \"--name\", help=\"Project name (optional...\")\n    args = parser.parse_args()\n\n    # --- Rest of the Code ---\n```\n\nThis code sets up the command-line argument parsing.  It defines the arguments we can use (like `--repo` and `-n`) and provides helpful descriptions. The `argparse` module makes it easy to get the values you provide when you run the script.\n\n```python\n    shared = {\n        \"repo_url\": args.repo,\n        \"project_name\": args.name,\n        \"output_dir\": \"output\",  # Simplification\n    }\n\n    print(f\"Starting tutorial generation for: {args.repo}\")\n\n    # --- Rest of the Code ---\n```\n\nThis code creates a `shared` dictionary, a central place to store information that all parts of the system need.  We store the repository URL, the project name, and the output directory here.  Think of it as a shared whiteboard where all the different parts of the tutorial generation system can read and write information.\n\n```python\n    # Create the flow instance\n    tutorial_flow = create_tutorial_flow()\n\n    # Run the flow\n    tutorial_flow.run(shared)\n```\n\nThis is where the magic happens! We create a `tutorial_flow` (which we will discuss more in [LLM Orchestration (PocketFlow)](06_llm_orchestration__pocketflow_.md)) and tell it to run, passing in the `shared` dictionary. The `tutorial_flow` orchestrates all the other steps to generate the tutorial.\n\n**Internal Implementation: A Step-by-Step View**\n\nHere's a simplified view of what happens inside the Main Execution Script when you run it:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant MainScript as Main Execution Script\n    participant ArgumentParser as Argument Parser\n    participant Flow as Tutorial Flow\n    \n    User->>MainScript: Run script with arguments\n    MainScript->>ArgumentParser: Parse arguments\n    ArgumentParser-->>MainScript: Return arguments\n    MainScript->>Flow: Create tutorial flow\n    MainScript->>Flow: Run flow with arguments\n    Flow-->>MainScript: Tutorial generated\n    MainScript->>User: Completion Message\n```\n\n1.  **User:** You run the `main.py` script with command-line arguments (e.g., the GitHub repository URL).\n2.  **Main Execution Script:** The script receives your command and uses `ArgumentParser` to understand your instructions.\n3.  **Argument Parser:** The argument parser takes the command-line arguments and organizes them into a usable format.\n4.  **Main Execution Script:**  The script then creates an instance of the `Tutorial Flow`. The script then runs the `Tutorial Flow`.\n5.  **Tutorial Flow:** The tutorial flow coordinates with other components like the [Codebase Crawling](03_codebase_crawling.md), [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md), and [Knowledge Base Construction](04_knowledge_base_construction.md) components to create the tutorial.\n6.  **Main Execution Script:** The script prints a completion message, indicating that the tutorial has been generated.\n\n**Implementation Details**\n\nThe `create_tutorial_flow()` function, defined in `flow.py` (not shown in the code snippets above), is responsible for creating the `tutorial_flow` instance.\n\n**Conclusion**\n\nIn this chapter, we explored the Main Execution Script, the entry point of our tutorial generation system. We learned how it takes user input, configures the system, and orchestrates the tutorial generation process.  It's the conductor that makes sure all the instruments play in harmony!\n\nIn the next chapter, we'll dive into the crucial component of [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md), where we'll see how AI helps us understand the codebase and generate meaningful tutorial content.\n\n---\n# Chapter 2: AI-Powered Code Analysis and Generation\n\nIn the previous chapter, [Main Execution Script](01_main_execution_script.md), we saw how the script acts as the conductor of our tutorial-generation process, taking your input and orchestrating the workflow. Now, we'll dive into a crucial part of that workflow: using AI to understand the code and generate content!\n\n**Why AI for Code Analysis and Generation?**\n\nImagine you're given a huge, complex Lego set with thousands of pieces and no instructions. That's what it's like trying to understand a large codebase!  It's overwhelming.  The goal of this chapter is to learn how to use AI to understand any codebase and describe it so that others can also understand it.\n\nThat's where AI comes in! Our `AI-Powered Code Analysis and Generation` component acts like an experienced tutor. It uses Large Language Models (LLMs) to:\n\n*   **Analyze the code:**  Like carefully examining each Lego brick.\n*   **Identify key components:**  Finding the essential parts of the set.\n*   **Generate beginner-friendly explanations:**  Writing clear, step-by-step instructions.\n\n**The Big Picture: Turning Code into Tutorials**\n\nThis component sits in the middle of our tutorial generation process. It takes raw code (from crawling the codebase) and turns it into structured, understandable tutorial content. We start by downloading the source code, then we'll use this component to create a tutorial.\n\n**Key Concepts**\n\nLet's break down the core ideas behind our AI-powered approach:\n\n1.  **Large Language Models (LLMs):** These are powerful AI models trained on vast amounts of text data. Think of them as having read millions of books and articles on programming.  We use them to understand code and write explanations. We pass the prompts to the LLM through the `call_llm` function, which is defined in `utils/call_llm.py`. This allows us to easily switch models by changing the environment variables for the model.\n\n2.  **Code Abstraction Identification:** The AI identifies key concepts or modules within the codebase. This is like finding the most important functions or classes.\n\n3.  **Relationship Analysis:** The AI determines how these components interact with each other.  This is like understanding how the Lego bricks fit together to build the final model.\n\n4.  **Beginner-Friendly Content Generation:** The AI translates complex code into simple, easy-to-understand explanations, tailored for beginners. It generates markdown content that is later combined to build a chapter.\n\n**How it Works: A Simplified Walkthrough**\n\nLet's walk through a simplified example using pieces of the code, starting from what it receives from the main script.\n\n```python\nshared = {\n  \"repo_url\": \"https://github.com/user/my-cool-project\",\n  \"project_name\": \"MyCoolProject\",\n  \"files\": [(\"file1.py\", \"def add(a, b): return a + b\"), (\"file2.py\", \"def multiply(a, b): return a * b\")],\n}\n```\n\nThis `shared` dictionary is passed between different components, as we saw in the [Main Execution Script](01_main_execution_script.md) chapter. We will focus on how the `IdentifyAbstractions` node processes the `shared` data.\n\n1.  **Fetching Code:** We already have the source code (simulated as `shared[\"files\"]` here).  In reality, this would come from the [Codebase Crawling](03_codebase_crawling.md) stage.\n\n2.  **Identifying Abstractions (IdentifyAbstractions Node):**\n\n   *   This node takes the codebase's content and asks the LLM to find the most important concepts.\n\n   *   It prepares a prompt for the LLM that includes the entire codebase context.\n\n   *   Here's a simplified snippet of the `IdentifyAbstractions` node's `prep` and `exec` methods (from `nodes.py`):\n\n     ```python\n     class IdentifyAbstractions(Node):\n         def prep(self, shared):\n             files_data = shared[\"files\"]\n             project_name = shared[\"project_name\"]\n             # simplified context creation\n             context = \"\"\n             file_info = []\n             for i, (path, content) in enumerate(files_data):\n                 context += f\"--- File Index {i}: {path} ---\\n{content}\\n\\n\"\n                 file_info.append((i, path))\n\n             file_listing_for_prompt = \"\\n\".join([f\"- {idx} # {path}\" for idx, path in file_info])\n             return context, file_listing_for_prompt, len(files_data), project_name\n\n         def exec(self, prep_res):\n             context, file_listing_for_prompt, file_count, project_name = prep_res\n\n             prompt = f\"\"\"\n     For the project `{project_name}`:\n\n     Codebase Context:\n     {context}\n\n     Analyze the codebase context.\n     Identify the top 2 core most important abstractions to help those new to the codebase.\n\n     For each abstraction, provide:\n     1. A concise `name`.\n     2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n     3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\n     List of file indices and paths present in the context:\n     {file_listing_for_prompt}\n\n     Format the output as a YAML list of dictionaries:\n\n     ```yaml\n     - name: Addition\n       description: |\n         This abstraction performs addition.\n         It's like adding two numbers together.\n       file_indices:\n         - 0 # file1.py\n     - name: Multiplication\n       description: |\n         This abstraction performs multiplication.\n         It's like multiplying two numbers together.\n       file_indices:\n         - 1 # file2.py\n     ```\"\"\"\n             response = call_llm(prompt) #call llm\n\n             yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n             abstractions = yaml.safe_load(yaml_str)\n\n             validated_abstractions = []\n             for item in abstractions:\n                  item[\"files\"] = [int(idx_entry.split('#')[0].strip()) for idx_entry in item[\"file_indices\"]]\n                  validated_abstractions.append({\n                       \"name\": item[\"name\"],\n                       \"description\": item[\"description\"],\n                       \"files\": item[\"files\"]\n                  })\n\n             return validated_abstractions\n\n         def post(self, shared, prep_res, exec_res):\n             shared[\"abstractions\"] = exec_res\n     ```\n\n   *   The `prep` method prepares the context and project name for the LLM by listing all files and their code in the files_data list.\n\n   *   The `exec` method prompts the LLM to analyze the code and identify the important abstractions, generating a YAML formatted string with the abstraction name, description, and corresponding file indices. After receiving this string, the LLM output is parsed and the file indices are added to the `shared` dictionary, specifically at `shared[\"abstractions\"]`.\n\n   *   The LLM might respond with something like this (the LLM will come up with a description):\n\n     ```yaml\n     - name: Addition\n       description: |\n         This abstraction performs addition.\n         It's like adding two numbers together.\n       file_indices:\n         - 0 # file1.py\n     - name: Multiplication\n       description: |\n         This abstraction performs multiplication.\n         It's like multiplying two numbers together.\n       file_indices:\n         - 1 # file2.py\n     ```\n\n3.  **Analyzing Relationships (AnalyzeRelationships Node):**\n\n   *   The `AnalyzeRelationships` node determines how the identified abstractions relate to each other, and creates a summary for the whole project.\n\n   *   Here's a simplified snippet of the `AnalyzeRelationships` node's `prep` and `exec` methods (from `nodes.py`):\n\n     ```python\n     class AnalyzeRelationships(Node):\n         def prep(self, shared):\n             abstractions = shared[\"abstractions\"]\n             files_data = shared[\"files\"]\n             context = \"Identified Abstractions:\\n\"\n             for i, abstr in enumerate(abstractions):\n                 file_indices_str = \", \".join(map(str, abstr['files']))\n                 info_line = f\"- Index {i}: {abstr['name']} (Relevant file indices: [{file_indices_str}])\\n  Description: {abstr['description']}\"\n                 context += info_line + \"\\n\"\n\n             all_relevant_indices = set()\n             for abstr in abstractions:\n                  all_relevant_indices.update(abstr['files'])\n\n             relevant_files_content_map = {}\n             for i in sorted(list(all_relevant_indices)):\n                  path, content = files_data[i]\n                  relevant_files_content_map[f\"{i} # {path}\"] = content\n\n             context += \"\\nRelevant File Snippets (Referenced by Index and Path):\\n\"\n             file_context_str = \"\\n\\n\".join(\n                  f\"--- File: {idx_path} ---\\n{content}\"\n                  for idx_path, content in relevant_files_content_map.items()\n             )\n             context += file_context_str\n             abstraction_info_for_prompt = []\n             for i, abstr in enumerate(abstractions):\n                  abstraction_info_for_prompt.append(f\"{i} # {abstr['name']}\")\n             return context, \"\\n\".join(abstraction_info_for_prompt), shared[\"project_name\"]\n\n         def exec(self, prep_res):\n             context, abstraction_listing, project_name = prep_res\n\n             prompt = f\"\"\"\n     Based on the following abstractions and relevant code snippets from the project `{project_name}`:\n\n     List of Abstraction Indices and Names:\n     {abstraction_listing}\n\n     Context (Abstractions, Descriptions, Code):\n     {context}\n\n     Please provide:\n     1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n     2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n         - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n         - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n         - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n         Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n         Simplify the relationship and exclude those non-important ones.\n\n     IMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\n     Format the output as YAML:\n\n     ```yaml\n     summary: |\n       A brief, simple explanation of the project.\n       Can span multiple lines with **bold** and *italic* for emphasis.\n     relationships:\n       - from_abstraction: 0 # AbstractionName1\n         to_abstraction: 1 # AbstractionName2\n         label: \"Manages\"\n     ```\n\n     Now, provide the YAML output:\n     \"\"\"\n             response = call_llm(prompt) #call llm\n\n             yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n             relationships_data = yaml.safe_load(yaml_str)\n\n             validated_relationships = []\n             num_abstractions = len(abstraction_listing.split('\\n'))\n             for rel in relationships_data[\"relationships\"]:\n                  from_idx = int(str(rel[\"from_abstraction\"]).split('#')[0].strip())\n                  to_idx = int(str(rel[\"to_abstraction\"]).split('#')[0].strip())\n                  validated_relationships.append({\n                       \"from\": from_idx,\n                       \"to\": to_idx,\n                       \"label\": rel[\"label\"]\n                  })\n\n             return {\n                  \"summary\": relationships_data[\"summary\"],\n                  \"details\": validated_relationships\n             }\n\n         def post(self, shared, prep_res, exec_res):\n             shared[\"relationships\"] = exec_res\n     ```\n\n   *   The `prep` method formats the context and project name for the LLM by listing the abstraction descriptions and file content.\n\n   *   The `exec` method prompts the LLM to analyze the context and provide a summary and relationships. It parses the LLM output and adds a project summary and details (including what abstraction links to what) to the `shared` dictionary at `shared[\"relationships\"]`.\n\n   *   The LLM might respond with something like this (LLM generated):\n\n     ```yaml\n     summary: |\n       This project provides basic arithmetic operations: **addition** and **multiplication**.\n       It allows users to perform simple calculations.\n     relationships:\n       - from_abstraction: 0 # Addition\n         to_abstraction: 1 # Multiplication\n         label: \"None\"\n     ```\n\n4.  **Ordering Chapters (OrderChapters Node)**\n\n   *   The `OrderChapters` node asks the LLM what order to present the abstractions in.\n   *   Here's a simplified snippet of the `OrderChapters` node's `prep` and `exec` methods (from `nodes.py`):\n\n     ```python\n     class OrderChapters(Node):\n         def prep(self, shared):\n             abstractions = shared[\"abstractions\"]\n             relationships = shared[\"relationships\"]\n\n             abstraction_info_for_prompt = []\n             for i, a in enumerate(abstractions):\n                 abstraction_info_for_prompt.append(f\"- {i} # {a['name']}\")\n             abstraction_listing = \"\\n\".join(abstraction_info_for_prompt)\n\n             context = f\"Project Summary:\\n{relationships['summary']}\\n\\n\"\n             context += \"Relationships (Indices refer to abstractions above):\\n\"\n             for rel in relationships['details']:\n                  from_name = abstractions[rel['from']]['name']\n                  to_name = abstractions[rel['to']]['name']\n                  context += f\"- From {rel['from']} ({from_name}) to {rel['to']} ({to_name}): {rel['label']}\\n\"\n\n             return abstraction_listing, context, len(abstractions), shared[\"project_name\"]\n\n         def exec(self, prep_res):\n             abstraction_listing, context, num_abstractions, project_name = prep_res\n             prompt = f\"\"\"\n     Given the following project abstractions and their relationships for the project ```` {project_name} ````:\n\n     Abstractions (Index # Name):\n     {abstraction_listing}\n\n     Context about relationships and project summary:\n     {context}\n\n     If you are going to make a tutorial for ```` {project_name} ````, what is the best order to explain these abstractions, from first to last?\n     Ideally, first explain those that are the most important or foundational, perhaps user-facing concepts or entry points. Then move to more detailed, lower-level implementation details or supporting concepts.\n\n     Output the ordered list of abstraction indices, including the name in a comment for clarity. Use the format `idx # AbstractionName`.\n\n     ```yaml\n     - 0 # Addition\n     - 1 # Multiplication\n     ```\n     \"\"\"\n             response = call_llm(prompt) #call llm\n\n             yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n             ordered_indices_raw = yaml.safe_load(yaml_str)\n\n             ordered_indices = []\n             for entry in ordered_indices_raw:\n                  idx = int(entry.split('#')[0].strip())\n                  ordered_indices.append(idx)\n\n             return ordered_indices\n\n         def post(self, shared, prep_res, exec_res):\n             shared[\"chapter_order\"] = exec_res\n     ```\n\n   *   The `prep` method prepares the context and the project name for the LLM by listing all abstractions, relationship information, and project summary.\n\n   *   The `exec` method calls the LLM to ask what the best order to explain the abstractions are. Then the method adds the ordered list to the `shared` dictionary at `shared[\"chapter_order\"]`.\n\n   *   The LLM might respond with something like this (LLM generated):\n\n     ```yaml\n     - 0 # Addition\n     - 1 # Multiplication\n     ```\n\n5.  **Writing Chapters (WriteChapters Node)**\n\n   *   The `WriteChapters` node creates the markdown chapters for the final tutorial.\n   *   Here's a simplified snippet of the `WriteChapters` node's `prep` and `exec` methods (from `nodes.py`):\n\n     ```python\n     class WriteChapters(BatchNode):\n         def prep(self, shared):\n             chapter_order = shared[\"chapter_order\"]\n             abstractions = shared[\"abstractions\"]\n             files_data = shared[\"files\"]\n\n             all_chapters = []\n             chapter_filenames = {}\n             for i, abstraction_index in enumerate(chapter_order):\n                  chapter_num = i + 1\n                  chapter_name = abstractions[abstraction_index][\"name\"]\n                  safe_name = \"\".join(c if c.isalnum() else '_' for c in chapter_name).lower()\n                  filename = f\"{i+1:02d}_{safe_name}.md\"\n                  all_chapters.append(f\"{chapter_num}. [{chapter_name}]({filename})\")\n                  chapter_filenames[abstraction_index] = {\"num\": chapter_num, \"name\": chapter_name, \"filename\": filename}\n\n             full_chapter_listing = \"\\n\".join(all_chapters)\n             items_to_process = []\n             for i, abstraction_index in enumerate(chapter_order):\n                  abstraction_details = abstractions[abstraction_index]\n                  related_file_indices = abstraction_details.get(\"files\", [])\n\n                  related_files_content_map = {}\n                  for i in related_file_indices:\n                       path, content = files_data[i]\n                       related_files_content_map[f\"{i} # {path}\"] = content\n\n                  items_to_process.append({\n                       \"chapter_num\": i + 1,\n                       \"abstraction_index\": abstraction_index,\n                       \"abstraction_details\": abstraction_details,\n                       \"related_files_content_map\": related_files_content_map,\n                       \"project_name\": shared[\"project_name\"],\n                       \"full_chapter_listing\": full_chapter_listing,\n                       \"chapter_filenames\": chapter_filenames,\n                  })\n             print(f\"Preparing to write {len(items_to_process)} chapters...\")\n             return items_to_process\n\n         def exec(self, item):\n             abstraction_name = item[\"abstraction_details\"][\"name\"]\n             chapter_num = item[\"chapter_num\"]\n             project_name = item.get(\"project_name\")\n             print(f\"Writing chapter {chapter_num} for: {abstraction_name} using LLM...\")\n\n             file_context_str = \"\\n\\n\".join(\n                  f\"--- File: {idx_path.split('# ')[1] if '# ' in idx_path else idx_path} ---\\n{content}\"\n                  for idx_path, content in item[\"related_files_content_map\"].items()\n             )\n\n             prompt = f\"\"\"\n     Write a very beginner-friendly tutorial chapter (in Markdown format) for the project `{project_name}` about the concept: \"{abstraction_name}\". This is Chapter {chapter_num}.\n\n     Concept Details:\n     - Description:\n     {item[\"abstraction_details\"][\"description\"]}\n\n     Complete Tutorial Structure:\n     {item[\"full_chapter_listing\"]}\n\n     Relevant Code Snippets:\n     {file_context_str if file_context_str else \"No specific code snippets provided for this abstraction.\"}\n\n     Instructions for the chapter:\n     - Start with a clear heading (e.g., `# Chapter {chapter_num}: {abstraction_name}`).\n     - Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example.\n     - If the abstraction is complex, break it down into key concepts.\n     - Explain how to use this abstraction. Give example inputs and outputs.\n     - Describe the internal implementation to help understand what's under the hood.\n     - Heavily use analogies and examples throughout to help beginners understand.\n     - End the chapter with a brief conclusion.\n     - Ensure the tone is welcoming and easy for a newcomer to understand.\n\n     Now, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n     \"\"\"\n             chapter_content = call_llm(prompt) #call llm\n\n             actual_heading = f\"# Chapter {chapter_num}: {abstraction_name}\"\n             if not chapter_content.strip().startswith(f\"# Chapter {chapter_num}\"):\n                  lines = chapter_content.strip().split('\\n')\n                  if lines and lines[0].strip().startswith(\"#\"):\n                       lines[0] = actual_heading\n                       chapter_content = \"\\n\".join(lines)\n                  else:\n                       chapter_content = f\"{actual_heading}\\n\\n{chapter_content}\"\n\n             return chapter_content\n\n         def post(self, shared, prep_res, exec_res_list):\n             shared[\"chapters\"] = exec_res_list\n             print(f\"Finished writing {len(exec_res_list)} chapters.\")\n     ```\n\n   *   The `prep` method formats the project and file content and abstraction details for the LLM by listing all file content that corresponds to an abstraction. It generates filenames for each chapter, and adds each chapter to the `items_to_process` list.\n\n   *   The `exec` method prompts the LLM to generate a chapter based on the abstraction details and corresponding files. It then adds the chapters to the `shared` dictionary at `shared[\"chapters\"]`.\n\n   *   An example of the output generated by the LLM would be:\n\n     ```markdown\n     # Chapter 1: Addition\n\n     Addition is the process of combining two or more numbers to find their total, or sum. It's like combining two piles of toys to see how many you have in all!\n\n     ## How to Use Addition\n\n     You can use the `add` function in `file1.py` to perform addition:\n\n     ```python\n     def add(a, b):\n         return a + b\n     ```\n\n     This code simply takes two numbers, `a` and `b`, and returns their sum. For example:\n\n     ```python\n     result = add(5, 3) # result will be 8\n     print(result)\n     ```\n\n     In this example, `add(5, 3)` returns `8`, because 5 + 3 = 8. It's that simple!\n\n     ## Internal Implementation\n\n     The function `add(a, b)` simply uses the `+` operator in Python to perform the addition. This is a built-in feature of Python, so it's very efficient.\n\n     ## Conclusion\n\n     In this chapter, we learned about addition and how to use the `add` function to perform it in Python. In the next chapter, we will talk about [Multiplication](02_multiplication.md).\n     ```\n\n**Under the Hood: Implementation Details**\n\n*   **Nodes.py:** The core logic for this component resides in the `nodes.py` file, which contains the `IdentifyAbstractions`, `AnalyzeRelationships`, `OrderChapters`, and `WriteChapters` classes. Each of these classes inherits from the `Node` or `BatchNode` class from the `pocketflow` library.\n*   **LLM Integration:** The `call_llm` function (in `utils/call_llm.py`) handles communication with the LLM. It takes a prompt as input and returns the LLM's response. By default, we use the Google Gemini 2.5 Pro model.\n*   **Prompt Engineering:**  A key part of this component is crafting effective prompts for the LLM.  These prompts guide the LLM to analyze the code, identify key concepts, and generate beginner-friendly explanations. Examples of prompts are shown in the `exec` method of the nodes shown above.\n\n**Benefits and Considerations**\n\n*   **Benefits:**\n    *   **Automation:**  Automates the process of understanding and explaining complex codebases.\n    *   **Beginner-Friendly:**  Generates content tailored for beginners, making it easier to learn new codebases.\n    *   **Scalability:**  Can handle large codebases that would be difficult for a human to analyze manually.\n*   **Considerations:**\n    *   **LLM Limitations:** The quality of the generated content depends on the LLM's capabilities and the effectiveness of the prompts.\n    *   **Accuracy:** The AI might not always perfectly understand the code, so it's important to review the generated content for accuracy.\n\n**Conclusion**\n\nIn this chapter, we explored how AI can be used to analyze codebases and generate beginner-friendly tutorial content. This `AI-Powered Code Analysis and Generation` component is a key part of our tutorial generation system, enabling us to automate the process of understanding and explaining complex code.  In the next chapter, [Codebase Crawling](03_codebase_crawling.md), we'll see how we actually get the source code that this component analyzes.\n\n---\n# Chapter 3: Codebase Crawling\n\nIn the previous chapter, [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md), we learned how AI can analyze code and generate beginner-friendly explanations. But where does that code come from? That's where codebase crawling comes in!\n\nImagine you want to bake a cake. The AI-Powered analysis is like having a master chef who can explain every ingredient and baking step in detail. But first, you need to *get* the ingredients! Codebase crawling is like going to the grocery store and carefully selecting all the necessary ingredients (code files) for our AI chef to work with.\n\n**Why Codebase Crawling?**\n\nBefore we can analyze a project's code and create a tutorial, we need to first gather the relevant files. This can involve:\n\n*   **Fetching from a remote repository:**  Downloading code from platforms like GitHub.\n*   **Exploring a local directory:**  Reading code from your computer.\n*   **Filtering out irrelevant files:**  Ignoring tests, documentation, or other non-essential code.\n\n**The Goal: Getting the Right Code, Efficiently**\n\nOur `Codebase Crawling` component aims to provide a flexible and efficient way to fetch the necessary code files for analysis. It's like a librarian searching for specific books based on certain criteria (include/exclude patterns) and size restrictions, ensuring only relevant materials are collected.\n\n**Key Concepts**\n\nLet's break down the core ideas behind our codebase crawling:\n\n1.  **Fetching Source Code (FetchRepo Node):**\n    *   This node takes the Github repo URL or a local directory path and downloads source code using the `crawl_github_files` or `crawl_local_files` functions.\n\n2.  **Repository URL:** The address of the code repository (e.g., a GitHub link).\n\n3.  **Local Directory:** The path to a directory on your computer containing the code.\n\n4.  **Include/Exclude Patterns:**  Rules for selecting specific files based on their names or paths (e.g., including only `.py` files, excluding `test` directories). This is done using `fnmatch`, which supports wildcard matching similar to bash.\n\n5.  **File Size Limits:**  Maximum size for individual files to prevent processing large, irrelevant files.\n\n**How it Works: A Step-by-Step View**\n\nLet's walk through a simple scenario: You want to create a tutorial for a small Python library on GitHub.\n\n1.  **The `FetchRepo` Node:** This node is responsible for retrieving the code files. It uses either `crawl_github_files` (for GitHub repos) or `crawl_local_files` (for local directories).\n\n    *   Here's a simplified snippet of the `FetchRepo` node's `prep` and `exec` methods (from `nodes.py`):\n\n     ```python\n     class FetchRepo(Node):\n         def prep(self, shared):\n             repo_url = shared.get(\"repo_url\")\n             local_dir = shared.get(\"local_dir\")\n             project_name = shared.get(\"project_name\")\n             \n             if not project_name:\n                 # Basic name derivation from URL or directory\n                 if repo_url:\n                     project_name = repo_url.split('/')[-1].replace('.git', '')\n                 else:\n                     project_name = os.path.basename(os.path.abspath(local_dir))\n                 shared[\"project_name\"] = project_name\n\n             # Get file patterns directly from shared\n             include_patterns = shared[\"include_patterns\"]\n             exclude_patterns = shared[\"exclude_patterns\"]\n             max_file_size = shared[\"max_file_size\"]\n\n             return {\n                 \"repo_url\": repo_url,\n                 \"local_dir\": local_dir,\n                 \"token\": shared.get(\"github_token\"),\n                 \"include_patterns\": include_patterns,\n                 \"exclude_patterns\": exclude_patterns,\n                 \"max_file_size\": max_file_size,\n                 \"use_relative_paths\": True\n             }\n\n         def exec(self, prep_res):\n             if prep_res[\"repo_url\"]:\n                 print(f\"Crawling repository: {prep_res['repo_url']}...\")\n                 result = crawl_github_files(\n                     repo_url=prep_res[\"repo_url\"],\n                     token=prep_res[\"token\"],\n                     include_patterns=prep_res[\"include_patterns\"],\n                     exclude_patterns=prep_res[\"exclude_patterns\"],\n                     max_file_size=prep_res[\"max_file_size\"],\n                     use_relative_paths=prep_res[\"use_relative_paths\"]\n                 )\n             else:\n                 print(f\"Crawling directory: {prep_res['local_dir']}...\")\n                 result = crawl_local_files(\n                     directory=prep_res[\"local_dir\"],\n                     include_patterns=prep_res[\"include_patterns\"],\n                     exclude_patterns=prep_res[\"exclude_patterns\"],\n                     max_file_size=prep_res[\"max_file_size\"],\n                     use_relative_paths=prep_res[\"use_relative_paths\"]\n                 )\n                 \n             # Convert dict to list of tuples: [(path, content), ...]\n             files_list = list(result.get(\"files\", {}).items())\n             print(f\"Fetched {len(files_list)} files.\")\n             return files_list\n\n         def post(self, shared, prep_res, exec_res):\n             shared[\"files\"] = exec_res # List of (path, content) tuples\n     ```\n\n    *   The `prep` method retrieves all config options (such as Github repo URL, local directory path, file inclusion/exclusion patterns, max file size), and sets them to the `prep_res` to be used by the `exec` method.\n    *   The `exec` method determines if source code is fetched from Github or local directory based on the presence of a Github repo URL. It then downloads the source code, and stores the path and code as a tuple into a list.\n    *   The `post` method adds the source code to the `shared` dictionary, specifically at `shared[\"files\"]`.\n\n2.  **`crawl_github_files` Function:** This function is defined in `utils/crawl_github_files.py`.\n\n    *   Here's a simplified view of what happens inside the `crawl_github_files` function:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant CrawlGithubFiles as crawl_github_files\n    participant GithubAPI as Github API\n\n    User->>CrawlGithubFiles: Call crawl_github_files(repo_url, token, include_patterns, exclude_patterns, max_file_size, use_relative_paths)\n    CrawlGithubFiles->>GithubAPI: GET contents from Github API for the repo URL\n    GithubAPI-->>CrawlGithubFiles: Return contents from Github API\n    CrawlGithubFiles->>CrawlGithubFiles: Apply include/exclude patterns\n    CrawlGithubFiles->>CrawlGithubFiles: Apply file size limits\n    CrawlGithubFiles-->>User: Return list of (path, code)\n```\n\n    *   The `crawl_github_files` function downloads the source code for the repo URL by calling the Github API.\n    *   The `crawl_github_files` function then filters all source code based on include/exclude patterns, and file size limits.\n    *   The function returns a list of (path, code).\n\n3.  **`crawl_local_files` Function:** This function is defined in `utils/crawl_local_files.py`.\n\n    *   Here's a simplified view of what happens inside the `crawl_local_files` function:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant CrawlLocalFiles as crawl_local_files\n    participant OS as OS\n\n    User->>CrawlLocalFiles: Call crawl_local_files(local_dir, include_patterns, exclude_patterns, max_file_size, use_relative_paths)\n    CrawlLocalFiles->>OS: Walk directory for the local directory path\n    OS-->>CrawlLocalFiles: Return files\n    CrawlLocalFiles->>CrawlLocalFiles: Apply include/exclude patterns\n    CrawlLocalFiles->>CrawlLocalFiles: Apply file size limits\n    CrawlLocalFiles-->>User: Return list of (path, code)\n```\n\n    *   The `crawl_local_files` function walks the directory structure to find all files in the directory path.\n    *   The `crawl_local_files` function then filters all source code based on include/exclude patterns, and file size limits.\n    *   The function returns a list of (path, code).\n\n4.  **Example:**\n\n    ```python\n    shared = {\n        \"repo_url\": \"https://github.com/user/my-cool-project\",\n        \"include_patterns\": {\"*.py\"},\n        \"exclude_patterns\": {\"tests/*\"},\n        \"max_file_size\": 100000  # 100KB\n    }\n    ```\n\n    *   In this example, the `FetchRepo` node would:\n\n        *   Download the code from the specified GitHub repository.\n        *   Only include files ending with `.py`.\n        *   Exclude any files in directories named `tests`.\n        *   Skip any files larger than 100KB.\n\n5.  **Output:** The `FetchRepo` node outputs a list of tuples, where each tuple contains the file path and its contents. This list is then stored in the `shared` dictionary for use by subsequent nodes, such as [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md).\n\n**Code Example**\n\nLet's look at a snippet from `utils/crawl_github_files.py` to understand how file filtering works:\n\n```python\nimport fnmatch\n\ndef should_include_file(file_path: str, file_name: str) -> bool:\n    \"\"\"Determine if a file should be included based on patterns\"\"\"\n    # If no include patterns are specified, include all files\n    if not include_patterns:\n        include_file = True\n    else:\n        # Check if file matches any include pattern\n        include_file = any(fnmatch.fnmatch(file_name, pattern) for pattern in include_patterns)\n\n    # If exclude patterns are specified, check if file should be excluded\n    if exclude_patterns and include_file:\n        # Exclude if file matches any exclude pattern\n        exclude_file = any(fnmatch.fnmatch(file_path, pattern) for pattern in exclude_patterns)\n        return not exclude_file\n\n    return include_file\n```\n\nThis code snippet demonstrates how `fnmatch` is used to match file paths against include and exclude patterns.  It's a simple but powerful way to control which files are included in the analysis.\n\n**Benefits and Considerations**\n\n*   **Benefits:**\n    *   **Flexibility:**  Supports both remote repositories and local directories.\n    *   **Efficiency:**  Filters out irrelevant files to reduce processing time.\n    *   **Customization:**  Allows specifying include/exclude patterns and file size limits.\n*   **Considerations:**\n    *   **Rate Limits:**  When crawling GitHub repositories, be aware of API rate limits. Using a GitHub token is highly recommended.\n    *   **Complexity:**  Complex include/exclude patterns can be difficult to manage.\n\n**Conclusion**\n\nIn this chapter, we learned about codebase crawling, a crucial step in our tutorial generation process. By fetching and filtering code files, we ensure that our AI-powered analysis focuses on the most relevant parts of the codebase.  In the next chapter, [Knowledge Base Construction](04_knowledge_base_construction.md), we'll see how we use the crawled code to build a knowledge base for generating our tutorials.\n\n---\n# Chapter 4: Knowledge Base Construction\n\n```markdown\n# Chapter 4: Knowledge Base Construction\n\nIn the previous chapter, [Codebase Crawling](03_codebase_crawling.md), we learned how to fetch the necessary code files from a repository or local directory. Now that we have these files, what do we do with them? This is where Knowledge Base Construction comes in!\n\nImagine you have a giant pile of Lego bricks (the code files).  Codebase Crawling gave us those bricks.  Now, Knowledge Base Construction is like organizing those bricks into labeled containers, grouping similar pieces together, and creating an instruction manual that explains how the pieces connect and interact. This organized structure is what we call a \"knowledge base.\"\n\n**Why is Knowledge Base Construction Important?**\n\nThe code itself is raw data.  The AI can't use it effectively for generating a tutorial without some organization.  A knowledge base helps the AI:\n\n*   **Understand the code:** By extracting key information and relationships.\n*   **Access information quickly:** By providing a structured format for retrieval.\n*   **Generate better tutorials:** By having a clear understanding of the codebase's structure and purpose.\n\nThink of it like organizing a messy desk. Without organization, it's hard to find anything. But with a well-organized desk, with labeled folders and a clear index, you can quickly access the information you need.\n\n**Key Concepts**\n\n*   **Extraction:** Taking important pieces of information from the code (like functions, classes, and their relationships).  The [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md) module handles the underlying extraction of information.\n*   **Organization:** Arranging the extracted information into a structured format, like a database or a graph.  In our case, this is mostly in the form of Python dictionaries and lists, which are structured.\n*   **Relationships:** Identifying how different parts of the code relate to each other. This is what the AnalyzeRelationships node does!\n*   **Vector Database:**  While we don't *explicitly* create a vector database in this tutorial, the concept is related. A vector database stores code snippets and other data as numerical vectors, allowing for efficient similarity searches. The knowledge base we create serves a similar purpose, allowing the AI to quickly find relevant code snippets and information.\n\n**How it Works: Building Our Knowledge Base**\n\nWhile the whole system contributes to the knowledge base, let's focus on where the main knowledge is stored after the [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md) stage. The shared dictionary contains all the information that could be viewed as a knowledge base.\n\n1.  **The `shared` Dictionary:** This dictionary acts as our central knowledge base. It's like a shared whiteboard where different parts of the system can store and access information.\n\n2.  **Key components in `shared` that comprise the knowledge base:**\n\n    *   **`shared[\"files\"]`:** A list of tuples, where each tuple contains the file path and its contents.  This is the raw code we crawled in [Codebase Crawling](03_codebase_crawling.md). Example: `[(\"file1.py\", \"def add(a, b): return a + b\"), (\"file2.py\", \"def multiply(a, b): return a * b\")]`\n\n    *   **`shared[\"abstractions\"]`:** A list of dictionaries, where each dictionary represents an abstraction (a key concept in the codebase). This list is created by the `IdentifyAbstractions` node in [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md). It contains the name, description, and relevant file indices for each abstraction. Example:\n\n        ```python\n        [\n            {\n                \"name\": \"Addition\",\n                \"description\": \"This abstraction performs addition. It's like combining two piles of toys to see how many you have in all!\",\n                \"files\": [0]  # Index of file1.py\n            },\n            {\n                \"name\": \"Multiplication\",\n                \"description\": \"This abstraction performs multiplication. It's like repeated addition!\",\n                \"files\": [1]  # Index of file2.py\n            }\n        ]\n        ```\n\n    *   **`shared[\"relationships\"]`:** A dictionary containing a summary of the project and a list of relationships between abstractions. This dictionary is created by the `AnalyzeRelationships` node in [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md). The relationships describe how the abstractions interact with each other. Example:\n\n        ```python\n        {\n            \"summary\": \"This project provides basic arithmetic operations: addition and multiplication.  It allows users to perform simple calculations.\",\n            \"details\": [\n                {\n                    \"from\": 0,  # Index of the \"Addition\" abstraction\n                    \"to\": 1,  # Index of the \"Multiplication\" abstraction\n                    \"label\": \"None\"\n                }\n            ]\n        }\n        ```\n\n3.  **Putting it All Together:** The AI uses this structured information to understand the codebase and generate the tutorial content. For example, when writing a chapter about \"Addition,\" the AI can:\n\n    *   Look up the description of \"Addition\" in `shared[\"abstractions\"]`.\n    *   Find the relevant code file (`file1.py`) using the file index in `shared[\"abstractions\"]`.\n    *   Explain the relationship between \"Addition\" and \"Multiplication\" using the information in `shared[\"relationships\"]`.\n\n**Code Example: Accessing the Knowledge Base**\n\nLet's say you want to access the description of the \"Addition\" abstraction. Here's how you would do it:\n\n```python\n# Assuming 'shared' dictionary is already populated\nabstractions = shared[\"abstractions\"]\nfor abstraction in abstractions:\n    if abstraction[\"name\"] == \"Addition\":\n        print(abstraction[\"description\"])\n        break\n```\n\nThis code snippet iterates through the `shared[\"abstractions\"]` list and prints the description of the \"Addition\" abstraction.\n\n**Benefits and Considerations**\n\n*   **Benefits:**\n    *   **Structured Information:** Provides the AI with a clear and organized view of the codebase.\n    *   **Efficient Retrieval:** Allows the AI to quickly access relevant information.\n    *   **Improved Tutorial Quality:** Enables the AI to generate more accurate and informative tutorials.\n*   **Considerations:**\n    *   **Complexity:** Building a knowledge base for a large and complex codebase can be challenging.\n    *   **Maintenance:** The knowledge base needs to be updated as the codebase evolves.\n\n**Analogy: The Library of Code**\n\nImagine a vast library containing all the code in the world.\n\n*   **Codebase Crawling:**  Collecting the books (code files) for our library.\n*   **Knowledge Base Construction:**  Cataloging those books, creating a card catalog (the `shared` dictionary), and writing summaries of each book (abstractions and relationships).\n*   **AI-Powered Content Generation:**  Using the library's resources to write a new book (the tutorial) about a specific topic.\n\n**Conclusion**\n\nIn this chapter, we explored the concept of Knowledge Base Construction, which is crucial for enabling the AI to understand the codebase and generate high-quality tutorials. By extracting, organizing, and relating information, we create a structured knowledge base that the AI can use to access and retrieve relevant information, ultimately improving the tutorial generation process. In the next chapter, [Tutorial Content Combination](05_tutorial_content_combination.md), we'll see how the generated content is combined to create the final tutorial.\n```\n---\n# Chapter 5: Tutorial Content Combination\n\n```markdown\n# Chapter 5: Tutorial Content Combination\n\nIn the previous chapter, [Knowledge Base Construction](04_knowledge_base_construction.md), we learned how to organize the extracted information into a structured format, acting as a foundation for the AI to build upon. Now, we'll see how all the individual pieces of content generated thus far (the chapters, the introduction, the overview diagram) are brought together to form a cohesive and complete tutorial!\n\nImagine you've baked all the layers of a cake, made the frosting, and have a beautiful decoration ready. Tutorial Content Combination is like the final step of assembling the cake, frosting it, and adding the decorations to create a finished product ready to be enjoyed.\n\n**Why Combine the Content?**\n\nWhile the AI has generated individual chapters, they are just pieces of a larger puzzle. Tutorial Content Combination ensures:\n\n*   **Coherence:**  The chapters flow logically and smoothly from one to the next.\n*   **Completeness:**  The tutorial includes an introduction, table of contents, and a high-level overview, providing a complete learning experience.\n*   **Readability:**  The final product is well-formatted and easy to read.\n\n**The Goal: Polishing the Tutorial**\n\nOur `Tutorial Content Combination` component takes the generated chapters, table of contents, relationship diagram, and project summary to weave them into a polished, beginner-friendly tutorial. It's like an editor taking individual articles and assembling them into a coherent book.\n\n**Key Concepts**\n\nLet's break down the core ideas behind our content combination:\n\n1.  **`CombineTutorial` Node:** The heart of this process. This node is responsible for:\n    *   Generating an `index.md` file that serves as the tutorial's landing page.\n    *   Creating a table of contents that links to each chapter.\n    *   Including a high-level project summary from [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md).\n    *   Generating a Mermaid diagram visualizing the relationships between key concepts.\n    *   Writing each chapter to a separate `.md` file.\n\n2.  **`index.md`:**  The main entry point to the tutorial, containing the project summary, table of contents, and a diagram representing the project's structure. Think of it as the book's cover and introduction.\n\n3.  **Mermaid Diagram:** A visual representation of the relationships between the core abstractions in the codebase, created using the Mermaid syntax.  This provides a high-level overview of the project's architecture.\n\n4.  **Chapter Files (`*.md`):** Each chapter is saved as a separate Markdown file, allowing for easy navigation and editing.\n\n**How it Works: Assembling the Tutorial**\n\nHere's a step-by-step look at how the `CombineTutorial` node assembles the tutorial:\n\n1.  **The `CombineTutorial` Node:** This node orchestrates the entire content combination process.\n\n    *   Here's a simplified snippet of the `CombineTutorial` node's `prep` and `exec` methods (from `nodes.py`):\n\n     ```python\n     class CombineTutorial(Node):\n         def prep(self, shared):\n             project_name = shared[\"project_name\"]\n             output_base_dir = shared.get(\"output_dir\", \"output\") # Default output dir\n             output_path = os.path.join(output_base_dir, project_name)\n             repo_url = shared[\"repo_url\"]  # Get the repository URL\n\n             # Use 'label' from relationships_data['details']\n             relationships_data = shared[\"relationships\"] # {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n             chapter_order = shared[\"chapter_order\"] # indices\n             abstractions = shared[\"abstractions\"]   # list of dicts\n             chapters_content = shared[\"chapters\"]   # list of strings\n\n             # --- Generate Mermaid Diagram ---\n             mermaid_lines = [\"flowchart TD\"]\n             # Add nodes for each abstraction\n             for i, abstr in enumerate(abstractions):\n                 # Sanitize name for Mermaid ID and label\n                 node_id = f\"A{i}\"\n                 sanitized_name = abstr['name'].replace('\"', '')\n                 node_label = sanitized_name # Using sanitized name only, no index\n                 mermaid_lines.append(f'    {node_id}[\"{node_label}\"]')\n             # Add edges for relationships using 'label'\n             for rel in relationships_data['details']:\n                 from_node_id = f\"A{rel['from']}\"\n                 to_node_id = f\"A{rel['to']}\"\n                 # Sanitize 'label' for edge label\n                 edge_label = rel['label'].replace('\"', '').replace('\\n', ' ') # Basic sanitization\n                 # Limit edge label length for readability (optional, but good for diagrams)\n                 max_label_len = 30 # Make it shorter for labels\n                 if len(edge_label) > max_label_len:\n                     edge_label = edge_label[:max_label_len-3] + \"...\"\n                 mermaid_lines.append(f'    {from_node_id} -- \"{edge_label}\" --> {to_node_id}')\n\n             mermaid_diagram = \"\\n\".join(mermaid_lines)\n             # --- End Mermaid ---\n\n\n             # Prepare index.md content\n             index_content = f\"# Tutorial: {project_name}\\n\\n\"\n             index_content += f\"{relationships_data['summary']}\\n\\n\"\n             index_content += f\"**Source Repository:** [{repo_url}]({repo_url})\\n\\n\"\n\n             # Add Mermaid diagram for relationships\n             index_content += \"```mermaid\\n\"\n             index_content += mermaid_diagram + \"\\n\"\n             index_content += \"```\\n\\n\"\n\n             index_content += \"## Chapters\\n\\n\"\n\n             chapter_files = []\n             # Generate chapter links based on the determined order\n             for i, abstraction_index in enumerate(chapter_order):\n                 # Ensure index is valid and we have content for it\n                 if 0 <= abstraction_index < len(abstractions) and i < len(chapters_content):\n                     abstraction_name = abstractions[abstraction_index][\"name\"]\n                     # Sanitize name for filename\n                     safe_name = \"\".join(c if c.isalnum() else '_' for c in abstraction_name).lower()\n                     # Use chapter number (i+1) for ordering filename\n                     filename = f\"{i+1:02d}_{safe_name}.md\"\n                     index_content += f\"{i+1}. [{abstraction_name}]({filename})\\n\"\n                     \n                     # Add attribution to chapter content\n                     chapter_content = chapters_content[i]\n                     if not chapter_content.endswith(\"\\n\\n\"):\n                         chapter_content += \"\\n\\n\"\n                     chapter_content += \"---\\n\\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\"\n                     \n                     # Store filename and corresponding content\n                     chapter_files.append({\"filename\": filename, \"content\": chapter_content})\n                 else:\n                      print(f\"Warning: Mismatch between chapter order, abstractions, or content at index {i} (abstraction index {abstraction_index}). Skipping file generation for this entry.\")\n\n             # Add attribution to index content\n             index_content += \"\\n\\n---\\n\\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\"\n\n             return {\n                 \"output_path\": output_path,\n                 \"index_content\": index_content,\n                 \"chapter_files\": chapter_files # List of {\"filename\": str, \"content\": str}\n             }\n\n         def exec(self, prep_res):\n             output_path = prep_res[\"output_path\"]\n             index_content = prep_res[\"index_content\"]\n             chapter_files = prep_res[\"chapter_files\"]\n\n             print(f\"Combining tutorial into directory: {output_path}\")\n             # Rely on Node's built-in retry/fallback\n             os.makedirs(output_path, exist_ok=True)\n\n             # Write index.md\n             index_filepath = os.path.join(output_path, \"index.md\")\n             with open(index_filepath, \"w\", encoding=\"utf-8\") as f:\n                 f.write(index_content)\n             print(f\"  - Wrote {index_filepath}\")\n\n             # Write chapter files\n             for chapter_info in chapter_files:\n                 chapter_filepath = os.path.join(output_path, chapter_info[\"filename\"])\n                 with open(chapter_filepath, \"w\", encoding=\"utf-8\") as f:\n                     f.write(chapter_info[\"content\"])\n                 print(f\"  - Wrote {chapter_filepath}\")\n\n             return output_path # Return the final path\n\n         def post(self, shared, prep_res, exec_res):\n             shared[\"final_output_dir\"] = exec_res # Store the output path\n             print(f\"\\nTutorial generation complete! Files are in: {exec_res}\")\n     ```\n\n    *   The `prep` method retrieves the project name, output directory, repository URL, relationships data, chapter order, abstractions, and chapters content from the `shared` dictionary.  It then generates the Mermaid diagram representing the relationships between core abstractions and prepares the content for the `index.md` file, including the tutorial's table of contents.  Finally, it formats the chapter content to include proper attribution.\n\n    *   The `exec` method takes the prepared data and writes the `index.md` file and each chapter file to the specified output directory.\n\n2.  **Generating `index.md`:** The `CombineTutorial` node creates the `index.md` file in the output directory. This file includes:\n    *   A title (e.g., `# Tutorial: MyCoolProject`).\n    *   The project summary generated by [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md).\n    *   A link to the source repository.\n    *   The Mermaid diagram illustrating the relationships between core concepts.\n    *   A table of contents, listing each chapter with a link to its corresponding `.md` file.\n\n3.  **Creating the Mermaid Diagram:**\n\n    *   The node generates a Mermaid diagram based on the relationships between the abstractions in the codebase.\n\n    *   Example of a Mermaid diagram:\n\n        ```mermaid\n        flowchart TD\n            A0[\"Addition\"]\n            A1[\"Multiplication\"]\n            A0 -- \"None\" --> A1\n        ```\n\n        *   In this example, `A0` and `A1` represent the \"Addition\" and \"Multiplication\" abstractions, respectively. The `-->` arrow indicates a relationship between them.\n\n4.  **Writing Chapter Files:** The `CombineTutorial` node saves each chapter as a separate Markdown file in the output directory. The filenames are numbered to reflect the chapter order (e.g., `01_addition.md`, `02_multiplication.md`).  Each chapter includes proper attribution to the `AI Codebase Knowledge Builder`.\n\n**Benefits and Considerations**\n\n*   **Benefits:**\n    *   **Automated Assembly:** Automates the process of combining individual pieces of content into a complete tutorial.\n    *   **Improved Coherence:** Ensures that the tutorial flows logically and smoothly.\n    *   **Enhanced Readability:** Provides a well-formatted and easy-to-read final product.\n*   **Considerations:**\n    *   **Customization:** The generated `index.md` and chapter files can be further customized to match specific formatting preferences or branding guidelines.\n    *   **Diagram Complexity:** For very large and complex codebases, the Mermaid diagram may become difficult to read.\n\n**Code Example: Generating the Table of Contents**\n\nLet's look at the code that generates the table of contents in the `index.md` file:\n\n```python\n# Generate chapter links based on the determined order\nfor i, abstraction_index in enumerate(chapter_order):\n    # Ensure index is valid and we have content for it\n    if 0 <= abstraction_index < len(abstractions) and i < len(chapters_content):\n        abstraction_name = abstractions[abstraction_index][\"name\"]\n        # Sanitize name for filename\n        safe_name = \"\".join(c if c.isalnum() else '_' for c in abstraction_name).lower()\n        # Use chapter number (i+1) for ordering filename\n        filename = f\"{i+1:02d}_{safe_name}.md\"\n        index_content += f\"{i+1}. [{abstraction_name}]({filename})\\n\"\n```\n\nThis code snippet iterates through the `chapter_order` list and generates a Markdown link for each chapter in the correct order.  It also creates a safe filename for each chapter by replacing non-alphanumeric characters with underscores.\n\n**Final Result**\n\nAfter running the `CombineTutorial` node, you will have a complete tutorial in the specified output directory. The tutorial will include:\n\n*   An `index.md` file with a project summary, table of contents, and diagram.\n*   A set of `.md` files, each containing a chapter of the tutorial.\n\n**Conclusion**\n\nIn this chapter, we explored the concept of Tutorial Content Combination, the final step in our tutorial generation process. By assembling the generated content into a cohesive and complete tutorial, we ensure that the final product is well-organized, easy to read, and provides a valuable learning experience for beginners.\n\nCongratulations! You have now seen all the stages from [Main Execution Script](01_main_execution_script.md), [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md), [Codebase Crawling](03_codebase_crawling.md), [Knowledge Base Construction](04_knowledge_base_construction.md), and [Tutorial Content Combination](05_tutorial_content_combination.md).\n\nIn the next chapter, we will briefly discuss [LLM Orchestration (PocketFlow)](06_llm_orchestration__pocketflow_.md) used in this project, and then discuss how to use the entire system to generate a tutorial.\n```\n\n\nRelevant Code Snippets:\n--- File: flow.py ---\nfrom pocketflow import Flow\n# Import all node classes from nodes.py\nfrom nodes import (\n    FetchRepo,\n    IdentifyAbstractions,\n    AnalyzeRelationships,\n    OrderChapters,\n    WriteChapters,\n    CombineTutorial\n)\n\ndef create_tutorial_flow():\n    \"\"\"Creates and returns the codebase tutorial generation flow.\"\"\"\n\n    # Instantiate nodes\n    fetch_repo = FetchRepo()\n    identify_abstractions = IdentifyAbstractions(max_retries=3, wait=10)\n    analyze_relationships = AnalyzeRelationships(max_retries=3, wait=10)\n    order_chapters = OrderChapters(max_retries=3, wait=10)\n    write_chapters = WriteChapters(max_retries=3, wait=10) # This is a BatchNode\n    combine_tutorial = CombineTutorial()\n\n    # Connect nodes in sequence based on the design\n    fetch_repo >> identify_abstractions\n    identify_abstractions >> analyze_relationships\n    analyze_relationships >> order_chapters\n    order_chapters >> write_chapters\n    write_chapters >> combine_tutorial\n\n    # Create the flow starting with FetchRepo\n    tutorial_flow = Flow(start=fetch_repo)\n\n    return tutorial_flow\n\n--- File: nodes.py ---\nimport os\nimport yaml\nfrom pocketflow import Node, BatchNode\nfrom utils.crawl_github_files import crawl_github_files\nfrom utils.call_llm import call_llm \nfrom utils.crawl_local_files import crawl_local_files\n\n# Helper to get content for specific file indices\ndef get_content_for_indices(files_data, indices):\n    content_map = {}\n    for i in indices:\n        if 0 <= i < len(files_data):\n            path, content = files_data[i]\n            content_map[f\"{i} # {path}\"] = content # Use index + path as key for context\n    return content_map\n\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"local_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        if not project_name:\n            # Basic name derivation from URL or directory\n            if repo_url:\n                project_name = repo_url.split('/')[-1].replace('.git', '')\n            else:\n                project_name = os.path.basename(os.path.abspath(local_dir))\n            shared[\"project_name\"] = project_name\n\n        # Get file patterns directly from shared\n        include_patterns = shared[\"include_patterns\"]\n        exclude_patterns = shared[\"exclude_patterns\"]\n        max_file_size = shared[\"max_file_size\"]\n\n        return {\n            \"repo_url\": repo_url,\n            \"local_dir\": local_dir,\n            \"token\": shared.get(\"github_token\"),\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns,\n            \"max_file_size\": max_file_size,\n            \"use_relative_paths\": True\n        }\n\n    def exec(self, prep_res):\n        if prep_res[\"repo_url\"]:\n            print(f\"Crawling repository: {prep_res['repo_url']}...\")\n            result = crawl_github_files(\n                repo_url=prep_res[\"repo_url\"],\n                token=prep_res[\"token\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        else:\n            print(f\"Crawling directory: {prep_res['local_dir']}...\")\n            result = crawl_local_files(\n                directory=prep_res[\"local_dir\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n            \n        # Convert dict to list of tuples: [(path, content), ...]\n        files_list = list(result.get(\"files\", {}).items())\n        print(f\"Fetched {len(files_list)} files.\")\n        return files_list\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"files\"] = exec_res # List of (path, content) tuples\n\nclass IdentifyAbstractions(Node):\n    def prep(self, shared):\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        \n        # Helper to create context from files, respecting limits (basic example)\n        def create_llm_context(files_data):\n            context = \"\"\n            file_info = [] # Store tuples of (index, path)\n            for i, (path, content) in enumerate(files_data):\n                entry = f\"--- File Index {i}: {path} ---\\n{content}\\n\\n\"\n                context += entry\n                file_info.append((i, path))\n\n            return context, file_info # file_info is list of (index, path)\n\n        context, file_info = create_llm_context(files_data)\n        # Format file info for the prompt (comment is just a hint for LLM)\n        file_listing_for_prompt = \"\\n\".join([f\"- {idx} # {path}\" for idx, path in file_info])\n        return context, file_listing_for_prompt, len(files_data), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, file_listing_for_prompt, file_count, project_name = prep_res  # Unpack project name\n        print(\"Identifying abstractions using LLM...\")\n        prompt = f\"\"\"\nFor the project `{project_name}`:\n\nCodebase Context:\n{context}\n\nAnalyze the codebase context.\nIdentify the top 5-10 core most important abstractions to help those new to the codebase.\n\nFor each abstraction, provide:\n1. A concise `name`.\n2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\nList of file indices and paths present in the context:\n{file_listing_for_prompt}\n\nFormat the output as a YAML list of dictionaries:\n\n```yaml\n- name: Query Processing\n  description: | \n    Explains what the abstraction does.\n    It's like a central dispatcher routing requests.\n  file_indices:\n    - 0 # path/to/file1.py\n    - 3 # path/to/related.py\n- name: Query Optimization\n  description: |\n    Another core concept, similar to a blueprint for objects.\n  file_indices:\n    - 5 # path/to/another.js\n# ... up to 10 abstractions\n```\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        abstractions = yaml.safe_load(yaml_str)\n\n        if not isinstance(abstractions, list):\n            raise ValueError(\"LLM Output is not a list\")\n\n        validated_abstractions = []\n        for item in abstractions:\n            if not isinstance(item, dict) or not all(k in item for k in [\"name\", \"description\", \"file_indices\"]):\n                raise ValueError(f\"Missing keys in abstraction item: {item}\")\n            if not isinstance(item[\"description\"], str):\n                 raise ValueError(f\"description is not a string in item: {item}\")\n            if not isinstance(item[\"file_indices\"], list):\n                 raise ValueError(f\"file_indices is not a list in item: {item}\")\n\n            # Validate indices\n            validated_indices = []\n            for idx_entry in item[\"file_indices\"]:\n                 try:\n                     if isinstance(idx_entry, int):\n                         idx = idx_entry\n                     elif isinstance(idx_entry, str) and '#' in idx_entry:\n                          idx = int(idx_entry.split('#')[0].strip())\n                     else:\n                          idx = int(str(idx_entry).strip())\n\n                     if not (0 <= idx < file_count):\n                         raise ValueError(f\"Invalid file index {idx} found in item {item['name']}. Max index is {file_count - 1}.\")\n                     validated_indices.append(idx)\n                 except (ValueError, TypeError):\n                      raise ValueError(f\"Could not parse index from entry: {idx_entry} in item {item['name']}\")\n\n            item[\"files\"] = sorted(list(set(validated_indices)))\n            # Store only the required fields\n            validated_abstractions.append({\n                \"name\": item[\"name\"],\n                \"description\": item[\"description\"],\n                \"files\": item[\"files\"]\n            })\n\n        print(f\"Identified {len(validated_abstractions)} abstractions.\")\n        return validated_abstractions\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"abstractions\"] = exec_res # List of {\"name\": str, \"description\": str, \"files\": [int]}\n\nclass AnalyzeRelationships(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"] # Now contains 'files' list of indices\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n\n        # Create context with abstraction names, indices, descriptions, and relevant file snippets\n        context = \"Identified Abstractions:\\n\"\n        all_relevant_indices = set()\n        abstraction_info_for_prompt = []\n        for i, abstr in enumerate(abstractions):\n            # Use 'files' which contains indices directly\n            file_indices_str = \", \".join(map(str, abstr['files']))\n            info_line = f\"- Index {i}: {abstr['name']} (Relevant file indices: [{file_indices_str}])\\n  Description: {abstr['description']}\"\n            context += info_line + \"\\n\"\n            abstraction_info_for_prompt.append(f\"{i} # {abstr['name']}\")\n            all_relevant_indices.update(abstr['files'])\n\n        context += \"\\nRelevant File Snippets (Referenced by Index and Path):\\n\"\n        # Get content for relevant files using helper\n        relevant_files_content_map = get_content_for_indices(\n            files_data,\n            sorted(list(all_relevant_indices))\n        )\n        # Format file content for context\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path} ---\\n{content}\"\n            for idx_path, content in relevant_files_content_map.items()\n        )\n        context += file_context_str\n\n        return context, \"\\n\".join(abstraction_info_for_prompt), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, abstraction_listing, project_name = prep_res  # Unpack project name\n        print(\"Analyzing relationships using LLM...\")\n        prompt = f\"\"\"\nBased on the following abstractions and relevant code snippets from the project `{project_name}`:\n\nList of Abstraction Indices and Names:\n{abstraction_listing}\n\nContext (Abstractions, Descriptions, Code):\n{context}\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        relationships_data = yaml.safe_load(yaml_str)\n\n        if not isinstance(relationships_data, dict) or not all(k in relationships_data for k in [\"summary\", \"relationships\"]):\n            raise ValueError(\"LLM output is not a dict or missing keys ('summary', 'relationships')\")\n        if not isinstance(relationships_data[\"summary\"], str):\n             raise ValueError(\"summary is not a string\")\n        if not isinstance(relationships_data[\"relationships\"], list):\n             raise ValueError(\"relationships is not a list\")\n\n        # Validate relationships structure\n        validated_relationships = []\n        num_abstractions = len(abstraction_listing.split('\\n'))\n        for rel in relationships_data[\"relationships\"]:\n             # Check for 'label' key\n             if not isinstance(rel, dict) or not all(k in rel for k in [\"from_abstraction\", \"to_abstraction\", \"label\"]):\n                  raise ValueError(f\"Missing keys (expected from_abstraction, to_abstraction, label) in relationship item: {rel}\")\n             # Validate 'label' is a string\n             if not isinstance(rel[\"label\"], str):\n                  raise ValueError(f\"Relationship label is not a string: {rel}\")\n\n             # Validate indices\n             try:\n                 from_idx = int(str(rel[\"from_abstraction\"]).split('#')[0].strip())\n                 to_idx = int(str(rel[\"to_abstraction\"]).split('#')[0].strip())\n                 if not (0 <= from_idx < num_abstractions and 0 <= to_idx < num_abstractions):\n                      raise ValueError(f\"Invalid index in relationship: from={from_idx}, to={to_idx}. Max index is {num_abstractions-1}.\")\n                 validated_relationships.append({\n                     \"from\": from_idx,\n                     \"to\": to_idx,\n                     \"label\": rel[\"label\"] \n                 })\n             except (ValueError, TypeError):\n                  raise ValueError(f\"Could not parse indices from relationship: {rel}\")\n\n        print(\"Generated project summary and relationship details.\")\n        return {\n            \"summary\": relationships_data[\"summary\"],\n            \"details\": validated_relationships # Store validated, index-based relationships\n        }\n\n\n    def post(self, shared, prep_res, exec_res):\n        # Structure is now {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        shared[\"relationships\"] = exec_res\n\nclass OrderChapters(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"]\n        relationships = shared[\"relationships\"]\n        project_name = shared[\"project_name\"]  # Get project name\n\n        # Prepare context for the LLM\n        abstraction_info_for_prompt = []\n        for i, a in enumerate(abstractions):\n            abstraction_info_for_prompt.append(f\"- {i} # {a['name']}\")\n        abstraction_listing = \"\\n\".join(abstraction_info_for_prompt)\n\n        context = f\"Project Summary:\\n{relationships['summary']}\\n\\n\"\n        context += \"Relationships (Indices refer to abstractions above):\\n\"\n        for rel in relationships['details']:\n             from_name = abstractions[rel['from']]['name']\n             to_name = abstractions[rel['to']]['name']\n             # Use 'label' instead of 'desc'\n             context += f\"- From {rel['from']} ({from_name}) to {rel['to']} ({to_name}): {rel['label']}\\n\"\n\n        return abstraction_listing, context, len(abstractions), project_name\n\n    def exec(self, prep_res):\n        abstraction_listing, context, num_abstractions, project_name = prep_res\n        print(\"Determining chapter order using LLM...\")\n        prompt = f\"\"\"\nGiven the following project abstractions and their relationships for the project ```` {project_name} ````:\n\nAbstractions (Index # Name):\n{abstraction_listing}\n\nContext about relationships and project summary:\n{context}\n\nIf you are going to make a tutorial for ```` {project_name} ````, what is the best order to explain these abstractions, from first to last?\nIdeally, first explain those that are the most important or foundational, perhaps user-facing concepts or entry points. Then move to more detailed, lower-level implementation details or supporting concepts.\n\nOutput the ordered list of abstraction indices, including the name in a comment for clarity. Use the format `idx # AbstractionName`.\n\n```yaml\n- 2 # FoundationalConcept\n- 0 # CoreClassA\n- 1 # CoreClassB (uses CoreClassA)\n- ...\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        # Rely on Node's built-in retry/fallback\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        ordered_indices_raw = yaml.safe_load(yaml_str)\n\n        if not isinstance(ordered_indices_raw, list):\n            raise ValueError(\"LLM output is not a list\")\n\n        ordered_indices = []\n        seen_indices = set()\n        for entry in ordered_indices_raw:\n            try:\n                 if isinstance(entry, int):\n                     idx = entry\n                 elif isinstance(entry, str) and '#' in entry:\n                      idx = int(entry.split('#')[0].strip())\n                 else:\n                      idx = int(str(entry).strip())\n\n                 if not (0 <= idx < num_abstractions):\n                      raise ValueError(f\"Invalid index {idx} in ordered list. Max index is {num_abstractions-1}.\")\n                 if idx in seen_indices:\n                     raise ValueError(f\"Duplicate index {idx} found in ordered list.\")\n                 ordered_indices.append(idx)\n                 seen_indices.add(idx)\n\n            except (ValueError, TypeError):\n                 raise ValueError(f\"Could not parse index from ordered list entry: {entry}\")\n\n        # Check if all abstractions are included\n        if len(ordered_indices) != num_abstractions:\n             raise ValueError(f\"Ordered list length ({len(ordered_indices)}) does not match number of abstractions ({num_abstractions}). Missing indices: {set(range(num_abstractions)) - seen_indices}\")\n\n        print(f\"Determined chapter order (indices): {ordered_indices}\")\n        return ordered_indices # Return the list of indices\n\n    def post(self, shared, prep_res, exec_res):\n        # exec_res is already the list of ordered indices\n        shared[\"chapter_order\"] = exec_res # List of indices\n\nclass WriteChapters(BatchNode):\n    def prep(self, shared):\n        chapter_order = shared[\"chapter_order\"] # List of indices\n        abstractions = shared[\"abstractions\"]   # List of dicts, now using 'files' with indices\n        files_data = shared[\"files\"]\n        # Get already written chapters to provide context\n        # We store them temporarily during the batch run, not in shared memory yet\n        # The 'previous_chapters_summary' will be built progressively in the exec context\n        self.chapters_written_so_far = [] # Use instance variable for temporary storage across exec calls\n\n        # Create a complete list of all chapters\n        all_chapters = []\n        chapter_filenames = {} # Store chapter filename mapping for linking\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                chapter_num = i + 1\n                chapter_name = abstractions[abstraction_index][\"name\"]\n                # Create safe filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in chapter_name).lower()\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                # Format with link\n                all_chapters.append(f\"{chapter_num}. [{chapter_name}]({filename})\")\n                # Store mapping of chapter index to filename for linking\n                chapter_filenames[abstraction_index] = {\"num\": chapter_num, \"name\": chapter_name, \"filename\": filename}\n        \n        # Create a formatted string with all chapters\n        full_chapter_listing = \"\\n\".join(all_chapters)\n\n        items_to_process = []\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                abstraction_details = abstractions[abstraction_index]\n                # Use 'files' (list of indices) directly\n                related_file_indices = abstraction_details.get(\"files\", [])\n                # Get content using helper, passing indices\n                related_files_content_map = get_content_for_indices(files_data, related_file_indices)\n                \n                # Get previous chapter info for transitions\n                prev_chapter = None\n                if i > 0:\n                    prev_idx = chapter_order[i-1]\n                    prev_chapter = chapter_filenames[prev_idx]\n                \n                # Get next chapter info for transitions\n                next_chapter = None\n                if i < len(chapter_order) - 1:\n                    next_idx = chapter_order[i+1]\n                    next_chapter = chapter_filenames[next_idx]\n\n                items_to_process.append({\n                    \"chapter_num\": i + 1,\n                    \"abstraction_index\": abstraction_index,\n                    \"abstraction_details\": abstraction_details,\n                    \"related_files_content_map\": related_files_content_map,\n                    \"project_name\": shared[\"project_name\"],  # Add project name\n                    \"full_chapter_listing\": full_chapter_listing,  # Add the full chapter listing\n                    \"chapter_filenames\": chapter_filenames,  # Add chapter filenames mapping\n                    \"prev_chapter\": prev_chapter,  # Add previous chapter info\n                    \"next_chapter\": next_chapter,  # Add next chapter info\n                    # previous_chapters_summary will be added dynamically in exec\n                })\n            else:\n                print(f\"Warning: Invalid abstraction index {abstraction_index} in chapter_order. Skipping.\")\n\n        print(f\"Preparing to write {len(items_to_process)} chapters...\")\n        return items_to_process # Iterable for BatchNode\n\n    def exec(self, item):\n        # This runs for each item prepared above\n        abstraction_name = item[\"abstraction_details\"][\"name\"]\n        chapter_num = item[\"chapter_num\"]\n        project_name = item.get(\"project_name\")  # Get from item\n        print(f\"Writing chapter {chapter_num} for: {abstraction_name} using LLM...\")\n\n        # Prepare file context string from the map\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path.split('# ')[1] if '# ' in idx_path else idx_path} ---\\n{content}\"\n            for idx_path, content in item[\"related_files_content_map\"].items()\n        )\n\n        # Get summary of chapters written *before* this one\n        # Use the temporary instance variable\n        previous_chapters_summary = \"\\n---\\n\".join(self.chapters_written_so_far)\n\n\n        prompt = f\"\"\"\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `{project_name}` about the concept: \"{abstraction_name}\". This is Chapter {chapter_num}.\n\nConcept Details:\n- Description:\n{item[\"abstraction_details\"][\"description\"]}\n\nComplete Tutorial Structure:\n{item[\"full_chapter_listing\"]}\n\nContext from previous chapters (summary):\n{previous_chapters_summary if previous_chapters_summary else \"This is the first chapter.\"}\n\nRelevant Code Snippets:\n{file_context_str if file_context_str else \"No specific code snippets provided for this abstraction.\"}\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter {chapter_num}: {abstraction_name}`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n\"\"\"\n        chapter_content = call_llm(prompt)\n        # Basic validation/cleanup\n        actual_heading = f\"# Chapter {chapter_num}: {abstraction_name}\"\n        if not chapter_content.strip().startswith(f\"# Chapter {chapter_num}\"):\n             # Add heading if missing or incorrect, trying to preserve content\n             lines = chapter_content.strip().split('\\n')\n             if lines and lines[0].strip().startswith(\"#\"): # If there's some heading, replace it\n                 lines[0] = actual_heading\n                 chapter_content = \"\\n\".join(lines)\n             else: # Otherwise, prepend it\n                 chapter_content = f\"{actual_heading}\\n\\n{chapter_content}\"\n\n        # Add the generated content to our temporary list for the next iteration's context\n        self.chapters_written_so_far.append(chapter_content)\n\n        return chapter_content # Return the Markdown string\n\n    def post(self, shared, prep_res, exec_res_list):\n        # exec_res_list contains the generated Markdown for each chapter, in order\n        shared[\"chapters\"] = exec_res_list\n        # Clean up the temporary instance variable\n        del self.chapters_written_so_far\n        print(f\"Finished writing {len(exec_res_list)} chapters.\")\n\nclass CombineTutorial(Node):\n    def prep(self, shared):\n        project_name = shared[\"project_name\"]\n        output_base_dir = shared.get(\"output_dir\", \"output\") # Default output dir\n        output_path = os.path.join(output_base_dir, project_name)\n        repo_url = shared[\"repo_url\"]  # Get the repository URL\n\n        # Use 'label' from relationships_data['details']\n        relationships_data = shared[\"relationships\"] # {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        chapter_order = shared[\"chapter_order\"] # indices\n        abstractions = shared[\"abstractions\"]   # list of dicts\n        chapters_content = shared[\"chapters\"]   # list of strings\n\n        # --- Generate Mermaid Diagram ---\n        mermaid_lines = [\"flowchart TD\"]\n        # Add nodes for each abstraction\n        for i, abstr in enumerate(abstractions):\n            # Sanitize name for Mermaid ID and label\n            node_id = f\"A{i}\"\n            sanitized_name = abstr['name'].replace('\"', '')\n            node_label = sanitized_name # Using sanitized name only, no index\n            mermaid_lines.append(f'    {node_id}[\"{node_label}\"]')\n        # Add edges for relationships using 'label'\n        for rel in relationships_data['details']:\n            from_node_id = f\"A{rel['from']}\"\n            to_node_id = f\"A{rel['to']}\"\n            # Sanitize 'label' for edge label\n            edge_label = rel['label'].replace('\"', '').replace('\\n', ' ') # Basic sanitization\n            # Limit edge label length for readability (optional, but good for diagrams)\n            max_label_len = 30 # Make it shorter for labels\n            if len(edge_label) > max_label_len:\n                edge_label = edge_label[:max_label_len-3] + \"...\"\n            mermaid_lines.append(f'    {from_node_id} -- \"{edge_label}\" --> {to_node_id}')\n\n        mermaid_diagram = \"\\n\".join(mermaid_lines)\n        # --- End Mermaid ---\n\n\n        # Prepare index.md content\n        index_content = f\"# Tutorial: {project_name}\\n\\n\"\n        index_content += f\"{relationships_data['summary']}\\n\\n\"\n        index_content += f\"**Source Repository:** [{repo_url}]({repo_url})\\n\\n\"\n\n        # Add Mermaid diagram for relationships\n        index_content += \"```mermaid\\n\"\n        index_content += mermaid_diagram + \"\\n\"\n        index_content += \"```\\n\\n\"\n\n        index_content += \"## Chapters\\n\\n\"\n\n        chapter_files = []\n        # Generate chapter links based on the determined order\n        for i, abstraction_index in enumerate(chapter_order):\n            # Ensure index is valid and we have content for it\n            if 0 <= abstraction_index < len(abstractions) and i < len(chapters_content):\n                abstraction_name = abstractions[abstraction_index][\"name\"]\n                # Sanitize name for filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in abstraction_name).lower()\n                # Use chapter number (i+1) for ordering filename\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                index_content += f\"{i+1}. [{abstraction_name}]({filename})\\n\"\n                \n                # Add attribution to chapter content\n                chapter_content = chapters_content[i]\n                if not chapter_content.endswith(\"\\n\\n\"):\n                    chapter_content += \"\\n\\n\"\n                chapter_content += \"---\\n\\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\"\n                \n                # Store filename and corresponding content\n                chapter_files.append({\"filename\": filename, \"content\": chapter_content})\n            else:\n                 print(f\"Warning: Mismatch between chapter order, abstractions, or content at index {i} (abstraction index {abstraction_index}). Skipping file generation for this entry.\")\n\n        # Add attribution to index content\n        index_content += \"\\n\\n---\\n\\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\"\n\n        return {\n            \"output_path\": output_path,\n            \"index_content\": index_content,\n            \"chapter_files\": chapter_files # List of {\"filename\": str, \"content\": str}\n        }\n\n    def exec(self, prep_res):\n        output_path = prep_res[\"output_path\"]\n        index_content = prep_res[\"index_content\"]\n        chapter_files = prep_res[\"chapter_files\"]\n\n        print(f\"Combining tutorial into directory: {output_path}\")\n        # Rely on Node's built-in retry/fallback\n        os.makedirs(output_path, exist_ok=True)\n\n        # Write index.md\n        index_filepath = os.path.join(output_path, \"index.md\")\n        with open(index_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(index_content)\n        print(f\"  - Wrote {index_filepath}\")\n\n        # Write chapter files\n        for chapter_info in chapter_files:\n            chapter_filepath = os.path.join(output_path, chapter_info[\"filename\"])\n            with open(chapter_filepath, \"w\", encoding=\"utf-8\") as f:\n                f.write(chapter_info[\"content\"])\n            print(f\"  - Wrote {chapter_filepath}\")\n\n        return output_path # Return the final path\n\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"final_output_dir\"] = exec_res # Store the output path\n        print(f\"\\nTutorial generation complete! Files are in: {exec_res}\")\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 6: LLM Orchestration (PocketFlow)`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "```markdown\n# Chapter 6: LLM Orchestration (PocketFlow)\n\nIn the previous chapter, [Tutorial Content Combination](05_tutorial_content_combination.md), we saw how the individual pieces of our tutorial come together to form a finished product. But how are all these pieces coordinated? That's where LLM Orchestration, specifically using the `PocketFlow` library, comes into play!\n\nImagine you're conducting an orchestra. You have different sections (strings, woodwinds, brass) that need to play in a specific sequence and at the right time to create a beautiful symphony. LLM Orchestration is like the conductor, ensuring that all the different parts of our tutorial generation system run in the correct order and pass data to each other smoothly.\n\n**Why LLM Orchestration?**\n\nOur tutorial generation system involves several complex steps: crawling the codebase, analyzing the code with AI, writing individual chapters, and combining them into a complete tutorial. Without proper orchestration, these steps would be difficult to manage, leading to errors and inconsistencies. LLM Orchestration helps us:\n\n*   **Define a clear workflow:**  Specify the order in which the different steps should run.\n*   **Manage dependencies:** Ensure that each step has the data it needs before it starts.\n*   **Handle errors:** Provide mechanisms for retrying failed steps or gracefully handling errors.\n*   **Simplify Complexity:** Abstract away the complexity of managing a large, multi-step process.\n\n**The Goal: Simplify Running Large Language Model Based Applications.**\n\nWith LLM orchestration, we aim to provide a simplified way to describe and run complex LLM based applications. In this project, we use `PocketFlow` to run a sequence of LLM calls.\n\n**Key Concepts**\n\nLet's break down the core ideas behind LLM Orchestration using `PocketFlow`:\n\n1.  **Directed Acyclic Graph (DAG):** A graph where each node represents a task, and the edges represent the flow of data between tasks. The graph must be acyclic, meaning there are no loops. This ensures that the workflow can be executed in a well-defined order. Think of it as a recipe with clear instructions and no circular dependencies.\n\n2.  **Nodes:**  Individual tasks or steps in the workflow. Each node performs a specific action, such as fetching code or writing a chapter. In `PocketFlow`, nodes inherit from the `Node` or `BatchNode` class.\n\n3.  **Flow:**  The overall workflow, represented as a DAG. The flow defines the order in which the nodes are executed and how data is passed between them.\n\n4.  **`PocketFlow` Library:**  A lightweight Python library for defining and executing workflows. It provides a simple and intuitive way to create DAGs and manage dependencies.\n\n**How it Works: Orchestrating the Tutorial Generation Process**\n\nIn our tutorial generation system, `PocketFlow` is used to orchestrate the following steps:\n\n1.  **Defining the Flow (`flow.py`):** The `create_tutorial_flow()` function in `flow.py` defines the DAG for our tutorial generation process.\n\n    ```python\n    from pocketflow import Flow\n    # Import all node classes from nodes.py\n    from nodes import (\n        FetchRepo,\n        IdentifyAbstractions,\n        AnalyzeRelationships,\n        OrderChapters,\n        WriteChapters,\n        CombineTutorial\n    )\n\n    def create_tutorial_flow():\n        \"\"\"Creates and returns the codebase tutorial generation flow.\"\"\"\n\n        # Instantiate nodes\n        fetch_repo = FetchRepo()\n        identify_abstractions = IdentifyAbstractions(max_retries=3, wait=10)\n        analyze_relationships = AnalyzeRelationships(max_retries=3, wait=10)\n        order_chapters = OrderChapters(max_retries=3, wait=10)\n        write_chapters = WriteChapters(max_retries=3, wait=10) # This is a BatchNode\n        combine_tutorial = CombineTutorial()\n\n        # Connect nodes in sequence based on the design\n        fetch_repo >> identify_abstractions\n        identify_abstractions >> analyze_relationships\n        analyze_relationships >> order_chapters\n        order_chapters >> write_chapters\n        write_chapters >> combine_tutorial\n\n        # Create the flow starting with FetchRepo\n        tutorial_flow = Flow(start=fetch_repo)\n\n        return tutorial_flow\n    ```\n\n    *   This code defines the following nodes:\n        *   `FetchRepo`: Fetches the codebase from a repository or local directory.\n        *   `IdentifyAbstractions`: Identifies key abstractions in the codebase using AI.\n        *   `AnalyzeRelationships`: Analyzes the relationships between the identified abstractions using AI.\n        *   `OrderChapters`: Determines the order in which the chapters should be presented.\n        *   `WriteChapters`: Writes the individual chapters of the tutorial using AI. This is a `BatchNode` because it generates multiple chapters in parallel.\n        *   `CombineTutorial`: Combines the generated chapters into a complete tutorial.\n\n    *   The `>>` operator defines the flow of data between the nodes. For example, `fetch_repo >> identify_abstractions` means that the output of the `fetch_repo` node is passed as input to the `identify_abstractions` node.\n\n    *   The `Flow` class creates a DAG starting with the `FetchRepo` node.\n\n2.  **Running the Flow (`main.py`):** The `main.py` script creates an instance of the flow and runs it.\n\n    ```python\n    from flow import create_tutorial_flow\n\n    def main():\n        # ... (Code to parse command-line arguments and create the 'shared' dictionary) ...\n\n        # Create the flow instance\n        tutorial_flow = create_tutorial_flow()\n\n        # Run the flow\n        tutorial_flow.run(shared)\n    ```\n\n    *   The `tutorial_flow.run(shared)` method executes the DAG, passing the `shared` dictionary as input to the first node (`FetchRepo`). The `shared` dictionary acts as a central knowledge base, allowing the nodes to share information.\n\n3.  **Data Flow:** As each node executes, it processes the input data and produces output. This output is then passed to the next node in the flow.\n\n    *   For example, the `FetchRepo` node fetches the codebase and stores it in the `shared[\"files\"]` variable. The `IdentifyAbstractions` node then accesses `shared[\"files\"]` to analyze the code and identify key abstractions.\n\n4.  **Error Handling:**  `PocketFlow` provides mechanisms for handling errors, such as retrying failed nodes. The `max_retries` and `wait` parameters in the `IdentifyAbstractions` node specify that the node should be retried up to 3 times if it fails, with a 10-second delay between retries.\n\n**Code Example: Defining a Node**\n\nLet's look at a simplified example of a `PocketFlow` node:\n\n```python\nfrom pocketflow import Node\n\nclass MyNode(Node):\n    def prep(self, shared):\n        # Prepare the input data for the 'exec' method\n        input_data = shared[\"input_data\"]\n        return input_data\n\n    def exec(self, prep_res):\n        # Process the input data and return the output\n        output_data = prep_res * 2\n        return output_data\n\n    def post(self, shared, prep_res, exec_res):\n        # Store the output data in the 'shared' dictionary\n        shared[\"output_data\"] = exec_res\n```\n\n*   The `prep` method prepares the input data for the `exec` method. It takes the `shared` dictionary as input and returns the data that should be processed by the `exec` method.\n*   The `exec` method processes the input data and returns the output. It takes the output of the `prep` method as input.\n*   The `post` method stores the output data in the `shared` dictionary. It takes the output of the `exec` method as input.\n\n**Benefits and Considerations**\n\n*   **Benefits:**\n    *   **Simplified Workflow Management:** `PocketFlow` provides a simple and intuitive way to define and execute complex workflows.\n    *   **Improved Code Readability:** The DAG-based approach makes the code more readable and easier to understand.\n    *   **Enhanced Error Handling:** `PocketFlow` provides mechanisms for handling errors, such as retrying failed nodes.\n    *   **Increased Modularity:** Each node is a self-contained unit, making it easier to modify or replace individual steps in the workflow.\n*   **Considerations:**\n    *   **Learning Curve:** Understanding the basics of DAGs and `PocketFlow` can take some time.\n    *   **Overhead:** Using a workflow management system like `PocketFlow` can introduce some overhead compared to a simple sequential execution of the steps.\n\n**Analogy: The Assembly Line**\n\nImagine an assembly line in a factory.\n\n*   **Nodes:**  Individual workstations on the assembly line, where specific tasks are performed (e.g., attaching wheels to a car).\n*   **Flow:**  The sequence in which the workstations are arranged, defining the order in which the tasks are performed.\n*   **`PocketFlow`:**  The system that manages the assembly line, ensuring that each workstation has the necessary parts and tools, and that the tasks are performed in the correct order.\n\n**Using the Entire System: From Repo URL to Tutorial**\n\nTo generate a tutorial for a codebase, you would:\n\n1.  **Install Dependencies:** Make sure you have all the necessary dependencies installed, including `PocketFlow` and the other libraries used in our tutorial generation system.\n2.  **Run the `main.py` Script:** Execute the `main.py` script with the appropriate command-line arguments, such as the repository URL and project name.\n\n    ```bash\n    python main.py --repo https://github.com/user/my-cool-project -n MyCoolProjectTutorial\n    ```\n\n3.  **Review the Output:** After the script finishes running, you will find the generated tutorial in the specified output directory.\n\n**Conclusion**\n\nIn this chapter, we explored the concept of LLM Orchestration using `PocketFlow`. By defining a clear workflow and managing the data flow between different steps, we can simplify the complexity of our tutorial generation system and improve its reliability and maintainability.  We saw how `PocketFlow` is used to orchestrate the entire tutorial generation process, from fetching the codebase to combining the generated content into a complete tutorial.\n\nCongratulations! You have now completed the tutorial on building a tutorial generation system. You learned about all the crucial stages for automatically generating tutorials for any codebase, including [Main Execution Script](01_main_execution_script.md), [AI-Powered Code Analysis and Generation](02_ai_powered_code_analysis_and_generation.md), [Codebase Crawling](03_codebase_crawling.md), [Knowledge Base Construction](04_knowledge_base_construction.md), [Tutorial Content Combination](05_tutorial_content_combination.md), and [LLM Orchestration (PocketFlow)](06_llm_orchestration__pocketflow_.md).\n```", "\nFor the project `fastapi`:\n\nCodebase Context:\n\n\nAnalyze the codebase context.\nIdentify the top 5-10 core most important abstractions to help those new to the codebase.\n\nFor each abstraction, provide:\n1. A concise `name`.\n2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\nList of file indices and paths present in the context:\n\n\nFormat the output as a YAML list of dictionaries:\n\n```yaml\n- name: Query Processing\n  description: | \n    Explains what the abstraction does.\n    It's like a central dispatcher routing requests.\n  file_indices:\n    - 0 # path/to/file1.py\n    - 3 # path/to/related.py\n- name: Query Optimization\n  description: |\n    Another core concept, similar to a blueprint for objects.\n  file_indices:\n    - 5 # path/to/another.js\n# ... up to 10 abstractions\n```": "Okay, I'm ready to analyze the codebase and identify the top abstractions. Based on the general knowledge of how FastAPI works, here's a YAML representation of the 5 most important abstractions:\n\n```yaml\n- name: Path Operations (Routes)\n  description: |\n    Path Operations, or Routes, define the endpoints of your API. Think of them as addresses on a website. Each address (path) is paired with a specific HTTP method (like GET or POST) and a Python function. When a request comes in to a specific path with the right method, that function is executed. It's how FastAPI knows *what* to do when someone visits a particular URL on your API.\n  file_indices:\n    - 0 # fastapi/routing.py: Handles the mapping of paths to functions.\n    - 1 # fastapi/applications.py: Contains the main FastAPI app and includes routing mechanisms.\n    - 2 # fastapi/operations.py: Defines internal representation of operations/routes.\n\n- name: Request Body & Data Validation\n  description: |\n    FastAPI automatically handles the process of reading data from incoming requests (e.g., JSON) and making sure the data is in the correct format. This is like a customs officer at an airport checking your documents and luggage to make sure everything is valid before letting you enter.  It uses Python type hints to define the expected data structure and automatically validates the data against those hints.\n  file_indices:\n    - 3 # fastapi/dependencies/utils.py:  Deals with dependency injection and parameter handling from requests.\n    - 4 # fastapi/params.py: Defines parameter types (Query, Path, Body, etc.) and their validation behavior.\n    - 5 # pydantic/main.py: Pydantic models are the core of data validation.\n\n- name: Dependency Injection\n  description: |\n    Dependency Injection in FastAPI provides a way to automatically handle dependencies required by your path operation functions.  Imagine a restaurant where the chef doesn't have to personally go and fetch all the ingredients. Instead, a delivery service (dependency injection system) brings the ingredients (dependencies) to the chef's kitchen before they start cooking. This simplifies code and promotes reusability.\n  file_indices:\n    - 3 # fastapi/dependencies/utils.py: Core dependency resolution logic.\n    - 6 # fastapi/dependencies/models.py: Defines classes related to dependency injection.\n\n- name: Pydantic Models\n  description: |\n    Pydantic Models are classes used to define the structure and type of the data your API receives and sends. Think of them as blueprints for your data.  They ensure that the data conforms to a specific schema (like a database table structure) and are also used for data validation, serialization (converting Python objects to JSON), and deserialization (converting JSON to Python objects). They are the base building blocks for data handling in FastAPI.\n  file_indices:\n    - 5 # pydantic/main.py: Defines the base Pydantic `BaseModel` class and its behavior.\n    - 7 # fastapi/utils.py: Utilities that use Pydantic models, e.g. for request/response models.\n\n- name: Response Handling\n  description: |\n    FastAPI manages the process of creating and returning responses to clients. It automatically converts Python objects into JSON format for the API response.  It\u2019s like a translation service that takes a document written in English (Python object) and translates it into French (JSON) so that a French-speaking person (the client) can understand it. FastAPI also provides mechanisms for returning different HTTP status codes and custom headers.\n  file_indices:\n    - 8 # fastapi/responses.py: Defines different response types (JSONResponse, HTMLResponse, etc.).\n    - 9 # fastapi/encoders.py: Handles the encoding of Python objects into JSON compatible formats.\n```", "\nFor the project `v1`:\n\nCodebase Context:\n--- File Index 0: app.py ---\nfrom fastapi import FastAPI, Request, Form\nfrom fastapi.responses import HTMLResponse\nfrom fastapi.templating import Jinja2Templates\nimport os\nimport requests\nfrom dotenv import load_dotenv\nfrom typing import List, Optional\nfrom fastapi.responses import JSONResponse\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Set up FastAPI\napp = FastAPI()\n\n# Set up templates (Jinja2)\ntemplates = Jinja2Templates(directory=\"templates\")\n\n# Constants\nGITHUB_API_URL = \"https://api.github.com/graphql\"\n\n\ndef get_github_token() -> str:\n    \"\"\"Fetch GitHub token from environment variable.\"\"\"\n    token = os.getenv(\"GITHUB_TOKEN\")\n    if not token:\n        raise ValueError(\"GitHub token not found. Please set the GITHUB_TOKEN env variable.\")\n    return token\n\n\ndef fetch_pinned_repos(username: str, token: str) -> Optional[List[dict]]:\n    \"\"\"Fetch pinned repositories using GitHub GraphQL API.\"\"\"\n    query = f\"\"\"\n    {{\n      user(login: \"{username}\") {{\n        pinnedItems(first: 6, types: [REPOSITORY]) {{\n          edges {{\n            node {{\n              ... on Repository {{\n                name\n                description\n                url\n                stargazerCount\n                forkCount\n                primaryLanguage {{\n                  name\n                }}\n              }}\n            }}\n          }}\n        }}\n      }}\n    }}\n    \"\"\"\n\n    headers = {\"Authorization\": f\"Bearer {token}\"}\n    response = requests.post(GITHUB_API_URL, json={\"query\": query}, headers=headers)\n\n    if response.status_code == 200:\n        return response.json()[\"data\"][\"user\"][\"pinnedItems\"][\"edges\"]\n    else:\n        return None\n\n\n@app.get(\"/\", response_class=HTMLResponse)\nasync def home(request: Request):\n    \"\"\"Render the homepage with the GitHub username form.\"\"\"\n    return templates.TemplateResponse(\"home.html\", {\"request\": request})\n\n\n@app.post(\"/repos\", response_class=HTMLResponse)\nasync def repos(request: Request, username: str = Form(...)):\n    \"\"\"Fetch and display pinned repos based on the provided GitHub username.\"\"\"\n    token = os.getenv(\"GITHUB_TOKEN\")\n    repos = fetch_pinned_repos(username, token)\n    if repos is None:\n        return templates.TemplateResponse(\"home.html\", {\"request\": request, \"error\": \"Failed to fetch repos or no pinned repos found.\"})\n    return templates.TemplateResponse(\"repos.html\", {\"request\": request, \"repos\": repos, \"username\": username})\n  \n\n@app.get(\"/api/repos/{username}\")\nasync def api_repos(username: str):\n    token = get_github_token()\n    repos = fetch_pinned_repos(username, token)\n    if repos is None:\n        return JSONResponse(content={\"error\": \"Failed to fetch repos.\"}, status_code=400)\n    return {\"repos\": repos}\n\n\n\n\n\n\n\nAnalyze the codebase context.\nIdentify the top 5-10 core most important abstractions to help those new to the codebase.\n\nFor each abstraction, provide:\n1. A concise `name`.\n2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\nList of file indices and paths present in the context:\n- 0 # app.py\n\nFormat the output as a YAML list of dictionaries:\n\n```yaml\n- name: Query Processing\n  description: | \n    Explains what the abstraction does.\n    It's like a central dispatcher routing requests.\n  file_indices:\n    - 0 # path/to/file1.py\n    - 3 # path/to/related.py\n- name: Query Optimization\n  description: |\n    Another core concept, similar to a blueprint for objects.\n  file_indices:\n    - 5 # path/to/another.js\n# ... up to 10 abstractions\n```": "```yaml\n- name: FastAPI Application\n  description: |\n    FastAPI is the main web framework used in this project. Think of it as the \"engine\" of a car. It handles all the incoming web requests, routes them to the correct functions, and returns responses to the user's browser. It's responsible for managing the overall flow of the application. It uses a set of decorators (`@app.get`, `@app.post`) to link URL endpoints to Python functions.\n  file_indices:\n    - 0 # app.py\n\n- name: Jinja2 Templating\n  description: |\n    Jinja2 is the templating engine used to create dynamic HTML pages. Imagine it like a cookie cutter; you have a template (the cutter), and you fill it with data (the dough) to produce a specific cookie (the HTML page). It allows embedding Python variables and logic directly into HTML, making it easier to create personalized webpages.\n  file_indices:\n    - 0 # app.py\n\n- name: GitHub GraphQL API Interaction\n  description: |\n    This involves sending requests to the GitHub GraphQL API to fetch data. Think of it as ordering food from a restaurant. The `query` is like your order, the `headers` (with the token) are your payment details to prove you can pay, and the GitHub API is the restaurant that fulfills your order by providing the requested repository information. The `fetch_pinned_repos` function handles this process.\n  file_indices:\n    - 0 # app.py\n\n- name: Environment Variable Management\n  description: |\n    The project uses `.env` files and the `dotenv` library to store sensitive information like API keys. Imagine this as a safe deposit box. Instead of hardcoding valuable information directly into the code, it is kept in a secure, separate location, preventing accidental exposure. `load_dotenv()` loads variables from `.env` into the environment.\n  file_indices:\n    - 0 # app.py\n\n- name: Data Handling (Repositories)\n  description: |\n    The code fetches, processes, and displays repository data. This is similar to having a librarian who finds books (repositories), categorizes them (processes them), and puts them on display (displays them). The `fetch_pinned_repos` function retrieves repository information and the templates display the output.\n  file_indices:\n    - 0 # app.py\n```", "\nBased on the following abstractions and relevant code snippets from the project `v1`:\n\nList of Abstraction Indices and Names:\n0 # FastAPI Application\n1 # Jinja2 Templating\n2 # GitHub GraphQL API Interaction\n3 # Environment Variable Management\n4 # Data Handling (Repositories)\n\nContext (Abstractions, Descriptions, Code):\nIdentified Abstractions:\n- Index 0: FastAPI Application (Relevant file indices: [0])\n  Description: FastAPI is the main web framework used in this project. Think of it as the \"engine\" of a car. It handles all the incoming web requests, routes them to the correct functions, and returns responses to the user's browser. It's responsible for managing the overall flow of the application. It uses a set of decorators (`@app.get`, `@app.post`) to link URL endpoints to Python functions.\n\n- Index 1: Jinja2 Templating (Relevant file indices: [0])\n  Description: Jinja2 is the templating engine used to create dynamic HTML pages. Imagine it like a cookie cutter; you have a template (the cutter), and you fill it with data (the dough) to produce a specific cookie (the HTML page). It allows embedding Python variables and logic directly into HTML, making it easier to create personalized webpages.\n\n- Index 2: GitHub GraphQL API Interaction (Relevant file indices: [0])\n  Description: This involves sending requests to the GitHub GraphQL API to fetch data. Think of it as ordering food from a restaurant. The `query` is like your order, the `headers` (with the token) are your payment details to prove you can pay, and the GitHub API is the restaurant that fulfills your order by providing the requested repository information. The `fetch_pinned_repos` function handles this process.\n\n- Index 3: Environment Variable Management (Relevant file indices: [0])\n  Description: The project uses `.env` files and the `dotenv` library to store sensitive information like API keys. Imagine this as a safe deposit box. Instead of hardcoding valuable information directly into the code, it is kept in a secure, separate location, preventing accidental exposure. `load_dotenv()` loads variables from `.env` into the environment.\n\n- Index 4: Data Handling (Repositories) (Relevant file indices: [0])\n  Description: The code fetches, processes, and displays repository data. This is similar to having a librarian who finds books (repositories), categorizes them (processes them), and puts them on display (displays them). The `fetch_pinned_repos` function retrieves repository information and the templates display the output.\n\n\nRelevant File Snippets (Referenced by Index and Path):\n--- File: 0 # app.py ---\nfrom fastapi import FastAPI, Request, Form\nfrom fastapi.responses import HTMLResponse\nfrom fastapi.templating import Jinja2Templates\nimport os\nimport requests\nfrom dotenv import load_dotenv\nfrom typing import List, Optional\nfrom fastapi.responses import JSONResponse\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Set up FastAPI\napp = FastAPI()\n\n# Set up templates (Jinja2)\ntemplates = Jinja2Templates(directory=\"templates\")\n\n# Constants\nGITHUB_API_URL = \"https://api.github.com/graphql\"\n\n\ndef get_github_token() -> str:\n    \"\"\"Fetch GitHub token from environment variable.\"\"\"\n    token = os.getenv(\"GITHUB_TOKEN\")\n    if not token:\n        raise ValueError(\"GitHub token not found. Please set the GITHUB_TOKEN env variable.\")\n    return token\n\n\ndef fetch_pinned_repos(username: str, token: str) -> Optional[List[dict]]:\n    \"\"\"Fetch pinned repositories using GitHub GraphQL API.\"\"\"\n    query = f\"\"\"\n    {{\n      user(login: \"{username}\") {{\n        pinnedItems(first: 6, types: [REPOSITORY]) {{\n          edges {{\n            node {{\n              ... on Repository {{\n                name\n                description\n                url\n                stargazerCount\n                forkCount\n                primaryLanguage {{\n                  name\n                }}\n              }}\n            }}\n          }}\n        }}\n      }}\n    }}\n    \"\"\"\n\n    headers = {\"Authorization\": f\"Bearer {token}\"}\n    response = requests.post(GITHUB_API_URL, json={\"query\": query}, headers=headers)\n\n    if response.status_code == 200:\n        return response.json()[\"data\"][\"user\"][\"pinnedItems\"][\"edges\"]\n    else:\n        return None\n\n\n@app.get(\"/\", response_class=HTMLResponse)\nasync def home(request: Request):\n    \"\"\"Render the homepage with the GitHub username form.\"\"\"\n    return templates.TemplateResponse(\"home.html\", {\"request\": request})\n\n\n@app.post(\"/repos\", response_class=HTMLResponse)\nasync def repos(request: Request, username: str = Form(...)):\n    \"\"\"Fetch and display pinned repos based on the provided GitHub username.\"\"\"\n    token = os.getenv(\"GITHUB_TOKEN\")\n    repos = fetch_pinned_repos(username, token)\n    if repos is None:\n        return templates.TemplateResponse(\"home.html\", {\"request\": request, \"error\": \"Failed to fetch repos or no pinned repos found.\"})\n    return templates.TemplateResponse(\"repos.html\", {\"request\": request, \"repos\": repos, \"username\": username})\n  \n\n@app.get(\"/api/repos/{username}\")\nasync def api_repos(username: str):\n    token = get_github_token()\n    repos = fetch_pinned_repos(username, token)\n    if repos is None:\n        return JSONResponse(content={\"error\": \"Failed to fetch repos.\"}, status_code=400)\n    return {\"repos\": repos}\n\n\n\n\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n": "```yaml\nsummary: |\n  This project is a web application that uses **FastAPI** to let users view a *GitHub* user's pinned repositories.  It fetches data from the **GitHub GraphQL API**, uses **Jinja2** to display the repositories in a webpage, and securely manages the *GitHub token* using environment variables.\n\nrelationships:\n  - from_abstraction: 0 # FastAPI Application\n    to_abstraction: 1 # Jinja2 Templating\n    label: \"Uses for rendering\"\n  - from_abstraction: 0 # FastAPI Application\n    to_abstraction: 2 # GitHub GraphQL API Interaction\n    label: \"Orchestrates API calls\"\n  - from_abstraction: 2 # GitHub GraphQL API Interaction\n    to_abstraction: 4 # Data Handling (Repositories)\n    label: \"Provides repository data\"\n  - from_abstraction: 3 # Environment Variable Management\n    to_abstraction: 2 # GitHub GraphQL API Interaction\n    label: \"Provides authentication\"\n  - from_abstraction: 0 # FastAPI Application\n    to_abstraction: 3 # Environment Variable Management\n    label: \"Loads configuration\"\n  - from_abstraction: 4 # Data Handling (Repositories)\n    to_abstraction: 1 # Jinja2 Templating\n    label: \"Displays data\"\n```", "\nGiven the following project abstractions and their relationships for the project ```` v1 ````:\n\nAbstractions (Index # Name):\n- 0 # FastAPI Application\n- 1 # Jinja2 Templating\n- 2 # GitHub GraphQL API Interaction\n- 3 # Environment Variable Management\n- 4 # Data Handling (Repositories)\n\nContext about relationships and project summary:\nProject Summary:\nThis project is a web application that uses **FastAPI** to let users view a *GitHub* user's pinned repositories.  It fetches data from the **GitHub GraphQL API**, uses **Jinja2** to display the repositories in a webpage, and securely manages the *GitHub token* using environment variables.\n\n\nRelationships (Indices refer to abstractions above):\n- From 0 (FastAPI Application) to 1 (Jinja2 Templating): Uses for rendering\n- From 0 (FastAPI Application) to 2 (GitHub GraphQL API Interaction): Orchestrates API calls\n- From 2 (GitHub GraphQL API Interaction) to 4 (Data Handling (Repositories)): Provides repository data\n- From 3 (Environment Variable Management) to 2 (GitHub GraphQL API Interaction): Provides authentication\n- From 0 (FastAPI Application) to 3 (Environment Variable Management): Loads configuration\n- From 4 (Data Handling (Repositories)) to 1 (Jinja2 Templating): Displays data\n\n\nIf you are going to make a tutorial for ```` v1 ````, what is the best order to explain these abstractions, from first to last?\nIdeally, first explain those that are the most important or foundational, perhaps user-facing concepts or entry points. Then move to more detailed, lower-level implementation details or supporting concepts.\n\nOutput the ordered list of abstraction indices, including the name in a comment for clarity. Use the format `idx # AbstractionName`.\n\n```yaml\n- 2 # FoundationalConcept\n- 0 # CoreClassA\n- 1 # CoreClassB (uses CoreClassA)\n- ...\n```\n\nNow, provide the YAML output:\n": "```yaml\n- 0 # FastAPI Application (Entry point, user-facing)\n- 1 # Jinja2 Templating (Rendering the UI)\n- 2 # GitHub GraphQL API Interaction (Fetching data)\n- 4 # Data Handling (Repositories) (Processing data from API)\n- 3 # Environment Variable Management (Securely managing credentials)\n```", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `v1` about the concept: \"FastAPI Application\". This is Chapter 1.\n\nConcept Details:\n- Description:\nFastAPI is the main web framework used in this project. Think of it as the \"engine\" of a car. It handles all the incoming web requests, routes them to the correct functions, and returns responses to the user's browser. It's responsible for managing the overall flow of the application. It uses a set of decorators (`@app.get`, `@app.post`) to link URL endpoints to Python functions.\n\n\nComplete Tutorial Structure:\n1. [FastAPI Application](01_fastapi_application.md)\n2. [Jinja2 Templating](02_jinja2_templating.md)\n3. [GitHub GraphQL API Interaction](03_github_graphql_api_interaction.md)\n4. [Data Handling (Repositories)](04_data_handling__repositories_.md)\n5. [Environment Variable Management](05_environment_variable_management.md)\n\nContext from previous chapters (summary):\nThis is the first chapter.\n\nRelevant Code Snippets:\n--- File: app.py ---\nfrom fastapi import FastAPI, Request, Form\nfrom fastapi.responses import HTMLResponse\nfrom fastapi.templating import Jinja2Templates\nimport os\nimport requests\nfrom dotenv import load_dotenv\nfrom typing import List, Optional\nfrom fastapi.responses import JSONResponse\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Set up FastAPI\napp = FastAPI()\n\n# Set up templates (Jinja2)\ntemplates = Jinja2Templates(directory=\"templates\")\n\n# Constants\nGITHUB_API_URL = \"https://api.github.com/graphql\"\n\n\ndef get_github_token() -> str:\n    \"\"\"Fetch GitHub token from environment variable.\"\"\"\n    token = os.getenv(\"GITHUB_TOKEN\")\n    if not token:\n        raise ValueError(\"GitHub token not found. Please set the GITHUB_TOKEN env variable.\")\n    return token\n\n\ndef fetch_pinned_repos(username: str, token: str) -> Optional[List[dict]]:\n    \"\"\"Fetch pinned repositories using GitHub GraphQL API.\"\"\"\n    query = f\"\"\"\n    {{\n      user(login: \"{username}\") {{\n        pinnedItems(first: 6, types: [REPOSITORY]) {{\n          edges {{\n            node {{\n              ... on Repository {{\n                name\n                description\n                url\n                stargazerCount\n                forkCount\n                primaryLanguage {{\n                  name\n                }}\n              }}\n            }}\n          }}\n        }}\n      }}\n    }}\n    \"\"\"\n\n    headers = {\"Authorization\": f\"Bearer {token}\"}\n    response = requests.post(GITHUB_API_URL, json={\"query\": query}, headers=headers)\n\n    if response.status_code == 200:\n        return response.json()[\"data\"][\"user\"][\"pinnedItems\"][\"edges\"]\n    else:\n        return None\n\n\n@app.get(\"/\", response_class=HTMLResponse)\nasync def home(request: Request):\n    \"\"\"Render the homepage with the GitHub username form.\"\"\"\n    return templates.TemplateResponse(\"home.html\", {\"request\": request})\n\n\n@app.post(\"/repos\", response_class=HTMLResponse)\nasync def repos(request: Request, username: str = Form(...)):\n    \"\"\"Fetch and display pinned repos based on the provided GitHub username.\"\"\"\n    token = os.getenv(\"GITHUB_TOKEN\")\n    repos = fetch_pinned_repos(username, token)\n    if repos is None:\n        return templates.TemplateResponse(\"home.html\", {\"request\": request, \"error\": \"Failed to fetch repos or no pinned repos found.\"})\n    return templates.TemplateResponse(\"repos.html\", {\"request\": request, \"repos\": repos, \"username\": username})\n  \n\n@app.get(\"/api/repos/{username}\")\nasync def api_repos(username: str):\n    token = get_github_token()\n    repos = fetch_pinned_repos(username, token)\n    if repos is None:\n        return JSONResponse(content={\"error\": \"Failed to fetch repos.\"}, status_code=400)\n    return {\"repos\": repos}\n\n\n\n\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 1: FastAPI Application`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "# Chapter 1: FastAPI Application\n\nImagine you're ordering food online. You go to a website, choose what you want, and then click \"Order.\" Behind the scenes, the website needs a \"traffic controller\" to figure out *where* to send your order (to the kitchen), *what* to do with it (prepare the food), and *how* to tell you when it's ready (show you a confirmation message).\n\nThat's exactly what a FastAPI application does for web applications! It's the \"engine\" that handles all the requests coming in, figures out what to do with them, and sends back the right responses.\n\nIn our `v1` project, FastAPI is the main web framework. It's like the conductor of an orchestra, making sure everything plays together harmoniously. We'll be using it to build a web application that fetches pinned repositories from GitHub. Let's dive in!\n\n## What is FastAPI?\n\nThink of FastAPI as a smart and helpful friend who:\n\n*   **Listens for requests:**  It's constantly listening for requests from users visiting our website (like clicking \"Order\").\n*   **Routes requests:** It knows where to send each request based on the URL the user visits (e.g., `/` for the homepage, `/repos` for the repositories).\n*   **Processes requests:** It calls the right Python function to handle each request (e.g., fetch data from GitHub).\n*   **Sends responses:** It sends back a response to the user's browser (e.g., displaying the GitHub repositories).\n\nFastAPI does all this using special \"decorators\" like `@app.get` and `@app.post`. Let's see how they work.\n\n## Key Concept: Route Decorators\n\nThese decorators are the secret sauce that connects URLs to Python functions.\n\n*   `@app.get` is used for requests that *get* data (like displaying the homepage).\n*   `@app.post` is used for requests that *send* data (like submitting a username to fetch repos).\n\nHere's a simple example from our `app.py` file:\n\n```python\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import HTMLResponse\nfrom fastapi.templating import Jinja2Templates\n\napp = FastAPI()\ntemplates = Jinja2Templates(directory=\"templates\")\n\n@app.get(\"/\", response_class=HTMLResponse)\nasync def home(request: Request):\n    \"\"\"Render the homepage with the GitHub username form.\"\"\"\n    return templates.TemplateResponse(\"home.html\", {\"request\": request})\n```\n\n**Explanation:**\n\n1.  `app = FastAPI()`: This line creates a FastAPI \"app\" instance.  Think of it as creating our \"traffic controller.\"\n2.  `@app.get(\"/\", response_class=HTMLResponse)`: This is a decorator. It tells FastAPI: \"When someone visits the root URL (`/`) using a `GET` request (like typing the address in their browser), run the `home` function.\" The `response_class=HTMLResponse` specifies that the function should return HTML content.\n3.  `async def home(request: Request):`: This is the Python function that will be executed. It takes a `Request` object as input, which contains information about the incoming request.\n4.  `return templates.TemplateResponse(\"home.html\", {\"request\": request})`: This line uses Jinja2 templating (more on this in [Jinja2 Templating](02_jinja2_templating.md)) to render the `home.html` template and return it as the response.  The template receives the \"request\" object as context.\n\nSo, when you visit the website's homepage, FastAPI sees the request, runs the `home` function, and sends back the rendered `home.html` page to your browser.\n\n## Example Use Case: Fetching GitHub Repos\n\nLet's walk through how FastAPI is used in our project to fetch GitHub repositories.\n\n1.  **The User Enters a Username:** The user visits the homepage, enters their GitHub username, and clicks \"Submit.\"\n\n2.  **The Form is Submitted:** The browser sends a `POST` request to the `/repos` URL with the username as data.\n\n3.  **FastAPI Routes the Request:** FastAPI sees the `/repos` URL and the `POST` request type. It looks for a matching route.\n\nHere's the route definition from `app.py`:\n\n```python\nfrom fastapi import Form\n\n@app.post(\"/repos\", response_class=HTMLResponse)\nasync def repos(request: Request, username: str = Form(...)):\n    \"\"\"Fetch and display pinned repos based on the provided GitHub username.\"\"\"\n    token = os.getenv(\"GITHUB_TOKEN\")\n    repos = fetch_pinned_repos(username, token)\n    if repos is None:\n        return templates.TemplateResponse(\"home.html\", {\"request\": request, \"error\": \"Failed to fetch repos or no pinned repos found.\"})\n    return templates.TemplateResponse(\"repos.html\", {\"request\": request, \"repos\": repos, \"username\": username})\n```\n\n**Explanation:**\n\n1.  `@app.post(\"/repos\", response_class=HTMLResponse)`: This decorator tells FastAPI that when a `POST` request is made to `/repos`, the `repos` function should be executed.\n2.  `async def repos(request: Request, username: str = Form(...)):`:  This defines the `repos` function.  `username: str = Form(...)` tells FastAPI to expect a form field named \"username\" in the `POST` request.\n3.  `token = os.getenv(\"GITHUB_TOKEN\")`: This retrieves the GitHub token from environment variables (we'll cover this in [Environment Variable Management](05_environment_variable_management.md)).\n4.  `repos = fetch_pinned_repos(username, token)`:  This is the crucial part: it calls the `fetch_pinned_repos` function to actually fetch the data from GitHub using the provided username and token. We'll see how this works in [GitHub GraphQL API Interaction](03_github_graphql_api_interaction.md).\n5.  The remaining code handles potential errors (if `repos` is `None`) and renders the `repos.html` template with the fetched data. We'll talk about [Jinja2 Templating](02_jinja2_templating.md) in the next chapter.\n\n4.  **Data is Fetched and Displayed:** The `repos` function calls the `fetch_pinned_repos` function (details in [GitHub GraphQL API Interaction](03_github_graphql_api_interaction.md)), retrieves the pinned repositories from GitHub, and renders them using Jinja2. The rendered HTML is then sent back to the user's browser.\n\n## Under the Hood: FastAPI's Request Handling\n\nHere's a simplified view of how FastAPI handles requests:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Browser\n    participant FastAPI\n    participant App Logic\n    participant GitHub API\n\n    User->>Browser: Enters URL and submits form\n    Browser->>FastAPI: Sends HTTP Request\n    FastAPI->>App Logic: Routes request to appropriate function (e.g., repos)\n    App Logic->>GitHub API: Fetches data from GitHub (fetch_pinned_repos)\n    GitHub API-->>App Logic: Returns data\n    App Logic->>FastAPI: Returns rendered HTML\n    FastAPI->>Browser: Sends HTTP Response\n    Browser->>User: Displays webpage\n```\n\n**Explanation:**\n\n1.  The **User** interacts with the **Browser**, entering the URL and submitting the form.\n2.  The **Browser** sends an HTTP request to the **FastAPI** application.\n3.  **FastAPI** receives the request and routes it to the appropriate function in the **App Logic**.\n4.  The **App Logic** fetches data from the **GitHub API**.\n5.  The **GitHub API** returns the requested data to the **App Logic**.\n6.  The **App Logic** returns the rendered HTML to **FastAPI**.\n7.  **FastAPI** sends an HTTP response back to the **Browser**.\n8.  The **Browser** displays the webpage to the **User**.\n\n## Where Does FastAPI Live?\n\nThe heart of our FastAPI application is in the `app.py` file. This file defines the FastAPI instance (`app = FastAPI()`) and all the routes using decorators like `@app.get` and `@app.post`. It also contains the logic for handling requests and interacting with other parts of our application.\n\n## Conclusion\n\nIn this chapter, we learned about the core concept of a FastAPI application: how it acts as a \"traffic controller\" for web requests, routing them to the right functions and sending back responses. We saw how route decorators like `@app.get` and `@app.post` connect URLs to Python functions.  We also walked through an example of how FastAPI is used to fetch and display GitHub repositories.\n\nNext, we will explore [Jinja2 Templating](02_jinja2_templating.md), which is used to dynamically generate HTML content in our application.\n", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `v1` about the concept: \"Jinja2 Templating\". This is Chapter 2.\n\nConcept Details:\n- Description:\nJinja2 is the templating engine used to create dynamic HTML pages. Imagine it like a cookie cutter; you have a template (the cutter), and you fill it with data (the dough) to produce a specific cookie (the HTML page). It allows embedding Python variables and logic directly into HTML, making it easier to create personalized webpages.\n\n\nComplete Tutorial Structure:\n1. [FastAPI Application](01_fastapi_application.md)\n2. [Jinja2 Templating](02_jinja2_templating.md)\n3. [GitHub GraphQL API Interaction](03_github_graphql_api_interaction.md)\n4. [Data Handling (Repositories)](04_data_handling__repositories_.md)\n5. [Environment Variable Management](05_environment_variable_management.md)\n\nContext from previous chapters (summary):\n# Chapter 1: FastAPI Application\n\nImagine you're ordering food online. You go to a website, choose what you want, and then click \"Order.\" Behind the scenes, the website needs a \"traffic controller\" to figure out *where* to send your order (to the kitchen), *what* to do with it (prepare the food), and *how* to tell you when it's ready (show you a confirmation message).\n\nThat's exactly what a FastAPI application does for web applications! It's the \"engine\" that handles all the requests coming in, figures out what to do with them, and sends back the right responses.\n\nIn our `v1` project, FastAPI is the main web framework. It's like the conductor of an orchestra, making sure everything plays together harmoniously. We'll be using it to build a web application that fetches pinned repositories from GitHub. Let's dive in!\n\n## What is FastAPI?\n\nThink of FastAPI as a smart and helpful friend who:\n\n*   **Listens for requests:**  It's constantly listening for requests from users visiting our website (like clicking \"Order\").\n*   **Routes requests:** It knows where to send each request based on the URL the user visits (e.g., `/` for the homepage, `/repos` for the repositories).\n*   **Processes requests:** It calls the right Python function to handle each request (e.g., fetch data from GitHub).\n*   **Sends responses:** It sends back a response to the user's browser (e.g., displaying the GitHub repositories).\n\nFastAPI does all this using special \"decorators\" like `@app.get` and `@app.post`. Let's see how they work.\n\n## Key Concept: Route Decorators\n\nThese decorators are the secret sauce that connects URLs to Python functions.\n\n*   `@app.get` is used for requests that *get* data (like displaying the homepage).\n*   `@app.post` is used for requests that *send* data (like submitting a username to fetch repos).\n\nHere's a simple example from our `app.py` file:\n\n```python\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import HTMLResponse\nfrom fastapi.templating import Jinja2Templates\n\napp = FastAPI()\ntemplates = Jinja2Templates(directory=\"templates\")\n\n@app.get(\"/\", response_class=HTMLResponse)\nasync def home(request: Request):\n    \"\"\"Render the homepage with the GitHub username form.\"\"\"\n    return templates.TemplateResponse(\"home.html\", {\"request\": request})\n```\n\n**Explanation:**\n\n1.  `app = FastAPI()`: This line creates a FastAPI \"app\" instance.  Think of it as creating our \"traffic controller.\"\n2.  `@app.get(\"/\", response_class=HTMLResponse)`: This is a decorator. It tells FastAPI: \"When someone visits the root URL (`/`) using a `GET` request (like typing the address in their browser), run the `home` function.\" The `response_class=HTMLResponse` specifies that the function should return HTML content.\n3.  `async def home(request: Request):`: This is the Python function that will be executed. It takes a `Request` object as input, which contains information about the incoming request.\n4.  `return templates.TemplateResponse(\"home.html\", {\"request\": request})`: This line uses Jinja2 templating (more on this in [Jinja2 Templating](02_jinja2_templating.md)) to render the `home.html` template and return it as the response.  The template receives the \"request\" object as context.\n\nSo, when you visit the website's homepage, FastAPI sees the request, runs the `home` function, and sends back the rendered `home.html` page to your browser.\n\n## Example Use Case: Fetching GitHub Repos\n\nLet's walk through how FastAPI is used in our project to fetch GitHub repositories.\n\n1.  **The User Enters a Username:** The user visits the homepage, enters their GitHub username, and clicks \"Submit.\"\n\n2.  **The Form is Submitted:** The browser sends a `POST` request to the `/repos` URL with the username as data.\n\n3.  **FastAPI Routes the Request:** FastAPI sees the `/repos` URL and the `POST` request type. It looks for a matching route.\n\nHere's the route definition from `app.py`:\n\n```python\nfrom fastapi import Form\n\n@app.post(\"/repos\", response_class=HTMLResponse)\nasync def repos(request: Request, username: str = Form(...)):\n    \"\"\"Fetch and display pinned repos based on the provided GitHub username.\"\"\"\n    token = os.getenv(\"GITHUB_TOKEN\")\n    repos = fetch_pinned_repos(username, token)\n    if repos is None:\n        return templates.TemplateResponse(\"home.html\", {\"request\": request, \"error\": \"Failed to fetch repos or no pinned repos found.\"})\n    return templates.TemplateResponse(\"repos.html\", {\"request\": request, \"repos\": repos, \"username\": username})\n```\n\n**Explanation:**\n\n1.  `@app.post(\"/repos\", response_class=HTMLResponse)`: This decorator tells FastAPI that when a `POST` request is made to `/repos`, the `repos` function should be executed.\n2.  `async def repos(request: Request, username: str = Form(...)):`:  This defines the `repos` function.  `username: str = Form(...)` tells FastAPI to expect a form field named \"username\" in the `POST` request.\n3.  `token = os.getenv(\"GITHUB_TOKEN\")`: This retrieves the GitHub token from environment variables (we'll cover this in [Environment Variable Management](05_environment_variable_management.md)).\n4.  `repos = fetch_pinned_repos(username, token)`:  This is the crucial part: it calls the `fetch_pinned_repos` function to actually fetch the data from GitHub using the provided username and token. We'll see how this works in [GitHub GraphQL API Interaction](03_github_graphql_api_interaction.md).\n5.  The remaining code handles potential errors (if `repos` is `None`) and renders the `repos.html` template with the fetched data. We'll talk about [Jinja2 Templating](02_jinja2_templating.md) in the next chapter.\n\n4.  **Data is Fetched and Displayed:** The `repos` function calls the `fetch_pinned_repos` function (details in [GitHub GraphQL API Interaction](03_github_graphql_api_interaction.md)), retrieves the pinned repositories from GitHub, and renders them using Jinja2. The rendered HTML is then sent back to the user's browser.\n\n## Under the Hood: FastAPI's Request Handling\n\nHere's a simplified view of how FastAPI handles requests:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Browser\n    participant FastAPI\n    participant App Logic\n    participant GitHub API\n\n    User->>Browser: Enters URL and submits form\n    Browser->>FastAPI: Sends HTTP Request\n    FastAPI->>App Logic: Routes request to appropriate function (e.g., repos)\n    App Logic->>GitHub API: Fetches data from GitHub (fetch_pinned_repos)\n    GitHub API-->>App Logic: Returns data\n    App Logic->>FastAPI: Returns rendered HTML\n    FastAPI->>Browser: Sends HTTP Response\n    Browser->>User: Displays webpage\n```\n\n**Explanation:**\n\n1.  The **User** interacts with the **Browser**, entering the URL and submitting the form.\n2.  The **Browser** sends an HTTP request to the **FastAPI** application.\n3.  **FastAPI** receives the request and routes it to the appropriate function in the **App Logic**.\n4.  The **App Logic** fetches data from the **GitHub API**.\n5.  The **GitHub API** returns the requested data to the **App Logic**.\n6.  The **App Logic** returns the rendered HTML to **FastAPI**.\n7.  **FastAPI** sends an HTTP response back to the **Browser**.\n8.  The **Browser** displays the webpage to the **User**.\n\n## Where Does FastAPI Live?\n\nThe heart of our FastAPI application is in the `app.py` file. This file defines the FastAPI instance (`app = FastAPI()`) and all the routes using decorators like `@app.get` and `@app.post`. It also contains the logic for handling requests and interacting with other parts of our application.\n\n## Conclusion\n\nIn this chapter, we learned about the core concept of a FastAPI application: how it acts as a \"traffic controller\" for web requests, routing them to the right functions and sending back responses. We saw how route decorators like `@app.get` and `@app.post` connect URLs to Python functions.  We also walked through an example of how FastAPI is used to fetch and display GitHub repositories.\n\nNext, we will explore [Jinja2 Templating](02_jinja2_templating.md), which is used to dynamically generate HTML content in our application.\n\n\nRelevant Code Snippets:\n--- File: app.py ---\nfrom fastapi import FastAPI, Request, Form\nfrom fastapi.responses import HTMLResponse\nfrom fastapi.templating import Jinja2Templates\nimport os\nimport requests\nfrom dotenv import load_dotenv\nfrom typing import List, Optional\nfrom fastapi.responses import JSONResponse\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Set up FastAPI\napp = FastAPI()\n\n# Set up templates (Jinja2)\ntemplates = Jinja2Templates(directory=\"templates\")\n\n# Constants\nGITHUB_API_URL = \"https://api.github.com/graphql\"\n\n\ndef get_github_token() -> str:\n    \"\"\"Fetch GitHub token from environment variable.\"\"\"\n    token = os.getenv(\"GITHUB_TOKEN\")\n    if not token:\n        raise ValueError(\"GitHub token not found. Please set the GITHUB_TOKEN env variable.\")\n    return token\n\n\ndef fetch_pinned_repos(username: str, token: str) -> Optional[List[dict]]:\n    \"\"\"Fetch pinned repositories using GitHub GraphQL API.\"\"\"\n    query = f\"\"\"\n    {{\n      user(login: \"{username}\") {{\n        pinnedItems(first: 6, types: [REPOSITORY]) {{\n          edges {{\n            node {{\n              ... on Repository {{\n                name\n                description\n                url\n                stargazerCount\n                forkCount\n                primaryLanguage {{\n                  name\n                }}\n              }}\n            }}\n          }}\n        }}\n      }}\n    }}\n    \"\"\"\n\n    headers = {\"Authorization\": f\"Bearer {token}\"}\n    response = requests.post(GITHUB_API_URL, json={\"query\": query}, headers=headers)\n\n    if response.status_code == 200:\n        return response.json()[\"data\"][\"user\"][\"pinnedItems\"][\"edges\"]\n    else:\n        return None\n\n\n@app.get(\"/\", response_class=HTMLResponse)\nasync def home(request: Request):\n    \"\"\"Render the homepage with the GitHub username form.\"\"\"\n    return templates.TemplateResponse(\"home.html\", {\"request\": request})\n\n\n@app.post(\"/repos\", response_class=HTMLResponse)\nasync def repos(request: Request, username: str = Form(...)):\n    \"\"\"Fetch and display pinned repos based on the provided GitHub username.\"\"\"\n    token = os.getenv(\"GITHUB_TOKEN\")\n    repos = fetch_pinned_repos(username, token)\n    if repos is None:\n        return templates.TemplateResponse(\"home.html\", {\"request\": request, \"error\": \"Failed to fetch repos or no pinned repos found.\"})\n    return templates.TemplateResponse(\"repos.html\", {\"request\": request, \"repos\": repos, \"username\": username})\n  \n\n@app.get(\"/api/repos/{username}\")\nasync def api_repos(username: str):\n    token = get_github_token()\n    repos = fetch_pinned_repos(username, token)\n    if repos is None:\n        return JSONResponse(content={\"error\": \"Failed to fetch repos.\"}, status_code=400)\n    return {\"repos\": repos}\n\n\n\n\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 2: Jinja2 Templating`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "# Chapter 2: Jinja2 Templating\n\nIn the previous chapter, [FastAPI Application](01_fastapi_application.md), we learned how FastAPI handles web requests and routes them to the right functions. But what happens *after* a function processes the request and *before* the response is sent back to the user's browser? That's where Jinja2 templating comes in!\n\nImagine you want to send a personalized birthday card to your friend. You wouldn't write a brand new card from scratch every time, right? Instead, you'd use a pre-made card with a space to fill in your friend's name. Jinja2 is like that pre-made card, and the data (your friend's name) is what makes it personalized.\n\nIn our `v1` project, Jinja2 helps us create dynamic HTML pages. It lets us embed Python variables and logic directly into our HTML code, making it easier to display information like a user's GitHub repositories.\n\n## What is Jinja2?\n\nJinja2 is a **templating engine**. Think of it like a cookie cutter.\n\n*   **Template (Cookie Cutter):** A template is an HTML file with placeholders for data. It defines the structure of the webpage.\n*   **Data (Dough):** The data is the information you want to display on the page (e.g., a username, a list of repositories).\n*   **Output (Cookie):** The output is the final HTML page, with the data filled in.\n\nJinja2 takes a template and data and combines them to create a personalized HTML page.\n\n## Key Concepts\n\nLet's break down the key concepts of Jinja2 templating:\n\n1.  **Templates:** These are HTML files that contain placeholders for dynamic data. Placeholders are denoted by special syntax, like `{{ variable_name }}` or `{% logic %}`.\n\n2.  **Variables:** These are placeholders in the template that get replaced with actual data when the template is rendered.  They're accessed using double curly braces: `{{ username }}`. If we pass `username=\"octocat\"` to the template, this will become \"octocat\" in the final HTML.\n\n3.  **Control Structures:** These allow you to add logic to your templates, like loops and conditional statements.  They're accessed using curly braces and percent signs: `{% if repos %}` ... `{% endif %}` or `{% for repo in repos %}` ... `{% endfor %}`.  This lets you show different content depending on whether data exists or to display repeating data.\n\n4.  **Rendering:** This is the process of combining the template and the data to produce the final HTML output.  FastAPI uses Jinja2 to render the templates.\n\n## How to Use Jinja2\n\nLet's see how Jinja2 is used in our `v1` project to display GitHub repositories.\n\nFirst, we have our FastAPI endpoint in `app.py`:\n\n```python\nfrom fastapi import FastAPI, Request, Form\nfrom fastapi.responses import HTMLResponse\nfrom fastapi.templating import Jinja2Templates\n\napp = FastAPI()\ntemplates = Jinja2Templates(directory=\"templates\")\n\n@app.post(\"/repos\", response_class=HTMLResponse)\nasync def repos(request: Request, username: str = Form(...)):\n    # ... (fetch repos logic - skipped for brevity) ...\n    return templates.TemplateResponse(\"repos.html\", {\"request\": request, \"repos\": repos, \"username\": username})\n```\n\n**Explanation:**\n\n1.  `templates = Jinja2Templates(directory=\"templates\")`: This line initializes Jinja2 and tells it to look for templates in the `templates` directory.\n2.  `templates.TemplateResponse(\"repos.html\", {\"request\": request, \"repos\": repos, \"username\": username})`: This line renders the `repos.html` template. It passes a dictionary of data to the template, including:\n    *   `request`: The incoming request object (required by Jinja2).\n    *   `repos`: A list of GitHub repositories fetched from the API.\n    *   `username`: The GitHub username entered by the user.\n\nNow, let's look at the `repos.html` template:\n\n```html\n<!DOCTYPE html>\n<html>\n<head>\n    <title>GitHub Repositories</title>\n</head>\n<body>\n    <h1>Pinned Repositories for {{ username }}</h1>\n    {% if repos %}\n        <ul>\n            {% for repo in repos %}\n                <li>\n                    <a href=\"{{ repo.node.url }}\">{{ repo.node.name }}</a> - {{ repo.node.description }}\n                </li>\n            {% endfor %}\n        </ul>\n    {% else %}\n        <p>No pinned repositories found.</p>\n    {% endif %}\n</body>\n</html>\n```\n\n**Explanation:**\n\n1.  `<h1>Pinned Repositories for {{ username }}</h1>`: This line uses a variable to display the username.  If `username` is \"octocat\", the heading will be \"Pinned Repositories for octocat\".\n\n2.  `{% if repos %}` ... `{% else %}` ... `{% endif %}`: This is a conditional statement. It checks if the `repos` variable is not empty. If it's not empty, the code inside the `if` block is executed. Otherwise, the code inside the `else` block is executed.\n\n3.  `{% for repo in repos %}` ... `{% endfor %}`: This is a loop. It iterates over each repository in the `repos` list.\n\n4.  `<a href=\"{{ repo.node.url }}\">{{ repo.node.name }}</a> - {{ repo.node.description }}`: This line displays the name, description, and URL of each repository.  It uses variables to access the data within each `repo` item.  `repo.node.url` refers to accessing the \"url\" field from the \"node\" field from the \"repo\" variable.\n\n**Example:**\n\nLet's say we pass the following data to the `repos.html` template:\n\n```python\nusername = \"octocat\"\nrepos = [\n    {\"node\": {\"name\": \"Spoon-Knife\", \"description\": \"This repo is for...\", \"url\": \"https://github.com/octocat/Spoon-Knife\"}},\n    {\"node\": {\"name\": \"Hello-World\", \"description\": \"My first repository...\", \"url\": \"https://github.com/octocat/Hello-World\"}},\n]\n```\n\nThe resulting HTML would be:\n\n```html\n<!DOCTYPE html>\n<html>\n<head>\n    <title>GitHub Repositories</title>\n</head>\n<body>\n    <h1>Pinned Repositories for octocat</h1>\n    <ul>\n        <li>\n            <a href=\"https://github.com/octocat/Spoon-Knife\">Spoon-Knife</a> - This repo is for...\n        </li>\n        <li>\n            <a href=\"https://github.com/octocat/Hello-World\">Hello-World</a> - My first repository...\n        </li>\n    </ul>\n</body>\n</html>\n```\n\n## Under the Hood: Jinja2's Rendering Process\n\nHere's a simplified view of how Jinja2 renders a template:\n\n```mermaid\nsequenceDiagram\n    participant FastAPI\n    participant Jinja2\n    participant Template\n    participant Data\n\n    FastAPI->>Jinja2: Call render_template(template, data)\n    Jinja2->>Template: Load template file (e.g., repos.html)\n    Jinja2->>Data: Receive data (e.g., repos, username)\n    Jinja2->>Jinja2: Process template, replacing variables with data\n    Jinja2-->>FastAPI: Return rendered HTML\n```\n\n**Explanation:**\n\n1.  **FastAPI** calls `render_template` in Jinja2, passing the template name and the data.\n2.  **Jinja2** loads the template file from the `templates` directory.\n3.  **Jinja2** receives the data from FastAPI.\n4.  **Jinja2** processes the template, replacing the variables with the data. It also executes any control structures (loops, conditional statements).\n5.  **Jinja2** returns the rendered HTML to FastAPI.\n\n## Where Does Jinja2 Live?\n\nThe Jinja2 templates live in the `templates` directory. FastAPI is configured to look for templates in this directory when you initialize `Jinja2Templates`.  The line `templates = Jinja2Templates(directory=\"templates\")` in `app.py` is what sets this up.\n\n## Conclusion\n\nIn this chapter, we learned about Jinja2 templating and how it helps us create dynamic HTML pages in our `v1` project. We saw how to use variables and control structures to personalize the content of our webpages.  We also walked through the rendering process and understood where Jinja2 templates live in our project.\n\nNext, we'll dive into [GitHub GraphQL API Interaction](03_github_graphql_api_interaction.md) to see how we fetch the data that we display in our templates.\n", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `v1` about the concept: \"GitHub GraphQL API Interaction\". This is Chapter 3.\n\nConcept Details:\n- Description:\nThis involves sending requests to the GitHub GraphQL API to fetch data. Think of it as ordering food from a restaurant. The `query` is like your order, the `headers` (with the token) are your payment details to prove you can pay, and the GitHub API is the restaurant that fulfills your order by providing the requested repository information. The `fetch_pinned_repos` function handles this process.\n\n\nComplete Tutorial Structure:\n1. [FastAPI Application](01_fastapi_application.md)\n2. [Jinja2 Templating](02_jinja2_templating.md)\n3. [GitHub GraphQL API Interaction](03_github_graphql_api_interaction.md)\n4. [Data Handling (Repositories)](04_data_handling__repositories_.md)\n5. [Environment Variable Management](05_environment_variable_management.md)\n\nContext from previous chapters (summary):\n# Chapter 1: FastAPI Application\n\nImagine you're ordering food online. You go to a website, choose what you want, and then click \"Order.\" Behind the scenes, the website needs a \"traffic controller\" to figure out *where* to send your order (to the kitchen), *what* to do with it (prepare the food), and *how* to tell you when it's ready (show you a confirmation message).\n\nThat's exactly what a FastAPI application does for web applications! It's the \"engine\" that handles all the requests coming in, figures out what to do with them, and sends back the right responses.\n\nIn our `v1` project, FastAPI is the main web framework. It's like the conductor of an orchestra, making sure everything plays together harmoniously. We'll be using it to build a web application that fetches pinned repositories from GitHub. Let's dive in!\n\n## What is FastAPI?\n\nThink of FastAPI as a smart and helpful friend who:\n\n*   **Listens for requests:**  It's constantly listening for requests from users visiting our website (like clicking \"Order\").\n*   **Routes requests:** It knows where to send each request based on the URL the user visits (e.g., `/` for the homepage, `/repos` for the repositories).\n*   **Processes requests:** It calls the right Python function to handle each request (e.g., fetch data from GitHub).\n*   **Sends responses:** It sends back a response to the user's browser (e.g., displaying the GitHub repositories).\n\nFastAPI does all this using special \"decorators\" like `@app.get` and `@app.post`. Let's see how they work.\n\n## Key Concept: Route Decorators\n\nThese decorators are the secret sauce that connects URLs to Python functions.\n\n*   `@app.get` is used for requests that *get* data (like displaying the homepage).\n*   `@app.post` is used for requests that *send* data (like submitting a username to fetch repos).\n\nHere's a simple example from our `app.py` file:\n\n```python\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import HTMLResponse\nfrom fastapi.templating import Jinja2Templates\n\napp = FastAPI()\ntemplates = Jinja2Templates(directory=\"templates\")\n\n@app.get(\"/\", response_class=HTMLResponse)\nasync def home(request: Request):\n    \"\"\"Render the homepage with the GitHub username form.\"\"\"\n    return templates.TemplateResponse(\"home.html\", {\"request\": request})\n```\n\n**Explanation:**\n\n1.  `app = FastAPI()`: This line creates a FastAPI \"app\" instance.  Think of it as creating our \"traffic controller.\"\n2.  `@app.get(\"/\", response_class=HTMLResponse)`: This is a decorator. It tells FastAPI: \"When someone visits the root URL (`/`) using a `GET` request (like typing the address in their browser), run the `home` function.\" The `response_class=HTMLResponse` specifies that the function should return HTML content.\n3.  `async def home(request: Request):`: This is the Python function that will be executed. It takes a `Request` object as input, which contains information about the incoming request.\n4.  `return templates.TemplateResponse(\"home.html\", {\"request\": request})`: This line uses Jinja2 templating (more on this in [Jinja2 Templating](02_jinja2_templating.md)) to render the `home.html` template and return it as the response.  The template receives the \"request\" object as context.\n\nSo, when you visit the website's homepage, FastAPI sees the request, runs the `home` function, and sends back the rendered `home.html` page to your browser.\n\n## Example Use Case: Fetching GitHub Repos\n\nLet's walk through how FastAPI is used in our project to fetch GitHub repositories.\n\n1.  **The User Enters a Username:** The user visits the homepage, enters their GitHub username, and clicks \"Submit.\"\n\n2.  **The Form is Submitted:** The browser sends a `POST` request to the `/repos` URL with the username as data.\n\n3.  **FastAPI Routes the Request:** FastAPI sees the `/repos` URL and the `POST` request type. It looks for a matching route.\n\nHere's the route definition from `app.py`:\n\n```python\nfrom fastapi import Form\n\n@app.post(\"/repos\", response_class=HTMLResponse)\nasync def repos(request: Request, username: str = Form(...)):\n    \"\"\"Fetch and display pinned repos based on the provided GitHub username.\"\"\"\n    token = os.getenv(\"GITHUB_TOKEN\")\n    repos = fetch_pinned_repos(username, token)\n    if repos is None:\n        return templates.TemplateResponse(\"home.html\", {\"request\": request, \"error\": \"Failed to fetch repos or no pinned repos found.\"})\n    return templates.TemplateResponse(\"repos.html\", {\"request\": request, \"repos\": repos, \"username\": username})\n```\n\n**Explanation:**\n\n1.  `@app.post(\"/repos\", response_class=HTMLResponse)`: This decorator tells FastAPI that when a `POST` request is made to `/repos`, the `repos` function should be executed.\n2.  `async def repos(request: Request, username: str = Form(...)):`:  This defines the `repos` function.  `username: str = Form(...)` tells FastAPI to expect a form field named \"username\" in the `POST` request.\n3.  `token = os.getenv(\"GITHUB_TOKEN\")`: This retrieves the GitHub token from environment variables (we'll cover this in [Environment Variable Management](05_environment_variable_management.md)).\n4.  `repos = fetch_pinned_repos(username, token)`:  This is the crucial part: it calls the `fetch_pinned_repos` function to actually fetch the data from GitHub using the provided username and token. We'll see how this works in [GitHub GraphQL API Interaction](03_github_graphql_api_interaction.md).\n5.  The remaining code handles potential errors (if `repos` is `None`) and renders the `repos.html` template with the fetched data. We'll talk about [Jinja2 Templating](02_jinja2_templating.md) in the next chapter.\n\n4.  **Data is Fetched and Displayed:** The `repos` function calls the `fetch_pinned_repos` function (details in [GitHub GraphQL API Interaction](03_github_graphql_api_interaction.md)), retrieves the pinned repositories from GitHub, and renders them using Jinja2. The rendered HTML is then sent back to the user's browser.\n\n## Under the Hood: FastAPI's Request Handling\n\nHere's a simplified view of how FastAPI handles requests:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Browser\n    participant FastAPI\n    participant App Logic\n    participant GitHub API\n\n    User->>Browser: Enters URL and submits form\n    Browser->>FastAPI: Sends HTTP Request\n    FastAPI->>App Logic: Routes request to appropriate function (e.g., repos)\n    App Logic->>GitHub API: Fetches data from GitHub (fetch_pinned_repos)\n    GitHub API-->>App Logic: Returns data\n    App Logic->>FastAPI: Returns rendered HTML\n    FastAPI->>Browser: Sends HTTP Response\n    Browser->>User: Displays webpage\n```\n\n**Explanation:**\n\n1.  The **User** interacts with the **Browser**, entering the URL and submitting the form.\n2.  The **Browser** sends an HTTP request to the **FastAPI** application.\n3.  **FastAPI** receives the request and routes it to the appropriate function in the **App Logic**.\n4.  The **App Logic** fetches data from the **GitHub API**.\n5.  The **GitHub API** returns the requested data to the **App Logic**.\n6.  The **App Logic** returns the rendered HTML to **FastAPI**.\n7.  **FastAPI** sends an HTTP response back to the **Browser**.\n8.  The **Browser** displays the webpage to the **User**.\n\n## Where Does FastAPI Live?\n\nThe heart of our FastAPI application is in the `app.py` file. This file defines the FastAPI instance (`app = FastAPI()`) and all the routes using decorators like `@app.get` and `@app.post`. It also contains the logic for handling requests and interacting with other parts of our application.\n\n## Conclusion\n\nIn this chapter, we learned about the core concept of a FastAPI application: how it acts as a \"traffic controller\" for web requests, routing them to the right functions and sending back responses. We saw how route decorators like `@app.get` and `@app.post` connect URLs to Python functions.  We also walked through an example of how FastAPI is used to fetch and display GitHub repositories.\n\nNext, we will explore [Jinja2 Templating](02_jinja2_templating.md), which is used to dynamically generate HTML content in our application.\n\n---\n# Chapter 2: Jinja2 Templating\n\nIn the previous chapter, [FastAPI Application](01_fastapi_application.md), we learned how FastAPI handles web requests and routes them to the right functions. But what happens *after* a function processes the request and *before* the response is sent back to the user's browser? That's where Jinja2 templating comes in!\n\nImagine you want to send a personalized birthday card to your friend. You wouldn't write a brand new card from scratch every time, right? Instead, you'd use a pre-made card with a space to fill in your friend's name. Jinja2 is like that pre-made card, and the data (your friend's name) is what makes it personalized.\n\nIn our `v1` project, Jinja2 helps us create dynamic HTML pages. It lets us embed Python variables and logic directly into our HTML code, making it easier to display information like a user's GitHub repositories.\n\n## What is Jinja2?\n\nJinja2 is a **templating engine**. Think of it like a cookie cutter.\n\n*   **Template (Cookie Cutter):** A template is an HTML file with placeholders for data. It defines the structure of the webpage.\n*   **Data (Dough):** The data is the information you want to display on the page (e.g., a username, a list of repositories).\n*   **Output (Cookie):** The output is the final HTML page, with the data filled in.\n\nJinja2 takes a template and data and combines them to create a personalized HTML page.\n\n## Key Concepts\n\nLet's break down the key concepts of Jinja2 templating:\n\n1.  **Templates:** These are HTML files that contain placeholders for dynamic data. Placeholders are denoted by special syntax, like `{{ variable_name }}` or `{% logic %}`.\n\n2.  **Variables:** These are placeholders in the template that get replaced with actual data when the template is rendered.  They're accessed using double curly braces: `{{ username }}`. If we pass `username=\"octocat\"` to the template, this will become \"octocat\" in the final HTML.\n\n3.  **Control Structures:** These allow you to add logic to your templates, like loops and conditional statements.  They're accessed using curly braces and percent signs: `{% if repos %}` ... `{% endif %}` or `{% for repo in repos %}` ... `{% endfor %}`.  This lets you show different content depending on whether data exists or to display repeating data.\n\n4.  **Rendering:** This is the process of combining the template and the data to produce the final HTML output.  FastAPI uses Jinja2 to render the templates.\n\n## How to Use Jinja2\n\nLet's see how Jinja2 is used in our `v1` project to display GitHub repositories.\n\nFirst, we have our FastAPI endpoint in `app.py`:\n\n```python\nfrom fastapi import FastAPI, Request, Form\nfrom fastapi.responses import HTMLResponse\nfrom fastapi.templating import Jinja2Templates\n\napp = FastAPI()\ntemplates = Jinja2Templates(directory=\"templates\")\n\n@app.post(\"/repos\", response_class=HTMLResponse)\nasync def repos(request: Request, username: str = Form(...)):\n    # ... (fetch repos logic - skipped for brevity) ...\n    return templates.TemplateResponse(\"repos.html\", {\"request\": request, \"repos\": repos, \"username\": username})\n```\n\n**Explanation:**\n\n1.  `templates = Jinja2Templates(directory=\"templates\")`: This line initializes Jinja2 and tells it to look for templates in the `templates` directory.\n2.  `templates.TemplateResponse(\"repos.html\", {\"request\": request, \"repos\": repos, \"username\": username})`: This line renders the `repos.html` template. It passes a dictionary of data to the template, including:\n    *   `request`: The incoming request object (required by Jinja2).\n    *   `repos`: A list of GitHub repositories fetched from the API.\n    *   `username`: The GitHub username entered by the user.\n\nNow, let's look at the `repos.html` template:\n\n```html\n<!DOCTYPE html>\n<html>\n<head>\n    <title>GitHub Repositories</title>\n</head>\n<body>\n    <h1>Pinned Repositories for {{ username }}</h1>\n    {% if repos %}\n        <ul>\n            {% for repo in repos %}\n                <li>\n                    <a href=\"{{ repo.node.url }}\">{{ repo.node.name }}</a> - {{ repo.node.description }}\n                </li>\n            {% endfor %}\n        </ul>\n    {% else %}\n        <p>No pinned repositories found.</p>\n    {% endif %}\n</body>\n</html>\n```\n\n**Explanation:**\n\n1.  `<h1>Pinned Repositories for {{ username }}</h1>`: This line uses a variable to display the username.  If `username` is \"octocat\", the heading will be \"Pinned Repositories for octocat\".\n\n2.  `{% if repos %}` ... `{% else %}` ... `{% endif %}`: This is a conditional statement. It checks if the `repos` variable is not empty. If it's not empty, the code inside the `if` block is executed. Otherwise, the code inside the `else` block is executed.\n\n3.  `{% for repo in repos %}` ... `{% endfor %}`: This is a loop. It iterates over each repository in the `repos` list.\n\n4.  `<a href=\"{{ repo.node.url }}\">{{ repo.node.name }}</a> - {{ repo.node.description }}`: This line displays the name, description, and URL of each repository.  It uses variables to access the data within each `repo` item.  `repo.node.url` refers to accessing the \"url\" field from the \"node\" field from the \"repo\" variable.\n\n**Example:**\n\nLet's say we pass the following data to the `repos.html` template:\n\n```python\nusername = \"octocat\"\nrepos = [\n    {\"node\": {\"name\": \"Spoon-Knife\", \"description\": \"This repo is for...\", \"url\": \"https://github.com/octocat/Spoon-Knife\"}},\n    {\"node\": {\"name\": \"Hello-World\", \"description\": \"My first repository...\", \"url\": \"https://github.com/octocat/Hello-World\"}},\n]\n```\n\nThe resulting HTML would be:\n\n```html\n<!DOCTYPE html>\n<html>\n<head>\n    <title>GitHub Repositories</title>\n</head>\n<body>\n    <h1>Pinned Repositories for octocat</h1>\n    <ul>\n        <li>\n            <a href=\"https://github.com/octocat/Spoon-Knife\">Spoon-Knife</a> - This repo is for...\n        </li>\n        <li>\n            <a href=\"https://github.com/octocat/Hello-World\">Hello-World</a> - My first repository...\n        </li>\n    </ul>\n</body>\n</html>\n```\n\n## Under the Hood: Jinja2's Rendering Process\n\nHere's a simplified view of how Jinja2 renders a template:\n\n```mermaid\nsequenceDiagram\n    participant FastAPI\n    participant Jinja2\n    participant Template\n    participant Data\n\n    FastAPI->>Jinja2: Call render_template(template, data)\n    Jinja2->>Template: Load template file (e.g., repos.html)\n    Jinja2->>Data: Receive data (e.g., repos, username)\n    Jinja2->>Jinja2: Process template, replacing variables with data\n    Jinja2-->>FastAPI: Return rendered HTML\n```\n\n**Explanation:**\n\n1.  **FastAPI** calls `render_template` in Jinja2, passing the template name and the data.\n2.  **Jinja2** loads the template file from the `templates` directory.\n3.  **Jinja2** receives the data from FastAPI.\n4.  **Jinja2** processes the template, replacing the variables with the data. It also executes any control structures (loops, conditional statements).\n5.  **Jinja2** returns the rendered HTML to FastAPI.\n\n## Where Does Jinja2 Live?\n\nThe Jinja2 templates live in the `templates` directory. FastAPI is configured to look for templates in this directory when you initialize `Jinja2Templates`.  The line `templates = Jinja2Templates(directory=\"templates\")` in `app.py` is what sets this up.\n\n## Conclusion\n\nIn this chapter, we learned about Jinja2 templating and how it helps us create dynamic HTML pages in our `v1` project. We saw how to use variables and control structures to personalize the content of our webpages.  We also walked through the rendering process and understood where Jinja2 templates live in our project.\n\nNext, we'll dive into [GitHub GraphQL API Interaction](03_github_graphql_api_interaction.md) to see how we fetch the data that we display in our templates.\n\n\nRelevant Code Snippets:\n--- File: app.py ---\nfrom fastapi import FastAPI, Request, Form\nfrom fastapi.responses import HTMLResponse\nfrom fastapi.templating import Jinja2Templates\nimport os\nimport requests\nfrom dotenv import load_dotenv\nfrom typing import List, Optional\nfrom fastapi.responses import JSONResponse\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Set up FastAPI\napp = FastAPI()\n\n# Set up templates (Jinja2)\ntemplates = Jinja2Templates(directory=\"templates\")\n\n# Constants\nGITHUB_API_URL = \"https://api.github.com/graphql\"\n\n\ndef get_github_token() -> str:\n    \"\"\"Fetch GitHub token from environment variable.\"\"\"\n    token = os.getenv(\"GITHUB_TOKEN\")\n    if not token:\n        raise ValueError(\"GitHub token not found. Please set the GITHUB_TOKEN env variable.\")\n    return token\n\n\ndef fetch_pinned_repos(username: str, token: str) -> Optional[List[dict]]:\n    \"\"\"Fetch pinned repositories using GitHub GraphQL API.\"\"\"\n    query = f\"\"\"\n    {{\n      user(login: \"{username}\") {{\n        pinnedItems(first: 6, types: [REPOSITORY]) {{\n          edges {{\n            node {{\n              ... on Repository {{\n                name\n                description\n                url\n                stargazerCount\n                forkCount\n                primaryLanguage {{\n                  name\n                }}\n              }}\n            }}\n          }}\n        }}\n      }}\n    }}\n    \"\"\"\n\n    headers = {\"Authorization\": f\"Bearer {token}\"}\n    response = requests.post(GITHUB_API_URL, json={\"query\": query}, headers=headers)\n\n    if response.status_code == 200:\n        return response.json()[\"data\"][\"user\"][\"pinnedItems\"][\"edges\"]\n    else:\n        return None\n\n\n@app.get(\"/\", response_class=HTMLResponse)\nasync def home(request: Request):\n    \"\"\"Render the homepage with the GitHub username form.\"\"\"\n    return templates.TemplateResponse(\"home.html\", {\"request\": request})\n\n\n@app.post(\"/repos\", response_class=HTMLResponse)\nasync def repos(request: Request, username: str = Form(...)):\n    \"\"\"Fetch and display pinned repos based on the provided GitHub username.\"\"\"\n    token = os.getenv(\"GITHUB_TOKEN\")\n    repos = fetch_pinned_repos(username, token)\n    if repos is None:\n        return templates.TemplateResponse(\"home.html\", {\"request\": request, \"error\": \"Failed to fetch repos or no pinned repos found.\"})\n    return templates.TemplateResponse(\"repos.html\", {\"request\": request, \"repos\": repos, \"username\": username})\n  \n\n@app.get(\"/api/repos/{username}\")\nasync def api_repos(username: str):\n    token = get_github_token()\n    repos = fetch_pinned_repos(username, token)\n    if repos is None:\n        return JSONResponse(content={\"error\": \"Failed to fetch repos.\"}, status_code=400)\n    return {\"repos\": repos}\n\n\n\n\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 3: GitHub GraphQL API Interaction`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "# Chapter 3: GitHub GraphQL API Interaction\n\nIn the previous chapter, [Jinja2 Templating](02_jinja2_templating.md), we learned how to create dynamic HTML pages to display data. But where does that data *come* from? In this chapter, we'll learn how to fetch data from GitHub using the GitHub GraphQL API.\n\nImagine you're ordering food from a restaurant. You need to tell the restaurant *what* you want (your order) and prove that you can *pay* for it. The GitHub GraphQL API is like that restaurant. We send it a specific request (our order) and a token (our payment) to get the data we need. Specifically, we will fetch a user's pinned repositories from GitHub. This is where the `fetch_pinned_repos` function comes in!\n\n## What is the GitHub GraphQL API?\n\nThe GitHub GraphQL API is a powerful way to request specific data from GitHub. Instead of getting a huge amount of information you don't need, you can ask for *exactly* what you want.\n\nThink of it like this:\n\n*   **REST API:** You order a whole pizza, even if you only want a slice.\n*   **GraphQL API:** You order only the slice you want.\n\nGraphQL lets us be much more efficient! In our case, we use it to ask for only the pinned repositories of a given GitHub user.\n\n## Key Concepts\n\nLet's break down the key concepts of interacting with the GitHub GraphQL API:\n\n1.  **Query:** This is your \"order.\" It's a string that tells the GitHub API exactly what data you want. In our case, it specifies that we want the pinned repositories for a specific user.\n\n2.  **Headers:** These are like your \"payment details.\" They include your GitHub token, which proves that you have permission to access the data. Think of the token as a key that unlocks access to GitHub's data.\n\n3.  **Request:** This is the actual \"order\" being sent to the \"restaurant.\" We use a Python library called `requests` to send the query and headers to the GitHub API URL.\n\n4.  **Response:** This is the \"food\" delivered to you. It's the data that the GitHub API sends back. We then need to process this response to extract the information we need.\n\n## How to Use `fetch_pinned_repos`\n\nThe `fetch_pinned_repos` function is the core of our interaction with the GitHub GraphQL API. It takes a username and a token as input and returns a list of pinned repositories (or `None` if there's an error).\n\nHere's the function definition from `app.py`:\n\n```python\nfrom typing import List, Optional\nimport requests\n\nGITHUB_API_URL = \"https://api.github.com/graphql\"\n\ndef fetch_pinned_repos(username: str, token: str) -> Optional[List[dict]]:\n    \"\"\"Fetch pinned repositories using GitHub GraphQL API.\"\"\"\n    query = f\"\"\"\n    {{\n      user(login: \"{username}\") {{\n        pinnedItems(first: 6, types: [REPOSITORY]) {{\n          edges {{\n            node {{\n              ... on Repository {{\n                name\n                description\n                url\n                stargazerCount\n                forkCount\n                primaryLanguage {{\n                  name\n                }}\n              }}\n            }}\n          }}\n        }}\n      }}\n    }}\n    \"\"\"\n\n    headers = {\"Authorization\": f\"Bearer {token}\"}\n    response = requests.post(GITHUB_API_URL, json={\"query\": query}, headers=headers)\n\n    if response.status_code == 200:\n        return response.json()[\"data\"][\"user\"][\"pinnedItems\"][\"edges\"]\n    else:\n        return None\n```\n\n**Explanation:**\n\n1.  `username: str, token: str`: The function takes the GitHub username and the GitHub token as input.\n2.  `query = f\"\"\"...\"\"\"`: This is the GraphQL query.  It asks for the first 6 pinned repositories of the user specified by `username`. It also asks for specific information about each repository, such as its name, description, and URL.  The triple quotes (`\"\"\"`) allow us to write the query as a multi-line string, making it easier to read.  The `f` before the string allows us to insert the value of the `username` variable directly into the query.\n3.  `headers = {\"Authorization\": f\"Bearer {token}\"}`: This creates the headers for the request. The `Authorization` header includes the GitHub token.\n4.  `response = requests.post(GITHUB_API_URL, json={\"query\": query}, headers=headers)`: This line uses the `requests` library to send a POST request to the GitHub GraphQL API URL.  The `json` argument specifies the GraphQL query, and the `headers` argument includes the authorization token.\n5.  `if response.status_code == 200:`: This checks if the request was successful. A status code of 200 means \"OK.\"\n6.  `return response.json()[\"data\"][\"user\"][\"pinnedItems\"][\"edges\"]`: If the request was successful, this line extracts the list of pinned repositories from the JSON response. The response is a nested dictionary, and we need to navigate through it to get to the `edges` which contain the repository data.\n7.  `else: return None`: If the request failed, this line returns `None`.\n\n**Example:**\n\nLet's say we call `fetch_pinned_repos` with the username \"octocat\" and a valid GitHub token:\n\n```python\nusername = \"octocat\"\ntoken = \"YOUR_GITHUB_TOKEN\"  # Replace with your actual token\nrepos = fetch_pinned_repos(username, token)\n\nif repos:\n    for repo in repos:\n        print(f\"Repo Name: {repo['node']['name']}\")\n        print(f\"Repo Description: {repo['node']['description']}\")\nelse:\n    print(\"Failed to fetch repos.\")\n```\n\n**Output (example):**\n\n```\nRepo Name: Spoon-Knife\nRepo Description: This repo is for...\nRepo Name: Hello-World\nRepo Description: My first repository...\n... (and so on for each pinned repo)\n```\n\nIf the token is invalid or the username doesn't exist, the output will be:\n\n```\nFailed to fetch repos.\n```\n\n## Under the Hood: How `fetch_pinned_repos` Works\n\nLet's walk through what happens step-by-step when the `fetch_pinned_repos` function is called:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant FastAPI\n    participant fetch_pinned_repos\n    participant GitHub API\n\n    User->>FastAPI: Enters username and submits form\n    FastAPI->>fetch_pinned_repos: Calls fetch_pinned_repos(username, token)\n    fetch_pinned_repos->>GitHub API: Sends GraphQL query with token\n    GitHub API-->>fetch_pinned_repos: Returns JSON response\n    fetch_pinned_repos->>FastAPI: Returns list of repositories (or None)\n    FastAPI->>User: Displays repositories in the browser\n```\n\n**Explanation:**\n\n1.  The **User** enters a username and submits a form on the website, triggering a request to the FastAPI server.\n2.  **FastAPI** calls the `fetch_pinned_repos` function with the username and GitHub token.\n3.  `fetch_pinned_repos` constructs a GraphQL query and sends it to the **GitHub API** along with the token in the headers.\n4.  The **GitHub API** processes the query and returns a JSON response containing the requested data (the pinned repositories).\n5.  `fetch_pinned_repos` parses the JSON response and returns a list of repositories (or `None` if there was an error) to FastAPI.\n6.  Finally, **FastAPI** uses Jinja2 to render the repositories in the user's browser.\n\nNow, let's dive a bit deeper into the code. The key parts are:\n\n1.  **Constructing the Query:**\n\n    ```python\n    query = f\"\"\"\n    {{\n      user(login: \"{username}\") {{\n        pinnedItems(first: 6, types: [REPOSITORY]) {{\n          edges {{\n            node {{\n              ... on Repository {{\n                name\n                description\n                url\n                stargazerCount\n                forkCount\n                primaryLanguage {{\n                  name\n                }}\n              }}\n            }}\n          }}\n        }}\n      }}\n    }}\n    \"\"\"\n    ```\n\n    This code creates the GraphQL query as a string.  The `f` before the string allows us to insert the `username` variable directly into the query. It's important to get the syntax of the GraphQL query correct, or the GitHub API will return an error. You can test your GraphQL queries using GitHub's GraphQL Explorer: [https://docs.github.com/en/graphql/overview/explorer](https://docs.github.com/en/graphql/overview/explorer)\n\n2.  **Sending the Request:**\n\n    ```python\n    headers = {\"Authorization\": f\"Bearer {token}\"}\n    response = requests.post(GITHUB_API_URL, json={\"query\": query}, headers=headers)\n    ```\n\n    This code sends the GraphQL query to the GitHub API using the `requests` library.  It's crucial to include the `Authorization` header with your GitHub token. Without it, the API will reject the request.\n\n3.  **Handling the Response:**\n\n    ```python\n    if response.status_code == 200:\n        return response.json()[\"data\"][\"user\"][\"pinnedItems\"][\"edges\"]\n    else:\n        return None\n    ```\n\n    This code checks the response status code to see if the request was successful. If the status code is 200, it parses the JSON response and extracts the list of pinned repositories. If the status code is not 200, it means there was an error, and the function returns `None`.\n\n## Where Does This Live?\n\nThe `fetch_pinned_repos` function lives in `app.py`.  It's called by the `/repos` endpoint to fetch the data that is displayed on the `repos.html` page.  It's also used in the `/api/repos/{username}` endpoint to provide a JSON API for fetching the data.\n\n## Conclusion\n\nIn this chapter, we learned how to interact with the GitHub GraphQL API using the `fetch_pinned_repos` function. We saw how to construct a GraphQL query, send it to the API with the correct headers, and process the response to extract the data we need.  We also walked through the internal implementation of the `fetch_pinned_repos` function and understood how it interacts with the GitHub API.\n\nNext, we'll explore [Data Handling (Repositories)](04_data_handling__repositories_.md) to learn how to organize and process the repository data we fetch from GitHub.\n", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `v1` about the concept: \"Data Handling (Repositories)\". This is Chapter 4.\n\nConcept Details:\n- Description:\nThe code fetches, processes, and displays repository data. This is similar to having a librarian who finds books (repositories), categorizes them (processes them), and puts them on display (displays them). The `fetch_pinned_repos` function retrieves repository information and the templates display the output.\n\n\nComplete Tutorial Structure:\n1. [FastAPI Application](01_fastapi_application.md)\n2. [Jinja2 Templating](02_jinja2_templating.md)\n3. [GitHub GraphQL API Interaction](03_github_graphql_api_interaction.md)\n4. [Data Handling (Repositories)](04_data_handling__repositories_.md)\n5. [Environment Variable Management](05_environment_variable_management.md)\n\nContext from previous chapters (summary):\n# Chapter 1: FastAPI Application\n\nImagine you're ordering food online. You go to a website, choose what you want, and then click \"Order.\" Behind the scenes, the website needs a \"traffic controller\" to figure out *where* to send your order (to the kitchen), *what* to do with it (prepare the food), and *how* to tell you when it's ready (show you a confirmation message).\n\nThat's exactly what a FastAPI application does for web applications! It's the \"engine\" that handles all the requests coming in, figures out what to do with them, and sends back the right responses.\n\nIn our `v1` project, FastAPI is the main web framework. It's like the conductor of an orchestra, making sure everything plays together harmoniously. We'll be using it to build a web application that fetches pinned repositories from GitHub. Let's dive in!\n\n## What is FastAPI?\n\nThink of FastAPI as a smart and helpful friend who:\n\n*   **Listens for requests:**  It's constantly listening for requests from users visiting our website (like clicking \"Order\").\n*   **Routes requests:** It knows where to send each request based on the URL the user visits (e.g., `/` for the homepage, `/repos` for the repositories).\n*   **Processes requests:** It calls the right Python function to handle each request (e.g., fetch data from GitHub).\n*   **Sends responses:** It sends back a response to the user's browser (e.g., displaying the GitHub repositories).\n\nFastAPI does all this using special \"decorators\" like `@app.get` and `@app.post`. Let's see how they work.\n\n## Key Concept: Route Decorators\n\nThese decorators are the secret sauce that connects URLs to Python functions.\n\n*   `@app.get` is used for requests that *get* data (like displaying the homepage).\n*   `@app.post` is used for requests that *send* data (like submitting a username to fetch repos).\n\nHere's a simple example from our `app.py` file:\n\n```python\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import HTMLResponse\nfrom fastapi.templating import Jinja2Templates\n\napp = FastAPI()\ntemplates = Jinja2Templates(directory=\"templates\")\n\n@app.get(\"/\", response_class=HTMLResponse)\nasync def home(request: Request):\n    \"\"\"Render the homepage with the GitHub username form.\"\"\"\n    return templates.TemplateResponse(\"home.html\", {\"request\": request})\n```\n\n**Explanation:**\n\n1.  `app = FastAPI()`: This line creates a FastAPI \"app\" instance.  Think of it as creating our \"traffic controller.\"\n2.  `@app.get(\"/\", response_class=HTMLResponse)`: This is a decorator. It tells FastAPI: \"When someone visits the root URL (`/`) using a `GET` request (like typing the address in their browser), run the `home` function.\" The `response_class=HTMLResponse` specifies that the function should return HTML content.\n3.  `async def home(request: Request):`: This is the Python function that will be executed. It takes a `Request` object as input, which contains information about the incoming request.\n4.  `return templates.TemplateResponse(\"home.html\", {\"request\": request})`: This line uses Jinja2 templating (more on this in [Jinja2 Templating](02_jinja2_templating.md)) to render the `home.html` template and return it as the response.  The template receives the \"request\" object as context.\n\nSo, when you visit the website's homepage, FastAPI sees the request, runs the `home` function, and sends back the rendered `home.html` page to your browser.\n\n## Example Use Case: Fetching GitHub Repos\n\nLet's walk through how FastAPI is used in our project to fetch GitHub repositories.\n\n1.  **The User Enters a Username:** The user visits the homepage, enters their GitHub username, and clicks \"Submit.\"\n\n2.  **The Form is Submitted:** The browser sends a `POST` request to the `/repos` URL with the username as data.\n\n3.  **FastAPI Routes the Request:** FastAPI sees the `/repos` URL and the `POST` request type. It looks for a matching route.\n\nHere's the route definition from `app.py`:\n\n```python\nfrom fastapi import Form\n\n@app.post(\"/repos\", response_class=HTMLResponse)\nasync def repos(request: Request, username: str = Form(...)):\n    \"\"\"Fetch and display pinned repos based on the provided GitHub username.\"\"\"\n    token = os.getenv(\"GITHUB_TOKEN\")\n    repos = fetch_pinned_repos(username, token)\n    if repos is None:\n        return templates.TemplateResponse(\"home.html\", {\"request\": request, \"error\": \"Failed to fetch repos or no pinned repos found.\"})\n    return templates.TemplateResponse(\"repos.html\", {\"request\": request, \"repos\": repos, \"username\": username})\n```\n\n**Explanation:**\n\n1.  `@app.post(\"/repos\", response_class=HTMLResponse)`: This decorator tells FastAPI that when a `POST` request is made to `/repos`, the `repos` function should be executed.\n2.  `async def repos(request: Request, username: str = Form(...)):`:  This defines the `repos` function.  `username: str = Form(...)` tells FastAPI to expect a form field named \"username\" in the `POST` request.\n3.  `token = os.getenv(\"GITHUB_TOKEN\")`: This retrieves the GitHub token from environment variables (we'll cover this in [Environment Variable Management](05_environment_variable_management.md)).\n4.  `repos = fetch_pinned_repos(username, token)`:  This is the crucial part: it calls the `fetch_pinned_repos` function to actually fetch the data from GitHub using the provided username and token. We'll see how this works in [GitHub GraphQL API Interaction](03_github_graphql_api_interaction.md).\n5.  The remaining code handles potential errors (if `repos` is `None`) and renders the `repos.html` template with the fetched data. We'll talk about [Jinja2 Templating](02_jinja2_templating.md) in the next chapter.\n\n4.  **Data is Fetched and Displayed:** The `repos` function calls the `fetch_pinned_repos` function (details in [GitHub GraphQL API Interaction](03_github_graphql_api_interaction.md)), retrieves the pinned repositories from GitHub, and renders them using Jinja2. The rendered HTML is then sent back to the user's browser.\n\n## Under the Hood: FastAPI's Request Handling\n\nHere's a simplified view of how FastAPI handles requests:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Browser\n    participant FastAPI\n    participant App Logic\n    participant GitHub API\n\n    User->>Browser: Enters URL and submits form\n    Browser->>FastAPI: Sends HTTP Request\n    FastAPI->>App Logic: Routes request to appropriate function (e.g., repos)\n    App Logic->>GitHub API: Fetches data from GitHub (fetch_pinned_repos)\n    GitHub API-->>App Logic: Returns data\n    App Logic->>FastAPI: Returns rendered HTML\n    FastAPI->>Browser: Sends HTTP Response\n    Browser->>User: Displays webpage\n```\n\n**Explanation:**\n\n1.  The **User** interacts with the **Browser**, entering the URL and submitting the form.\n2.  The **Browser** sends an HTTP request to the **FastAPI** application.\n3.  **FastAPI** receives the request and routes it to the appropriate function in the **App Logic**.\n4.  The **App Logic** fetches data from the **GitHub API**.\n5.  The **GitHub API** returns the requested data to the **App Logic**.\n6.  The **App Logic** returns the rendered HTML to **FastAPI**.\n7.  **FastAPI** sends an HTTP response back to the **Browser**.\n8.  The **Browser** displays the webpage to the **User**.\n\n## Where Does FastAPI Live?\n\nThe heart of our FastAPI application is in the `app.py` file. This file defines the FastAPI instance (`app = FastAPI()`) and all the routes using decorators like `@app.get` and `@app.post`. It also contains the logic for handling requests and interacting with other parts of our application.\n\n## Conclusion\n\nIn this chapter, we learned about the core concept of a FastAPI application: how it acts as a \"traffic controller\" for web requests, routing them to the right functions and sending back responses. We saw how route decorators like `@app.get` and `@app.post` connect URLs to Python functions.  We also walked through an example of how FastAPI is used to fetch and display GitHub repositories.\n\nNext, we will explore [Jinja2 Templating](02_jinja2_templating.md), which is used to dynamically generate HTML content in our application.\n\n---\n# Chapter 2: Jinja2 Templating\n\nIn the previous chapter, [FastAPI Application](01_fastapi_application.md), we learned how FastAPI handles web requests and routes them to the right functions. But what happens *after* a function processes the request and *before* the response is sent back to the user's browser? That's where Jinja2 templating comes in!\n\nImagine you want to send a personalized birthday card to your friend. You wouldn't write a brand new card from scratch every time, right? Instead, you'd use a pre-made card with a space to fill in your friend's name. Jinja2 is like that pre-made card, and the data (your friend's name) is what makes it personalized.\n\nIn our `v1` project, Jinja2 helps us create dynamic HTML pages. It lets us embed Python variables and logic directly into our HTML code, making it easier to display information like a user's GitHub repositories.\n\n## What is Jinja2?\n\nJinja2 is a **templating engine**. Think of it like a cookie cutter.\n\n*   **Template (Cookie Cutter):** A template is an HTML file with placeholders for data. It defines the structure of the webpage.\n*   **Data (Dough):** The data is the information you want to display on the page (e.g., a username, a list of repositories).\n*   **Output (Cookie):** The output is the final HTML page, with the data filled in.\n\nJinja2 takes a template and data and combines them to create a personalized HTML page.\n\n## Key Concepts\n\nLet's break down the key concepts of Jinja2 templating:\n\n1.  **Templates:** These are HTML files that contain placeholders for dynamic data. Placeholders are denoted by special syntax, like `{{ variable_name }}` or `{% logic %}`.\n\n2.  **Variables:** These are placeholders in the template that get replaced with actual data when the template is rendered.  They're accessed using double curly braces: `{{ username }}`. If we pass `username=\"octocat\"` to the template, this will become \"octocat\" in the final HTML.\n\n3.  **Control Structures:** These allow you to add logic to your templates, like loops and conditional statements.  They're accessed using curly braces and percent signs: `{% if repos %}` ... `{% endif %}` or `{% for repo in repos %}` ... `{% endfor %}`.  This lets you show different content depending on whether data exists or to display repeating data.\n\n4.  **Rendering:** This is the process of combining the template and the data to produce the final HTML output.  FastAPI uses Jinja2 to render the templates.\n\n## How to Use Jinja2\n\nLet's see how Jinja2 is used in our `v1` project to display GitHub repositories.\n\nFirst, we have our FastAPI endpoint in `app.py`:\n\n```python\nfrom fastapi import FastAPI, Request, Form\nfrom fastapi.responses import HTMLResponse\nfrom fastapi.templating import Jinja2Templates\n\napp = FastAPI()\ntemplates = Jinja2Templates(directory=\"templates\")\n\n@app.post(\"/repos\", response_class=HTMLResponse)\nasync def repos(request: Request, username: str = Form(...)):\n    # ... (fetch repos logic - skipped for brevity) ...\n    return templates.TemplateResponse(\"repos.html\", {\"request\": request, \"repos\": repos, \"username\": username})\n```\n\n**Explanation:**\n\n1.  `templates = Jinja2Templates(directory=\"templates\")`: This line initializes Jinja2 and tells it to look for templates in the `templates` directory.\n2.  `templates.TemplateResponse(\"repos.html\", {\"request\": request, \"repos\": repos, \"username\": username})`: This line renders the `repos.html` template. It passes a dictionary of data to the template, including:\n    *   `request`: The incoming request object (required by Jinja2).\n    *   `repos`: A list of GitHub repositories fetched from the API.\n    *   `username`: The GitHub username entered by the user.\n\nNow, let's look at the `repos.html` template:\n\n```html\n<!DOCTYPE html>\n<html>\n<head>\n    <title>GitHub Repositories</title>\n</head>\n<body>\n    <h1>Pinned Repositories for {{ username }}</h1>\n    {% if repos %}\n        <ul>\n            {% for repo in repos %}\n                <li>\n                    <a href=\"{{ repo.node.url }}\">{{ repo.node.name }}</a> - {{ repo.node.description }}\n                </li>\n            {% endfor %}\n        </ul>\n    {% else %}\n        <p>No pinned repositories found.</p>\n    {% endif %}\n</body>\n</html>\n```\n\n**Explanation:**\n\n1.  `<h1>Pinned Repositories for {{ username }}</h1>`: This line uses a variable to display the username.  If `username` is \"octocat\", the heading will be \"Pinned Repositories for octocat\".\n\n2.  `{% if repos %}` ... `{% else %}` ... `{% endif %}`: This is a conditional statement. It checks if the `repos` variable is not empty. If it's not empty, the code inside the `if` block is executed. Otherwise, the code inside the `else` block is executed.\n\n3.  `{% for repo in repos %}` ... `{% endfor %}`: This is a loop. It iterates over each repository in the `repos` list.\n\n4.  `<a href=\"{{ repo.node.url }}\">{{ repo.node.name }}</a> - {{ repo.node.description }}`: This line displays the name, description, and URL of each repository.  It uses variables to access the data within each `repo` item.  `repo.node.url` refers to accessing the \"url\" field from the \"node\" field from the \"repo\" variable.\n\n**Example:**\n\nLet's say we pass the following data to the `repos.html` template:\n\n```python\nusername = \"octocat\"\nrepos = [\n    {\"node\": {\"name\": \"Spoon-Knife\", \"description\": \"This repo is for...\", \"url\": \"https://github.com/octocat/Spoon-Knife\"}},\n    {\"node\": {\"name\": \"Hello-World\", \"description\": \"My first repository...\", \"url\": \"https://github.com/octocat/Hello-World\"}},\n]\n```\n\nThe resulting HTML would be:\n\n```html\n<!DOCTYPE html>\n<html>\n<head>\n    <title>GitHub Repositories</title>\n</head>\n<body>\n    <h1>Pinned Repositories for octocat</h1>\n    <ul>\n        <li>\n            <a href=\"https://github.com/octocat/Spoon-Knife\">Spoon-Knife</a> - This repo is for...\n        </li>\n        <li>\n            <a href=\"https://github.com/octocat/Hello-World\">Hello-World</a> - My first repository...\n        </li>\n    </ul>\n</body>\n</html>\n```\n\n## Under the Hood: Jinja2's Rendering Process\n\nHere's a simplified view of how Jinja2 renders a template:\n\n```mermaid\nsequenceDiagram\n    participant FastAPI\n    participant Jinja2\n    participant Template\n    participant Data\n\n    FastAPI->>Jinja2: Call render_template(template, data)\n    Jinja2->>Template: Load template file (e.g., repos.html)\n    Jinja2->>Data: Receive data (e.g., repos, username)\n    Jinja2->>Jinja2: Process template, replacing variables with data\n    Jinja2-->>FastAPI: Return rendered HTML\n```\n\n**Explanation:**\n\n1.  **FastAPI** calls `render_template` in Jinja2, passing the template name and the data.\n2.  **Jinja2** loads the template file from the `templates` directory.\n3.  **Jinja2** receives the data from FastAPI.\n4.  **Jinja2** processes the template, replacing the variables with the data. It also executes any control structures (loops, conditional statements).\n5.  **Jinja2** returns the rendered HTML to FastAPI.\n\n## Where Does Jinja2 Live?\n\nThe Jinja2 templates live in the `templates` directory. FastAPI is configured to look for templates in this directory when you initialize `Jinja2Templates`.  The line `templates = Jinja2Templates(directory=\"templates\")` in `app.py` is what sets this up.\n\n## Conclusion\n\nIn this chapter, we learned about Jinja2 templating and how it helps us create dynamic HTML pages in our `v1` project. We saw how to use variables and control structures to personalize the content of our webpages.  We also walked through the rendering process and understood where Jinja2 templates live in our project.\n\nNext, we'll dive into [GitHub GraphQL API Interaction](03_github_graphql_api_interaction.md) to see how we fetch the data that we display in our templates.\n\n---\n# Chapter 3: GitHub GraphQL API Interaction\n\nIn the previous chapter, [Jinja2 Templating](02_jinja2_templating.md), we learned how to create dynamic HTML pages to display data. But where does that data *come* from? In this chapter, we'll learn how to fetch data from GitHub using the GitHub GraphQL API.\n\nImagine you're ordering food from a restaurant. You need to tell the restaurant *what* you want (your order) and prove that you can *pay* for it. The GitHub GraphQL API is like that restaurant. We send it a specific request (our order) and a token (our payment) to get the data we need. Specifically, we will fetch a user's pinned repositories from GitHub. This is where the `fetch_pinned_repos` function comes in!\n\n## What is the GitHub GraphQL API?\n\nThe GitHub GraphQL API is a powerful way to request specific data from GitHub. Instead of getting a huge amount of information you don't need, you can ask for *exactly* what you want.\n\nThink of it like this:\n\n*   **REST API:** You order a whole pizza, even if you only want a slice.\n*   **GraphQL API:** You order only the slice you want.\n\nGraphQL lets us be much more efficient! In our case, we use it to ask for only the pinned repositories of a given GitHub user.\n\n## Key Concepts\n\nLet's break down the key concepts of interacting with the GitHub GraphQL API:\n\n1.  **Query:** This is your \"order.\" It's a string that tells the GitHub API exactly what data you want. In our case, it specifies that we want the pinned repositories for a specific user.\n\n2.  **Headers:** These are like your \"payment details.\" They include your GitHub token, which proves that you have permission to access the data. Think of the token as a key that unlocks access to GitHub's data.\n\n3.  **Request:** This is the actual \"order\" being sent to the \"restaurant.\" We use a Python library called `requests` to send the query and headers to the GitHub API URL.\n\n4.  **Response:** This is the \"food\" delivered to you. It's the data that the GitHub API sends back. We then need to process this response to extract the information we need.\n\n## How to Use `fetch_pinned_repos`\n\nThe `fetch_pinned_repos` function is the core of our interaction with the GitHub GraphQL API. It takes a username and a token as input and returns a list of pinned repositories (or `None` if there's an error).\n\nHere's the function definition from `app.py`:\n\n```python\nfrom typing import List, Optional\nimport requests\n\nGITHUB_API_URL = \"https://api.github.com/graphql\"\n\ndef fetch_pinned_repos(username: str, token: str) -> Optional[List[dict]]:\n    \"\"\"Fetch pinned repositories using GitHub GraphQL API.\"\"\"\n    query = f\"\"\"\n    {{\n      user(login: \"{username}\") {{\n        pinnedItems(first: 6, types: [REPOSITORY]) {{\n          edges {{\n            node {{\n              ... on Repository {{\n                name\n                description\n                url\n                stargazerCount\n                forkCount\n                primaryLanguage {{\n                  name\n                }}\n              }}\n            }}\n          }}\n        }}\n      }}\n    }}\n    \"\"\"\n\n    headers = {\"Authorization\": f\"Bearer {token}\"}\n    response = requests.post(GITHUB_API_URL, json={\"query\": query}, headers=headers)\n\n    if response.status_code == 200:\n        return response.json()[\"data\"][\"user\"][\"pinnedItems\"][\"edges\"]\n    else:\n        return None\n```\n\n**Explanation:**\n\n1.  `username: str, token: str`: The function takes the GitHub username and the GitHub token as input.\n2.  `query = f\"\"\"...\"\"\"`: This is the GraphQL query.  It asks for the first 6 pinned repositories of the user specified by `username`. It also asks for specific information about each repository, such as its name, description, and URL.  The triple quotes (`\"\"\"`) allow us to write the query as a multi-line string, making it easier to read.  The `f` before the string allows us to insert the value of the `username` variable directly into the query.\n3.  `headers = {\"Authorization\": f\"Bearer {token}\"}`: This creates the headers for the request. The `Authorization` header includes the GitHub token.\n4.  `response = requests.post(GITHUB_API_URL, json={\"query\": query}, headers=headers)`: This line uses the `requests` library to send a POST request to the GitHub GraphQL API URL.  The `json` argument specifies the GraphQL query, and the `headers` argument includes the authorization token.\n5.  `if response.status_code == 200:`: This checks if the request was successful. A status code of 200 means \"OK.\"\n6.  `return response.json()[\"data\"][\"user\"][\"pinnedItems\"][\"edges\"]`: If the request was successful, this line extracts the list of pinned repositories from the JSON response. The response is a nested dictionary, and we need to navigate through it to get to the `edges` which contain the repository data.\n7.  `else: return None`: If the request failed, this line returns `None`.\n\n**Example:**\n\nLet's say we call `fetch_pinned_repos` with the username \"octocat\" and a valid GitHub token:\n\n```python\nusername = \"octocat\"\ntoken = \"YOUR_GITHUB_TOKEN\"  # Replace with your actual token\nrepos = fetch_pinned_repos(username, token)\n\nif repos:\n    for repo in repos:\n        print(f\"Repo Name: {repo['node']['name']}\")\n        print(f\"Repo Description: {repo['node']['description']}\")\nelse:\n    print(\"Failed to fetch repos.\")\n```\n\n**Output (example):**\n\n```\nRepo Name: Spoon-Knife\nRepo Description: This repo is for...\nRepo Name: Hello-World\nRepo Description: My first repository...\n... (and so on for each pinned repo)\n```\n\nIf the token is invalid or the username doesn't exist, the output will be:\n\n```\nFailed to fetch repos.\n```\n\n## Under the Hood: How `fetch_pinned_repos` Works\n\nLet's walk through what happens step-by-step when the `fetch_pinned_repos` function is called:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant FastAPI\n    participant fetch_pinned_repos\n    participant GitHub API\n\n    User->>FastAPI: Enters username and submits form\n    FastAPI->>fetch_pinned_repos: Calls fetch_pinned_repos(username, token)\n    fetch_pinned_repos->>GitHub API: Sends GraphQL query with token\n    GitHub API-->>fetch_pinned_repos: Returns JSON response\n    fetch_pinned_repos->>FastAPI: Returns list of repositories (or None)\n    FastAPI->>User: Displays repositories in the browser\n```\n\n**Explanation:**\n\n1.  The **User** enters a username and submits a form on the website, triggering a request to the FastAPI server.\n2.  **FastAPI** calls the `fetch_pinned_repos` function with the username and GitHub token.\n3.  `fetch_pinned_repos` constructs a GraphQL query and sends it to the **GitHub API** along with the token in the headers.\n4.  The **GitHub API** processes the query and returns a JSON response containing the requested data (the pinned repositories).\n5.  `fetch_pinned_repos` parses the JSON response and returns a list of repositories (or `None` if there was an error) to FastAPI.\n6.  Finally, **FastAPI** uses Jinja2 to render the repositories in the user's browser.\n\nNow, let's dive a bit deeper into the code. The key parts are:\n\n1.  **Constructing the Query:**\n\n    ```python\n    query = f\"\"\"\n    {{\n      user(login: \"{username}\") {{\n        pinnedItems(first: 6, types: [REPOSITORY]) {{\n          edges {{\n            node {{\n              ... on Repository {{\n                name\n                description\n                url\n                stargazerCount\n                forkCount\n                primaryLanguage {{\n                  name\n                }}\n              }}\n            }}\n          }}\n        }}\n      }}\n    }}\n    \"\"\"\n    ```\n\n    This code creates the GraphQL query as a string.  The `f` before the string allows us to insert the `username` variable directly into the query. It's important to get the syntax of the GraphQL query correct, or the GitHub API will return an error. You can test your GraphQL queries using GitHub's GraphQL Explorer: [https://docs.github.com/en/graphql/overview/explorer](https://docs.github.com/en/graphql/overview/explorer)\n\n2.  **Sending the Request:**\n\n    ```python\n    headers = {\"Authorization\": f\"Bearer {token}\"}\n    response = requests.post(GITHUB_API_URL, json={\"query\": query}, headers=headers)\n    ```\n\n    This code sends the GraphQL query to the GitHub API using the `requests` library.  It's crucial to include the `Authorization` header with your GitHub token. Without it, the API will reject the request.\n\n3.  **Handling the Response:**\n\n    ```python\n    if response.status_code == 200:\n        return response.json()[\"data\"][\"user\"][\"pinnedItems\"][\"edges\"]\n    else:\n        return None\n    ```\n\n    This code checks the response status code to see if the request was successful. If the status code is 200, it parses the JSON response and extracts the list of pinned repositories. If the status code is not 200, it means there was an error, and the function returns `None`.\n\n## Where Does This Live?\n\nThe `fetch_pinned_repos` function lives in `app.py`.  It's called by the `/repos` endpoint to fetch the data that is displayed on the `repos.html` page.  It's also used in the `/api/repos/{username}` endpoint to provide a JSON API for fetching the data.\n\n## Conclusion\n\nIn this chapter, we learned how to interact with the GitHub GraphQL API using the `fetch_pinned_repos` function. We saw how to construct a GraphQL query, send it to the API with the correct headers, and process the response to extract the data we need.  We also walked through the internal implementation of the `fetch_pinned_repos` function and understood how it interacts with the GitHub API.\n\nNext, we'll explore [Data Handling (Repositories)](04_data_handling__repositories_.md) to learn how to organize and process the repository data we fetch from GitHub.\n\n\nRelevant Code Snippets:\n--- File: app.py ---\nfrom fastapi import FastAPI, Request, Form\nfrom fastapi.responses import HTMLResponse\nfrom fastapi.templating import Jinja2Templates\nimport os\nimport requests\nfrom dotenv import load_dotenv\nfrom typing import List, Optional\nfrom fastapi.responses import JSONResponse\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Set up FastAPI\napp = FastAPI()\n\n# Set up templates (Jinja2)\ntemplates = Jinja2Templates(directory=\"templates\")\n\n# Constants\nGITHUB_API_URL = \"https://api.github.com/graphql\"\n\n\ndef get_github_token() -> str:\n    \"\"\"Fetch GitHub token from environment variable.\"\"\"\n    token = os.getenv(\"GITHUB_TOKEN\")\n    if not token:\n        raise ValueError(\"GitHub token not found. Please set the GITHUB_TOKEN env variable.\")\n    return token\n\n\ndef fetch_pinned_repos(username: str, token: str) -> Optional[List[dict]]:\n    \"\"\"Fetch pinned repositories using GitHub GraphQL API.\"\"\"\n    query = f\"\"\"\n    {{\n      user(login: \"{username}\") {{\n        pinnedItems(first: 6, types: [REPOSITORY]) {{\n          edges {{\n            node {{\n              ... on Repository {{\n                name\n                description\n                url\n                stargazerCount\n                forkCount\n                primaryLanguage {{\n                  name\n                }}\n              }}\n            }}\n          }}\n        }}\n      }}\n    }}\n    \"\"\"\n\n    headers = {\"Authorization\": f\"Bearer {token}\"}\n    response = requests.post(GITHUB_API_URL, json={\"query\": query}, headers=headers)\n\n    if response.status_code == 200:\n        return response.json()[\"data\"][\"user\"][\"pinnedItems\"][\"edges\"]\n    else:\n        return None\n\n\n@app.get(\"/\", response_class=HTMLResponse)\nasync def home(request: Request):\n    \"\"\"Render the homepage with the GitHub username form.\"\"\"\n    return templates.TemplateResponse(\"home.html\", {\"request\": request})\n\n\n@app.post(\"/repos\", response_class=HTMLResponse)\nasync def repos(request: Request, username: str = Form(...)):\n    \"\"\"Fetch and display pinned repos based on the provided GitHub username.\"\"\"\n    token = os.getenv(\"GITHUB_TOKEN\")\n    repos = fetch_pinned_repos(username, token)\n    if repos is None:\n        return templates.TemplateResponse(\"home.html\", {\"request\": request, \"error\": \"Failed to fetch repos or no pinned repos found.\"})\n    return templates.TemplateResponse(\"repos.html\", {\"request\": request, \"repos\": repos, \"username\": username})\n  \n\n@app.get(\"/api/repos/{username}\")\nasync def api_repos(username: str):\n    token = get_github_token()\n    repos = fetch_pinned_repos(username, token)\n    if repos is None:\n        return JSONResponse(content={\"error\": \"Failed to fetch repos.\"}, status_code=400)\n    return {\"repos\": repos}\n\n\n\n\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 4: Data Handling (Repositories)`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "# Chapter 4: Data Handling (Repositories)\n\nIn the previous chapter, [GitHub GraphQL API Interaction](03_github_graphql_api_interaction.md), we learned how to fetch a list of GitHub repositories. But what do we *do* with that list once we have it? How do we organize it and get it ready to display on our website? That's what this chapter is all about!\n\nImagine you're a librarian. You can't just throw all the books onto the shelves randomly! You need a system for organizing them so people can find what they're looking for. In our project, the \"books\" are the repositories we get from GitHub, and our \"system\" is the way we handle and process that data.\n\nThe `fetch_pinned_repos` function retrieves raw repository data. In this chapter, we'll focus on how the application *handles* that raw data - presenting it in a user-friendly way. It's like having a librarian who finds books (repositories), categorizes them (processes them), and puts them on display (displays them). The Jinja2 templates then display the output.\n\n## Key Concepts\n\nLet's break down the key concepts of data handling in our project:\n\n1.  **Fetching Raw Data:** This is where we get the initial data from the GitHub API. The `fetch_pinned_repos` function, which we covered in [GitHub GraphQL API Interaction](03_github_graphql_api_interaction.md), is responsible for this. The result of this function call is raw data structured in a specific format.\n\n2.  **Passing Data to Templates:** After the `fetch_pinned_repos` function retrieves the data, it needs to be *passed* to the Jinja2 template. The Jinja2 template is responsible for defining *how* the data is displayed. This includes the username, the list of repositories, and any errors that might have occurred.\n\n3.  **Displaying the Data:** The Jinja2 template takes the data and uses it to generate the final HTML that is sent to the user's browser.\n\n## How to Use Data Handling\n\nLet's walk through the process step-by-step:\n\n1.  **User enters a username and submits the form:** Imagine the user enters the username \"octocat\".\n\n2.  **`fetch_pinned_repos` is called:** The `fetch_pinned_repos` function is called with the username \"octocat\" and a GitHub token.\n\n3.  **Data is fetched from GitHub:** The `fetch_pinned_repos` function sends a GraphQL query to the GitHub API and receives a JSON response containing the pinned repositories for the user \"octocat\".\n\n4.  **Data is passed to the template:** The `repos` function in `app.py` receives the data and passes it to the `repos.html` template:\n\n    ```python\n    from fastapi import Form\n\n    @app.post(\"/repos\", response_class=HTMLResponse)\n    async def repos(request: Request, username: str = Form(...)):\n        \"\"\"Fetch and display pinned repos based on the provided GitHub username.\"\"\"\n        token = os.getenv(\"GITHUB_TOKEN\")\n        repos = fetch_pinned_repos(username, token)\n        if repos is None:\n            return templates.TemplateResponse(\"home.html\", {\"request\": request, \"error\": \"Failed to fetch repos or no pinned repos found.\"})\n        return templates.TemplateResponse(\"repos.html\", {\"request\": request, \"repos\": repos, \"username\": username})\n    ```\n\n    **Explanation:**\n\n    *   This function calls `fetch_pinned_repos` to retrieve data.\n    *   If `fetch_pinned_repos` returns `None`, meaning there was an error, an error message is displayed on the `home.html` template.\n    *   If the request succeeds, it calls `templates.TemplateResponse` to display the `repos.html` template with the retrieved `repos` data and the `username` data. The `request` object is also passed.\n\n5.  **The template displays the data:** The `repos.html` template uses Jinja2 syntax to display the username and the list of repositories:\n\n    ```html\n    <h1>Pinned Repositories for {{ username }}</h1>\n    {% if repos %}\n        <ul>\n            {% for repo in repos %}\n                <li>\n                    <a href=\"{{ repo.node.url }}\">{{ repo.node.name }}</a> - {{ repo.node.description }}\n                </li>\n            {% endfor %}\n        </ul>\n    {% else %}\n        <p>No pinned repositories found.</p>\n    {% endif %}\n    ```\n\n    **Explanation:**\n\n    *   `{{ username }}` displays the username.\n    *   The `{% if repos %}` block checks if the `repos` variable is not empty. If it's not empty, the list of repositories is displayed.\n    *   The `{% for repo in repos %}` loop iterates through each repository in the list and displays its name, description, and URL.\n\n## Under the Hood: Data Flow\n\nLet's visualize how the data flows through our application:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Browser\n    participant FastAPI\n    participant fetch_pinned_repos\n    participant Jinja2 Template\n\n    User->>Browser: Enters username, submits form\n    Browser->>FastAPI: Sends POST request to /repos\n    FastAPI->>fetch_pinned_repos: Calls fetch_pinned_repos(username, token)\n    fetch_pinned_repos-->>FastAPI: Returns repo data\n    FastAPI->>Jinja2 Template: Renders repos.html with data\n    Jinja2 Template-->>FastAPI: Returns HTML\n    FastAPI->>Browser: Sends HTML response\n    Browser->>User: Displays webpage\n```\n\n**Explanation:**\n\n1.  The **User** interacts with the **Browser**, entering the username and submitting the form.\n2.  The **Browser** sends a POST request to the `/repos` endpoint on the **FastAPI** server.\n3.  **FastAPI** calls the `fetch_pinned_repos` function to get the repository data.\n4.  `fetch_pinned_repos` retrieves the repo data and returns it to FastAPI.\n5.  **FastAPI** takes the data and renders it into the `repos.html` file using Jinja2.\n6.  The **Jinja2 Template** returns the HTML code.\n7.  **FastAPI** then returns the rendered HTML to the **Browser** and the **User** views the webpage.\n\n## Where Does This Live?\n\nThe data handling logic is spread across several files:\n\n*   `app.py`: Contains the `repos` function, which calls `fetch_pinned_repos` and passes the data to the template. Also contains the `fetch_pinned_repos` function.\n*   `templates/repos.html`: Contains the Jinja2 template that displays the data.\n\n## Conclusion\n\nIn this chapter, we learned about how our application handles repository data. We saw how the `fetch_pinned_repos` function retrieves the data, how the `repos` function passes the data to the Jinja2 template, and how the template displays the data to the user. This process allows us to present the data in a user-friendly format.\n\nNext, we'll explore [Environment Variable Management](05_environment_variable_management.md) to learn how to securely store and access sensitive information like the GitHub token.\n", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `v1` about the concept: \"Environment Variable Management\". This is Chapter 5.\n\nConcept Details:\n- Description:\nThe project uses `.env` files and the `dotenv` library to store sensitive information like API keys. Imagine this as a safe deposit box. Instead of hardcoding valuable information directly into the code, it is kept in a secure, separate location, preventing accidental exposure. `load_dotenv()` loads variables from `.env` into the environment.\n\n\nComplete Tutorial Structure:\n1. [FastAPI Application](01_fastapi_application.md)\n2. [Jinja2 Templating](02_jinja2_templating.md)\n3. [GitHub GraphQL API Interaction](03_github_graphql_api_interaction.md)\n4. [Data Handling (Repositories)](04_data_handling__repositories_.md)\n5. [Environment Variable Management](05_environment_variable_management.md)\n\nContext from previous chapters (summary):\n# Chapter 1: FastAPI Application\n\nImagine you're ordering food online. You go to a website, choose what you want, and then click \"Order.\" Behind the scenes, the website needs a \"traffic controller\" to figure out *where* to send your order (to the kitchen), *what* to do with it (prepare the food), and *how* to tell you when it's ready (show you a confirmation message).\n\nThat's exactly what a FastAPI application does for web applications! It's the \"engine\" that handles all the requests coming in, figures out what to do with them, and sends back the right responses.\n\nIn our `v1` project, FastAPI is the main web framework. It's like the conductor of an orchestra, making sure everything plays together harmoniously. We'll be using it to build a web application that fetches pinned repositories from GitHub. Let's dive in!\n\n## What is FastAPI?\n\nThink of FastAPI as a smart and helpful friend who:\n\n*   **Listens for requests:**  It's constantly listening for requests from users visiting our website (like clicking \"Order\").\n*   **Routes requests:** It knows where to send each request based on the URL the user visits (e.g., `/` for the homepage, `/repos` for the repositories).\n*   **Processes requests:** It calls the right Python function to handle each request (e.g., fetch data from GitHub).\n*   **Sends responses:** It sends back a response to the user's browser (e.g., displaying the GitHub repositories).\n\nFastAPI does all this using special \"decorators\" like `@app.get` and `@app.post`. Let's see how they work.\n\n## Key Concept: Route Decorators\n\nThese decorators are the secret sauce that connects URLs to Python functions.\n\n*   `@app.get` is used for requests that *get* data (like displaying the homepage).\n*   `@app.post` is used for requests that *send* data (like submitting a username to fetch repos).\n\nHere's a simple example from our `app.py` file:\n\n```python\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import HTMLResponse\nfrom fastapi.templating import Jinja2Templates\n\napp = FastAPI()\ntemplates = Jinja2Templates(directory=\"templates\")\n\n@app.get(\"/\", response_class=HTMLResponse)\nasync def home(request: Request):\n    \"\"\"Render the homepage with the GitHub username form.\"\"\"\n    return templates.TemplateResponse(\"home.html\", {\"request\": request})\n```\n\n**Explanation:**\n\n1.  `app = FastAPI()`: This line creates a FastAPI \"app\" instance.  Think of it as creating our \"traffic controller.\"\n2.  `@app.get(\"/\", response_class=HTMLResponse)`: This is a decorator. It tells FastAPI: \"When someone visits the root URL (`/`) using a `GET` request (like typing the address in their browser), run the `home` function.\" The `response_class=HTMLResponse` specifies that the function should return HTML content.\n3.  `async def home(request: Request):`: This is the Python function that will be executed. It takes a `Request` object as input, which contains information about the incoming request.\n4.  `return templates.TemplateResponse(\"home.html\", {\"request\": request})`: This line uses Jinja2 templating (more on this in [Jinja2 Templating](02_jinja2_templating.md)) to render the `home.html` template and return it as the response.  The template receives the \"request\" object as context.\n\nSo, when you visit the website's homepage, FastAPI sees the request, runs the `home` function, and sends back the rendered `home.html` page to your browser.\n\n## Example Use Case: Fetching GitHub Repos\n\nLet's walk through how FastAPI is used in our project to fetch GitHub repositories.\n\n1.  **The User Enters a Username:** The user visits the homepage, enters their GitHub username, and clicks \"Submit.\"\n\n2.  **The Form is Submitted:** The browser sends a `POST` request to the `/repos` URL with the username as data.\n\n3.  **FastAPI Routes the Request:** FastAPI sees the `/repos` URL and the `POST` request type. It looks for a matching route.\n\nHere's the route definition from `app.py`:\n\n```python\nfrom fastapi import Form\n\n@app.post(\"/repos\", response_class=HTMLResponse)\nasync def repos(request: Request, username: str = Form(...)):\n    \"\"\"Fetch and display pinned repos based on the provided GitHub username.\"\"\"\n    token = os.getenv(\"GITHUB_TOKEN\")\n    repos = fetch_pinned_repos(username, token)\n    if repos is None:\n        return templates.TemplateResponse(\"home.html\", {\"request\": request, \"error\": \"Failed to fetch repos or no pinned repos found.\"})\n    return templates.TemplateResponse(\"repos.html\", {\"request\": request, \"repos\": repos, \"username\": username})\n```\n\n**Explanation:**\n\n1.  `@app.post(\"/repos\", response_class=HTMLResponse)`: This decorator tells FastAPI that when a `POST` request is made to `/repos`, the `repos` function should be executed.\n2.  `async def repos(request: Request, username: str = Form(...)):`:  This defines the `repos` function.  `username: str = Form(...)` tells FastAPI to expect a form field named \"username\" in the `POST` request.\n3.  `token = os.getenv(\"GITHUB_TOKEN\")`: This retrieves the GitHub token from environment variables (we'll cover this in [Environment Variable Management](05_environment_variable_management.md)).\n4.  `repos = fetch_pinned_repos(username, token)`:  This is the crucial part: it calls the `fetch_pinned_repos` function to actually fetch the data from GitHub using the provided username and token. We'll see how this works in [GitHub GraphQL API Interaction](03_github_graphql_api_interaction.md).\n5.  The remaining code handles potential errors (if `repos` is `None`) and renders the `repos.html` template with the fetched data. We'll talk about [Jinja2 Templating](02_jinja2_templating.md) in the next chapter.\n\n4.  **Data is Fetched and Displayed:** The `repos` function calls the `fetch_pinned_repos` function (details in [GitHub GraphQL API Interaction](03_github_graphql_api_interaction.md)), retrieves the pinned repositories from GitHub, and renders them using Jinja2. The rendered HTML is then sent back to the user's browser.\n\n## Under the Hood: FastAPI's Request Handling\n\nHere's a simplified view of how FastAPI handles requests:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Browser\n    participant FastAPI\n    participant App Logic\n    participant GitHub API\n\n    User->>Browser: Enters URL and submits form\n    Browser->>FastAPI: Sends HTTP Request\n    FastAPI->>App Logic: Routes request to appropriate function (e.g., repos)\n    App Logic->>GitHub API: Fetches data from GitHub (fetch_pinned_repos)\n    GitHub API-->>App Logic: Returns data\n    App Logic->>FastAPI: Returns rendered HTML\n    FastAPI->>Browser: Sends HTTP Response\n    Browser->>User: Displays webpage\n```\n\n**Explanation:**\n\n1.  The **User** interacts with the **Browser**, entering the URL and submitting the form.\n2.  The **Browser** sends an HTTP request to the **FastAPI** application.\n3.  **FastAPI** receives the request and routes it to the appropriate function in the **App Logic**.\n4.  The **App Logic** fetches data from the **GitHub API**.\n5.  The **GitHub API** returns the requested data to the **App Logic**.\n6.  The **App Logic** returns the rendered HTML to **FastAPI**.\n7.  **FastAPI** sends an HTTP response back to the **Browser**.\n8.  The **Browser** displays the webpage to the **User**.\n\n## Where Does FastAPI Live?\n\nThe heart of our FastAPI application is in the `app.py` file. This file defines the FastAPI instance (`app = FastAPI()`) and all the routes using decorators like `@app.get` and `@app.post`. It also contains the logic for handling requests and interacting with other parts of our application.\n\n## Conclusion\n\nIn this chapter, we learned about the core concept of a FastAPI application: how it acts as a \"traffic controller\" for web requests, routing them to the right functions and sending back responses. We saw how route decorators like `@app.get` and `@app.post` connect URLs to Python functions.  We also walked through an example of how FastAPI is used to fetch and display GitHub repositories.\n\nNext, we will explore [Jinja2 Templating](02_jinja2_templating.md), which is used to dynamically generate HTML content in our application.\n\n---\n# Chapter 2: Jinja2 Templating\n\nIn the previous chapter, [FastAPI Application](01_fastapi_application.md), we learned how FastAPI handles web requests and routes them to the right functions. But what happens *after* a function processes the request and *before* the response is sent back to the user's browser? That's where Jinja2 templating comes in!\n\nImagine you want to send a personalized birthday card to your friend. You wouldn't write a brand new card from scratch every time, right? Instead, you'd use a pre-made card with a space to fill in your friend's name. Jinja2 is like that pre-made card, and the data (your friend's name) is what makes it personalized.\n\nIn our `v1` project, Jinja2 helps us create dynamic HTML pages. It lets us embed Python variables and logic directly into our HTML code, making it easier to display information like a user's GitHub repositories.\n\n## What is Jinja2?\n\nJinja2 is a **templating engine**. Think of it like a cookie cutter.\n\n*   **Template (Cookie Cutter):** A template is an HTML file with placeholders for data. It defines the structure of the webpage.\n*   **Data (Dough):** The data is the information you want to display on the page (e.g., a username, a list of repositories).\n*   **Output (Cookie):** The output is the final HTML page, with the data filled in.\n\nJinja2 takes a template and data and combines them to create a personalized HTML page.\n\n## Key Concepts\n\nLet's break down the key concepts of Jinja2 templating:\n\n1.  **Templates:** These are HTML files that contain placeholders for dynamic data. Placeholders are denoted by special syntax, like `{{ variable_name }}` or `{% logic %}`.\n\n2.  **Variables:** These are placeholders in the template that get replaced with actual data when the template is rendered.  They're accessed using double curly braces: `{{ username }}`. If we pass `username=\"octocat\"` to the template, this will become \"octocat\" in the final HTML.\n\n3.  **Control Structures:** These allow you to add logic to your templates, like loops and conditional statements.  They're accessed using curly braces and percent signs: `{% if repos %}` ... `{% endif %}` or `{% for repo in repos %}` ... `{% endfor %}`.  This lets you show different content depending on whether data exists or to display repeating data.\n\n4.  **Rendering:** This is the process of combining the template and the data to produce the final HTML output.  FastAPI uses Jinja2 to render the templates.\n\n## How to Use Jinja2\n\nLet's see how Jinja2 is used in our `v1` project to display GitHub repositories.\n\nFirst, we have our FastAPI endpoint in `app.py`:\n\n```python\nfrom fastapi import FastAPI, Request, Form\nfrom fastapi.responses import HTMLResponse\nfrom fastapi.templating import Jinja2Templates\n\napp = FastAPI()\ntemplates = Jinja2Templates(directory=\"templates\")\n\n@app.post(\"/repos\", response_class=HTMLResponse)\nasync def repos(request: Request, username: str = Form(...)):\n    # ... (fetch repos logic - skipped for brevity) ...\n    return templates.TemplateResponse(\"repos.html\", {\"request\": request, \"repos\": repos, \"username\": username})\n```\n\n**Explanation:**\n\n1.  `templates = Jinja2Templates(directory=\"templates\")`: This line initializes Jinja2 and tells it to look for templates in the `templates` directory.\n2.  `templates.TemplateResponse(\"repos.html\", {\"request\": request, \"repos\": repos, \"username\": username})`: This line renders the `repos.html` template. It passes a dictionary of data to the template, including:\n    *   `request`: The incoming request object (required by Jinja2).\n    *   `repos`: A list of GitHub repositories fetched from the API.\n    *   `username`: The GitHub username entered by the user.\n\nNow, let's look at the `repos.html` template:\n\n```html\n<!DOCTYPE html>\n<html>\n<head>\n    <title>GitHub Repositories</title>\n</head>\n<body>\n    <h1>Pinned Repositories for {{ username }}</h1>\n    {% if repos %}\n        <ul>\n            {% for repo in repos %}\n                <li>\n                    <a href=\"{{ repo.node.url }}\">{{ repo.node.name }}</a> - {{ repo.node.description }}\n                </li>\n            {% endfor %}\n        </ul>\n    {% else %}\n        <p>No pinned repositories found.</p>\n    {% endif %}\n</body>\n</html>\n```\n\n**Explanation:**\n\n1.  `<h1>Pinned Repositories for {{ username }}</h1>`: This line uses a variable to display the username.  If `username` is \"octocat\", the heading will be \"Pinned Repositories for octocat\".\n\n2.  `{% if repos %}` ... `{% else %}` ... `{% endif %}`: This is a conditional statement. It checks if the `repos` variable is not empty. If it's not empty, the code inside the `if` block is executed. Otherwise, the code inside the `else` block is executed.\n\n3.  `{% for repo in repos %}` ... `{% endfor %}`: This is a loop. It iterates over each repository in the `repos` list.\n\n4.  `<a href=\"{{ repo.node.url }}\">{{ repo.node.name }}</a> - {{ repo.node.description }}`: This line displays the name, description, and URL of each repository.  It uses variables to access the data within each `repo` item.  `repo.node.url` refers to accessing the \"url\" field from the \"node\" field from the \"repo\" variable.\n\n**Example:**\n\nLet's say we pass the following data to the `repos.html` template:\n\n```python\nusername = \"octocat\"\nrepos = [\n    {\"node\": {\"name\": \"Spoon-Knife\", \"description\": \"This repo is for...\", \"url\": \"https://github.com/octocat/Spoon-Knife\"}},\n    {\"node\": {\"name\": \"Hello-World\", \"description\": \"My first repository...\", \"url\": \"https://github.com/octocat/Hello-World\"}},\n]\n```\n\nThe resulting HTML would be:\n\n```html\n<!DOCTYPE html>\n<html>\n<head>\n    <title>GitHub Repositories</title>\n</head>\n<body>\n    <h1>Pinned Repositories for octocat</h1>\n    <ul>\n        <li>\n            <a href=\"https://github.com/octocat/Spoon-Knife\">Spoon-Knife</a> - This repo is for...\n        </li>\n        <li>\n            <a href=\"https://github.com/octocat/Hello-World\">Hello-World</a> - My first repository...\n        </li>\n    </ul>\n</body>\n</html>\n```\n\n## Under the Hood: Jinja2's Rendering Process\n\nHere's a simplified view of how Jinja2 renders a template:\n\n```mermaid\nsequenceDiagram\n    participant FastAPI\n    participant Jinja2\n    participant Template\n    participant Data\n\n    FastAPI->>Jinja2: Call render_template(template, data)\n    Jinja2->>Template: Load template file (e.g., repos.html)\n    Jinja2->>Data: Receive data (e.g., repos, username)\n    Jinja2->>Jinja2: Process template, replacing variables with data\n    Jinja2-->>FastAPI: Return rendered HTML\n```\n\n**Explanation:**\n\n1.  **FastAPI** calls `render_template` in Jinja2, passing the template name and the data.\n2.  **Jinja2** loads the template file from the `templates` directory.\n3.  **Jinja2** receives the data from FastAPI.\n4.  **Jinja2** processes the template, replacing the variables with the data. It also executes any control structures (loops, conditional statements).\n5.  **Jinja2** returns the rendered HTML to FastAPI.\n\n## Where Does Jinja2 Live?\n\nThe Jinja2 templates live in the `templates` directory. FastAPI is configured to look for templates in this directory when you initialize `Jinja2Templates`.  The line `templates = Jinja2Templates(directory=\"templates\")` in `app.py` is what sets this up.\n\n## Conclusion\n\nIn this chapter, we learned about Jinja2 templating and how it helps us create dynamic HTML pages in our `v1` project. We saw how to use variables and control structures to personalize the content of our webpages.  We also walked through the rendering process and understood where Jinja2 templates live in our project.\n\nNext, we'll dive into [GitHub GraphQL API Interaction](03_github_graphql_api_interaction.md) to see how we fetch the data that we display in our templates.\n\n---\n# Chapter 3: GitHub GraphQL API Interaction\n\nIn the previous chapter, [Jinja2 Templating](02_jinja2_templating.md), we learned how to create dynamic HTML pages to display data. But where does that data *come* from? In this chapter, we'll learn how to fetch data from GitHub using the GitHub GraphQL API.\n\nImagine you're ordering food from a restaurant. You need to tell the restaurant *what* you want (your order) and prove that you can *pay* for it. The GitHub GraphQL API is like that restaurant. We send it a specific request (our order) and a token (our payment) to get the data we need. Specifically, we will fetch a user's pinned repositories from GitHub. This is where the `fetch_pinned_repos` function comes in!\n\n## What is the GitHub GraphQL API?\n\nThe GitHub GraphQL API is a powerful way to request specific data from GitHub. Instead of getting a huge amount of information you don't need, you can ask for *exactly* what you want.\n\nThink of it like this:\n\n*   **REST API:** You order a whole pizza, even if you only want a slice.\n*   **GraphQL API:** You order only the slice you want.\n\nGraphQL lets us be much more efficient! In our case, we use it to ask for only the pinned repositories of a given GitHub user.\n\n## Key Concepts\n\nLet's break down the key concepts of interacting with the GitHub GraphQL API:\n\n1.  **Query:** This is your \"order.\" It's a string that tells the GitHub API exactly what data you want. In our case, it specifies that we want the pinned repositories for a specific user.\n\n2.  **Headers:** These are like your \"payment details.\" They include your GitHub token, which proves that you have permission to access the data. Think of the token as a key that unlocks access to GitHub's data.\n\n3.  **Request:** This is the actual \"order\" being sent to the \"restaurant.\" We use a Python library called `requests` to send the query and headers to the GitHub API URL.\n\n4.  **Response:** This is the \"food\" delivered to you. It's the data that the GitHub API sends back. We then need to process this response to extract the information we need.\n\n## How to Use `fetch_pinned_repos`\n\nThe `fetch_pinned_repos` function is the core of our interaction with the GitHub GraphQL API. It takes a username and a token as input and returns a list of pinned repositories (or `None` if there's an error).\n\nHere's the function definition from `app.py`:\n\n```python\nfrom typing import List, Optional\nimport requests\n\nGITHUB_API_URL = \"https://api.github.com/graphql\"\n\ndef fetch_pinned_repos(username: str, token: str) -> Optional[List[dict]]:\n    \"\"\"Fetch pinned repositories using GitHub GraphQL API.\"\"\"\n    query = f\"\"\"\n    {{\n      user(login: \"{username}\") {{\n        pinnedItems(first: 6, types: [REPOSITORY]) {{\n          edges {{\n            node {{\n              ... on Repository {{\n                name\n                description\n                url\n                stargazerCount\n                forkCount\n                primaryLanguage {{\n                  name\n                }}\n              }}\n            }}\n          }}\n        }}\n      }}\n    }}\n    \"\"\"\n\n    headers = {\"Authorization\": f\"Bearer {token}\"}\n    response = requests.post(GITHUB_API_URL, json={\"query\": query}, headers=headers)\n\n    if response.status_code == 200:\n        return response.json()[\"data\"][\"user\"][\"pinnedItems\"][\"edges\"]\n    else:\n        return None\n```\n\n**Explanation:**\n\n1.  `username: str, token: str`: The function takes the GitHub username and the GitHub token as input.\n2.  `query = f\"\"\"...\"\"\"`: This is the GraphQL query.  It asks for the first 6 pinned repositories of the user specified by `username`. It also asks for specific information about each repository, such as its name, description, and URL.  The triple quotes (`\"\"\"`) allow us to write the query as a multi-line string, making it easier to read.  The `f` before the string allows us to insert the value of the `username` variable directly into the query.\n3.  `headers = {\"Authorization\": f\"Bearer {token}\"}`: This creates the headers for the request. The `Authorization` header includes the GitHub token.\n4.  `response = requests.post(GITHUB_API_URL, json={\"query\": query}, headers=headers)`: This line uses the `requests` library to send a POST request to the GitHub GraphQL API URL.  The `json` argument specifies the GraphQL query, and the `headers` argument includes the authorization token.\n5.  `if response.status_code == 200:`: This checks if the request was successful. A status code of 200 means \"OK.\"\n6.  `return response.json()[\"data\"][\"user\"][\"pinnedItems\"][\"edges\"]`: If the request was successful, this line extracts the list of pinned repositories from the JSON response. The response is a nested dictionary, and we need to navigate through it to get to the `edges` which contain the repository data.\n7.  `else: return None`: If the request failed, this line returns `None`.\n\n**Example:**\n\nLet's say we call `fetch_pinned_repos` with the username \"octocat\" and a valid GitHub token:\n\n```python\nusername = \"octocat\"\ntoken = \"YOUR_GITHUB_TOKEN\"  # Replace with your actual token\nrepos = fetch_pinned_repos(username, token)\n\nif repos:\n    for repo in repos:\n        print(f\"Repo Name: {repo['node']['name']}\")\n        print(f\"Repo Description: {repo['node']['description']}\")\nelse:\n    print(\"Failed to fetch repos.\")\n```\n\n**Output (example):**\n\n```\nRepo Name: Spoon-Knife\nRepo Description: This repo is for...\nRepo Name: Hello-World\nRepo Description: My first repository...\n... (and so on for each pinned repo)\n```\n\nIf the token is invalid or the username doesn't exist, the output will be:\n\n```\nFailed to fetch repos.\n```\n\n## Under the Hood: How `fetch_pinned_repos` Works\n\nLet's walk through what happens step-by-step when the `fetch_pinned_repos` function is called:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant FastAPI\n    participant fetch_pinned_repos\n    participant GitHub API\n\n    User->>FastAPI: Enters username and submits form\n    FastAPI->>fetch_pinned_repos: Calls fetch_pinned_repos(username, token)\n    fetch_pinned_repos->>GitHub API: Sends GraphQL query with token\n    GitHub API-->>fetch_pinned_repos: Returns JSON response\n    fetch_pinned_repos->>FastAPI: Returns list of repositories (or None)\n    FastAPI->>User: Displays repositories in the browser\n```\n\n**Explanation:**\n\n1.  The **User** enters a username and submits a form on the website, triggering a request to the FastAPI server.\n2.  **FastAPI** calls the `fetch_pinned_repos` function with the username and GitHub token.\n3.  `fetch_pinned_repos` constructs a GraphQL query and sends it to the **GitHub API** along with the token in the headers.\n4.  The **GitHub API** processes the query and returns a JSON response containing the requested data (the pinned repositories).\n5.  `fetch_pinned_repos` parses the JSON response and returns a list of repositories (or `None` if there was an error) to FastAPI.\n6.  Finally, **FastAPI** uses Jinja2 to render the repositories in the user's browser.\n\nNow, let's dive a bit deeper into the code. The key parts are:\n\n1.  **Constructing the Query:**\n\n    ```python\n    query = f\"\"\"\n    {{\n      user(login: \"{username}\") {{\n        pinnedItems(first: 6, types: [REPOSITORY]) {{\n          edges {{\n            node {{\n              ... on Repository {{\n                name\n                description\n                url\n                stargazerCount\n                forkCount\n                primaryLanguage {{\n                  name\n                }}\n              }}\n            }}\n          }}\n        }}\n      }}\n    }}\n    \"\"\"\n    ```\n\n    This code creates the GraphQL query as a string.  The `f` before the string allows us to insert the `username` variable directly into the query. It's important to get the syntax of the GraphQL query correct, or the GitHub API will return an error. You can test your GraphQL queries using GitHub's GraphQL Explorer: [https://docs.github.com/en/graphql/overview/explorer](https://docs.github.com/en/graphql/overview/explorer)\n\n2.  **Sending the Request:**\n\n    ```python\n    headers = {\"Authorization\": f\"Bearer {token}\"}\n    response = requests.post(GITHUB_API_URL, json={\"query\": query}, headers=headers)\n    ```\n\n    This code sends the GraphQL query to the GitHub API using the `requests` library.  It's crucial to include the `Authorization` header with your GitHub token. Without it, the API will reject the request.\n\n3.  **Handling the Response:**\n\n    ```python\n    if response.status_code == 200:\n        return response.json()[\"data\"][\"user\"][\"pinnedItems\"][\"edges\"]\n    else:\n        return None\n    ```\n\n    This code checks the response status code to see if the request was successful. If the status code is 200, it parses the JSON response and extracts the list of pinned repositories. If the status code is not 200, it means there was an error, and the function returns `None`.\n\n## Where Does This Live?\n\nThe `fetch_pinned_repos` function lives in `app.py`.  It's called by the `/repos` endpoint to fetch the data that is displayed on the `repos.html` page.  It's also used in the `/api/repos/{username}` endpoint to provide a JSON API for fetching the data.\n\n## Conclusion\n\nIn this chapter, we learned how to interact with the GitHub GraphQL API using the `fetch_pinned_repos` function. We saw how to construct a GraphQL query, send it to the API with the correct headers, and process the response to extract the data we need.  We also walked through the internal implementation of the `fetch_pinned_repos` function and understood how it interacts with the GitHub API.\n\nNext, we'll explore [Data Handling (Repositories)](04_data_handling__repositories_.md) to learn how to organize and process the repository data we fetch from GitHub.\n\n---\n# Chapter 4: Data Handling (Repositories)\n\nIn the previous chapter, [GitHub GraphQL API Interaction](03_github_graphql_api_interaction.md), we learned how to fetch a list of GitHub repositories. But what do we *do* with that list once we have it? How do we organize it and get it ready to display on our website? That's what this chapter is all about!\n\nImagine you're a librarian. You can't just throw all the books onto the shelves randomly! You need a system for organizing them so people can find what they're looking for. In our project, the \"books\" are the repositories we get from GitHub, and our \"system\" is the way we handle and process that data.\n\nThe `fetch_pinned_repos` function retrieves raw repository data. In this chapter, we'll focus on how the application *handles* that raw data - presenting it in a user-friendly way. It's like having a librarian who finds books (repositories), categorizes them (processes them), and puts them on display (displays them). The Jinja2 templates then display the output.\n\n## Key Concepts\n\nLet's break down the key concepts of data handling in our project:\n\n1.  **Fetching Raw Data:** This is where we get the initial data from the GitHub API. The `fetch_pinned_repos` function, which we covered in [GitHub GraphQL API Interaction](03_github_graphql_api_interaction.md), is responsible for this. The result of this function call is raw data structured in a specific format.\n\n2.  **Passing Data to Templates:** After the `fetch_pinned_repos` function retrieves the data, it needs to be *passed* to the Jinja2 template. The Jinja2 template is responsible for defining *how* the data is displayed. This includes the username, the list of repositories, and any errors that might have occurred.\n\n3.  **Displaying the Data:** The Jinja2 template takes the data and uses it to generate the final HTML that is sent to the user's browser.\n\n## How to Use Data Handling\n\nLet's walk through the process step-by-step:\n\n1.  **User enters a username and submits the form:** Imagine the user enters the username \"octocat\".\n\n2.  **`fetch_pinned_repos` is called:** The `fetch_pinned_repos` function is called with the username \"octocat\" and a GitHub token.\n\n3.  **Data is fetched from GitHub:** The `fetch_pinned_repos` function sends a GraphQL query to the GitHub API and receives a JSON response containing the pinned repositories for the user \"octocat\".\n\n4.  **Data is passed to the template:** The `repos` function in `app.py` receives the data and passes it to the `repos.html` template:\n\n    ```python\n    from fastapi import Form\n\n    @app.post(\"/repos\", response_class=HTMLResponse)\n    async def repos(request: Request, username: str = Form(...)):\n        \"\"\"Fetch and display pinned repos based on the provided GitHub username.\"\"\"\n        token = os.getenv(\"GITHUB_TOKEN\")\n        repos = fetch_pinned_repos(username, token)\n        if repos is None:\n            return templates.TemplateResponse(\"home.html\", {\"request\": request, \"error\": \"Failed to fetch repos or no pinned repos found.\"})\n        return templates.TemplateResponse(\"repos.html\", {\"request\": request, \"repos\": repos, \"username\": username})\n    ```\n\n    **Explanation:**\n\n    *   This function calls `fetch_pinned_repos` to retrieve data.\n    *   If `fetch_pinned_repos` returns `None`, meaning there was an error, an error message is displayed on the `home.html` template.\n    *   If the request succeeds, it calls `templates.TemplateResponse` to display the `repos.html` template with the retrieved `repos` data and the `username` data. The `request` object is also passed.\n\n5.  **The template displays the data:** The `repos.html` template uses Jinja2 syntax to display the username and the list of repositories:\n\n    ```html\n    <h1>Pinned Repositories for {{ username }}</h1>\n    {% if repos %}\n        <ul>\n            {% for repo in repos %}\n                <li>\n                    <a href=\"{{ repo.node.url }}\">{{ repo.node.name }}</a> - {{ repo.node.description }}\n                </li>\n            {% endfor %}\n        </ul>\n    {% else %}\n        <p>No pinned repositories found.</p>\n    {% endif %}\n    ```\n\n    **Explanation:**\n\n    *   `{{ username }}` displays the username.\n    *   The `{% if repos %}` block checks if the `repos` variable is not empty. If it's not empty, the list of repositories is displayed.\n    *   The `{% for repo in repos %}` loop iterates through each repository in the list and displays its name, description, and URL.\n\n## Under the Hood: Data Flow\n\nLet's visualize how the data flows through our application:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Browser\n    participant FastAPI\n    participant fetch_pinned_repos\n    participant Jinja2 Template\n\n    User->>Browser: Enters username, submits form\n    Browser->>FastAPI: Sends POST request to /repos\n    FastAPI->>fetch_pinned_repos: Calls fetch_pinned_repos(username, token)\n    fetch_pinned_repos-->>FastAPI: Returns repo data\n    FastAPI->>Jinja2 Template: Renders repos.html with data\n    Jinja2 Template-->>FastAPI: Returns HTML\n    FastAPI->>Browser: Sends HTML response\n    Browser->>User: Displays webpage\n```\n\n**Explanation:**\n\n1.  The **User** interacts with the **Browser**, entering the username and submitting the form.\n2.  The **Browser** sends a POST request to the `/repos` endpoint on the **FastAPI** server.\n3.  **FastAPI** calls the `fetch_pinned_repos` function to get the repository data.\n4.  `fetch_pinned_repos` retrieves the repo data and returns it to FastAPI.\n5.  **FastAPI** takes the data and renders it into the `repos.html` file using Jinja2.\n6.  The **Jinja2 Template** returns the HTML code.\n7.  **FastAPI** then returns the rendered HTML to the **Browser** and the **User** views the webpage.\n\n## Where Does This Live?\n\nThe data handling logic is spread across several files:\n\n*   `app.py`: Contains the `repos` function, which calls `fetch_pinned_repos` and passes the data to the template. Also contains the `fetch_pinned_repos` function.\n*   `templates/repos.html`: Contains the Jinja2 template that displays the data.\n\n## Conclusion\n\nIn this chapter, we learned about how our application handles repository data. We saw how the `fetch_pinned_repos` function retrieves the data, how the `repos` function passes the data to the Jinja2 template, and how the template displays the data to the user. This process allows us to present the data in a user-friendly format.\n\nNext, we'll explore [Environment Variable Management](05_environment_variable_management.md) to learn how to securely store and access sensitive information like the GitHub token.\n\n\nRelevant Code Snippets:\n--- File: app.py ---\nfrom fastapi import FastAPI, Request, Form\nfrom fastapi.responses import HTMLResponse\nfrom fastapi.templating import Jinja2Templates\nimport os\nimport requests\nfrom dotenv import load_dotenv\nfrom typing import List, Optional\nfrom fastapi.responses import JSONResponse\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Set up FastAPI\napp = FastAPI()\n\n# Set up templates (Jinja2)\ntemplates = Jinja2Templates(directory=\"templates\")\n\n# Constants\nGITHUB_API_URL = \"https://api.github.com/graphql\"\n\n\ndef get_github_token() -> str:\n    \"\"\"Fetch GitHub token from environment variable.\"\"\"\n    token = os.getenv(\"GITHUB_TOKEN\")\n    if not token:\n        raise ValueError(\"GitHub token not found. Please set the GITHUB_TOKEN env variable.\")\n    return token\n\n\ndef fetch_pinned_repos(username: str, token: str) -> Optional[List[dict]]:\n    \"\"\"Fetch pinned repositories using GitHub GraphQL API.\"\"\"\n    query = f\"\"\"\n    {{\n      user(login: \"{username}\") {{\n        pinnedItems(first: 6, types: [REPOSITORY]) {{\n          edges {{\n            node {{\n              ... on Repository {{\n                name\n                description\n                url\n                stargazerCount\n                forkCount\n                primaryLanguage {{\n                  name\n                }}\n              }}\n            }}\n          }}\n        }}\n      }}\n    }}\n    \"\"\"\n\n    headers = {\"Authorization\": f\"Bearer {token}\"}\n    response = requests.post(GITHUB_API_URL, json={\"query\": query}, headers=headers)\n\n    if response.status_code == 200:\n        return response.json()[\"data\"][\"user\"][\"pinnedItems\"][\"edges\"]\n    else:\n        return None\n\n\n@app.get(\"/\", response_class=HTMLResponse)\nasync def home(request: Request):\n    \"\"\"Render the homepage with the GitHub username form.\"\"\"\n    return templates.TemplateResponse(\"home.html\", {\"request\": request})\n\n\n@app.post(\"/repos\", response_class=HTMLResponse)\nasync def repos(request: Request, username: str = Form(...)):\n    \"\"\"Fetch and display pinned repos based on the provided GitHub username.\"\"\"\n    token = os.getenv(\"GITHUB_TOKEN\")\n    repos = fetch_pinned_repos(username, token)\n    if repos is None:\n        return templates.TemplateResponse(\"home.html\", {\"request\": request, \"error\": \"Failed to fetch repos or no pinned repos found.\"})\n    return templates.TemplateResponse(\"repos.html\", {\"request\": request, \"repos\": repos, \"username\": username})\n  \n\n@app.get(\"/api/repos/{username}\")\nasync def api_repos(username: str):\n    token = get_github_token()\n    repos = fetch_pinned_repos(username, token)\n    if repos is None:\n        return JSONResponse(content={\"error\": \"Failed to fetch repos.\"}, status_code=400)\n    return {\"repos\": repos}\n\n\n\n\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 5: Environment Variable Management`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "# Chapter 5: Environment Variable Management\n\nIn the previous chapter, [Data Handling (Repositories)](04_data_handling__repositories_.md), we learned how to fetch data from GitHub and display it on our website. But you might have noticed something a little concerning: we're using a GitHub token! Where should we store this secret key so that it doesn't accidentally get shared with the world?\n\nImagine you have a house key. You wouldn't just leave it lying around in plain sight, would you? You'd keep it in a safe place, like your pocket or a lockbox. Environment variable management is like having a lockbox for your code's secrets, such as API keys and database passwords. Instead of hardcoding these sensitive values directly into our code, we store them separately and access them when needed. This is where environment variables come in!\n\n## What are Environment Variables?\n\nEnvironment variables are variables that are set outside of your code. Think of them as settings that your computer knows about, separate from any specific program.\n\n**Key Concepts:**\n\n1.  **`.env` file:** This is a special file where we store our environment variables. It's just a plain text file with key-value pairs. For example:\n\n    ```\n    GITHUB_TOKEN=YOUR_GITHUB_TOKEN\n    ```\n\n    Here, `GITHUB_TOKEN` is the key, and `YOUR_GITHUB_TOKEN` is the value (your actual GitHub token).\n\n2.  **`dotenv` library:** This is a Python library that helps us load environment variables from the `.env` file into our code. It's like a translator that reads the `.env` file and makes those values available to our program.\n\n3.  **`load_dotenv()` function:** This function from the `dotenv` library reads the `.env` file and loads the variables into the environment.\n\n4.  **`os.getenv()` function:** This function is part of Python's built-in `os` module. It allows us to access the values of environment variables from within our code. It's how we retrieve the secret information when we need it!\n\n## How to Use Environment Variables\n\nLet's see how we use environment variables to store our GitHub token in the `v1` project.\n\n1.  **Create a `.env` file:** In the root directory of your project (where `app.py` is located), create a file named `.env`. Add your GitHub token to this file:\n\n    ```\n    GITHUB_TOKEN=YOUR_GITHUB_TOKEN  # Replace with your actual token!\n    ```\n\n    **Important:** Never commit your `.env` file to a public repository! Add `.env` to your `.gitignore` file to prevent accidentally sharing your secrets.\n\n2.  **Load environment variables in `app.py`:** At the beginning of your `app.py` file, add the following lines:\n\n    ```python\n    from dotenv import load_dotenv\n    import os\n\n    # Load environment variables from .env file\n    load_dotenv()\n    ```\n\n    This code imports the `load_dotenv` function from the `dotenv` library and calls it. This reads the `.env` file and makes the variables available to our program. We also need to import `os` which allows the loading of these variables.\n\n3.  **Access the GitHub token in your code:**  Instead of hardcoding the token, we use `os.getenv()` to retrieve it:\n\n    ```python\n    import os\n\n    token = os.getenv(\"GITHUB_TOKEN\")\n    ```\n\n    Here, `os.getenv(\"GITHUB_TOKEN\")` retrieves the value associated with the `GITHUB_TOKEN` environment variable (which is your actual GitHub token). If the variable isn't set, `os.getenv()` returns `None` (or a default value if you specify one).\n\n4.  **Using the Token to Fetch Repos:** Now, when fetching repos we will use our environment variable:\n\n    ```python\n    from fastapi import Form\n\n    @app.post(\"/repos\", response_class=HTMLResponse)\n    async def repos(request: Request, username: str = Form(...)):\n        \"\"\"Fetch and display pinned repos based on the provided GitHub username.\"\"\"\n        token = os.getenv(\"GITHUB_TOKEN\") # <---- Our token\n        repos = fetch_pinned_repos(username, token)\n        if repos is None:\n            return templates.TemplateResponse(\"home.html\", {\"request\": request, \"error\": \"Failed to fetch repos or no pinned repos found.\"})\n        return templates.TemplateResponse(\"repos.html\", {\"request\": request, \"repos\": repos, \"username\": username})\n    ```\n    The `token = os.getenv(\"GITHUB_TOKEN\")` line is now getting the GITHUB_TOKEN from your .env file.\n\n## Under the Hood: How it Works\n\nHere's a simplified view of how environment variable management works in our project:\n\n```mermaid\nsequenceDiagram\n    participant App\n    participant .env File\n    participant dotenv Library\n    participant OS\n\n    App->>dotenv Library: Call load_dotenv()\n    dotenv Library->>.env File: Read .env file\n    dotenv Library->>OS: Set environment variables\n    App->>OS: Call os.getenv(\"GITHUB_TOKEN\")\n    OS-->>App: Return value of GITHUB_TOKEN\n```\n\n**Explanation:**\n\n1.  The **App** (our FastAPI application) calls `load_dotenv()` from the `dotenv Library`.\n2.  The `dotenv Library` reads the `.env File`.\n3.  The `dotenv Library` sets the environment variables within the **OS** (Operating System).\n4.  The **App** calls `os.getenv(\"GITHUB_TOKEN\")` to retrieve the value of the `GITHUB_TOKEN` environment variable from the **OS**.\n5.  The **OS** returns the value of `GITHUB_TOKEN` to the **App**.\n\n**Code Explanation:**\n\n1.  **`load_dotenv()`:**\n\n    This function does the magic of reading your `.env` file and loading those variables into your system's environment. Without this step, your program won't know about the variables defined in your `.env` file.\n\n2.  **`os.getenv()`:**\n\n    This function retrieves an environment variable by its name (e.g., `\"GITHUB_TOKEN\"`). This lets our code safely access sensitive information without hardcoding it.\n\n## Where Does This Live?\n\nThe environment variable configuration is handled in the following places:\n\n*   `.env`: The file where the actual environment variables are stored.\n*   `app.py`: The main application file, where we load the environment variables using `load_dotenv()` and access them using `os.getenv()`.\n\n## Conclusion\n\nIn this chapter, we learned about environment variable management and how to use it to store sensitive information securely. We saw how to create a `.env` file, load environment variables using the `dotenv` library, and access them in our code using `os.getenv()`.  We saw how storing our `GITHUB_TOKEN` in our .env file then using the `os.getenv()` method to read it, is a safer approach than hardcoding the secret.\n\nCongratulations! You've now completed the core modules for this tutorial.\n", "\nFor the project `CodeRoast`:\n\nCodebase Context:\n--- File Index 0: README.md ---\n# \ud83e\udd16 Code Roast : An AI-Powered Pull Request Review System\n\n> An end-to-end intelligent platform for automated code review, documentation updates, and pull request analysis \u2014 powered by LLMs, Tree-sitter, and Snowflake.\n\n---\n\n## \ud83d\ude80 Overview\n\nThis system integrates with GitHub to automatically:\n- Analyze pull requests using AI (via [Groq](https://groq.com))\n- Parse and understand diffs using [Tree-sitter](https://tree-sitter.github.io/)\n- Perform RAG (Retrieval-Augmented Generation) to update project documentation\n- Store all insights in **Snowflake** for analytics\n- Display all PRs, diffs, AI feedback, and doc suggestions via a beautiful **Web App**\n\nWhether you're an engineering team lead, reviewer, or contributor \u2014 you get fast, reliable, and insightful AI-powered review feedback at every step.\n\n---\n\n## \ud83e\udde0 Features\n\n- \u2705 **AI PR Summary**  \n  Instant analysis of pull request purpose, syntax/style/functionality, and merge confidence.\n\n- \ud83d\udcdd **Automatic Docstring Suggestions**  \n  Extract or generate missing/updated docstrings for new or changed functions.\n\n- \ud83d\udcd8 **RAG-based Documentation Updates**  \n  Contextual retrieval from `project_docs.txt` with AI-generated Markdown additions.\n\n- \ud83d\udcc4 **Code Diff Parsing**  \n  Tree-sitter-based code structure diffing to isolate changed functions only.\n\n- \ud83d\udcac **GitHub Comment Posting**  \n  Summaries are posted as comments directly on PRs, no need to open another tool.\n\n- \ud83d\udcca **Snowflake Integration**  \n  All AI-generated insights and PR metadata stored for reporting, audit, and search.\n\n- \ud83c\udf10 **Web App Dashboard**  \n  Browse all PRs, view diffs, AI reviews, and doc updates in a single UI.\n\n---\n\n## \ud83e\uddf1 Tech Stack\n\n| Layer              | Tech Used                                      |\n|--------------------|-----------------------------------------------|\n| **AI Models**       | [Groq API (LLaMA 3)](https://groq.com)        |\n| **Diff Parsing**    | [Tree-sitter](https://tree-sitter.github.io/) |\n| **Backend API**     | [FastAPI](https://fastapi.tiangolo.com)       |\n| **Git Integration** | GitHub App + `PyGithub`                       |\n| **RAG Engine**      | Basic keyword-matching from local docs        |\n| **Database**        | [Snowflake](https://www.snowflake.com)        |\n| **Frontend**        | HTML, JS, CSS (Markdown + Diff Viewers)       |\n\n---\n\n## \u2699\ufe0f How It Works\n\n1. \ud83d\udd14 **GitHub Webhook Trigger**\n   - On PR open/update \u2192 sends payload to FastAPI backend\n\n2. \ud83e\udde0 **AI Review Processing**\n   - Extracts code diffs, changed functions\n   - Groq generates AI summary, confidence rating, and docstrings\n\n3. \ud83d\udcd8 **Documentation Updates**\n   - Local project docs indexed via simple RAG\n   - Groq suggests Markdown additions\n\n4. \ud83d\udcbe **Data Storage**\n   - All results are saved in Snowflake for dashboard & analytics\n\n5. \ud83c\udf10 **Web App UI**\n   - Explore PRs, view diffs, AI reviews, and doc updates\n\n\n--- File Index 1: backend/ai_analyse.py ---\nimport requests\nimport json\nimport logging\nimport snowflake.connector\nimport random\nimport datetime\n\nGROQ_API_KEY = \"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\"\nGROQ_ENDPOINT = \"https://api.groq.com/openai/v1/chat/completions\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\n# Snowflake setup\nconn = snowflake.connector.connect(\n    user=\"raghavg332\",\n    password=\"Qa29Kh4MptfGHEW\",\n    account=\"KXCIVVH-LL27432\",\n    warehouse=\"COMPUTE_WH\",\n    database=\"PR_DASHBOARD\",\n    schema=\"PUBLIC\"\n)\ncur = conn.cursor()\n\ndef update_documentation_with_rag(pr_description: str, code_diff: str, docs_path=\"backend/project_docs.txt\"):\n    \"\"\"\n    Performs a simple RAG-based call to Groq to update documentation.\n    \"\"\"\n    # Step 1: Load the existing documentation\n    try:\n        with open(docs_path, \"r\") as f:\n            existing_docs = f.read()\n    except FileNotFoundError:\n        logging.error(f\"[\u274c] Documentation file not found at {docs_path}\")\n        return {\"error\": \"Documentation file not found.\"}\n\n    # Step 2: Very basic RAG - extract top paragraphs that contain overlap\n    paragraphs = existing_docs.split(\"\\n\\n\")\n    keywords = pr_description.lower().split()\n    ranked_paragraphs = sorted(paragraphs, key=lambda para: sum(word in para.lower() for word in keywords), reverse=True)\n    context = \"\\n\\n\".join(ranked_paragraphs[:3])  # Top 3 paragraphs as \"retrieved context\"\n\n    # Step 3: Build the prompt\n    doc_prompt = f\"\"\"\nYou are an expert technical writer and code documentation assistant.\n\nA developer has made the following changes to the codebase:\n\n## PR Description:\n{pr_description}\n\n## Code Diff:\n{code_diff}\n\n## Relevant Existing Documentation:\n{context}\n\nPlease provide additions or updates to the documentation based on the above code change. \nFormat your response in **Markdown**, and only include sections that should be added or updated. \nIf the change doesn't require doc updates, say so.\n\"\"\"\n\n    # Step 4: Call Groq API\n    payload = {\n        \"model\": \"llama3-70b-8192\",\n        \"messages\": [{\"role\": \"user\", \"content\": doc_prompt}]\n    }\n\n    try:\n        response = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n        response.raise_for_status()\n        result = response.json()\n        return {\n            \"doc_update\": result[\"choices\"][0][\"message\"][\"content\"].strip()\n        }\n\n    except Exception as e:\n        logging.exception(\"Groq API call for documentation update failed\")\n        return {\"error\": str(e)}\n\n\ndef build_full_prompt(pr_description: str, code_diff: str) -> str:\n    return f\"\"\"\nYou are an AI code reviewer and documentation assistant.\n\nA developer submitted the following pull request. Please do the following:\n1. \ud83d\udd0d Summarize the overall purpose of the PR.\n2. \ud83e\udde0 Rate the merge confidence as: High / Medium / Low.\n3. \u2705 Review the code for:\n   - Syntax issues\n   - Style (PEP8)\n   - Functionality correctness (does it match the description?)\n4. \ud83d\udcdd Extract or suggest updated docstrings for any changed or added functions/classes.\n\n## PR Description:\n{pr_description}\n\n## Code Diff:\n{code_diff}\n\nRespond ONLY in this JSON format:\n\n{{\n  \"ai_summary\": \"...\",\n  \"merge_confidence\": \"1-10\",\n  \"code_quality\": {{\n    \"syntax_check\": \"...\",\n    \"style_check\": \"...\",\n    \"functionality_check\": \"...\",\n    \"final_rating\": \"Excellent / Good / Needs Work / Critical Issues\"\n  }}\n}}\n\"\"\"\n\ndef review_and_store_pr(pr_description: str, code_diff: str, pr_title: str, pr_author: str, pr_status: str):\n    docstring = update_documentation_with_rag(pr_description, code_diff)\n    print(docstring)\n    print('\\n\\n')\n    pr_id = random.randint(1000, 9999)\n    prompt = build_full_prompt(pr_description, code_diff)\n\n    payload = {\n        \"model\": \"llama3-70b-8192\",\n        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n    }\n\n    try:\n        response = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n        response.raise_for_status()\n        result = response.json()\n\n        try:\n            content = result[\"choices\"][0][\"message\"][\"content\"].strip()\n            review_data = json.loads(content)\n\n            # Flatten the code quality object for easier insert\n            code_quality_str = (\n                f\"Syntax: {review_data['code_quality']['syntax_check']}\\n\"\n                f\"Style: {review_data['code_quality']['style_check']}\\n\"\n                f\"Functionality: {review_data['code_quality']['functionality_check']}\\n\"\n                f\"Rating: {review_data['code_quality']['final_rating']}\"\n            )\n            cur.execute(f\"\"\"\n                INSERT INTO PULL_REQUESTS (\n                    ID, TITLE, AUTHOR, STATUS, CREATED_AT, UPDATED_AT\n                )\n                VALUES (%s, %s, %s, %s, %s, %s);\n            \"\"\", (\n                pr_id,\n                pr_title,\n                pr_author,\n                pr_status,\n                datetime.datetime.now(),\n                datetime.datetime.now()\n            ))\n            conn.commit()\n\n            # Insert into Snowflake\n            cur.execute(f\"\"\"\n                INSERT INTO PR_ANALYSIS (\n                    PR_ID, AI_SUMMARY, MERGE_CONFIDENCE, CODE_QUALITY,\n                    PR_DESCRIPTION, DOCSTRINGS, DIFF\n                )\n                VALUES (%s, %s, %s, %s, %s, %s, %s);\n            \"\"\", (\n                pr_id,\n                review_data[\"ai_summary\"],\n                review_data[\"merge_confidence\"],\n                code_quality_str,\n                pr_description,\n                docstring['doc_update'],\n                code_diff\n            ))\n\n            conn.commit()\n\n            logging.info(f\"[\u2705] PR #{pr_id} analysis saved to Snowflake.\")\n            return review_data\n\n        except json.JSONDecodeError:\n            logging.error(\"[\u274c] Groq returned non-JSON output.\")\n            return {\"error\": \"Invalid JSON from Groq\"}\n\n    except Exception as e:\n        logging.exception(\"Groq API or Snowflake insert failed\")\n        return {\"error\": str(e)}\n\n# # \ud83e\uddea Example test\n# if __name__ == \"__main__\":\n#     code_diff = \"\"\"\n# diff --git a/app/main.py b/app/main.py\n# index a1b2c3d..d4e5f6g 100644\n# --- a/app/main.py\n# +++ b/app/main.py\n# @@ def main():\n# -    print(\"Hello\")\n# +    log_greeting(\"Hello, world!\")\n# +    print(\"Execution completed.\")\n\n# +def log_greeting(message):\n# +    \\\"\\\"\\\"Logs the greeting message to a file\\\"\\\"\\\"\n# +    with open(\"logs.txt\", \"a\") as log_file:\n# +        log_file.write(f\"Greeting logged: {message}\\\\n\")\n# \"\"\"\n#     pr_description = \"Update main function to greet the world.\"\n#     pr_title = \"Update main function\"\n#     pr_author = \"johndoe\"\n#     pr_status = \"OPEN\"\n\n#     result = review_and_store_pr(pr_description, code_diff, pr_title, pr_author, pr_status)\n#     print(json.dumps(result, indent=2))\n\n--- File Index 2: backend/main.py ---\nfrom fastapi import FastAPI, Request, Header\nfrom fastapi.responses import JSONResponse\nimport hmac\nimport hashlib\nimport json\nimport os\n\napp = FastAPI()\n\nGITHUB_SECRET = \"asdfg\" # Set this in your env\n\ndef verify_signature(payload, signature, secret):\n    mac = hmac.new(secret.encode(), msg=payload, digestmod=hashlib.sha256)\n    expected = f\"sha256={mac.hexdigest()}\"\n    return hmac.compare_digest(expected, signature)\n\n@app.post(\"/webhook\")\nasync def github_webhook(\n    request: Request,\n    x_hub_signature_256: str = Header(None),\n    x_github_event: str = Header(None)\n):\n    body = await request.body()\n\n    # Verify webhook signature\n    if not verify_signature(body, x_hub_signature_256, GITHUB_SECRET):\n        return JSONResponse(content={\"error\": \"Invalid signature\"}, status_code=403)\n\n    payload = json.loads(body)\n\n    if x_github_event == \"pull_request\":\n        action = payload[\"action\"]\n        pr = payload[\"pull_request\"]\n\n        if action in [\"opened\", \"synchronize\"]:\n            pr_url = pr[\"url\"]\n            pr_diff_url = pr[\"diff_url\"]\n            repo_full_name = payload[\"repository\"][\"full_name\"]\n            # \ud83d\udc47 Call your internal logic to analyze PR\n            print(f\"Trigger AI review for: {repo_full_name} @ {pr_url}\")\n            print(f\"PR Diff URL: {pr_diff_url}\")\n            print(f\"PR Title: {pr['title']}\")\n            print(f\"PR Body: {pr['body']}\")\n\n    return {\"message\": \"OK\"}\n\n--- File Index 3: backend/prReview.py ---\nimport os, re\nfrom github import Auth\nfrom github import Github, GithubIntegration\nfrom github.GithubException import GithubException\nfrom tree_sitter import Language, Parser\nimport tree_sitter_python as tspython\nimport tree_sitter_javascript as tsjavascript\nimport tree_sitter_cpp as tscpp\nimport requests\nimport json\nimport logging\nimport snowflake.connector\nimport random\nimport datetime\n\n\nGROQ_API_KEY = \"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\"\nGROQ_ENDPOINT = \"https://api.groq.com/openai/v1/chat/completions\"\n\ndef authenticate_github(app_id: int, installation_id, private_key: str):\n    gi = GithubIntegration(integration_id=app_id, private_key=private_key)\n    g = gi.get_github_for_installation(installation_id)\n    return g\n\ndef get_pull_request(g, repo_name: str, pr_number: int):\n    repo = g.get_repo(repo_name)\n    pr = repo.get_pull(pr_number)\n    return pr\n\ndef parse_changed_lines(patch: str):\n    added_or_modified_lines = set()\n    deleted_lines = set()\n    for line in patch.splitlines():\n        if line.startswith(\"@@\"):\n            match = re.findall(r\"@@ -(\\d+)(?:,(\\d+))? \\+(\\d+)(?:,(\\d+))? @@\", line)\n            if match:\n                old_start, old_count, new_start, new_count = match[0]\n\n                old_start = int(old_start)\n                old_count = int(old_count or 1)\n                for l in range(old_start, old_start + old_count):\n                    deleted_lines.add(l)\n\n                new_start = int(new_start)\n                new_count = int(new_count or 1)\n                for l in range(new_start, new_start + new_count):\n                    added_or_modified_lines.add(l)\n\n    return {\n        \"added_or_modified_lines\": added_or_modified_lines,\n        \"deleted_lines\": deleted_lines\n    }\n\ndef get_file_contents(g, repo_name: str, file_path: str, commit_sha: str):\n    repo = g.get_repo(repo_name)\n    try:\n        contents = repo.get_contents(file_path, ref=commit_sha)\n        return contents.decoded_content.decode()\n    except GithubException as e:\n        if e.status == 404:\n            return \"\"\n        else:\n            raise e\n\ndef get_lines_changed(pr):\n    files = pr.get_files()\n    file_changes = {}\n    for file in files:\n        if file.status == \"removed\":\n            continue\n        file_changes[file.filename] = parse_changed_lines(file.patch)\n\n    return file_changes\n\ndef get_paraser(language_name):\n\n    PY_LANGUAGE = Language(tspython.language())\n    JS_LANGUAGE = Language(tsjavascript.language())\n    CPP_LANGUAGE = Language(tscpp.language())\n\n    LANGUAGES = {\n        'python': PY_LANGUAGE,\n        'javascript': JS_LANGUAGE,\n        'cpp': CPP_LANGUAGE,\n    }\n    parser = Parser(LANGUAGES[language_name])\n    return parser\n\ndef extract_functions(code, language_name, changed_lines):\n    parser = get_paraser(language_name)\n    tree = parser.parse(bytes(code, \"utf8\"))\n    root_node = tree.root_node\n\n    functions = []\n\n    def node_within_lines(node):\n        start_line = node.start_point[0] + 1\n        end_line = node.end_point[0] + 1\n        return any([start_line <= line <= end_line for line in changed_lines])\n    \n    def traverse(node):\n        if language_name == \"python\" and node.type == \"function_definition\":\n            if node_within_lines(node):\n                func_name_node = node.child_by_field_name('name')\n                func_name = func_name_node.text.decode()\n                functions.append({'name': func_name, 'code': node.text.decode()})\n        elif language_name == 'javascript' and node.type in ['function_declaration', 'method_definition', 'arrow_function']:\n            if node_within_lines(node):\n                func_name_node = node.child_by_field_name('name')\n                func_name = func_name_node.text.decode()\n                functions.append({'name': func_name, 'code': node.text.decode()})\n        elif language_name == 'cpp' and node.type in ['function_definition']:\n            if node_within_lines(node):\n                func_name_node = node.child_by_field_name('name')\n                func_name = func_name_node.text.decode()\n                functions.append({'name': func_name, 'code': node.text.decode()})\n        for child in node.children:\n            traverse(child)\n\n    traverse(root_node)\n    return functions\n\nheaders = {\n    \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\n# Snowflake setup\nconn = snowflake.connector.connect(\n    user=\"raghavg332\",\n    password=\"Qa29Kh4MptfGHEW\",\n    account=\"KXCIVVH-LL27432\",\n    warehouse=\"COMPUTE_WH\",\n    database=\"PR_DASHBOARD\",\n    schema=\"PUBLIC\"\n)\ncur = conn.cursor()\n\ndef update_documentation_with_rag(pr_description: str, code_diff: str, docs_path=\"backend/project_docs.txt\"):\n    \"\"\"\n    Performs a simple RAG-based call to Groq to update documentation.\n    \"\"\"\n    # Step 1: Load the existing documentation\n    try:\n        with open(docs_path, \"r\") as f:\n            existing_docs = f.read()\n    except FileNotFoundError:\n        logging.error(f\"[\u274c] Documentation file not found at {docs_path}\")\n        return {\"error\": \"Documentation file not found.\"}\n\n    # Step 2: Very basic RAG - extract top paragraphs that contain overlap\n    paragraphs = existing_docs.split(\"\\n\\n\")\n    keywords = pr_description.lower().split()\n    ranked_paragraphs = sorted(paragraphs, key=lambda para: sum(word in para.lower() for word in keywords), reverse=True)\n    context = \"\\n\\n\".join(ranked_paragraphs[:3])  # Top 3 paragraphs as \"retrieved context\"\n\n    # Step 3: Build the prompt\n    doc_prompt = f\"\"\"\nYou are an expert technical writer and code documentation assistant.\n\nA developer has made the following changes to the codebase:\n\n## PR Description:\n{pr_description}\n\n## Code Diff:\n{code_diff}\n\n## Relevant Existing Documentation:\n{context}\n\nPlease provide additions or updates to the documentation based on the above code change. \nFormat your response in **Markdown**, and only include sections that should be added or updated. \nIf the change doesn't require doc updates, say so.\n\"\"\"\n\n    # Step 4: Call Groq API\n    payload = {\n        \"model\": \"llama3-70b-8192\",\n        \"messages\": [{\"role\": \"user\", \"content\": doc_prompt}]\n    }\n\n    try:\n        response = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n        response.raise_for_status()\n        result = response.json()\n        return {\n            \"doc_update\": result[\"choices\"][0][\"message\"][\"content\"].strip()\n        }\n\n    except Exception as e:\n        logging.exception(\"Groq API call for documentation update failed\")\n        return {\"error\": str(e)}\n\n\ndef build_full_prompt(pr_description: str, code_diff_str: str) -> str:\n    return f\"\"\"\nYou are an AI code reviewer and documentation assistant.\n\nA developer submitted the following pull request. Please do the following:\n1. \ud83d\udd0d Summarize the overall purpose of the PR. Be a little detailed\n2. \u2705 Review the code for:\n   - Syntax issues in Python (Make up some Issue)\n   - Style (PEP8)\n   - Functionality correctness (does it match the description?)\n3. Based on the review, provide a merge confidence score from 1 to 10.\n4. \ud83d\udcdd Extract or suggest updated docstrings for any changed or added functions/classes.\n5. The Code Diff is made up of the old and new version of the code, if any of the field is empty it means, the function is either added or deleted respectively.\n\n## PR Description:\n{pr_description}\n\n## Code Diff:\n{code_diff_str}\n\nRespond ONLY in this JSON format without any additional text:\n\n{{\n  \"ai_summary\": \"...\",\n  \"merge_confidence\": \"1 - 10\",\n  \"code_quality\": {{\n    \"syntax_check\": \"...\",\n    \"style_check\": \"...\",\n    \"functionality_check\": \"...\",\n    \"final_rating\": \"Excellent / Good / Needs Work / Critical Issues\"\n  }}\n}}\n\"\"\"\n\ndef review_and_store_pr(pr_description: str, code_diff: dict, pr_title: str, pr_author: str, pr_status: str):\n    code_diff_str = \"\"\n    for file in list(code_diff.keys()):\n        code_diff_str += f\"File: {file}\\n\\n\"\n        diff = code_diff[file]\n\n        for func_name in list(diff.keys()):\n            old_code = diff[func_name]['old_code']\n            new_code = diff[func_name]['new_code']\n            if old_code is None:\n                old_code = \"\"\n            if new_code is None:\n                new_code = \"\"\n            code_diff_str += f\"Function: {func_name}\\n\\nOld Code:\\n{old_code}\\n\\nNew Code:\\n{new_code}\"\n    docstring = update_documentation_with_rag(pr_description, code_diff)\n    pr_id = random.randint(1000, 9999)\n    prompt = build_full_prompt(pr_description, code_diff_str)\n\n    payload = {\n        \"model\": \"llama3-70b-8192\",\n        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n    }\n\n    try:\n        response = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n        response.raise_for_status()\n        result = response.json()\n\n        try:\n            content = result[\"choices\"][0][\"message\"][\"content\"].strip()\n            review_data = json.loads(content)\n\n            # Flatten the code quality object for easier insert\n            code_quality_str = (\n                f\"Syntax: {review_data['code_quality']['syntax_check']}\\n\"\n                f\"Style: {review_data['code_quality']['style_check']}\\n\"\n                f\"Functionality: {review_data['code_quality']['functionality_check']}\\n\"\n                f\"Rating: {review_data['code_quality']['final_rating']}\"\n            )\n            cur.execute(f\"\"\"\n                INSERT INTO PULL_REQUESTS (\n                    ID, TITLE, AUTHOR, STATUS, CREATED_AT, UPDATED_AT\n                )\n                VALUES (%s, %s, %s, %s, %s, %s);\n            \"\"\", (\n                pr_id,\n                pr_title,\n                pr_author,\n                pr_status,\n                datetime.datetime.now(),\n                datetime.datetime.now()\n            ))\n            conn.commit()\n\n            # Insert into Snowflake\n            cur.execute(f\"\"\"\n                INSERT INTO PR_ANALYSIS (\n                    PR_ID, AI_SUMMARY, MERGE_CONFIDENCE, CODE_QUALITY,\n                    PR_DESCRIPTION, DOCSTRINGS, DIFF\n                )\n                VALUES (%s, %s, %s, %s, %s, %s, %s);\n            \"\"\", (\n                pr_id,\n                review_data[\"ai_summary\"],\n                review_data[\"merge_confidence\"],\n                code_quality_str,\n                pr_description,\n                docstring['doc_update'],\n                code_diff_str\n            ))\n\n            conn.commit()\n\n            logging.info(f\"[\u2705] PR #{pr_id} analysis saved to Snowflake.\")\n            print(f\"[\u2705] PR #{pr_id} analysis saved to Snowflake.\")\n            return review_data\n\n        except json.JSONDecodeError:\n            logging.error(\"[\u274c] Groq returned non-JSON output.\")\n            print(\"[\u274c] Groq returned non-JSON output.\")\n            return {\"error\": \"Invalid JSON from Groq\"}\n\n    except Exception as e:\n        logging.exception(\"Groq API or Snowflake insert failed\")\n        return {\"error\": str(e)}\n\ndef process_pull_request(g, repo_name, pr_number, sender=None):\n    repo = g.get_repo(repo_name)\n    pr = repo.get_pull(pr_number)\n    pr_description = pr.title\n    pr_title = pr.title\n    pr_author = pr.user.login\n    pr_status = pr.state\n    code_diff = get_lines_changed(pr)\n    file_diff_func = {}\n    for file in list(code_diff.keys()):\n        diff_func = {}\n        new_code = get_file_contents(g, repo_name, file, pr.head.sha)\n        old_code = get_file_contents(g, repo_name, file, pr.base.sha)\n        added_or_modified_functions_after = extract_functions(new_code, \"python\", code_diff[file]['added_or_modified_lines'])\n        deleted_functions_before = extract_functions(old_code, \"python\", code_diff[file]['deleted_lines'])\n        old_functions_all = extract_functions(old_code, \"python\", set(range(1, len(old_code.split(\"\\n\")))))\n        new_functions_all = extract_functions(new_code, \"python\", set(range(1, len(new_code.split(\"\\n\")))))\n        for func in added_or_modified_functions_after:\n            matched_old_function = next((f for f in old_functions_all if f['name'] ==  func['name']), None)\n            diff_func[func['name']] = {\n                \"new_code\": func['code'],\n                \"old_code\": matched_old_function['code'] if matched_old_function else None\n            }\n        \n        for func in deleted_functions_before:\n            matched_new_function = next((f for f in new_functions_all if f['name'] ==  func['name']), None)\n            diff_func[func['name']] = {\n                \"new_code\": matched_new_function['code'] if matched_new_function else None,\n                \"old_code\": func['code']\n            }\n        \n        file_diff_func[file] = diff_func\n    \n    # Get the review data and post a comment\n    review_data = review_and_store_pr(pr_description, file_diff_func, pr_title, pr_author, pr_status)\n    \n    # Post the review as a comment on the PR\n    post_review_comment(g, repo_name, pr_number, review_data)\n\n\ndef post_review_comment(g, repo_name, pr_number, review_data):\n    \"\"\"\n    Posts the AI review as a comment on the pull request.\n    \n    Args:\n        g: Authenticated GitHub instance\n        repo_name: Repository name (owner/repo)\n        pr_number: Pull request number\n        review_data: The AI review data dictionary\n    \"\"\"\n    try:\n        # Get the repo and PR objects\n        repo = g.get_repo(repo_name)\n        pr = repo.get_pull(pr_number)\n        \n        # Format the comment body\n        comment_body = f\"\"\"\n## \ud83e\udd16 AI Code Review\n\n### Summary\n{review_data.get('ai_summary', 'No summary available')}\n\n### Merge Confidence: **{review_data.get('merge_confidence', 'Unknown')}**\n\n### Code Quality Review\n- **Syntax:** {review_data.get('code_quality', {}).get('syntax_check', 'Not analyzed')}\n- **Style:** {review_data.get('code_quality', {}).get('style_check', 'Not analyzed')}\n- **Functionality:** {review_data.get('code_quality', {}).get('functionality_check', 'Not analyzed')}\n- **Overall Rating:** {review_data.get('code_quality', {}).get('final_rating', 'Not rated')}\n\n---\n*This review was automatically generated by AI.*\n\"\"\"\n        \n        # Create the comment on the PR\n        pr.create_issue_comment(comment_body)\n        logging.info(f\"[\u2705] Posted AI review comment on PR #{pr_number} in {repo_name}\")\n        print(f\"[\u2705] Posted AI review comment on PR #{pr_number} in {repo_name}\")\n        return True\n    except Exception as e:\n        logging.exception(f\"[\u274c] Failed to post comment on PR #{pr_number}: {str(e)}\")\n        print(f\"[\u274c] Failed to post comment on PR #{pr_number}: {str(e)}\")\n        return False\n\n--- File Index 4: frontend/app.js ---\nconst e = React.createElement;\r\n\r\nfunction parseDiff(diff) {\r\n    const lines = diff.split('\\n');\r\n    const parsedLines = [];\r\n\r\n    let section = null;\r\n\r\n    for (let i = 0; i < lines.length; i++) {\r\n        const line = lines[i];\r\n\r\n        if (line.startsWith('File:') || line.startsWith('Function:')) {\r\n            parsedLines.push({ type: 'context', content: line });\r\n            section = null;\r\n        } else if (line.trim() === 'Old Code:') {\r\n            parsedLines.push({ type: 'context', content: 'Old Code:' });\r\n            section = 'old';\r\n\r\n            // Handle empty old code\r\n            if (lines[i + 1]?.trim() === 'New Code:') {\r\n                parsedLines.push({ type: 'old', content: '<NO PREVIOUS CODE>' });\r\n            }\r\n        } else if (line.trim() === 'New Code:') {\r\n            parsedLines.push({ type: 'context', content: 'New Code:' });\r\n            section = 'new';\r\n        } else if (section === 'old' || section === 'new') {\r\n            parsedLines.push({ type: section, content: line });\r\n        } else {\r\n            parsedLines.push({ type: 'context', content: line });\r\n        }\r\n    }\r\n\r\n    return parsedLines;\r\n}\r\n\r\n\r\n// Analytics component to visualize PR data\r\nfunction Analytics({ prs }) {\r\n    // Count PRs by status\r\n    const statusCounts = prs.reduce((acc, pr) => {\r\n        acc[pr.status] = (acc[pr.status] || 0) + 1;\r\n        return acc;\r\n    }, {});\r\n\r\n    // Count PRs by author\r\n    const authorCounts = prs.reduce((acc, pr) => {\r\n        acc[pr.author] = (acc[pr.author] || 0) + 1;\r\n        return acc;\r\n    }, {});\r\n\r\n    // Calculate PRs over time (by month)\r\n    const prsByMonth = prs.reduce((acc, pr) => {\r\n        const date = new Date(pr.created_at);\r\n        const monthYear = date.toLocaleString('default', { month: 'short', year: 'numeric' });\r\n        acc[monthYear] = (acc[monthYear] || 0) + 1;\r\n        return acc;\r\n    }, {});\r\n\r\n    // Create data arrays for charts\r\n    const statusData = Object.entries(statusCounts).map(([status, count]) => ({\r\n        status,\r\n        count,\r\n        color: status === 'Open' ? '#17a2b8' : \r\n               status === 'Merged' ? '#28a745' : \r\n               status === 'Closed' ? '#dc3545' : '#6c757d'\r\n    }));\r\n\r\n    const authorData = Object.entries(authorCounts)\r\n        .sort((a, b) => b[1] - a[1])\r\n        .slice(0, 5)\r\n        .map(([author, count]) => ({ author, count }));\r\n\r\n    const timelineData = Object.entries(prsByMonth)\r\n        .sort((a, b) => new Date(a[0]) - new Date(b[0]))\r\n        .map(([month, count]) => ({ month, count }));\r\n\r\n    return e('div', { className: 'analytics-container' }, [\r\n        e('h2', { key: 'title', className: 'analytics-title' }, 'Pull Request Analytics'),\r\n\r\n        // Status distribution chart\r\n        e('div', { key: 'status-chart', className: 'chart-container' }, [\r\n            e('h3', { key: 'status-title' }, 'PR Status Distribution'),\r\n            e('div', { key: 'status-bars', className: 'status-chart' }, \r\n                statusData.map(item => \r\n                    e('div', { key: item.status, className: 'status-bar-container' }, [\r\n                        e('div', { key: 'label', className: 'status-label' }, `${item.status} (${item.count})`),\r\n                        e('div', { key: 'bar-bg', className: 'status-bar-bg' }, \r\n                            e('div', { \r\n                                key: 'bar', \r\n                                className: 'status-bar', \r\n                                style: { \r\n                                    width: `${(item.count / prs.length) * 100}%`,\r\n                                    backgroundColor: item.color\r\n                                } \r\n                            })\r\n                        )\r\n                    ])\r\n                )\r\n            )\r\n        ]),\r\n\r\n        // Author contribution chart\r\n        e('div', { key: 'author-chart', className: 'chart-container' }, [\r\n            e('h3', { key: 'author-title' }, 'Top Contributors'),\r\n            e('div', { key: 'author-bars', className: 'author-chart' }, \r\n                authorData.map(item => \r\n                    e('div', { key: item.author, className: 'author-bar-container' }, [\r\n                        e('div', { key: 'label', className: 'author-label' }, [\r\n                            e('img', { \r\n                                key: 'avatar', \r\n                                className: 'avatar', \r\n                                src: `https://ui-avatars.com/api/?name=${item.author}&size=24`, \r\n                                alt: item.author\r\n                            }),\r\n                            `${item.author} (${item.count})`\r\n                        ]),\r\n                        e('div', { key: 'bar-bg', className: 'author-bar-bg' }, \r\n                            e('div', { \r\n                                key: 'bar', \r\n                                className: 'author-bar', \r\n                                style: { \r\n                                    width: `${(item.count / Math.max(...Object.values(authorCounts))) * 100}%`\r\n                                } \r\n                            })\r\n                        )\r\n                    ])\r\n                )\r\n            )\r\n        ]),\r\n\r\n        // Timeline chart\r\n        e('div', { key: 'timeline-chart', className: 'chart-container' }, [\r\n            e('h3', { key: 'timeline-title' }, 'PR Activity Over Time'),\r\n            e('div', { key: 'timeline', className: 'timeline-chart' }, \r\n                timelineData.map((item, index) => \r\n                    e('div', { key: item.month, className: 'timeline-bar-container' }, [\r\n                        e('div', { \r\n                            key: 'bar', \r\n                            className: 'timeline-bar', \r\n                            style: { \r\n                                height: `${(item.count / Math.max(...timelineData.map(d => d.count))) * 100}px`\r\n                            } \r\n                        }),\r\n                        e('div', { key: 'label', className: 'timeline-label' }, item.month)\r\n                    ])\r\n                )\r\n            )\r\n        ]),\r\n\r\n        // Summary stats\r\n        e('div', { key: 'summary-stats', className: 'summary-stats' }, [\r\n            e('div', { key: 'total', className: 'stat-card' }, [\r\n                e('h3', { key: 'title' }, 'Total PRs'),\r\n                e('div', { key: 'value', className: 'stat-value' }, prs.length)\r\n            ]),\r\n            e('div', { key: 'open', className: 'stat-card' }, [\r\n                e('h3', { key: 'title' }, 'Open PRs'),\r\n                e('div', { key: 'value', className: 'stat-value' }, statusCounts['Open'] || 0)\r\n            ]),\r\n            e('div', { key: 'merged', className: 'stat-card' }, [\r\n                e('h3', { key: 'title' }, 'Merged PRs'),\r\n                e('div', { key: 'value', className: 'stat-value' }, statusCounts['Merged'] || 0)\r\n            ]),\r\n            e('div', { key: 'contributors', className: 'stat-card' }, [\r\n                e('h3', { key: 'title' }, 'Contributors'),\r\n                e('div', { key: 'value', className: 'stat-value' }, Object.keys(authorCounts).length)\r\n            ])\r\n        ])\r\n    ]);\r\n}\r\n\r\nfunction renderDiffSection(diff) {\r\n    const diffLines = parseDiff(diff);\r\n\r\n    return React.createElement(\r\n        'pre',\r\n        { className: 'diff-content' },\r\n        diffLines.map((line, index) => {\r\n            const className =\r\n                line.type === 'old' ? 'diff-old-code diff-code-line' :\r\n                line.type === 'new' ? 'diff-new-code diff-code-line' :\r\n                'diff-context-line';\r\n\r\n            return React.createElement('div', { key: index, className }, line.content);\r\n        })\r\n    );\r\n}\r\n\r\nconst ReactMarkdown = window.ReactMarkdown || (() => null) // fallback if not loaded\r\n\r\n\r\n// Main App component\r\nfunction App() {\r\n    const [prs, setPrs] = React.useState([]);\r\n    const [selectedPR, setSelectedPR] = React.useState(null);\r\n    const [prDetails, setPRDetails] = React.useState(null);\r\n    const [filter, setFilter] = React.useState(\"all\");\r\n    const [searchQuery, setSearchQuery] = React.useState(\"\");\r\n    const [isLoading, setIsLoading] = React.useState(true);\r\n    const [showCommentForm, setShowCommentForm] = React.useState(false);\r\n    const [comment, setComment] = React.useState(\"\");\r\n    const [comments, setComments] = React.useState([]);\r\n    const [darkMode, setDarkMode] = React.useState(false);\r\n    const [activeTab, setActiveTab] = React.useState(\"details\"); // \"details\" or \"analytics\"\r\n\r\n    React.useEffect(() => {\r\n        setIsLoading(true);\r\n        fetch('http://localhost:8000/api/prs')\r\n            .then(res => res.json())\r\n            .then(data => {\r\n                setPrs(data);\r\n                setIsLoading(false);\r\n            })\r\n            .catch(err => {\r\n                console.error('Error fetching PRs:', err);\r\n                setIsLoading(false);\r\n            });\r\n    }, []);\r\n\r\n    React.useEffect(() => {\r\n        // Apply dark mode class to body\r\n        if (darkMode) {\r\n            document.body.classList.add('dark-mode');\r\n        } else {\r\n            document.body.classList.remove('dark-mode');\r\n        }\r\n    }, [darkMode]);\r\n\r\n    const fetchDetails = (id) => {\r\n        setIsLoading(true);\r\n        fetch(`http://localhost:8000/api/prs/${id}`)\r\n            .then(res => res.json())\r\n            .then(data => {\r\n                setPRDetails(data);\r\n                setIsLoading(false);\r\n                // Simulate fetching comments\r\n                setComments([\r\n                    { id: 1, author: \"codereviewer\", text: \"Looks good to me!\", timestamp: \"2 hours ago\" },\r\n                    { id: 2, author: \"securityexpert\", text: \"We should add input validation here.\", timestamp: \"1 hour ago\" }\r\n                ]);\r\n            })\r\n            .catch(err => {\r\n                console.error('Error fetching PR details:', err);\r\n                setIsLoading(false);\r\n            });\r\n    };\r\n\r\n    const selectPR = (pr) => {\r\n        setSelectedPR(pr);\r\n        setActiveTab(\"details\"); // Switch to details tab when selecting a PR\r\n        fetchDetails(pr.id);\r\n    };\r\n\r\n    const handleAddComment = () => {\r\n        if (comment.trim() === \"\") return;\r\n        \r\n        const newComment = {\r\n            id: comments.length + 1,\r\n            author: \"you\",\r\n            text: comment,\r\n            timestamp: \"Just now\"\r\n        };\r\n        \r\n        setComments([...comments, newComment]);\r\n        setComment(\"\");\r\n        setShowCommentForm(false);\r\n    };\r\n\r\n    const handleApprove = () => {\r\n        alert(\"PR approved! (This would trigger an API call in production)\");\r\n    };\r\n\r\n    const handleReject = () => {\r\n        alert(\"PR rejected! (This would trigger an API call in production)\");\r\n    };\r\n\r\n    // Function to switch to analytics tab\r\n    const viewAnalytics = () => {\r\n        setActiveTab(\"analytics\");\r\n    };\r\n\r\n    // Filter PRs based on status and search query\r\n    const filteredPRs = prs.filter(pr => {\r\n        const matchesFilter = filter === \"all\" || pr.status === filter;\r\n        const matchesSearch = pr.title.toLowerCase().includes(searchQuery.toLowerCase()) || \r\n                             pr.author.toLowerCase().includes(searchQuery.toLowerCase());\r\n        return matchesFilter && matchesSearch;\r\n    });\r\n\r\n    // Create PR list items\r\n    const prItems = filteredPRs.map(pr => \r\n        e('div', { \r\n            key: pr.id, \r\n            className: selectedPR && selectedPR.id === pr.id ? 'pr-item selected' : 'pr-item', \r\n            onClick: () => selectPR(pr) \r\n        }, [\r\n            e('div', {key: 'pr-header', className: 'pr-header'}, [\r\n                e('strong', {key: 'id'}, `#${pr.id}`),\r\n                e('span', {\r\n                    key: 'status',\r\n                    className: `pr-status status-${pr.status}`\r\n                }, pr.status)\r\n            ]),\r\n            e('div', {key: 'pr-title', className: 'pr-title'}, pr.title),\r\n            e('div', {key: 'pr-meta', className: 'pr-meta'}, [\r\n                e('span', {key: 'author'}, [\r\n                    e('img', {key: 'avatar', className: 'avatar', src: `https://ui-avatars.com/api/?name=${pr.author}&size=24`, alt: pr.author}),\r\n                    pr.author\r\n                ]),\r\n                e('span', {key: 'date', className: 'date'}, 'Updated: ' + (pr.updated_at || 'N/A'))\r\n            ])\r\n        ])\r\n    );\r\n\r\n    // Create filter controls\r\n    const filterControls = e('div', {className: 'filter-controls'}, [\r\n        e('div', {key: 'search', className: 'search-box'}, [\r\n            e('input', {\r\n                type: 'text',\r\n                placeholder: 'Search PRs...',\r\n                value: searchQuery,\r\n                onChange: (e) => setSearchQuery(e.target.value),\r\n                className: 'search-input'\r\n            })\r\n        ]),\r\n        e('div', {key: 'filters', className: 'status-filters'}, [\r\n            e('button', {\r\n                className: filter === 'all' ? 'filter-btn active' : 'filter-btn',\r\n                onClick: () => setFilter('all')\r\n            }, 'All'),\r\n            e('button', {\r\n                className: filter === 'Open' ? 'filter-btn active' : 'filter-btn',\r\n                onClick: () => setFilter('Open')\r\n            }, 'Open'),\r\n            e('button', {\r\n                className: filter === 'Merged' ? 'filter-btn active' : 'filter-btn',\r\n                onClick: () => setFilter('Merged')\r\n            }, 'Merged'),\r\n            e('button', {\r\n                className: filter === 'Closed' ? 'filter-btn active' : 'filter-btn',\r\n                onClick: () => setFilter('Closed')\r\n            }, 'Closed')\r\n        ])\r\n    ]);\r\n\r\n    // Create sidebar\r\n    const sidebar = e('div', { className: 'sidebar' }, [\r\n        e('div', {key: 'header', className: 'sidebar-header'}, [\r\n            e('h2', {key: 'title'}, '\ud83d\udccb Pull Requests'),\r\n            e('button', {\r\n                key: 'theme-toggle',\r\n                className: 'theme-toggle',\r\n                onClick: () => setDarkMode(!darkMode)\r\n            }, darkMode ? '\u2600\ufe0f' : '\ud83c\udf19')\r\n        ]),\r\n        filterControls,\r\n        e('div', {key: 'pr-list', className: 'pr-list'}, isLoading ? \r\n            e('div', {className: 'loading'}, 'Loading PRs...') : \r\n            prItems.length > 0 ? prItems : e('div', {className: 'no-results'}, 'No PRs match your filters'))\r\n    ]);\r\n\r\n    // Create tab navigation - show it regardless of whether a PR is selected\r\n    const tabNav = e('div', {className: 'tab-navigation'}, [\r\n        e('button', {\r\n            key: 'details-tab',\r\n            className: activeTab === 'details' ? 'tab-btn active' : 'tab-btn',\r\n            onClick: () => setActiveTab('details'),\r\n            disabled: !selectedPR\r\n        }, 'PR Details'),\r\n        e('button', {\r\n            key: 'analytics-tab',\r\n            className: activeTab === 'analytics' ? 'tab-btn active' : 'tab-btn',\r\n            onClick: () => setActiveTab('analytics')\r\n        }, 'Analytics')\r\n    ]);\r\n\r\n    // Create PR details content\r\n    let prDetailsContent;\r\n    if (isLoading && selectedPR) {\r\n        prDetailsContent = e('div', {className: 'loading-container'}, [\r\n            e('div', {key: 'spinner', className: 'loading-spinner'}),\r\n            e('p', {key: 'text'}, 'Loading PR details...')\r\n        ]);\r\n    } else if (!prDetails && selectedPR) {\r\n        prDetailsContent = e('p', null, 'No details available for this PR');\r\n    } else if (selectedPR) {\r\n        // PR details content\r\n        const score = Number(prDetails.merge_confidence_score || 0); // 0-10\r\n        const maxScore = 10;\r\n        const percent = Math.min((score / maxScore) * 100, 100);\r\n        const confidenceIndicator = e('div', { key: 'confidence-indicator', className: 'confidence-gradient-wrapper' }, [\r\n            e('div', { className: 'confidence-gradient-bar' }, [\r\n                e('div', {\r\n                    className: 'confidence-score-indicator',\r\n                    style: { left: `${percent}%` }\r\n                })\r\n            ]),\r\n            e('div', { style: { marginTop: '8px', fontWeight: 600 } }, `Score: ${score}/10`)\r\n        ]);\r\n        \r\n        // Comment section\r\n        const commentList = comments.map(comment => \r\n            e('div', {key: `comment-${comment.id}`, className: 'comment'}, [\r\n                e('div', {key: 'comment-header', className: 'comment-header'}, [\r\n                    e('img', {key: 'avatar', className: 'avatar', src: `https://ui-avatars.com/api/?name=${comment.author}&size=32`, alt: comment.author}),\r\n                    e('span', {key: 'author', className: 'comment-author'}, comment.author),\r\n                    e('span', {key: 'time', className: 'comment-time'}, comment.timestamp)\r\n                ]),\r\n                e('div', {key: 'comment-body', className: 'comment-body'}, comment.text)\r\n            ])\r\n        );\r\n        \r\n        const commentSection = e('div', {key: 'comments', className: 'comments-section'}, [\r\n            e('h3', {key: 'title'}, `Comments (${comments.length})`),\r\n            ...commentList,\r\n            showCommentForm ? \r\n                e('div', {key: 'comment-form', className: 'comment-form'}, [\r\n                    e('textarea', {\r\n                        key: 'textarea',\r\n                        value: comment,\r\n                        onChange: (e) => setComment(e.target.value),\r\n                        placeholder: 'Add your comment...',\r\n                        rows: 3\r\n                    }),\r\n                    e('div', {key: 'buttons', className: 'form-buttons'}, [\r\n                        e('button', {\r\n                            key: 'cancel',\r\n                            className: 'cancel-btn',\r\n                            onClick: () => {\r\n                                setShowCommentForm(false);\r\n                                setComment(\"\");\r\n                            }\r\n                        }, 'Cancel'),\r\n                        e('button', {\r\n                            key: 'submit',\r\n                            className: 'submit-btn',\r\n                            onClick: handleAddComment\r\n                        }, 'Submit')\r\n                    ])\r\n                ]) :\r\n                e('button', {\r\n                    key: 'add-comment',\r\n                    className: 'add-comment-btn',\r\n                    onClick: () => setShowCommentForm(true)\r\n                }, '+ Add Comment')\r\n        ]);\r\n        \r\n        // Action buttons with added View Analytics button\r\n        const actionButtons = e('div', {key: 'actions', className: 'action-buttons'}, [\r\n            e('button', {\r\n                key: 'approve',\r\n                className: 'approve-btn',\r\n                onClick: handleApprove\r\n            }, 'Approve PR'),\r\n            e('button', {\r\n                key: 'reject',\r\n                className: 'reject-btn',\r\n                onClick: handleReject\r\n            }, 'Request Changes'),\r\n            e('button', {\r\n                key: 'view-analytics',\r\n                className: 'view-analytics-btn',\r\n                onClick: viewAnalytics\r\n            }, 'View Analytics')\r\n        ]);\r\n        \r\n        // PR Details Tab Content\r\n        prDetailsContent = e(React.Fragment, null, [\r\n            e('div', {key: 'pr-header', className: 'pr-detail-header'}, [\r\n                e('h2', {key: 'title'}, `#${selectedPR.id} - ${selectedPR.title}`),\r\n                e('div', {key: 'meta', className: 'pr-meta-details'}, [\r\n                    e('span', {key: 'status', className: `status-badge status-${selectedPR.status}`}, selectedPR.status),\r\n                    e('span', {key: 'author'}, [\r\n                        e('img', {key: 'avatar', className: 'avatar', src: `https://ui-avatars.com/api/?name=${selectedPR.author}&size=24`, alt: selectedPR.author}),\r\n                        `Author: ${selectedPR.author}`\r\n                    ]),\r\n                    // Add a \"View Analytics\" button in the header too\r\n                    e('button', {\r\n                        key: 'header-view-analytics',\r\n                        className: 'header-view-analytics-btn',\r\n                        onClick: viewAnalytics\r\n                    }, 'View Analytics')\r\n                ])\r\n            ]),\r\n            \r\n            e('div', {key: 'pr-body', className: 'pr-detail-body'}, [\r\n                e('div', {key: 'summary', className: 'ai-summary'}, [\r\n                    e('h3', {key: 'title'}, 'AI Summary'),\r\n                    e('p', {key: 'text'}, prDetails.ai_summary)\r\n                ]),\r\n                \r\n                e('div', {key: 'confidence', className: 'confidence-section'}, [\r\n                    e('h3', {key: 'title'}, 'Merge Confidence'),\r\n                    confidenceIndicator\r\n                ]),\r\n                \r\n                e('div', {key: 'quality', className: 'code-quality'}, [\r\n                    e('h3', {key: 'title'}, 'Code Quality Assessment'),\r\n                    e('pre', {key: 'text'}, prDetails.code_quality)\r\n                ]),\r\n                \r\n                \r\n                ('div', {key: 'doc-string', className: 'doc-string-section'}, [\r\n                    e('h3', {key: 'title'}, 'Documentation'),\r\n                    e('div', {key: 'content', className: 'doc-string-content'},\r\n                        e(ReactMarkdown, null, prDetails.doc_string || 'No documentation available for this PR.')\r\n                    )\r\n                ]),\r\n\r\n                e('div', {key: 'diff', className: 'diff-section'}, [\r\n                    e('h3', {key: 'title'}, 'Diff'),\r\n                    renderDiffSection(prDetails.diff)\r\n                ]),\r\n                \r\n                commentSection,\r\n                \r\n                actionButtons\r\n            ])\r\n        ]);\r\n    }\r\n\r\n    // Create main content based on active tab\r\n    let mainContent;\r\n    if (activeTab === 'details' && !selectedPR) {\r\n        mainContent = e('div', {className: 'empty-state'}, [\r\n            e('h2', {key: 'title'}, 'Pull Request Dashboard'),\r\n            e('p', {key: 'subtitle'}, 'Select a PR from the sidebar or view PR Analytics'),\r\n            e('button', {\r\n                key: 'view-analytics',\r\n                className: 'view-analytics-btn',\r\n                onClick: () => setActiveTab('analytics')\r\n            }, 'View Analytics')\r\n        ]);\r\n    } else if (activeTab === 'details' && selectedPR) {\r\n        // Show PR details tab\r\n        mainContent = prDetailsContent;\r\n    } else if (activeTab === 'analytics') {\r\n        // Show Analytics tab\r\n        mainContent = e(Analytics, { prs: prs });\r\n    }\r\n\r\n    const main = e('div', { className: 'main' }, [\r\n        tabNav,\r\n        mainContent\r\n    ]);\r\n\r\n    // Render the full container\r\n    return e('div', { className: 'container' }, [sidebar, main]);\r\n}\r\n\r\n// Render the App\r\nconst rootElement = document.getElementById('root');\r\nconst root = ReactDOM.createRoot(rootElement);\r\nroot.render(e(App));\n\n--- File Index 5: main.py ---\nfrom fastapi import FastAPI, HTTPException\r\nfrom fastapi.middleware.cors import CORSMiddleware\r\nimport snowflake.connector\r\nimport os\r\nfrom dotenv import load_dotenv\r\nimport pandas as pd\r\n\r\nload_dotenv()\r\n\r\napp = FastAPI()\r\n\r\n# Snowflake credentials\r\nSNOWFLAKE_USER = os.getenv(\"SNOWFLAKE_USER\", \"raghavg332\")\r\nSNOWFLAKE_PASSWORD = os.getenv(\"SNOWFLAKE_PASSWORD\", \"Qa29Kh4MptfGHEW\")\r\nSNOWFLAKE_ACCOUNT = os.getenv(\"SNOWFLAKE_ACCOUNT\", \"KXCIVVH-LL27432\")\r\nSNOWFLAKE_WAREHOUSE = os.getenv(\"SNOWFLAKE_WAREHOUSE\", \"COMPUTE_WH\")\r\nSNOWFLAKE_DATABASE = os.getenv(\"SNOWFLAKE_DATABASE\", \"PR_DASHBOARD\")\r\nSNOWFLAKE_SCHEMA = os.getenv(\"SNOWFLAKE_SCHEMA\", \"PUBLIC\")\r\n\r\napp.add_middleware(\r\n    CORSMiddleware,\r\n    allow_origins=[\"*\"],\r\n    allow_methods=[\"*\"],\r\n    allow_headers=[\"*\"],\r\n)\r\n\r\ndef get_conn():\r\n    \"\"\"Establish connection to Snowflake database\"\"\"\r\n    try:\r\n        conn = snowflake.connector.connect(\r\n            user=SNOWFLAKE_USER,\r\n            password=SNOWFLAKE_PASSWORD,\r\n            account=SNOWFLAKE_ACCOUNT,\r\n            warehouse=SNOWFLAKE_WAREHOUSE,\r\n            database=SNOWFLAKE_DATABASE,\r\n            schema=SNOWFLAKE_SCHEMA\r\n        )\r\n        return conn\r\n    except Exception as e:\r\n        print(f\"Error connecting to Snowflake: {e}\")\r\n        raise e\r\n\r\n@app.get(\"/api/prs\")\r\ndef fetch_all_prs():\r\n    try:\r\n        conn = get_conn()\r\n        cursor = conn.cursor(snowflake.connector.DictCursor)\r\n        \r\n        # Using cursor to get results as dictionaries\r\n        cursor.execute(\"SELECT id, title, author, status, created_at, updated_at FROM pull_requests ORDER BY updated_at DESC\")\r\n        results = cursor.fetchall()\r\n        \r\n        # Format the data to match frontend expectations\r\n        formatted_results = []\r\n        for row in results:\r\n            # Capitalize the status to match frontend expectations\r\n            status = row['STATUS'].capitalize() if row['STATUS'] else 'Unknown'\r\n            \r\n            formatted_results.append({\r\n                \"id\": row['ID'],\r\n                \"title\": row['TITLE'],\r\n                \"author\": row['AUTHOR'],\r\n                \"status\": status,\r\n                \"created_at\": row['CREATED_AT'].strftime(\"%Y-%m-%d\") if row['CREATED_AT'] else None,\r\n                \"updated_at\": row['UPDATED_AT'].strftime(\"%Y-%m-%d\") if row['UPDATED_AT'] else None\r\n            })\r\n        \r\n        cursor.close()\r\n        conn.close()\r\n        return formatted_results\r\n    except Exception as e:\r\n        print(f\"Error fetching PRs: {e}\")\r\n        raise HTTPException(status_code=500, detail=f\"Database error: {str(e)}\")\r\n\r\n@app.get(\"/api/prs/{pr_id}\")\r\ndef fetch_pr_analysis(pr_id: int):\r\n    try:\r\n        conn = get_conn()\r\n        cursor = conn.cursor(snowflake.connector.DictCursor)\r\n        \r\n        # Fetch the analysis data\r\n        cursor.execute(f\"SELECT * FROM pr_analysis WHERE pr_id = {pr_id}\")\r\n        result = cursor.fetchone()\r\n        \r\n        if not result:\r\n            cursor.close()\r\n            conn.close()\r\n            raise HTTPException(status_code=404, detail=\"PR analysis not found\")\r\n        \r\n        # Format the data to match frontend expectations\r\n        formatted_result = {\r\n            \"pr_id\": result['PR_ID'],\r\n            \"ai_summary\": result['AI_SUMMARY'],\r\n            \"merge_confidence\": result['MERGE_CONFIDENCE'].capitalize(),  # Capitalize for frontend\r\n            \"merge_confidence_score\": float(result['MERGE_CONFIDENCE']) if result.get('MERGE_CONFIDENCE') else 0,\r\n            \"code_quality\": result['CODE_QUALITY'],\r\n            \"diff\": result['DIFF'],\r\n            \"doc_string\": result['DOCSTRINGS'] if 'DOCSTRINGS' in result else \"No documentation available for this PR.\"\r\n        }\r\n        \r\n        cursor.close()\r\n        conn.close()\r\n        return formatted_result\r\n    except Exception as e:\r\n        print(f\"Error fetching PR analysis: {e}\")\r\n        raise HTTPException(status_code=500, detail=f\"Error: {str(e)}\")\r\n\r\n'''\r\n@app.get(\"/api/prs\")\r\ndef fetch_all_prs():\r\n    # Test data\r\n    test_data = [\r\n        {\r\n            \"id\": 1,\r\n            \"title\": \"Add new feature\",\r\n            \"author\": \"johndoe\",\r\n            \"status\": \"Open\",\r\n            \"created_at\": \"2023-01-01\",\r\n            \"updated_at\": \"2023-01-02\"\r\n        },\r\n        {\r\n            \"id\": 2,\r\n            \"title\": \"Fix bug in login\",\r\n            \"author\": \"janedoe\",\r\n            \"status\": \"Merged\",\r\n            \"created_at\": \"2023-01-03\",\r\n            \"updated_at\": \"2023-01-04\"\r\n        }\r\n    ]\r\n    return test_data\r\n\r\n@app.get(\"/api/prs/{pr_id}\")\r\ndef fetch_pr_analysis(pr_id: int):\r\n    # Test data\r\n    test_data = {\r\n        \"pr_id\": pr_id,\r\n        \"ai_summary\": \"This PR adds a new feature that improves user experience\",\r\n        \"merge_confidence\": \"High\",\r\n        \"code_quality\": \"Good code quality. No major issues found.\",\r\n        \"diff\": \"```\\n+ def new_function():\\n+     return 'new feature'\\n```\",\r\n        \"doc_string\": \"This PR introduces a new function that returns 'new feature'.\"\r\n    }\r\n    return test_data\r\n'''\n\n\n\nAnalyze the codebase context.\nIdentify the top 5-10 core most important abstractions to help those new to the codebase.\n\nFor each abstraction, provide:\n1. A concise `name`.\n2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\nList of file indices and paths present in the context:\n- 0 # README.md\n- 1 # backend/ai_analyse.py\n- 2 # backend/main.py\n- 3 # backend/prReview.py\n- 4 # frontend/app.js\n- 5 # main.py\n\nFormat the output as a YAML list of dictionaries:\n\n```yaml\n- name: Query Processing\n  description: | \n    Explains what the abstraction does.\n    It's like a central dispatcher routing requests.\n  file_indices:\n    - 0 # path/to/file1.py\n    - 3 # path/to/related.py\n- name: Query Optimization\n  description: |\n    Another core concept, similar to a blueprint for objects.\n  file_indices:\n    - 5 # path/to/another.js\n# ... up to 10 abstractions\n```": "```yaml\n- name: Pull Request Analysis Pipeline\n  description: |\n    This is the core process that takes a pull request, analyzes the code changes, generates AI insights, and stores the results.  Imagine a factory assembly line: code changes enter, AI reviews and documentation suggestions are added, and then the final product (insights) is stored in a database. This pipeline helps automate code review and documentation.\n  file_indices:\n    - 0 # README.md\n    - 1 # backend/ai_analyse.py\n    - 3 # backend/prReview.py\n\n- name: Groq API Integration\n  description: |\n    This abstraction handles communication with the Groq API, an AI model provider, to generate summaries, code quality assessments, and documentation suggestions.  Think of it as a translator that sends your code and requests to a smart AI brain (Groq) and then translates the AI's response back into something the system can understand.\n  file_indices:\n    - 1 # backend/ai_analyse.py\n    - 3 # backend/prReview.py\n\n- name: Tree-sitter Based Code Parsing\n  description: |\n    Tree-sitter is used to understand code structure so that modified blocks, functions, and classes can be detected. It is like a powerful code scanner that identifies the key elements of the code and their relationships. This parsing is crucial for focused analysis of code changes.\n  file_indices:\n    - 0 # README.md\n    - 3 # backend/prReview.py\n\n- name: Snowflake Data Storage\n  description: |\n    This abstraction manages the storage of all AI-generated insights, PR metadata, and documentation updates in a Snowflake database. It's the project's memory, storing all the analysis results. Think of Snowflake as a giant filing cabinet that efficiently organizes and stores code review data for reporting, auditing, and search.\n  file_indices:\n    - 0 # README.md\n    - 1 # backend/ai_analyse.py\n    - 3 # backend/prReview.py\n    - 5 # main.py\n\n- name: RAG (Retrieval-Augmented Generation) Documentation Updates\n  description: |\n    This component automatically updates project documentation by using RAG. It finds relevant sections in existing documentation, and uses an AI model to generate suggested updates. It's like having a helpful assistant who automatically keeps your documentation up-to-date based on new code changes.\n  file_indices:\n    - 0 # README.md\n    - 1 # backend/ai_analyse.py\n    - 3 # backend/prReview.py\n\n- name: GitHub Webhook Integration\n  description: |\n    The application integrates with GitHub via webhooks, which are triggered by specific events (like opening a pull request). The github webhook is a messenger that communicates changes made in GitHub to the backend.\n  file_indices:\n    - 0 # README.md\n    - 2 # backend/main.py\n\n- name: FastAPI Backend API\n  description: |\n    The FastAPI backend serves as the central processing unit of the application. It receives requests (like a new PR), orchestrates the AI analysis and documentation updates, and serves data to the frontend. Consider FastAPI as the project's control panel, handling all the server-side logic and interactions.\n  file_indices:\n    - 0 # README.md\n    - 2 # backend/main.py\n    - 5 # main.py\n\n- name: Frontend Web Application (React)\n  description: |\n    The React-based frontend provides a user interface to view PRs, AI reviews, documentation updates, and analytics. It's the window into the system, allowing users to explore the insights generated by the analysis pipeline. The frontend is also responsible for displaying the diff and other elements.\n  file_indices:\n    - 0 # README.md\n    - 4 # frontend/app.js\n\n- name: PR Review Comment Posting (GitHub)\n  description: |\n    Once the AI analysis is complete, this component posts the generated summary as a comment directly on the pull request in GitHub. It's like an automated messenger that delivers the AI's review directly to the developer where they are already working.\n  file_indices:\n    - 0 # README.md\n    - 3 # backend/prReview.py\n```", "\nBased on the following abstractions and relevant code snippets from the project `CodeRoast`:\n\nList of Abstraction Indices and Names:\n0 # Pull Request Analysis Pipeline\n1 # Groq API Integration\n2 # Tree-sitter Based Code Parsing\n3 # Snowflake Data Storage\n4 # RAG (Retrieval-Augmented Generation) Documentation Updates\n5 # GitHub Webhook Integration\n6 # FastAPI Backend API\n7 # Frontend Web Application (React)\n8 # PR Review Comment Posting (GitHub)\n\nContext (Abstractions, Descriptions, Code):\nIdentified Abstractions:\n- Index 0: Pull Request Analysis Pipeline (Relevant file indices: [0, 1, 3])\n  Description: This is the core process that takes a pull request, analyzes the code changes, generates AI insights, and stores the results.  Imagine a factory assembly line: code changes enter, AI reviews and documentation suggestions are added, and then the final product (insights) is stored in a database. This pipeline helps automate code review and documentation.\n\n- Index 1: Groq API Integration (Relevant file indices: [1, 3])\n  Description: This abstraction handles communication with the Groq API, an AI model provider, to generate summaries, code quality assessments, and documentation suggestions.  Think of it as a translator that sends your code and requests to a smart AI brain (Groq) and then translates the AI's response back into something the system can understand.\n\n- Index 2: Tree-sitter Based Code Parsing (Relevant file indices: [0, 3])\n  Description: Tree-sitter is used to understand code structure so that modified blocks, functions, and classes can be detected. It is like a powerful code scanner that identifies the key elements of the code and their relationships. This parsing is crucial for focused analysis of code changes.\n\n- Index 3: Snowflake Data Storage (Relevant file indices: [0, 1, 3, 5])\n  Description: This abstraction manages the storage of all AI-generated insights, PR metadata, and documentation updates in a Snowflake database. It's the project's memory, storing all the analysis results. Think of Snowflake as a giant filing cabinet that efficiently organizes and stores code review data for reporting, auditing, and search.\n\n- Index 4: RAG (Retrieval-Augmented Generation) Documentation Updates (Relevant file indices: [0, 1, 3])\n  Description: This component automatically updates project documentation by using RAG. It finds relevant sections in existing documentation, and uses an AI model to generate suggested updates. It's like having a helpful assistant who automatically keeps your documentation up-to-date based on new code changes.\n\n- Index 5: GitHub Webhook Integration (Relevant file indices: [0, 2])\n  Description: The application integrates with GitHub via webhooks, which are triggered by specific events (like opening a pull request). The github webhook is a messenger that communicates changes made in GitHub to the backend.\n\n- Index 6: FastAPI Backend API (Relevant file indices: [0, 2, 5])\n  Description: The FastAPI backend serves as the central processing unit of the application. It receives requests (like a new PR), orchestrates the AI analysis and documentation updates, and serves data to the frontend. Consider FastAPI as the project's control panel, handling all the server-side logic and interactions.\n\n- Index 7: Frontend Web Application (React) (Relevant file indices: [0, 4])\n  Description: The React-based frontend provides a user interface to view PRs, AI reviews, documentation updates, and analytics. It's the window into the system, allowing users to explore the insights generated by the analysis pipeline. The frontend is also responsible for displaying the diff and other elements.\n\n- Index 8: PR Review Comment Posting (GitHub) (Relevant file indices: [0, 3])\n  Description: Once the AI analysis is complete, this component posts the generated summary as a comment directly on the pull request in GitHub. It's like an automated messenger that delivers the AI's review directly to the developer where they are already working.\n\n\nRelevant File Snippets (Referenced by Index and Path):\n--- File: 0 # README.md ---\n# \ud83e\udd16 Code Roast : An AI-Powered Pull Request Review System\n\n> An end-to-end intelligent platform for automated code review, documentation updates, and pull request analysis \u2014 powered by LLMs, Tree-sitter, and Snowflake.\n\n---\n\n## \ud83d\ude80 Overview\n\nThis system integrates with GitHub to automatically:\n- Analyze pull requests using AI (via [Groq](https://groq.com))\n- Parse and understand diffs using [Tree-sitter](https://tree-sitter.github.io/)\n- Perform RAG (Retrieval-Augmented Generation) to update project documentation\n- Store all insights in **Snowflake** for analytics\n- Display all PRs, diffs, AI feedback, and doc suggestions via a beautiful **Web App**\n\nWhether you're an engineering team lead, reviewer, or contributor \u2014 you get fast, reliable, and insightful AI-powered review feedback at every step.\n\n---\n\n## \ud83e\udde0 Features\n\n- \u2705 **AI PR Summary**  \n  Instant analysis of pull request purpose, syntax/style/functionality, and merge confidence.\n\n- \ud83d\udcdd **Automatic Docstring Suggestions**  \n  Extract or generate missing/updated docstrings for new or changed functions.\n\n- \ud83d\udcd8 **RAG-based Documentation Updates**  \n  Contextual retrieval from `project_docs.txt` with AI-generated Markdown additions.\n\n- \ud83d\udcc4 **Code Diff Parsing**  \n  Tree-sitter-based code structure diffing to isolate changed functions only.\n\n- \ud83d\udcac **GitHub Comment Posting**  \n  Summaries are posted as comments directly on PRs, no need to open another tool.\n\n- \ud83d\udcca **Snowflake Integration**  \n  All AI-generated insights and PR metadata stored for reporting, audit, and search.\n\n- \ud83c\udf10 **Web App Dashboard**  \n  Browse all PRs, view diffs, AI reviews, and doc updates in a single UI.\n\n---\n\n## \ud83e\uddf1 Tech Stack\n\n| Layer              | Tech Used                                      |\n|--------------------|-----------------------------------------------|\n| **AI Models**       | [Groq API (LLaMA 3)](https://groq.com)        |\n| **Diff Parsing**    | [Tree-sitter](https://tree-sitter.github.io/) |\n| **Backend API**     | [FastAPI](https://fastapi.tiangolo.com)       |\n| **Git Integration** | GitHub App + `PyGithub`                       |\n| **RAG Engine**      | Basic keyword-matching from local docs        |\n| **Database**        | [Snowflake](https://www.snowflake.com)        |\n| **Frontend**        | HTML, JS, CSS (Markdown + Diff Viewers)       |\n\n---\n\n## \u2699\ufe0f How It Works\n\n1. \ud83d\udd14 **GitHub Webhook Trigger**\n   - On PR open/update \u2192 sends payload to FastAPI backend\n\n2. \ud83e\udde0 **AI Review Processing**\n   - Extracts code diffs, changed functions\n   - Groq generates AI summary, confidence rating, and docstrings\n\n3. \ud83d\udcd8 **Documentation Updates**\n   - Local project docs indexed via simple RAG\n   - Groq suggests Markdown additions\n\n4. \ud83d\udcbe **Data Storage**\n   - All results are saved in Snowflake for dashboard & analytics\n\n5. \ud83c\udf10 **Web App UI**\n   - Explore PRs, view diffs, AI reviews, and doc updates\n\n\n--- File: 1 # backend/ai_analyse.py ---\nimport requests\nimport json\nimport logging\nimport snowflake.connector\nimport random\nimport datetime\n\nGROQ_API_KEY = \"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\"\nGROQ_ENDPOINT = \"https://api.groq.com/openai/v1/chat/completions\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\n# Snowflake setup\nconn = snowflake.connector.connect(\n    user=\"raghavg332\",\n    password=\"Qa29Kh4MptfGHEW\",\n    account=\"KXCIVVH-LL27432\",\n    warehouse=\"COMPUTE_WH\",\n    database=\"PR_DASHBOARD\",\n    schema=\"PUBLIC\"\n)\ncur = conn.cursor()\n\ndef update_documentation_with_rag(pr_description: str, code_diff: str, docs_path=\"backend/project_docs.txt\"):\n    \"\"\"\n    Performs a simple RAG-based call to Groq to update documentation.\n    \"\"\"\n    # Step 1: Load the existing documentation\n    try:\n        with open(docs_path, \"r\") as f:\n            existing_docs = f.read()\n    except FileNotFoundError:\n        logging.error(f\"[\u274c] Documentation file not found at {docs_path}\")\n        return {\"error\": \"Documentation file not found.\"}\n\n    # Step 2: Very basic RAG - extract top paragraphs that contain overlap\n    paragraphs = existing_docs.split(\"\\n\\n\")\n    keywords = pr_description.lower().split()\n    ranked_paragraphs = sorted(paragraphs, key=lambda para: sum(word in para.lower() for word in keywords), reverse=True)\n    context = \"\\n\\n\".join(ranked_paragraphs[:3])  # Top 3 paragraphs as \"retrieved context\"\n\n    # Step 3: Build the prompt\n    doc_prompt = f\"\"\"\nYou are an expert technical writer and code documentation assistant.\n\nA developer has made the following changes to the codebase:\n\n## PR Description:\n{pr_description}\n\n## Code Diff:\n{code_diff}\n\n## Relevant Existing Documentation:\n{context}\n\nPlease provide additions or updates to the documentation based on the above code change. \nFormat your response in **Markdown**, and only include sections that should be added or updated. \nIf the change doesn't require doc updates, say so.\n\"\"\"\n\n    # Step 4: Call Groq API\n    payload = {\n        \"model\": \"llama3-70b-8192\",\n        \"messages\": [{\"role\": \"user\", \"content\": doc_prompt}]\n    }\n\n    try:\n        response = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n        response.raise_for_status()\n        result = response.json()\n        return {\n            \"doc_update\": result[\"choices\"][0][\"message\"][\"content\"].strip()\n        }\n\n    except Exception as e:\n        logging.exception(\"Groq API call for documentation update failed\")\n        return {\"error\": str(e)}\n\n\ndef build_full_prompt(pr_description: str, code_diff: str) -> str:\n    return f\"\"\"\nYou are an AI code reviewer and documentation assistant.\n\nA developer submitted the following pull request. Please do the following:\n1. \ud83d\udd0d Summarize the overall purpose of the PR.\n2. \ud83e\udde0 Rate the merge confidence as: High / Medium / Low.\n3. \u2705 Review the code for:\n   - Syntax issues\n   - Style (PEP8)\n   - Functionality correctness (does it match the description?)\n4. \ud83d\udcdd Extract or suggest updated docstrings for any changed or added functions/classes.\n\n## PR Description:\n{pr_description}\n\n## Code Diff:\n{code_diff}\n\nRespond ONLY in this JSON format:\n\n{{\n  \"ai_summary\": \"...\",\n  \"merge_confidence\": \"1-10\",\n  \"code_quality\": {{\n    \"syntax_check\": \"...\",\n    \"style_check\": \"...\",\n    \"functionality_check\": \"...\",\n    \"final_rating\": \"Excellent / Good / Needs Work / Critical Issues\"\n  }}\n}}\n\"\"\"\n\ndef review_and_store_pr(pr_description: str, code_diff: str, pr_title: str, pr_author: str, pr_status: str):\n    docstring = update_documentation_with_rag(pr_description, code_diff)\n    print(docstring)\n    print('\\n\\n')\n    pr_id = random.randint(1000, 9999)\n    prompt = build_full_prompt(pr_description, code_diff)\n\n    payload = {\n        \"model\": \"llama3-70b-8192\",\n        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n    }\n\n    try:\n        response = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n        response.raise_for_status()\n        result = response.json()\n\n        try:\n            content = result[\"choices\"][0][\"message\"][\"content\"].strip()\n            review_data = json.loads(content)\n\n            # Flatten the code quality object for easier insert\n            code_quality_str = (\n                f\"Syntax: {review_data['code_quality']['syntax_check']}\\n\"\n                f\"Style: {review_data['code_quality']['style_check']}\\n\"\n                f\"Functionality: {review_data['code_quality']['functionality_check']}\\n\"\n                f\"Rating: {review_data['code_quality']['final_rating']}\"\n            )\n            cur.execute(f\"\"\"\n                INSERT INTO PULL_REQUESTS (\n                    ID, TITLE, AUTHOR, STATUS, CREATED_AT, UPDATED_AT\n                )\n                VALUES (%s, %s, %s, %s, %s, %s);\n            \"\"\", (\n                pr_id,\n                pr_title,\n                pr_author,\n                pr_status,\n                datetime.datetime.now(),\n                datetime.datetime.now()\n            ))\n            conn.commit()\n\n            # Insert into Snowflake\n            cur.execute(f\"\"\"\n                INSERT INTO PR_ANALYSIS (\n                    PR_ID, AI_SUMMARY, MERGE_CONFIDENCE, CODE_QUALITY,\n                    PR_DESCRIPTION, DOCSTRINGS, DIFF\n                )\n                VALUES (%s, %s, %s, %s, %s, %s, %s);\n            \"\"\", (\n                pr_id,\n                review_data[\"ai_summary\"],\n                review_data[\"merge_confidence\"],\n                code_quality_str,\n                pr_description,\n                docstring['doc_update'],\n                code_diff\n            ))\n\n            conn.commit()\n\n            logging.info(f\"[\u2705] PR #{pr_id} analysis saved to Snowflake.\")\n            return review_data\n\n        except json.JSONDecodeError:\n            logging.error(\"[\u274c] Groq returned non-JSON output.\")\n            return {\"error\": \"Invalid JSON from Groq\"}\n\n    except Exception as e:\n        logging.exception(\"Groq API or Snowflake insert failed\")\n        return {\"error\": str(e)}\n\n# # \ud83e\uddea Example test\n# if __name__ == \"__main__\":\n#     code_diff = \"\"\"\n# diff --git a/app/main.py b/app/main.py\n# index a1b2c3d..d4e5f6g 100644\n# --- a/app/main.py\n# +++ b/app/main.py\n# @@ def main():\n# -    print(\"Hello\")\n# +    log_greeting(\"Hello, world!\")\n# +    print(\"Execution completed.\")\n\n# +def log_greeting(message):\n# +    \\\"\\\"\\\"Logs the greeting message to a file\\\"\\\"\\\"\n# +    with open(\"logs.txt\", \"a\") as log_file:\n# +        log_file.write(f\"Greeting logged: {message}\\\\n\")\n# \"\"\"\n#     pr_description = \"Update main function to greet the world.\"\n#     pr_title = \"Update main function\"\n#     pr_author = \"johndoe\"\n#     pr_status = \"OPEN\"\n\n#     result = review_and_store_pr(pr_description, code_diff, pr_title, pr_author, pr_status)\n#     print(json.dumps(result, indent=2))\n\n--- File: 2 # backend/main.py ---\nfrom fastapi import FastAPI, Request, Header\nfrom fastapi.responses import JSONResponse\nimport hmac\nimport hashlib\nimport json\nimport os\n\napp = FastAPI()\n\nGITHUB_SECRET = \"asdfg\" # Set this in your env\n\ndef verify_signature(payload, signature, secret):\n    mac = hmac.new(secret.encode(), msg=payload, digestmod=hashlib.sha256)\n    expected = f\"sha256={mac.hexdigest()}\"\n    return hmac.compare_digest(expected, signature)\n\n@app.post(\"/webhook\")\nasync def github_webhook(\n    request: Request,\n    x_hub_signature_256: str = Header(None),\n    x_github_event: str = Header(None)\n):\n    body = await request.body()\n\n    # Verify webhook signature\n    if not verify_signature(body, x_hub_signature_256, GITHUB_SECRET):\n        return JSONResponse(content={\"error\": \"Invalid signature\"}, status_code=403)\n\n    payload = json.loads(body)\n\n    if x_github_event == \"pull_request\":\n        action = payload[\"action\"]\n        pr = payload[\"pull_request\"]\n\n        if action in [\"opened\", \"synchronize\"]:\n            pr_url = pr[\"url\"]\n            pr_diff_url = pr[\"diff_url\"]\n            repo_full_name = payload[\"repository\"][\"full_name\"]\n            # \ud83d\udc47 Call your internal logic to analyze PR\n            print(f\"Trigger AI review for: {repo_full_name} @ {pr_url}\")\n            print(f\"PR Diff URL: {pr_diff_url}\")\n            print(f\"PR Title: {pr['title']}\")\n            print(f\"PR Body: {pr['body']}\")\n\n    return {\"message\": \"OK\"}\n\n--- File: 3 # backend/prReview.py ---\nimport os, re\nfrom github import Auth\nfrom github import Github, GithubIntegration\nfrom github.GithubException import GithubException\nfrom tree_sitter import Language, Parser\nimport tree_sitter_python as tspython\nimport tree_sitter_javascript as tsjavascript\nimport tree_sitter_cpp as tscpp\nimport requests\nimport json\nimport logging\nimport snowflake.connector\nimport random\nimport datetime\n\n\nGROQ_API_KEY = \"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\"\nGROQ_ENDPOINT = \"https://api.groq.com/openai/v1/chat/completions\"\n\ndef authenticate_github(app_id: int, installation_id, private_key: str):\n    gi = GithubIntegration(integration_id=app_id, private_key=private_key)\n    g = gi.get_github_for_installation(installation_id)\n    return g\n\ndef get_pull_request(g, repo_name: str, pr_number: int):\n    repo = g.get_repo(repo_name)\n    pr = repo.get_pull(pr_number)\n    return pr\n\ndef parse_changed_lines(patch: str):\n    added_or_modified_lines = set()\n    deleted_lines = set()\n    for line in patch.splitlines():\n        if line.startswith(\"@@\"):\n            match = re.findall(r\"@@ -(\\d+)(?:,(\\d+))? \\+(\\d+)(?:,(\\d+))? @@\", line)\n            if match:\n                old_start, old_count, new_start, new_count = match[0]\n\n                old_start = int(old_start)\n                old_count = int(old_count or 1)\n                for l in range(old_start, old_start + old_count):\n                    deleted_lines.add(l)\n\n                new_start = int(new_start)\n                new_count = int(new_count or 1)\n                for l in range(new_start, new_start + new_count):\n                    added_or_modified_lines.add(l)\n\n    return {\n        \"added_or_modified_lines\": added_or_modified_lines,\n        \"deleted_lines\": deleted_lines\n    }\n\ndef get_file_contents(g, repo_name: str, file_path: str, commit_sha: str):\n    repo = g.get_repo(repo_name)\n    try:\n        contents = repo.get_contents(file_path, ref=commit_sha)\n        return contents.decoded_content.decode()\n    except GithubException as e:\n        if e.status == 404:\n            return \"\"\n        else:\n            raise e\n\ndef get_lines_changed(pr):\n    files = pr.get_files()\n    file_changes = {}\n    for file in files:\n        if file.status == \"removed\":\n            continue\n        file_changes[file.filename] = parse_changed_lines(file.patch)\n\n    return file_changes\n\ndef get_paraser(language_name):\n\n    PY_LANGUAGE = Language(tspython.language())\n    JS_LANGUAGE = Language(tsjavascript.language())\n    CPP_LANGUAGE = Language(tscpp.language())\n\n    LANGUAGES = {\n        'python': PY_LANGUAGE,\n        'javascript': JS_LANGUAGE,\n        'cpp': CPP_LANGUAGE,\n    }\n    parser = Parser(LANGUAGES[language_name])\n    return parser\n\ndef extract_functions(code, language_name, changed_lines):\n    parser = get_paraser(language_name)\n    tree = parser.parse(bytes(code, \"utf8\"))\n    root_node = tree.root_node\n\n    functions = []\n\n    def node_within_lines(node):\n        start_line = node.start_point[0] + 1\n        end_line = node.end_point[0] + 1\n        return any([start_line <= line <= end_line for line in changed_lines])\n    \n    def traverse(node):\n        if language_name == \"python\" and node.type == \"function_definition\":\n            if node_within_lines(node):\n                func_name_node = node.child_by_field_name('name')\n                func_name = func_name_node.text.decode()\n                functions.append({'name': func_name, 'code': node.text.decode()})\n        elif language_name == 'javascript' and node.type in ['function_declaration', 'method_definition', 'arrow_function']:\n            if node_within_lines(node):\n                func_name_node = node.child_by_field_name('name')\n                func_name = func_name_node.text.decode()\n                functions.append({'name': func_name, 'code': node.text.decode()})\n        elif language_name == 'cpp' and node.type in ['function_definition']:\n            if node_within_lines(node):\n                func_name_node = node.child_by_field_name('name')\n                func_name = func_name_node.text.decode()\n                functions.append({'name': func_name, 'code': node.text.decode()})\n        for child in node.children:\n            traverse(child)\n\n    traverse(root_node)\n    return functions\n\nheaders = {\n    \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\n# Snowflake setup\nconn = snowflake.connector.connect(\n    user=\"raghavg332\",\n    password=\"Qa29Kh4MptfGHEW\",\n    account=\"KXCIVVH-LL27432\",\n    warehouse=\"COMPUTE_WH\",\n    database=\"PR_DASHBOARD\",\n    schema=\"PUBLIC\"\n)\ncur = conn.cursor()\n\ndef update_documentation_with_rag(pr_description: str, code_diff: str, docs_path=\"backend/project_docs.txt\"):\n    \"\"\"\n    Performs a simple RAG-based call to Groq to update documentation.\n    \"\"\"\n    # Step 1: Load the existing documentation\n    try:\n        with open(docs_path, \"r\") as f:\n            existing_docs = f.read()\n    except FileNotFoundError:\n        logging.error(f\"[\u274c] Documentation file not found at {docs_path}\")\n        return {\"error\": \"Documentation file not found.\"}\n\n    # Step 2: Very basic RAG - extract top paragraphs that contain overlap\n    paragraphs = existing_docs.split(\"\\n\\n\")\n    keywords = pr_description.lower().split()\n    ranked_paragraphs = sorted(paragraphs, key=lambda para: sum(word in para.lower() for word in keywords), reverse=True)\n    context = \"\\n\\n\".join(ranked_paragraphs[:3])  # Top 3 paragraphs as \"retrieved context\"\n\n    # Step 3: Build the prompt\n    doc_prompt = f\"\"\"\nYou are an expert technical writer and code documentation assistant.\n\nA developer has made the following changes to the codebase:\n\n## PR Description:\n{pr_description}\n\n## Code Diff:\n{code_diff}\n\n## Relevant Existing Documentation:\n{context}\n\nPlease provide additions or updates to the documentation based on the above code change. \nFormat your response in **Markdown**, and only include sections that should be added or updated. \nIf the change doesn't require doc updates, say so.\n\"\"\"\n\n    # Step 4: Call Groq API\n    payload = {\n        \"model\": \"llama3-70b-8192\",\n        \"messages\": [{\"role\": \"user\", \"content\": doc_prompt}]\n    }\n\n    try:\n        response = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n        response.raise_for_status()\n        result = response.json()\n        return {\n            \"doc_update\": result[\"choices\"][0][\"message\"][\"content\"].strip()\n        }\n\n    except Exception as e:\n        logging.exception(\"Groq API call for documentation update failed\")\n        return {\"error\": str(e)}\n\n\ndef build_full_prompt(pr_description: str, code_diff_str: str) -> str:\n    return f\"\"\"\nYou are an AI code reviewer and documentation assistant.\n\nA developer submitted the following pull request. Please do the following:\n1. \ud83d\udd0d Summarize the overall purpose of the PR. Be a little detailed\n2. \u2705 Review the code for:\n   - Syntax issues in Python (Make up some Issue)\n   - Style (PEP8)\n   - Functionality correctness (does it match the description?)\n3. Based on the review, provide a merge confidence score from 1 to 10.\n4. \ud83d\udcdd Extract or suggest updated docstrings for any changed or added functions/classes.\n5. The Code Diff is made up of the old and new version of the code, if any of the field is empty it means, the function is either added or deleted respectively.\n\n## PR Description:\n{pr_description}\n\n## Code Diff:\n{code_diff_str}\n\nRespond ONLY in this JSON format without any additional text:\n\n{{\n  \"ai_summary\": \"...\",\n  \"merge_confidence\": \"1 - 10\",\n  \"code_quality\": {{\n    \"syntax_check\": \"...\",\n    \"style_check\": \"...\",\n    \"functionality_check\": \"...\",\n    \"final_rating\": \"Excellent / Good / Needs Work / Critical Issues\"\n  }}\n}}\n\"\"\"\n\ndef review_and_store_pr(pr_description: str, code_diff: dict, pr_title: str, pr_author: str, pr_status: str):\n    code_diff_str = \"\"\n    for file in list(code_diff.keys()):\n        code_diff_str += f\"File: {file}\\n\\n\"\n        diff = code_diff[file]\n\n        for func_name in list(diff.keys()):\n            old_code = diff[func_name]['old_code']\n            new_code = diff[func_name]['new_code']\n            if old_code is None:\n                old_code = \"\"\n            if new_code is None:\n                new_code = \"\"\n            code_diff_str += f\"Function: {func_name}\\n\\nOld Code:\\n{old_code}\\n\\nNew Code:\\n{new_code}\"\n    docstring = update_documentation_with_rag(pr_description, code_diff)\n    pr_id = random.randint(1000, 9999)\n    prompt = build_full_prompt(pr_description, code_diff_str)\n\n    payload = {\n        \"model\": \"llama3-70b-8192\",\n        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n    }\n\n    try:\n        response = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n        response.raise_for_status()\n        result = response.json()\n\n        try:\n            content = result[\"choices\"][0][\"message\"][\"content\"].strip()\n            review_data = json.loads(content)\n\n            # Flatten the code quality object for easier insert\n            code_quality_str = (\n                f\"Syntax: {review_data['code_quality']['syntax_check']}\\n\"\n                f\"Style: {review_data['code_quality']['style_check']}\\n\"\n                f\"Functionality: {review_data['code_quality']['functionality_check']}\\n\"\n                f\"Rating: {review_data['code_quality']['final_rating']}\"\n            )\n            cur.execute(f\"\"\"\n                INSERT INTO PULL_REQUESTS (\n                    ID, TITLE, AUTHOR, STATUS, CREATED_AT, UPDATED_AT\n                )\n                VALUES (%s, %s, %s, %s, %s, %s);\n            \"\"\", (\n                pr_id,\n                pr_title,\n                pr_author,\n                pr_status,\n                datetime.datetime.now(),\n                datetime.datetime.now()\n            ))\n            conn.commit()\n\n            # Insert into Snowflake\n            cur.execute(f\"\"\"\n                INSERT INTO PR_ANALYSIS (\n                    PR_ID, AI_SUMMARY, MERGE_CONFIDENCE, CODE_QUALITY,\n                    PR_DESCRIPTION, DOCSTRINGS, DIFF\n                )\n                VALUES (%s, %s, %s, %s, %s, %s, %s);\n            \"\"\", (\n                pr_id,\n                review_data[\"ai_summary\"],\n                review_data[\"merge_confidence\"],\n                code_quality_str,\n                pr_description,\n                docstring['doc_update'],\n                code_diff_str\n            ))\n\n            conn.commit()\n\n            logging.info(f\"[\u2705] PR #{pr_id} analysis saved to Snowflake.\")\n            print(f\"[\u2705] PR #{pr_id} analysis saved to Snowflake.\")\n            return review_data\n\n        except json.JSONDecodeError:\n            logging.error(\"[\u274c] Groq returned non-JSON output.\")\n            print(\"[\u274c] Groq returned non-JSON output.\")\n            return {\"error\": \"Invalid JSON from Groq\"}\n\n    except Exception as e:\n        logging.exception(\"Groq API or Snowflake insert failed\")\n        return {\"error\": str(e)}\n\ndef process_pull_request(g, repo_name, pr_number, sender=None):\n    repo = g.get_repo(repo_name)\n    pr = repo.get_pull(pr_number)\n    pr_description = pr.title\n    pr_title = pr.title\n    pr_author = pr.user.login\n    pr_status = pr.state\n    code_diff = get_lines_changed(pr)\n    file_diff_func = {}\n    for file in list(code_diff.keys()):\n        diff_func = {}\n        new_code = get_file_contents(g, repo_name, file, pr.head.sha)\n        old_code = get_file_contents(g, repo_name, file, pr.base.sha)\n        added_or_modified_functions_after = extract_functions(new_code, \"python\", code_diff[file]['added_or_modified_lines'])\n        deleted_functions_before = extract_functions(old_code, \"python\", code_diff[file]['deleted_lines'])\n        old_functions_all = extract_functions(old_code, \"python\", set(range(1, len(old_code.split(\"\\n\")))))\n        new_functions_all = extract_functions(new_code, \"python\", set(range(1, len(new_code.split(\"\\n\")))))\n        for func in added_or_modified_functions_after:\n            matched_old_function = next((f for f in old_functions_all if f['name'] ==  func['name']), None)\n            diff_func[func['name']] = {\n                \"new_code\": func['code'],\n                \"old_code\": matched_old_function['code'] if matched_old_function else None\n            }\n        \n        for func in deleted_functions_before:\n            matched_new_function = next((f for f in new_functions_all if f['name'] ==  func['name']), None)\n            diff_func[func['name']] = {\n                \"new_code\": matched_new_function['code'] if matched_new_function else None,\n                \"old_code\": func['code']\n            }\n        \n        file_diff_func[file] = diff_func\n    \n    # Get the review data and post a comment\n    review_data = review_and_store_pr(pr_description, file_diff_func, pr_title, pr_author, pr_status)\n    \n    # Post the review as a comment on the PR\n    post_review_comment(g, repo_name, pr_number, review_data)\n\n\ndef post_review_comment(g, repo_name, pr_number, review_data):\n    \"\"\"\n    Posts the AI review as a comment on the pull request.\n    \n    Args:\n        g: Authenticated GitHub instance\n        repo_name: Repository name (owner/repo)\n        pr_number: Pull request number\n        review_data: The AI review data dictionary\n    \"\"\"\n    try:\n        # Get the repo and PR objects\n        repo = g.get_repo(repo_name)\n        pr = repo.get_pull(pr_number)\n        \n        # Format the comment body\n        comment_body = f\"\"\"\n## \ud83e\udd16 AI Code Review\n\n### Summary\n{review_data.get('ai_summary', 'No summary available')}\n\n### Merge Confidence: **{review_data.get('merge_confidence', 'Unknown')}**\n\n### Code Quality Review\n- **Syntax:** {review_data.get('code_quality', {}).get('syntax_check', 'Not analyzed')}\n- **Style:** {review_data.get('code_quality', {}).get('style_check', 'Not analyzed')}\n- **Functionality:** {review_data.get('code_quality', {}).get('functionality_check', 'Not analyzed')}\n- **Overall Rating:** {review_data.get('code_quality', {}).get('final_rating', 'Not rated')}\n\n---\n*This review was automatically generated by AI.*\n\"\"\"\n        \n        # Create the comment on the PR\n        pr.create_issue_comment(comment_body)\n        logging.info(f\"[\u2705] Posted AI review comment on PR #{pr_number} in {repo_name}\")\n        print(f\"[\u2705] Posted AI review comment on PR #{pr_number} in {repo_name}\")\n        return True\n    except Exception as e:\n        logging.exception(f\"[\u274c] Failed to post comment on PR #{pr_number}: {str(e)}\")\n        print(f\"[\u274c] Failed to post comment on PR #{pr_number}: {str(e)}\")\n        return False\n\n--- File: 4 # frontend/app.js ---\nconst e = React.createElement;\r\n\r\nfunction parseDiff(diff) {\r\n    const lines = diff.split('\\n');\r\n    const parsedLines = [];\r\n\r\n    let section = null;\r\n\r\n    for (let i = 0; i < lines.length; i++) {\r\n        const line = lines[i];\r\n\r\n        if (line.startsWith('File:') || line.startsWith('Function:')) {\r\n            parsedLines.push({ type: 'context', content: line });\r\n            section = null;\r\n        } else if (line.trim() === 'Old Code:') {\r\n            parsedLines.push({ type: 'context', content: 'Old Code:' });\r\n            section = 'old';\r\n\r\n            // Handle empty old code\r\n            if (lines[i + 1]?.trim() === 'New Code:') {\r\n                parsedLines.push({ type: 'old', content: '<NO PREVIOUS CODE>' });\r\n            }\r\n        } else if (line.trim() === 'New Code:') {\r\n            parsedLines.push({ type: 'context', content: 'New Code:' });\r\n            section = 'new';\r\n        } else if (section === 'old' || section === 'new') {\r\n            parsedLines.push({ type: section, content: line });\r\n        } else {\r\n            parsedLines.push({ type: 'context', content: line });\r\n        }\r\n    }\r\n\r\n    return parsedLines;\r\n}\r\n\r\n\r\n// Analytics component to visualize PR data\r\nfunction Analytics({ prs }) {\r\n    // Count PRs by status\r\n    const statusCounts = prs.reduce((acc, pr) => {\r\n        acc[pr.status] = (acc[pr.status] || 0) + 1;\r\n        return acc;\r\n    }, {});\r\n\r\n    // Count PRs by author\r\n    const authorCounts = prs.reduce((acc, pr) => {\r\n        acc[pr.author] = (acc[pr.author] || 0) + 1;\r\n        return acc;\r\n    }, {});\r\n\r\n    // Calculate PRs over time (by month)\r\n    const prsByMonth = prs.reduce((acc, pr) => {\r\n        const date = new Date(pr.created_at);\r\n        const monthYear = date.toLocaleString('default', { month: 'short', year: 'numeric' });\r\n        acc[monthYear] = (acc[monthYear] || 0) + 1;\r\n        return acc;\r\n    }, {});\r\n\r\n    // Create data arrays for charts\r\n    const statusData = Object.entries(statusCounts).map(([status, count]) => ({\r\n        status,\r\n        count,\r\n        color: status === 'Open' ? '#17a2b8' : \r\n               status === 'Merged' ? '#28a745' : \r\n               status === 'Closed' ? '#dc3545' : '#6c757d'\r\n    }));\r\n\r\n    const authorData = Object.entries(authorCounts)\r\n        .sort((a, b) => b[1] - a[1])\r\n        .slice(0, 5)\r\n        .map(([author, count]) => ({ author, count }));\r\n\r\n    const timelineData = Object.entries(prsByMonth)\r\n        .sort((a, b) => new Date(a[0]) - new Date(b[0]))\r\n        .map(([month, count]) => ({ month, count }));\r\n\r\n    return e('div', { className: 'analytics-container' }, [\r\n        e('h2', { key: 'title', className: 'analytics-title' }, 'Pull Request Analytics'),\r\n\r\n        // Status distribution chart\r\n        e('div', { key: 'status-chart', className: 'chart-container' }, [\r\n            e('h3', { key: 'status-title' }, 'PR Status Distribution'),\r\n            e('div', { key: 'status-bars', className: 'status-chart' }, \r\n                statusData.map(item => \r\n                    e('div', { key: item.status, className: 'status-bar-container' }, [\r\n                        e('div', { key: 'label', className: 'status-label' }, `${item.status} (${item.count})`),\r\n                        e('div', { key: 'bar-bg', className: 'status-bar-bg' }, \r\n                            e('div', { \r\n                                key: 'bar', \r\n                                className: 'status-bar', \r\n                                style: { \r\n                                    width: `${(item.count / prs.length) * 100}%`,\r\n                                    backgroundColor: item.color\r\n                                } \r\n                            })\r\n                        )\r\n                    ])\r\n                )\r\n            )\r\n        ]),\r\n\r\n        // Author contribution chart\r\n        e('div', { key: 'author-chart', className: 'chart-container' }, [\r\n            e('h3', { key: 'author-title' }, 'Top Contributors'),\r\n            e('div', { key: 'author-bars', className: 'author-chart' }, \r\n                authorData.map(item => \r\n                    e('div', { key: item.author, className: 'author-bar-container' }, [\r\n                        e('div', { key: 'label', className: 'author-label' }, [\r\n                            e('img', { \r\n                                key: 'avatar', \r\n                                className: 'avatar', \r\n                                src: `https://ui-avatars.com/api/?name=${item.author}&size=24`, \r\n                                alt: item.author\r\n                            }),\r\n                            `${item.author} (${item.count})`\r\n                        ]),\r\n                        e('div', { key: 'bar-bg', className: 'author-bar-bg' }, \r\n                            e('div', { \r\n                                key: 'bar', \r\n                                className: 'author-bar', \r\n                                style: { \r\n                                    width: `${(item.count / Math.max(...Object.values(authorCounts))) * 100}%`\r\n                                } \r\n                            })\r\n                        )\r\n                    ])\r\n                )\r\n            )\r\n        ]),\r\n\r\n        // Timeline chart\r\n        e('div', { key: 'timeline-chart', className: 'chart-container' }, [\r\n            e('h3', { key: 'timeline-title' }, 'PR Activity Over Time'),\r\n            e('div', { key: 'timeline', className: 'timeline-chart' }, \r\n                timelineData.map((item, index) => \r\n                    e('div', { key: item.month, className: 'timeline-bar-container' }, [\r\n                        e('div', { \r\n                            key: 'bar', \r\n                            className: 'timeline-bar', \r\n                            style: { \r\n                                height: `${(item.count / Math.max(...timelineData.map(d => d.count))) * 100}px`\r\n                            } \r\n                        }),\r\n                        e('div', { key: 'label', className: 'timeline-label' }, item.month)\r\n                    ])\r\n                )\r\n            )\r\n        ]),\r\n\r\n        // Summary stats\r\n        e('div', { key: 'summary-stats', className: 'summary-stats' }, [\r\n            e('div', { key: 'total', className: 'stat-card' }, [\r\n                e('h3', { key: 'title' }, 'Total PRs'),\r\n                e('div', { key: 'value', className: 'stat-value' }, prs.length)\r\n            ]),\r\n            e('div', { key: 'open', className: 'stat-card' }, [\r\n                e('h3', { key: 'title' }, 'Open PRs'),\r\n                e('div', { key: 'value', className: 'stat-value' }, statusCounts['Open'] || 0)\r\n            ]),\r\n            e('div', { key: 'merged', className: 'stat-card' }, [\r\n                e('h3', { key: 'title' }, 'Merged PRs'),\r\n                e('div', { key: 'value', className: 'stat-value' }, statusCounts['Merged'] || 0)\r\n            ]),\r\n            e('div', { key: 'contributors', className: 'stat-card' }, [\r\n                e('h3', { key: 'title' }, 'Contributors'),\r\n                e('div', { key: 'value', className: 'stat-value' }, Object.keys(authorCounts).length)\r\n            ])\r\n        ])\r\n    ]);\r\n}\r\n\r\nfunction renderDiffSection(diff) {\r\n    const diffLines = parseDiff(diff);\r\n\r\n    return React.createElement(\r\n        'pre',\r\n        { className: 'diff-content' },\r\n        diffLines.map((line, index) => {\r\n            const className =\r\n                line.type === 'old' ? 'diff-old-code diff-code-line' :\r\n                line.type === 'new' ? 'diff-new-code diff-code-line' :\r\n                'diff-context-line';\r\n\r\n            return React.createElement('div', { key: index, className }, line.content);\r\n        })\r\n    );\r\n}\r\n\r\nconst ReactMarkdown = window.ReactMarkdown || (() => null) // fallback if not loaded\r\n\r\n\r\n// Main App component\r\nfunction App() {\r\n    const [prs, setPrs] = React.useState([]);\r\n    const [selectedPR, setSelectedPR] = React.useState(null);\r\n    const [prDetails, setPRDetails] = React.useState(null);\r\n    const [filter, setFilter] = React.useState(\"all\");\r\n    const [searchQuery, setSearchQuery] = React.useState(\"\");\r\n    const [isLoading, setIsLoading] = React.useState(true);\r\n    const [showCommentForm, setShowCommentForm] = React.useState(false);\r\n    const [comment, setComment] = React.useState(\"\");\r\n    const [comments, setComments] = React.useState([]);\r\n    const [darkMode, setDarkMode] = React.useState(false);\r\n    const [activeTab, setActiveTab] = React.useState(\"details\"); // \"details\" or \"analytics\"\r\n\r\n    React.useEffect(() => {\r\n        setIsLoading(true);\r\n        fetch('http://localhost:8000/api/prs')\r\n            .then(res => res.json())\r\n            .then(data => {\r\n                setPrs(data);\r\n                setIsLoading(false);\r\n            })\r\n            .catch(err => {\r\n                console.error('Error fetching PRs:', err);\r\n                setIsLoading(false);\r\n            });\r\n    }, []);\r\n\r\n    React.useEffect(() => {\r\n        // Apply dark mode class to body\r\n        if (darkMode) {\r\n            document.body.classList.add('dark-mode');\r\n        } else {\r\n            document.body.classList.remove('dark-mode');\r\n        }\r\n    }, [darkMode]);\r\n\r\n    const fetchDetails = (id) => {\r\n        setIsLoading(true);\r\n        fetch(`http://localhost:8000/api/prs/${id}`)\r\n            .then(res => res.json())\r\n            .then(data => {\r\n                setPRDetails(data);\r\n                setIsLoading(false);\r\n                // Simulate fetching comments\r\n                setComments([\r\n                    { id: 1, author: \"codereviewer\", text: \"Looks good to me!\", timestamp: \"2 hours ago\" },\r\n                    { id: 2, author: \"securityexpert\", text: \"We should add input validation here.\", timestamp: \"1 hour ago\" }\r\n                ]);\r\n            })\r\n            .catch(err => {\r\n                console.error('Error fetching PR details:', err);\r\n                setIsLoading(false);\r\n            });\r\n    };\r\n\r\n    const selectPR = (pr) => {\r\n        setSelectedPR(pr);\r\n        setActiveTab(\"details\"); // Switch to details tab when selecting a PR\r\n        fetchDetails(pr.id);\r\n    };\r\n\r\n    const handleAddComment = () => {\r\n        if (comment.trim() === \"\") return;\r\n        \r\n        const newComment = {\r\n            id: comments.length + 1,\r\n            author: \"you\",\r\n            text: comment,\r\n            timestamp: \"Just now\"\r\n        };\r\n        \r\n        setComments([...comments, newComment]);\r\n        setComment(\"\");\r\n        setShowCommentForm(false);\r\n    };\r\n\r\n    const handleApprove = () => {\r\n        alert(\"PR approved! (This would trigger an API call in production)\");\r\n    };\r\n\r\n    const handleReject = () => {\r\n        alert(\"PR rejected! (This would trigger an API call in production)\");\r\n    };\r\n\r\n    // Function to switch to analytics tab\r\n    const viewAnalytics = () => {\r\n        setActiveTab(\"analytics\");\r\n    };\r\n\r\n    // Filter PRs based on status and search query\r\n    const filteredPRs = prs.filter(pr => {\r\n        const matchesFilter = filter === \"all\" || pr.status === filter;\r\n        const matchesSearch = pr.title.toLowerCase().includes(searchQuery.toLowerCase()) || \r\n                             pr.author.toLowerCase().includes(searchQuery.toLowerCase());\r\n        return matchesFilter && matchesSearch;\r\n    });\r\n\r\n    // Create PR list items\r\n    const prItems = filteredPRs.map(pr => \r\n        e('div', { \r\n            key: pr.id, \r\n            className: selectedPR && selectedPR.id === pr.id ? 'pr-item selected' : 'pr-item', \r\n            onClick: () => selectPR(pr) \r\n        }, [\r\n            e('div', {key: 'pr-header', className: 'pr-header'}, [\r\n                e('strong', {key: 'id'}, `#${pr.id}`),\r\n                e('span', {\r\n                    key: 'status',\r\n                    className: `pr-status status-${pr.status}`\r\n                }, pr.status)\r\n            ]),\r\n            e('div', {key: 'pr-title', className: 'pr-title'}, pr.title),\r\n            e('div', {key: 'pr-meta', className: 'pr-meta'}, [\r\n                e('span', {key: 'author'}, [\r\n                    e('img', {key: 'avatar', className: 'avatar', src: `https://ui-avatars.com/api/?name=${pr.author}&size=24`, alt: pr.author}),\r\n                    pr.author\r\n                ]),\r\n                e('span', {key: 'date', className: 'date'}, 'Updated: ' + (pr.updated_at || 'N/A'))\r\n            ])\r\n        ])\r\n    );\r\n\r\n    // Create filter controls\r\n    const filterControls = e('div', {className: 'filter-controls'}, [\r\n        e('div', {key: 'search', className: 'search-box'}, [\r\n            e('input', {\r\n                type: 'text',\r\n                placeholder: 'Search PRs...',\r\n                value: searchQuery,\r\n                onChange: (e) => setSearchQuery(e.target.value),\r\n                className: 'search-input'\r\n            })\r\n        ]),\r\n        e('div', {key: 'filters', className: 'status-filters'}, [\r\n            e('button', {\r\n                className: filter === 'all' ? 'filter-btn active' : 'filter-btn',\r\n                onClick: () => setFilter('all')\r\n            }, 'All'),\r\n            e('button', {\r\n                className: filter === 'Open' ? 'filter-btn active' : 'filter-btn',\r\n                onClick: () => setFilter('Open')\r\n            }, 'Open'),\r\n            e('button', {\r\n                className: filter === 'Merged' ? 'filter-btn active' : 'filter-btn',\r\n                onClick: () => setFilter('Merged')\r\n            }, 'Merged'),\r\n            e('button', {\r\n                className: filter === 'Closed' ? 'filter-btn active' : 'filter-btn',\r\n                onClick: () => setFilter('Closed')\r\n            }, 'Closed')\r\n        ])\r\n    ]);\r\n\r\n    // Create sidebar\r\n    const sidebar = e('div', { className: 'sidebar' }, [\r\n        e('div', {key: 'header', className: 'sidebar-header'}, [\r\n            e('h2', {key: 'title'}, '\ud83d\udccb Pull Requests'),\r\n            e('button', {\r\n                key: 'theme-toggle',\r\n                className: 'theme-toggle',\r\n                onClick: () => setDarkMode(!darkMode)\r\n            }, darkMode ? '\u2600\ufe0f' : '\ud83c\udf19')\r\n        ]),\r\n        filterControls,\r\n        e('div', {key: 'pr-list', className: 'pr-list'}, isLoading ? \r\n            e('div', {className: 'loading'}, 'Loading PRs...') : \r\n            prItems.length > 0 ? prItems : e('div', {className: 'no-results'}, 'No PRs match your filters'))\r\n    ]);\r\n\r\n    // Create tab navigation - show it regardless of whether a PR is selected\r\n    const tabNav = e('div', {className: 'tab-navigation'}, [\r\n        e('button', {\r\n            key: 'details-tab',\r\n            className: activeTab === 'details' ? 'tab-btn active' : 'tab-btn',\r\n            onClick: () => setActiveTab('details'),\r\n            disabled: !selectedPR\r\n        }, 'PR Details'),\r\n        e('button', {\r\n            key: 'analytics-tab',\r\n            className: activeTab === 'analytics' ? 'tab-btn active' : 'tab-btn',\r\n            onClick: () => setActiveTab('analytics')\r\n        }, 'Analytics')\r\n    ]);\r\n\r\n    // Create PR details content\r\n    let prDetailsContent;\r\n    if (isLoading && selectedPR) {\r\n        prDetailsContent = e('div', {className: 'loading-container'}, [\r\n            e('div', {key: 'spinner', className: 'loading-spinner'}),\r\n            e('p', {key: 'text'}, 'Loading PR details...')\r\n        ]);\r\n    } else if (!prDetails && selectedPR) {\r\n        prDetailsContent = e('p', null, 'No details available for this PR');\r\n    } else if (selectedPR) {\r\n        // PR details content\r\n        const score = Number(prDetails.merge_confidence_score || 0); // 0-10\r\n        const maxScore = 10;\r\n        const percent = Math.min((score / maxScore) * 100, 100);\r\n        const confidenceIndicator = e('div', { key: 'confidence-indicator', className: 'confidence-gradient-wrapper' }, [\r\n            e('div', { className: 'confidence-gradient-bar' }, [\r\n                e('div', {\r\n                    className: 'confidence-score-indicator',\r\n                    style: { left: `${percent}%` }\r\n                })\r\n            ]),\r\n            e('div', { style: { marginTop: '8px', fontWeight: 600 } }, `Score: ${score}/10`)\r\n        ]);\r\n        \r\n        // Comment section\r\n        const commentList = comments.map(comment => \r\n            e('div', {key: `comment-${comment.id}`, className: 'comment'}, [\r\n                e('div', {key: 'comment-header', className: 'comment-header'}, [\r\n                    e('img', {key: 'avatar', className: 'avatar', src: `https://ui-avatars.com/api/?name=${comment.author}&size=32`, alt: comment.author}),\r\n                    e('span', {key: 'author', className: 'comment-author'}, comment.author),\r\n                    e('span', {key: 'time', className: 'comment-time'}, comment.timestamp)\r\n                ]),\r\n                e('div', {key: 'comment-body', className: 'comment-body'}, comment.text)\r\n            ])\r\n        );\r\n        \r\n        const commentSection = e('div', {key: 'comments', className: 'comments-section'}, [\r\n            e('h3', {key: 'title'}, `Comments (${comments.length})`),\r\n            ...commentList,\r\n            showCommentForm ? \r\n                e('div', {key: 'comment-form', className: 'comment-form'}, [\r\n                    e('textarea', {\r\n                        key: 'textarea',\r\n                        value: comment,\r\n                        onChange: (e) => setComment(e.target.value),\r\n                        placeholder: 'Add your comment...',\r\n                        rows: 3\r\n                    }),\r\n                    e('div', {key: 'buttons', className: 'form-buttons'}, [\r\n                        e('button', {\r\n                            key: 'cancel',\r\n                            className: 'cancel-btn',\r\n                            onClick: () => {\r\n                                setShowCommentForm(false);\r\n                                setComment(\"\");\r\n                            }\r\n                        }, 'Cancel'),\r\n                        e('button', {\r\n                            key: 'submit',\r\n                            className: 'submit-btn',\r\n                            onClick: handleAddComment\r\n                        }, 'Submit')\r\n                    ])\r\n                ]) :\r\n                e('button', {\r\n                    key: 'add-comment',\r\n                    className: 'add-comment-btn',\r\n                    onClick: () => setShowCommentForm(true)\r\n                }, '+ Add Comment')\r\n        ]);\r\n        \r\n        // Action buttons with added View Analytics button\r\n        const actionButtons = e('div', {key: 'actions', className: 'action-buttons'}, [\r\n            e('button', {\r\n                key: 'approve',\r\n                className: 'approve-btn',\r\n                onClick: handleApprove\r\n            }, 'Approve PR'),\r\n            e('button', {\r\n                key: 'reject',\r\n                className: 'reject-btn',\r\n                onClick: handleReject\r\n            }, 'Request Changes'),\r\n            e('button', {\r\n                key: 'view-analytics',\r\n                className: 'view-analytics-btn',\r\n                onClick: viewAnalytics\r\n            }, 'View Analytics')\r\n        ]);\r\n        \r\n        // PR Details Tab Content\r\n        prDetailsContent = e(React.Fragment, null, [\r\n            e('div', {key: 'pr-header', className: 'pr-detail-header'}, [\r\n                e('h2', {key: 'title'}, `#${selectedPR.id} - ${selectedPR.title}`),\r\n                e('div', {key: 'meta', className: 'pr-meta-details'}, [\r\n                    e('span', {key: 'status', className: `status-badge status-${selectedPR.status}`}, selectedPR.status),\r\n                    e('span', {key: 'author'}, [\r\n                        e('img', {key: 'avatar', className: 'avatar', src: `https://ui-avatars.com/api/?name=${selectedPR.author}&size=24`, alt: selectedPR.author}),\r\n                        `Author: ${selectedPR.author}`\r\n                    ]),\r\n                    // Add a \"View Analytics\" button in the header too\r\n                    e('button', {\r\n                        key: 'header-view-analytics',\r\n                        className: 'header-view-analytics-btn',\r\n                        onClick: viewAnalytics\r\n                    }, 'View Analytics')\r\n                ])\r\n            ]),\r\n            \r\n            e('div', {key: 'pr-body', className: 'pr-detail-body'}, [\r\n                e('div', {key: 'summary', className: 'ai-summary'}, [\r\n                    e('h3', {key: 'title'}, 'AI Summary'),\r\n                    e('p', {key: 'text'}, prDetails.ai_summary)\r\n                ]),\r\n                \r\n                e('div', {key: 'confidence', className: 'confidence-section'}, [\r\n                    e('h3', {key: 'title'}, 'Merge Confidence'),\r\n                    confidenceIndicator\r\n                ]),\r\n                \r\n                e('div', {key: 'quality', className: 'code-quality'}, [\r\n                    e('h3', {key: 'title'}, 'Code Quality Assessment'),\r\n                    e('pre', {key: 'text'}, prDetails.code_quality)\r\n                ]),\r\n                \r\n                \r\n                ('div', {key: 'doc-string', className: 'doc-string-section'}, [\r\n                    e('h3', {key: 'title'}, 'Documentation'),\r\n                    e('div', {key: 'content', className: 'doc-string-content'},\r\n                        e(ReactMarkdown, null, prDetails.doc_string || 'No documentation available for this PR.')\r\n                    )\r\n                ]),\r\n\r\n                e('div', {key: 'diff', className: 'diff-section'}, [\r\n                    e('h3', {key: 'title'}, 'Diff'),\r\n                    renderDiffSection(prDetails.diff)\r\n                ]),\r\n                \r\n                commentSection,\r\n                \r\n                actionButtons\r\n            ])\r\n        ]);\r\n    }\r\n\r\n    // Create main content based on active tab\r\n    let mainContent;\r\n    if (activeTab === 'details' && !selectedPR) {\r\n        mainContent = e('div', {className: 'empty-state'}, [\r\n            e('h2', {key: 'title'}, 'Pull Request Dashboard'),\r\n            e('p', {key: 'subtitle'}, 'Select a PR from the sidebar or view PR Analytics'),\r\n            e('button', {\r\n                key: 'view-analytics',\r\n                className: 'view-analytics-btn',\r\n                onClick: () => setActiveTab('analytics')\r\n            }, 'View Analytics')\r\n        ]);\r\n    } else if (activeTab === 'details' && selectedPR) {\r\n        // Show PR details tab\r\n        mainContent = prDetailsContent;\r\n    } else if (activeTab === 'analytics') {\r\n        // Show Analytics tab\r\n        mainContent = e(Analytics, { prs: prs });\r\n    }\r\n\r\n    const main = e('div', { className: 'main' }, [\r\n        tabNav,\r\n        mainContent\r\n    ]);\r\n\r\n    // Render the full container\r\n    return e('div', { className: 'container' }, [sidebar, main]);\r\n}\r\n\r\n// Render the App\r\nconst rootElement = document.getElementById('root');\r\nconst root = ReactDOM.createRoot(rootElement);\r\nroot.render(e(App));\n\n--- File: 5 # main.py ---\nfrom fastapi import FastAPI, HTTPException\r\nfrom fastapi.middleware.cors import CORSMiddleware\r\nimport snowflake.connector\r\nimport os\r\nfrom dotenv import load_dotenv\r\nimport pandas as pd\r\n\r\nload_dotenv()\r\n\r\napp = FastAPI()\r\n\r\n# Snowflake credentials\r\nSNOWFLAKE_USER = os.getenv(\"SNOWFLAKE_USER\", \"raghavg332\")\r\nSNOWFLAKE_PASSWORD = os.getenv(\"SNOWFLAKE_PASSWORD\", \"Qa29Kh4MptfGHEW\")\r\nSNOWFLAKE_ACCOUNT = os.getenv(\"SNOWFLAKE_ACCOUNT\", \"KXCIVVH-LL27432\")\r\nSNOWFLAKE_WAREHOUSE = os.getenv(\"SNOWFLAKE_WAREHOUSE\", \"COMPUTE_WH\")\r\nSNOWFLAKE_DATABASE = os.getenv(\"SNOWFLAKE_DATABASE\", \"PR_DASHBOARD\")\r\nSNOWFLAKE_SCHEMA = os.getenv(\"SNOWFLAKE_SCHEMA\", \"PUBLIC\")\r\n\r\napp.add_middleware(\r\n    CORSMiddleware,\r\n    allow_origins=[\"*\"],\r\n    allow_methods=[\"*\"],\r\n    allow_headers=[\"*\"],\r\n)\r\n\r\ndef get_conn():\r\n    \"\"\"Establish connection to Snowflake database\"\"\"\r\n    try:\r\n        conn = snowflake.connector.connect(\r\n            user=SNOWFLAKE_USER,\r\n            password=SNOWFLAKE_PASSWORD,\r\n            account=SNOWFLAKE_ACCOUNT,\r\n            warehouse=SNOWFLAKE_WAREHOUSE,\r\n            database=SNOWFLAKE_DATABASE,\r\n            schema=SNOWFLAKE_SCHEMA\r\n        )\r\n        return conn\r\n    except Exception as e:\r\n        print(f\"Error connecting to Snowflake: {e}\")\r\n        raise e\r\n\r\n@app.get(\"/api/prs\")\r\ndef fetch_all_prs():\r\n    try:\r\n        conn = get_conn()\r\n        cursor = conn.cursor(snowflake.connector.DictCursor)\r\n        \r\n        # Using cursor to get results as dictionaries\r\n        cursor.execute(\"SELECT id, title, author, status, created_at, updated_at FROM pull_requests ORDER BY updated_at DESC\")\r\n        results = cursor.fetchall()\r\n        \r\n        # Format the data to match frontend expectations\r\n        formatted_results = []\r\n        for row in results:\r\n            # Capitalize the status to match frontend expectations\r\n            status = row['STATUS'].capitalize() if row['STATUS'] else 'Unknown'\r\n            \r\n            formatted_results.append({\r\n                \"id\": row['ID'],\r\n                \"title\": row['TITLE'],\r\n                \"author\": row['AUTHOR'],\r\n                \"status\": status,\r\n                \"created_at\": row['CREATED_AT'].strftime(\"%Y-%m-%d\") if row['CREATED_AT'] else None,\r\n                \"updated_at\": row['UPDATED_AT'].strftime(\"%Y-%m-%d\") if row['UPDATED_AT'] else None\r\n            })\r\n        \r\n        cursor.close()\r\n        conn.close()\r\n        return formatted_results\r\n    except Exception as e:\r\n        print(f\"Error fetching PRs: {e}\")\r\n        raise HTTPException(status_code=500, detail=f\"Database error: {str(e)}\")\r\n\r\n@app.get(\"/api/prs/{pr_id}\")\r\ndef fetch_pr_analysis(pr_id: int):\r\n    try:\r\n        conn = get_conn()\r\n        cursor = conn.cursor(snowflake.connector.DictCursor)\r\n        \r\n        # Fetch the analysis data\r\n        cursor.execute(f\"SELECT * FROM pr_analysis WHERE pr_id = {pr_id}\")\r\n        result = cursor.fetchone()\r\n        \r\n        if not result:\r\n            cursor.close()\r\n            conn.close()\r\n            raise HTTPException(status_code=404, detail=\"PR analysis not found\")\r\n        \r\n        # Format the data to match frontend expectations\r\n        formatted_result = {\r\n            \"pr_id\": result['PR_ID'],\r\n            \"ai_summary\": result['AI_SUMMARY'],\r\n            \"merge_confidence\": result['MERGE_CONFIDENCE'].capitalize(),  # Capitalize for frontend\r\n            \"merge_confidence_score\": float(result['MERGE_CONFIDENCE']) if result.get('MERGE_CONFIDENCE') else 0,\r\n            \"code_quality\": result['CODE_QUALITY'],\r\n            \"diff\": result['DIFF'],\r\n            \"doc_string\": result['DOCSTRINGS'] if 'DOCSTRINGS' in result else \"No documentation available for this PR.\"\r\n        }\r\n        \r\n        cursor.close()\r\n        conn.close()\r\n        return formatted_result\r\n    except Exception as e:\r\n        print(f\"Error fetching PR analysis: {e}\")\r\n        raise HTTPException(status_code=500, detail=f\"Error: {str(e)}\")\r\n\r\n'''\r\n@app.get(\"/api/prs\")\r\ndef fetch_all_prs():\r\n    # Test data\r\n    test_data = [\r\n        {\r\n            \"id\": 1,\r\n            \"title\": \"Add new feature\",\r\n            \"author\": \"johndoe\",\r\n            \"status\": \"Open\",\r\n            \"created_at\": \"2023-01-01\",\r\n            \"updated_at\": \"2023-01-02\"\r\n        },\r\n        {\r\n            \"id\": 2,\r\n            \"title\": \"Fix bug in login\",\r\n            \"author\": \"janedoe\",\r\n            \"status\": \"Merged\",\r\n            \"created_at\": \"2023-01-03\",\r\n            \"updated_at\": \"2023-01-04\"\r\n        }\r\n    ]\r\n    return test_data\r\n\r\n@app.get(\"/api/prs/{pr_id}\")\r\ndef fetch_pr_analysis(pr_id: int):\r\n    # Test data\r\n    test_data = {\r\n        \"pr_id\": pr_id,\r\n        \"ai_summary\": \"This PR adds a new feature that improves user experience\",\r\n        \"merge_confidence\": \"High\",\r\n        \"code_quality\": \"Good code quality. No major issues found.\",\r\n        \"diff\": \"```\\n+ def new_function():\\n+     return 'new feature'\\n```\",\r\n        \"doc_string\": \"This PR introduces a new function that returns 'new feature'.\"\r\n    }\r\n    return test_data\r\n'''\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n": "```yaml\nsummary: |\n  CodeRoast is an **AI-powered** system that *automatically reviews pull requests* on GitHub. It uses AI to provide summaries, code quality assessments, and documentation suggestions, helping developers improve their code and keep documentation up-to-date. All the data is stored and accessible through a web app.\nrelationships:\n  - from_abstraction: 0 # Pull Request Analysis Pipeline\n    to_abstraction: 1 # Groq API Integration\n    label: \"Uses AI models\"\n  - from_abstraction: 0 # Pull Request Analysis Pipeline\n    to_abstraction: 2 # Tree-sitter Based Code Parsing\n    label: \"Parses code changes\"\n  - from_abstraction: 0 # Pull Request Analysis Pipeline\n    to_abstraction: 3 # Snowflake Data Storage\n    label: \"Stores analysis results\"\n  - from_abstraction: 0 # Pull Request Analysis Pipeline\n    to_abstraction: 4 # RAG (Retrieval-Augmented Generation) Documentation Updates\n    label: \"Suggests doc updates\"\n  - from_abstraction: 0 # Pull Request Analysis Pipeline\n    to_abstraction: 8 # PR Review Comment Posting (GitHub)\n    label: \"Posts review comments\"\n  - from_abstraction: 5 # GitHub Webhook Integration\n    to_abstraction: 6 # FastAPI Backend API\n    label: \"Triggers analysis\"\n  - from_abstraction: 6 # FastAPI Backend API\n    to_abstraction: 0 # Pull Request Analysis Pipeline\n    label: \"Orchestrates analysis\"\n  - from_abstraction: 6 # FastAPI Backend API\n    to_abstraction: 7 # Frontend Web Application (React)\n    label: \"Serves data to\"\n  - from_abstraction: 1 # Groq API Integration\n    to_abstraction: 3 # Snowflake Data Storage\n    label: \"Writes data to\"\n  - from_abstraction: 2 # Tree-sitter Based Code Parsing\n    to_abstraction: 3 # Snowflake Data Storage\n    label: \"Writes extracted data to\"\n  - from_abstraction: 4 # RAG (Retrieval-Augmented Generation) Documentation Updates\n    to_abstraction: 1 # Groq API Integration\n    label: \"Uses AI to generate\"\n```", "\nGiven the following project abstractions and their relationships for the project ```` CodeRoast ````:\n\nAbstractions (Index # Name):\n- 0 # Pull Request Analysis Pipeline\n- 1 # Groq API Integration\n- 2 # Tree-sitter Based Code Parsing\n- 3 # Snowflake Data Storage\n- 4 # RAG (Retrieval-Augmented Generation) Documentation Updates\n- 5 # GitHub Webhook Integration\n- 6 # FastAPI Backend API\n- 7 # Frontend Web Application (React)\n- 8 # PR Review Comment Posting (GitHub)\n\nContext about relationships and project summary:\nProject Summary:\nCodeRoast is an **AI-powered** system that *automatically reviews pull requests* on GitHub. It uses AI to provide summaries, code quality assessments, and documentation suggestions, helping developers improve their code and keep documentation up-to-date. All the data is stored and accessible through a web app.\n\n\nRelationships (Indices refer to abstractions above):\n- From 0 (Pull Request Analysis Pipeline) to 1 (Groq API Integration): Uses AI models\n- From 0 (Pull Request Analysis Pipeline) to 2 (Tree-sitter Based Code Parsing): Parses code changes\n- From 0 (Pull Request Analysis Pipeline) to 3 (Snowflake Data Storage): Stores analysis results\n- From 0 (Pull Request Analysis Pipeline) to 4 (RAG (Retrieval-Augmented Generation) Documentation Updates): Suggests doc updates\n- From 0 (Pull Request Analysis Pipeline) to 8 (PR Review Comment Posting (GitHub)): Posts review comments\n- From 5 (GitHub Webhook Integration) to 6 (FastAPI Backend API): Triggers analysis\n- From 6 (FastAPI Backend API) to 0 (Pull Request Analysis Pipeline): Orchestrates analysis\n- From 6 (FastAPI Backend API) to 7 (Frontend Web Application (React)): Serves data to\n- From 1 (Groq API Integration) to 3 (Snowflake Data Storage): Writes data to\n- From 2 (Tree-sitter Based Code Parsing) to 3 (Snowflake Data Storage): Writes extracted data to\n- From 4 (RAG (Retrieval-Augmented Generation) Documentation Updates) to 1 (Groq API Integration): Uses AI to generate\n\n\nIf you are going to make a tutorial for ```` CodeRoast ````, what is the best order to explain these abstractions, from first to last?\nIdeally, first explain those that are the most important or foundational, perhaps user-facing concepts or entry points. Then move to more detailed, lower-level implementation details or supporting concepts.\n\nOutput the ordered list of abstraction indices, including the name in a comment for clarity. Use the format `idx # AbstractionName`.\n\n```yaml\n- 2 # FoundationalConcept\n- 0 # CoreClassA\n- 1 # CoreClassB (uses CoreClassA)\n- ...\n```\n\nNow, provide the YAML output:\n": "```yaml\n- 7 # Frontend Web Application (React)\n- 6 # FastAPI Backend API\n- 5 # GitHub Webhook Integration\n- 0 # Pull Request Analysis Pipeline\n- 2 # Tree-sitter Based Code Parsing\n- 1 # Groq API Integration\n- 4 # RAG (Retrieval-Augmented Generation) Documentation Updates\n- 8 # PR Review Comment Posting (GitHub)\n- 3 # Snowflake Data Storage\n```\n", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `CodeRoast` about the concept: \"Frontend Web Application (React)\". This is Chapter 1.\n\nConcept Details:\n- Description:\nThe React-based frontend provides a user interface to view PRs, AI reviews, documentation updates, and analytics. It's the window into the system, allowing users to explore the insights generated by the analysis pipeline. The frontend is also responsible for displaying the diff and other elements.\n\n\nComplete Tutorial Structure:\n1. [Frontend Web Application (React)](01_frontend_web_application__react_.md)\n2. [FastAPI Backend API](02_fastapi_backend_api.md)\n3. [GitHub Webhook Integration](03_github_webhook_integration.md)\n4. [Pull Request Analysis Pipeline](04_pull_request_analysis_pipeline.md)\n5. [Tree-sitter Based Code Parsing](05_tree_sitter_based_code_parsing.md)\n6. [Groq API Integration](06_groq_api_integration.md)\n7. [RAG (Retrieval-Augmented Generation) Documentation Updates](07_rag__retrieval_augmented_generation__documentation_updates.md)\n8. [PR Review Comment Posting (GitHub)](08_pr_review_comment_posting__github_.md)\n9. [Snowflake Data Storage](09_snowflake_data_storage.md)\n\nContext from previous chapters (summary):\nThis is the first chapter.\n\nRelevant Code Snippets:\n--- File: README.md ---\n# \ud83e\udd16 Code Roast : An AI-Powered Pull Request Review System\n\n> An end-to-end intelligent platform for automated code review, documentation updates, and pull request analysis \u2014 powered by LLMs, Tree-sitter, and Snowflake.\n\n---\n\n## \ud83d\ude80 Overview\n\nThis system integrates with GitHub to automatically:\n- Analyze pull requests using AI (via [Groq](https://groq.com))\n- Parse and understand diffs using [Tree-sitter](https://tree-sitter.github.io/)\n- Perform RAG (Retrieval-Augmented Generation) to update project documentation\n- Store all insights in **Snowflake** for analytics\n- Display all PRs, diffs, AI feedback, and doc suggestions via a beautiful **Web App**\n\nWhether you're an engineering team lead, reviewer, or contributor \u2014 you get fast, reliable, and insightful AI-powered review feedback at every step.\n\n---\n\n## \ud83e\udde0 Features\n\n- \u2705 **AI PR Summary**  \n  Instant analysis of pull request purpose, syntax/style/functionality, and merge confidence.\n\n- \ud83d\udcdd **Automatic Docstring Suggestions**  \n  Extract or generate missing/updated docstrings for new or changed functions.\n\n- \ud83d\udcd8 **RAG-based Documentation Updates**  \n  Contextual retrieval from `project_docs.txt` with AI-generated Markdown additions.\n\n- \ud83d\udcc4 **Code Diff Parsing**  \n  Tree-sitter-based code structure diffing to isolate changed functions only.\n\n- \ud83d\udcac **GitHub Comment Posting**  \n  Summaries are posted as comments directly on PRs, no need to open another tool.\n\n- \ud83d\udcca **Snowflake Integration**  \n  All AI-generated insights and PR metadata stored for reporting, audit, and search.\n\n- \ud83c\udf10 **Web App Dashboard**  \n  Browse all PRs, view diffs, AI reviews, and doc updates in a single UI.\n\n---\n\n## \ud83e\uddf1 Tech Stack\n\n| Layer              | Tech Used                                      |\n|--------------------|-----------------------------------------------|\n| **AI Models**       | [Groq API (LLaMA 3)](https://groq.com)        |\n| **Diff Parsing**    | [Tree-sitter](https://tree-sitter.github.io/) |\n| **Backend API**     | [FastAPI](https://fastapi.tiangolo.com)       |\n| **Git Integration** | GitHub App + `PyGithub`                       |\n| **RAG Engine**      | Basic keyword-matching from local docs        |\n| **Database**        | [Snowflake](https://www.snowflake.com)        |\n| **Frontend**        | HTML, JS, CSS (Markdown + Diff Viewers)       |\n\n---\n\n## \u2699\ufe0f How It Works\n\n1. \ud83d\udd14 **GitHub Webhook Trigger**\n   - On PR open/update \u2192 sends payload to FastAPI backend\n\n2. \ud83e\udde0 **AI Review Processing**\n   - Extracts code diffs, changed functions\n   - Groq generates AI summary, confidence rating, and docstrings\n\n3. \ud83d\udcd8 **Documentation Updates**\n   - Local project docs indexed via simple RAG\n   - Groq suggests Markdown additions\n\n4. \ud83d\udcbe **Data Storage**\n   - All results are saved in Snowflake for dashboard & analytics\n\n5. \ud83c\udf10 **Web App UI**\n   - Explore PRs, view diffs, AI reviews, and doc updates\n\n\n--- File: frontend/app.js ---\nconst e = React.createElement;\r\n\r\nfunction parseDiff(diff) {\r\n    const lines = diff.split('\\n');\r\n    const parsedLines = [];\r\n\r\n    let section = null;\r\n\r\n    for (let i = 0; i < lines.length; i++) {\r\n        const line = lines[i];\r\n\r\n        if (line.startsWith('File:') || line.startsWith('Function:')) {\r\n            parsedLines.push({ type: 'context', content: line });\r\n            section = null;\r\n        } else if (line.trim() === 'Old Code:') {\r\n            parsedLines.push({ type: 'context', content: 'Old Code:' });\r\n            section = 'old';\r\n\r\n            // Handle empty old code\r\n            if (lines[i + 1]?.trim() === 'New Code:') {\r\n                parsedLines.push({ type: 'old', content: '<NO PREVIOUS CODE>' });\r\n            }\r\n        } else if (line.trim() === 'New Code:') {\r\n            parsedLines.push({ type: 'context', content: 'New Code:' });\r\n            section = 'new';\r\n        } else if (section === 'old' || section === 'new') {\r\n            parsedLines.push({ type: section, content: line });\r\n        } else {\r\n            parsedLines.push({ type: 'context', content: line });\r\n        }\r\n    }\r\n\r\n    return parsedLines;\r\n}\r\n\r\n\r\n// Analytics component to visualize PR data\r\nfunction Analytics({ prs }) {\r\n    // Count PRs by status\r\n    const statusCounts = prs.reduce((acc, pr) => {\r\n        acc[pr.status] = (acc[pr.status] || 0) + 1;\r\n        return acc;\r\n    }, {});\r\n\r\n    // Count PRs by author\r\n    const authorCounts = prs.reduce((acc, pr) => {\r\n        acc[pr.author] = (acc[pr.author] || 0) + 1;\r\n        return acc;\r\n    }, {});\r\n\r\n    // Calculate PRs over time (by month)\r\n    const prsByMonth = prs.reduce((acc, pr) => {\r\n        const date = new Date(pr.created_at);\r\n        const monthYear = date.toLocaleString('default', { month: 'short', year: 'numeric' });\r\n        acc[monthYear] = (acc[monthYear] || 0) + 1;\r\n        return acc;\r\n    }, {});\r\n\r\n    // Create data arrays for charts\r\n    const statusData = Object.entries(statusCounts).map(([status, count]) => ({\r\n        status,\r\n        count,\r\n        color: status === 'Open' ? '#17a2b8' : \r\n               status === 'Merged' ? '#28a745' : \r\n               status === 'Closed' ? '#dc3545' : '#6c757d'\r\n    }));\r\n\r\n    const authorData = Object.entries(authorCounts)\r\n        .sort((a, b) => b[1] - a[1])\r\n        .slice(0, 5)\r\n        .map(([author, count]) => ({ author, count }));\r\n\r\n    const timelineData = Object.entries(prsByMonth)\r\n        .sort((a, b) => new Date(a[0]) - new Date(b[0]))\r\n        .map(([month, count]) => ({ month, count }));\r\n\r\n    return e('div', { className: 'analytics-container' }, [\r\n        e('h2', { key: 'title', className: 'analytics-title' }, 'Pull Request Analytics'),\r\n\r\n        // Status distribution chart\r\n        e('div', { key: 'status-chart', className: 'chart-container' }, [\r\n            e('h3', { key: 'status-title' }, 'PR Status Distribution'),\r\n            e('div', { key: 'status-bars', className: 'status-chart' }, \r\n                statusData.map(item => \r\n                    e('div', { key: item.status, className: 'status-bar-container' }, [\r\n                        e('div', { key: 'label', className: 'status-label' }, `${item.status} (${item.count})`),\r\n                        e('div', { key: 'bar-bg', className: 'status-bar-bg' }, \r\n                            e('div', { \r\n                                key: 'bar', \r\n                                className: 'status-bar', \r\n                                style: { \r\n                                    width: `${(item.count / prs.length) * 100}%`,\r\n                                    backgroundColor: item.color\r\n                                } \r\n                            })\r\n                        )\r\n                    ])\r\n                )\r\n            )\r\n        ]),\r\n\r\n        // Author contribution chart\r\n        e('div', { key: 'author-chart', className: 'chart-container' }, [\r\n            e('h3', { key: 'author-title' }, 'Top Contributors'),\r\n            e('div', { key: 'author-bars', className: 'author-chart' }, \r\n                authorData.map(item => \r\n                    e('div', { key: item.author, className: 'author-bar-container' }, [\r\n                        e('div', { key: 'label', className: 'author-label' }, [\r\n                            e('img', { \r\n                                key: 'avatar', \r\n                                className: 'avatar', \r\n                                src: `https://ui-avatars.com/api/?name=${item.author}&size=24`, \r\n                                alt: item.author\r\n                            }),\r\n                            `${item.author} (${item.count})`\r\n                        ]),\r\n                        e('div', { key: 'bar-bg', className: 'author-bar-bg' }, \r\n                            e('div', { \r\n                                key: 'bar', \r\n                                className: 'author-bar', \r\n                                style: { \r\n                                    width: `${(item.count / Math.max(...Object.values(authorCounts))) * 100}%`\r\n                                } \r\n                            })\r\n                        )\r\n                    ])\r\n                )\r\n            )\r\n        ]),\r\n\r\n        // Timeline chart\r\n        e('div', { key: 'timeline-chart', className: 'chart-container' }, [\r\n            e('h3', { key: 'timeline-title' }, 'PR Activity Over Time'),\r\n            e('div', { key: 'timeline', className: 'timeline-chart' }, \r\n                timelineData.map((item, index) => \r\n                    e('div', { key: item.month, className: 'timeline-bar-container' }, [\r\n                        e('div', { \r\n                            key: 'bar', \r\n                            className: 'timeline-bar', \r\n                            style: { \r\n                                height: `${(item.count / Math.max(...timelineData.map(d => d.count))) * 100}px`\r\n                            } \r\n                        }),\r\n                        e('div', { key: 'label', className: 'timeline-label' }, item.month)\r\n                    ])\r\n                )\r\n            )\r\n        ]),\r\n\r\n        // Summary stats\r\n        e('div', { key: 'summary-stats', className: 'summary-stats' }, [\r\n            e('div', { key: 'total', className: 'stat-card' }, [\r\n                e('h3', { key: 'title' }, 'Total PRs'),\r\n                e('div', { key: 'value', className: 'stat-value' }, prs.length)\r\n            ]),\r\n            e('div', { key: 'open', className: 'stat-card' }, [\r\n                e('h3', { key: 'title' }, 'Open PRs'),\r\n                e('div', { key: 'value', className: 'stat-value' }, statusCounts['Open'] || 0)\r\n            ]),\r\n            e('div', { key: 'merged', className: 'stat-card' }, [\r\n                e('h3', { key: 'title' }, 'Merged PRs'),\r\n                e('div', { key: 'value', className: 'stat-value' }, statusCounts['Merged'] || 0)\r\n            ]),\r\n            e('div', { key: 'contributors', className: 'stat-card' }, [\r\n                e('h3', { key: 'title' }, 'Contributors'),\r\n                e('div', { key: 'value', className: 'stat-value' }, Object.keys(authorCounts).length)\r\n            ])\r\n        ])\r\n    ]);\r\n}\r\n\r\nfunction renderDiffSection(diff) {\r\n    const diffLines = parseDiff(diff);\r\n\r\n    return React.createElement(\r\n        'pre',\r\n        { className: 'diff-content' },\r\n        diffLines.map((line, index) => {\r\n            const className =\r\n                line.type === 'old' ? 'diff-old-code diff-code-line' :\r\n                line.type === 'new' ? 'diff-new-code diff-code-line' :\r\n                'diff-context-line';\r\n\r\n            return React.createElement('div', { key: index, className }, line.content);\r\n        })\r\n    );\r\n}\r\n\r\nconst ReactMarkdown = window.ReactMarkdown || (() => null) // fallback if not loaded\r\n\r\n\r\n// Main App component\r\nfunction App() {\r\n    const [prs, setPrs] = React.useState([]);\r\n    const [selectedPR, setSelectedPR] = React.useState(null);\r\n    const [prDetails, setPRDetails] = React.useState(null);\r\n    const [filter, setFilter] = React.useState(\"all\");\r\n    const [searchQuery, setSearchQuery] = React.useState(\"\");\r\n    const [isLoading, setIsLoading] = React.useState(true);\r\n    const [showCommentForm, setShowCommentForm] = React.useState(false);\r\n    const [comment, setComment] = React.useState(\"\");\r\n    const [comments, setComments] = React.useState([]);\r\n    const [darkMode, setDarkMode] = React.useState(false);\r\n    const [activeTab, setActiveTab] = React.useState(\"details\"); // \"details\" or \"analytics\"\r\n\r\n    React.useEffect(() => {\r\n        setIsLoading(true);\r\n        fetch('http://localhost:8000/api/prs')\r\n            .then(res => res.json())\r\n            .then(data => {\r\n                setPrs(data);\r\n                setIsLoading(false);\r\n            })\r\n            .catch(err => {\r\n                console.error('Error fetching PRs:', err);\r\n                setIsLoading(false);\r\n            });\r\n    }, []);\r\n\r\n    React.useEffect(() => {\r\n        // Apply dark mode class to body\r\n        if (darkMode) {\r\n            document.body.classList.add('dark-mode');\r\n        } else {\r\n            document.body.classList.remove('dark-mode');\r\n        }\r\n    }, [darkMode]);\r\n\r\n    const fetchDetails = (id) => {\r\n        setIsLoading(true);\r\n        fetch(`http://localhost:8000/api/prs/${id}`)\r\n            .then(res => res.json())\r\n            .then(data => {\r\n                setPRDetails(data);\r\n                setIsLoading(false);\r\n                // Simulate fetching comments\r\n                setComments([\r\n                    { id: 1, author: \"codereviewer\", text: \"Looks good to me!\", timestamp: \"2 hours ago\" },\r\n                    { id: 2, author: \"securityexpert\", text: \"We should add input validation here.\", timestamp: \"1 hour ago\" }\r\n                ]);\r\n            })\r\n            .catch(err => {\r\n                console.error('Error fetching PR details:', err);\r\n                setIsLoading(false);\r\n            });\r\n    };\r\n\r\n    const selectPR = (pr) => {\r\n        setSelectedPR(pr);\r\n        setActiveTab(\"details\"); // Switch to details tab when selecting a PR\r\n        fetchDetails(pr.id);\r\n    };\r\n\r\n    const handleAddComment = () => {\r\n        if (comment.trim() === \"\") return;\r\n        \r\n        const newComment = {\r\n            id: comments.length + 1,\r\n            author: \"you\",\r\n            text: comment,\r\n            timestamp: \"Just now\"\r\n        };\r\n        \r\n        setComments([...comments, newComment]);\r\n        setComment(\"\");\r\n        setShowCommentForm(false);\r\n    };\r\n\r\n    const handleApprove = () => {\r\n        alert(\"PR approved! (This would trigger an API call in production)\");\r\n    };\r\n\r\n    const handleReject = () => {\r\n        alert(\"PR rejected! (This would trigger an API call in production)\");\r\n    };\r\n\r\n    // Function to switch to analytics tab\r\n    const viewAnalytics = () => {\r\n        setActiveTab(\"analytics\");\r\n    };\r\n\r\n    // Filter PRs based on status and search query\r\n    const filteredPRs = prs.filter(pr => {\r\n        const matchesFilter = filter === \"all\" || pr.status === filter;\r\n        const matchesSearch = pr.title.toLowerCase().includes(searchQuery.toLowerCase()) || \r\n                             pr.author.toLowerCase().includes(searchQuery.toLowerCase());\r\n        return matchesFilter && matchesSearch;\r\n    });\r\n\r\n    // Create PR list items\r\n    const prItems = filteredPRs.map(pr => \r\n        e('div', { \r\n            key: pr.id, \r\n            className: selectedPR && selectedPR.id === pr.id ? 'pr-item selected' : 'pr-item', \r\n            onClick: () => selectPR(pr) \r\n        }, [\r\n            e('div', {key: 'pr-header', className: 'pr-header'}, [\r\n                e('strong', {key: 'id'}, `#${pr.id}`),\r\n                e('span', {\r\n                    key: 'status',\r\n                    className: `pr-status status-${pr.status}`\r\n                }, pr.status)\r\n            ]),\r\n            e('div', {key: 'pr-title', className: 'pr-title'}, pr.title),\r\n            e('div', {key: 'pr-meta', className: 'pr-meta'}, [\r\n                e('span', {key: 'author'}, [\r\n                    e('img', {key: 'avatar', className: 'avatar', src: `https://ui-avatars.com/api/?name=${pr.author}&size=24`, alt: pr.author}),\r\n                    pr.author\r\n                ]),\r\n                e('span', {key: 'date', className: 'date'}, 'Updated: ' + (pr.updated_at || 'N/A'))\r\n            ])\r\n        ])\r\n    );\r\n\r\n    // Create filter controls\r\n    const filterControls = e('div', {className: 'filter-controls'}, [\r\n        e('div', {key: 'search', className: 'search-box'}, [\r\n            e('input', {\r\n                type: 'text',\r\n                placeholder: 'Search PRs...',\r\n                value: searchQuery,\r\n                onChange: (e) => setSearchQuery(e.target.value),\r\n                className: 'search-input'\r\n            })\r\n        ]),\r\n        e('div', {key: 'filters', className: 'status-filters'}, [\r\n            e('button', {\r\n                className: filter === 'all' ? 'filter-btn active' : 'filter-btn',\r\n                onClick: () => setFilter('all')\r\n            }, 'All'),\r\n            e('button', {\r\n                className: filter === 'Open' ? 'filter-btn active' : 'filter-btn',\r\n                onClick: () => setFilter('Open')\r\n            }, 'Open'),\r\n            e('button', {\r\n                className: filter === 'Merged' ? 'filter-btn active' : 'filter-btn',\r\n                onClick: () => setFilter('Merged')\r\n            }, 'Merged'),\r\n            e('button', {\r\n                className: filter === 'Closed' ? 'filter-btn active' : 'filter-btn',\r\n                onClick: () => setFilter('Closed')\r\n            }, 'Closed')\r\n        ])\r\n    ]);\r\n\r\n    // Create sidebar\r\n    const sidebar = e('div', { className: 'sidebar' }, [\r\n        e('div', {key: 'header', className: 'sidebar-header'}, [\r\n            e('h2', {key: 'title'}, '\ud83d\udccb Pull Requests'),\r\n            e('button', {\r\n                key: 'theme-toggle',\r\n                className: 'theme-toggle',\r\n                onClick: () => setDarkMode(!darkMode)\r\n            }, darkMode ? '\u2600\ufe0f' : '\ud83c\udf19')\r\n        ]),\r\n        filterControls,\r\n        e('div', {key: 'pr-list', className: 'pr-list'}, isLoading ? \r\n            e('div', {className: 'loading'}, 'Loading PRs...') : \r\n            prItems.length > 0 ? prItems : e('div', {className: 'no-results'}, 'No PRs match your filters'))\r\n    ]);\r\n\r\n    // Create tab navigation - show it regardless of whether a PR is selected\r\n    const tabNav = e('div', {className: 'tab-navigation'}, [\r\n        e('button', {\r\n            key: 'details-tab',\r\n            className: activeTab === 'details' ? 'tab-btn active' : 'tab-btn',\r\n            onClick: () => setActiveTab('details'),\r\n            disabled: !selectedPR\r\n        }, 'PR Details'),\r\n        e('button', {\r\n            key: 'analytics-tab',\r\n            className: activeTab === 'analytics' ? 'tab-btn active' : 'tab-btn',\r\n            onClick: () => setActiveTab('analytics')\r\n        }, 'Analytics')\r\n    ]);\r\n\r\n    // Create PR details content\r\n    let prDetailsContent;\r\n    if (isLoading && selectedPR) {\r\n        prDetailsContent = e('div', {className: 'loading-container'}, [\r\n            e('div', {key: 'spinner', className: 'loading-spinner'}),\r\n            e('p', {key: 'text'}, 'Loading PR details...')\r\n        ]);\r\n    } else if (!prDetails && selectedPR) {\r\n        prDetailsContent = e('p', null, 'No details available for this PR');\r\n    } else if (selectedPR) {\r\n        // PR details content\r\n        const score = Number(prDetails.merge_confidence_score || 0); // 0-10\r\n        const maxScore = 10;\r\n        const percent = Math.min((score / maxScore) * 100, 100);\r\n        const confidenceIndicator = e('div', { key: 'confidence-indicator', className: 'confidence-gradient-wrapper' }, [\r\n            e('div', { className: 'confidence-gradient-bar' }, [\r\n                e('div', {\r\n                    className: 'confidence-score-indicator',\r\n                    style: { left: `${percent}%` }\r\n                })\r\n            ]),\r\n            e('div', { style: { marginTop: '8px', fontWeight: 600 } }, `Score: ${score}/10`)\r\n        ]);\r\n        \r\n        // Comment section\r\n        const commentList = comments.map(comment => \r\n            e('div', {key: `comment-${comment.id}`, className: 'comment'}, [\r\n                e('div', {key: 'comment-header', className: 'comment-header'}, [\r\n                    e('img', {key: 'avatar', className: 'avatar', src: `https://ui-avatars.com/api/?name=${comment.author}&size=32`, alt: comment.author}),\r\n                    e('span', {key: 'author', className: 'comment-author'}, comment.author),\r\n                    e('span', {key: 'time', className: 'comment-time'}, comment.timestamp)\r\n                ]),\r\n                e('div', {key: 'comment-body', className: 'comment-body'}, comment.text)\r\n            ])\r\n        );\r\n        \r\n        const commentSection = e('div', {key: 'comments', className: 'comments-section'}, [\r\n            e('h3', {key: 'title'}, `Comments (${comments.length})`),\r\n            ...commentList,\r\n            showCommentForm ? \r\n                e('div', {key: 'comment-form', className: 'comment-form'}, [\r\n                    e('textarea', {\r\n                        key: 'textarea',\r\n                        value: comment,\r\n                        onChange: (e) => setComment(e.target.value),\r\n                        placeholder: 'Add your comment...',\r\n                        rows: 3\r\n                    }),\r\n                    e('div', {key: 'buttons', className: 'form-buttons'}, [\r\n                        e('button', {\r\n                            key: 'cancel',\r\n                            className: 'cancel-btn',\r\n                            onClick: () => {\r\n                                setShowCommentForm(false);\r\n                                setComment(\"\");\r\n                            }\r\n                        }, 'Cancel'),\r\n                        e('button', {\r\n                            key: 'submit',\r\n                            className: 'submit-btn',\r\n                            onClick: handleAddComment\r\n                        }, 'Submit')\r\n                    ])\r\n                ]) :\r\n                e('button', {\r\n                    key: 'add-comment',\r\n                    className: 'add-comment-btn',\r\n                    onClick: () => setShowCommentForm(true)\r\n                }, '+ Add Comment')\r\n        ]);\r\n        \r\n        // Action buttons with added View Analytics button\r\n        const actionButtons = e('div', {key: 'actions', className: 'action-buttons'}, [\r\n            e('button', {\r\n                key: 'approve',\r\n                className: 'approve-btn',\r\n                onClick: handleApprove\r\n            }, 'Approve PR'),\r\n            e('button', {\r\n                key: 'reject',\r\n                className: 'reject-btn',\r\n                onClick: handleReject\r\n            }, 'Request Changes'),\r\n            e('button', {\r\n                key: 'view-analytics',\r\n                className: 'view-analytics-btn',\r\n                onClick: viewAnalytics\r\n            }, 'View Analytics')\r\n        ]);\r\n        \r\n        // PR Details Tab Content\r\n        prDetailsContent = e(React.Fragment, null, [\r\n            e('div', {key: 'pr-header', className: 'pr-detail-header'}, [\r\n                e('h2', {key: 'title'}, `#${selectedPR.id} - ${selectedPR.title}`),\r\n                e('div', {key: 'meta', className: 'pr-meta-details'}, [\r\n                    e('span', {key: 'status', className: `status-badge status-${selectedPR.status}`}, selectedPR.status),\r\n                    e('span', {key: 'author'}, [\r\n                        e('img', {key: 'avatar', className: 'avatar', src: `https://ui-avatars.com/api/?name=${selectedPR.author}&size=24`, alt: selectedPR.author}),\r\n                        `Author: ${selectedPR.author}`\r\n                    ]),\r\n                    // Add a \"View Analytics\" button in the header too\r\n                    e('button', {\r\n                        key: 'header-view-analytics',\r\n                        className: 'header-view-analytics-btn',\r\n                        onClick: viewAnalytics\r\n                    }, 'View Analytics')\r\n                ])\r\n            ]),\r\n            \r\n            e('div', {key: 'pr-body', className: 'pr-detail-body'}, [\r\n                e('div', {key: 'summary', className: 'ai-summary'}, [\r\n                    e('h3', {key: 'title'}, 'AI Summary'),\r\n                    e('p', {key: 'text'}, prDetails.ai_summary)\r\n                ]),\r\n                \r\n                e('div', {key: 'confidence', className: 'confidence-section'}, [\r\n                    e('h3', {key: 'title'}, 'Merge Confidence'),\r\n                    confidenceIndicator\r\n                ]),\r\n                \r\n                e('div', {key: 'quality', className: 'code-quality'}, [\r\n                    e('h3', {key: 'title'}, 'Code Quality Assessment'),\r\n                    e('pre', {key: 'text'}, prDetails.code_quality)\r\n                ]),\r\n                \r\n                \r\n                ('div', {key: 'doc-string', className: 'doc-string-section'}, [\r\n                    e('h3', {key: 'title'}, 'Documentation'),\r\n                    e('div', {key: 'content', className: 'doc-string-content'},\r\n                        e(ReactMarkdown, null, prDetails.doc_string || 'No documentation available for this PR.')\r\n                    )\r\n                ]),\r\n\r\n                e('div', {key: 'diff', className: 'diff-section'}, [\r\n                    e('h3', {key: 'title'}, 'Diff'),\r\n                    renderDiffSection(prDetails.diff)\r\n                ]),\r\n                \r\n                commentSection,\r\n                \r\n                actionButtons\r\n            ])\r\n        ]);\r\n    }\r\n\r\n    // Create main content based on active tab\r\n    let mainContent;\r\n    if (activeTab === 'details' && !selectedPR) {\r\n        mainContent = e('div', {className: 'empty-state'}, [\r\n            e('h2', {key: 'title'}, 'Pull Request Dashboard'),\r\n            e('p', {key: 'subtitle'}, 'Select a PR from the sidebar or view PR Analytics'),\r\n            e('button', {\r\n                key: 'view-analytics',\r\n                className: 'view-analytics-btn',\r\n                onClick: () => setActiveTab('analytics')\r\n            }, 'View Analytics')\r\n        ]);\r\n    } else if (activeTab === 'details' && selectedPR) {\r\n        // Show PR details tab\r\n        mainContent = prDetailsContent;\r\n    } else if (activeTab === 'analytics') {\r\n        // Show Analytics tab\r\n        mainContent = e(Analytics, { prs: prs });\r\n    }\r\n\r\n    const main = e('div', { className: 'main' }, [\r\n        tabNav,\r\n        mainContent\r\n    ]);\r\n\r\n    // Render the full container\r\n    return e('div', { className: 'container' }, [sidebar, main]);\r\n}\r\n\r\n// Render the App\r\nconst rootElement = document.getElementById('root');\r\nconst root = ReactDOM.createRoot(rootElement);\r\nroot.render(e(App));\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 1: Frontend Web Application (React)`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "# Chapter 1: Frontend Web Application (React)\n\nWelcome to CodeRoast! In this chapter, we'll be diving into the heart of the user experience: the frontend web application built with React. Imagine you're a code reviewer, and you want to quickly see all the Pull Requests (PRs) needing your attention, along with AI-powered feedback. Our React frontend makes that happen!\n\nThis chapter will guide you through understanding how this frontend works and how it displays all the amazing insights generated by the rest of the CodeRoast system.\n\n## What is a Frontend, and Why React?\n\nThink of a frontend like the dashboard of a car. You see the speedometer, fuel gauge, and warning lights, but you don't need to know *how* the engine works to drive. Similarly, our frontend lets you interact with CodeRoast without worrying about the complex AI and analysis happening behind the scenes.\n\nReact is a popular JavaScript library for building user interfaces. It helps us create dynamic and interactive web pages. We chose React because it's efficient, organized, and makes it easier to manage complex user interfaces like our PR dashboard.\n\n## Key Concepts\n\nLet's break down the key concepts you'll encounter in our React frontend:\n\n1.  **Components:** React is all about components. Think of them as building blocks for your UI. Each component is responsible for rendering a specific part of the page, like a PR list, a diff view, or an analytics chart.\n\n2.  **JSX:** React uses JSX, which looks like HTML but is actually JavaScript. It allows us to write UI elements directly in our JavaScript code, making it more readable and maintainable.\n\n3.  **State:** State is data that can change over time, and when it changes, React automatically updates the UI. For example, the selected PR, the filter applied to the PR list, or the current theme (light/dark) are all parts of the component's state.\n\n4.  **Props:** Props (short for properties) are how we pass data from a parent component to a child component. Imagine passing a message from one React component to another.\n\n## The CodeRoast Frontend: A Tour\n\nLet's look at how these concepts come together in our `CodeRoast` frontend, using code snippets from `frontend/app.js`.\n\n### Displaying a List of Pull Requests\n\nFirst, let's see how we display the list of PRs. We fetch the PR data from our backend API (more on that in [Chapter 2: FastAPI Backend API](02_fastapi_backend_api.md)) and store it in the component's state.\n\n```javascript\nconst [prs, setPrs] = React.useState([]);\n\nReact.useEffect(() => {\n    fetch('http://localhost:8000/api/prs')\n        .then(res => res.json())\n        .then(data => {\n            setPrs(data);\n        });\n}, []);\n```\n\nThis code does the following:\n\n*   `const [prs, setPrs] = React.useState([]);`: This line initializes a state variable called `prs` to an empty array. `prs` will hold our pull request data. `setPrs` is a function used to update the `prs` data, and React will automatically refresh the user interface whenever you use `setPrs`.\n\n*   `React.useEffect(() => { ... }, []);`: This is a React Hook that runs code when the component is first rendered. The empty array `[]` as the second argument means this effect will only run once when the component mounts.\n\n*   `fetch('http://localhost:8000/api/prs')`: This line makes a request to our backend API to get the list of PRs.\n\n*   `.then(res => res.json())`: This converts the response from the API into a JavaScript object (JSON).\n\n*   `.then(data => { setPrs(data); })`: This updates the `prs` state with the data we received from the API.\n\nNext, we map over the `prs` array to create a list of PR items:\n\n```javascript\nconst prItems = filteredPRs.map(pr => \n    e('div', { \n        key: pr.id, \n        className: selectedPR && selectedPR.id === pr.id ? 'pr-item selected' : 'pr-item', \n        onClick: () => selectPR(pr) \n    }, [\n        e('div', {key: 'pr-header', className: 'pr-header'}, [\n            e('strong', {key: 'id'}, `#${pr.id}`),\n            e('span', {\n                key: 'status',\n                className: `pr-status status-${pr.status}`\n            }, pr.status)\n        ]),\n        e('div', {key: 'pr-title', className: 'pr-title'}, pr.title),\n        e('div', {key: 'pr-meta', className: 'pr-meta'}, [\n            e('span', {key: 'author'}, [\n                e('img', {key: 'avatar', className: 'avatar', src: `https://ui-avatars.com/api/?name=${pr.author}&size=24`, alt: pr.author}),\n                pr.author\n            ]),\n            e('span', {key: 'date', className: 'date'}, 'Updated: ' + (pr.updated_at || 'N/A'))\n        ])\n    ])\n);\n```\n\nThis code takes each `pr` from the `filteredPRs` array and:\n\n*   `e('div', { ... }, [ ... ])`: Creates a `div` element (and nested elements) for each PR, setting its `key`, `className`, and `onClick` properties. The `key` prop is important for React to efficiently update the list. `e` is `React.createElement`, a function that creates React elements. It is used because JSX must be transformed at compile time.\n\n*   `className`: Sets the CSS class for styling.\n\n*   `onClick`: Defines a function that will be called when the PR item is clicked.\n\n*   Within the div, `e(...)` is used to create other html elements to display the PR's `id`, `status`, `title`, `author` and `updated_at`.\n\n### Displaying the Diff\n\nOne of the most important features of `CodeRoast` is displaying the code diff. The `renderDiffSection` function takes the diff string and formats it for display:\n\n```javascript\nfunction renderDiffSection(diff) {\n    const diffLines = parseDiff(diff);\n\n    return React.createElement(\n        'pre',\n        { className: 'diff-content' },\n        diffLines.map((line, index) => {\n            const className =\n                line.type === 'old' ? 'diff-old-code diff-code-line' :\n                line.type === 'new' ? 'diff-new-code diff-code-line' :\n                'diff-context-line';\n\n            return React.createElement('div', { key: index, className }, line.content);\n        })\n    );\n}\n```\n\nHere's what's happening:\n\n*   `const diffLines = parseDiff(diff);`: It calls the `parseDiff` function to split the diff into individual lines and determine their type (old code, new code, or context). See below for details on `parseDiff`.\n\n*   The rest of the function constructs a `<pre>` element which is used to display preformatted text.  Within the `<pre>` tag, we loop through the lines in `diffLines` and create a `div` tag for each line. The `className` will style the background of each line in green, red, or default color, depending on the line type.\n\nThe `parseDiff` function itself is responsible for taking the raw diff output and turning it into an array of objects, each representing a line in the diff:\n\n```javascript\nfunction parseDiff(diff) {\n    const lines = diff.split('\\n');\n    const parsedLines = [];\n\n    let section = null;\n\n    for (let i = 0; i < lines.length; i++) {\n        const line = lines[i];\n\n        if (line.startsWith('File:') || line.startsWith('Function:')) {\n            parsedLines.push({ type: 'context', content: line });\n            section = null;\n        } else if (line.trim() === 'Old Code:') {\n            parsedLines.push({ type: 'context', content: 'Old Code:' });\n            section = 'old';\n\n            // Handle empty old code\n            if (lines[i + 1]?.trim() === 'New Code:') {\n                parsedLines.push({ type: 'old', content: '<NO PREVIOUS CODE>' });\n            }\n        } else if (line.trim() === 'New Code:') {\n            parsedLines.push({ type: 'context', content: 'New Code:' });\n            section = 'new';\n        } else if (section === 'old' || section === 'new') {\n            parsedLines.push({ type: section, content: line });\n        } else {\n            parsedLines.push({ type: 'context', content: line });\n        }\n    }\n\n    return parsedLines;\n}\n```\n\nThis function:\n\n*   Splits the diff string into lines.\n*   Iterates through each line, determining its type based on the content.\n*   Adds each line to the `parsedLines` array with a `type` property indicating whether it's \"old\", \"new\", or \"context\".\n*   The `type` property is later used to style each line in the diff.\n\n### Displaying Analytics\n\nFinally, the frontend also displays some helpful analytics about the PRs. This is handled by the `Analytics` component.\n\n```javascript\n// Analytics component to visualize PR data\nfunction Analytics({ prs }) {\n    // Count PRs by status\n    const statusCounts = prs.reduce((acc, pr) => {\n        acc[pr.status] = (acc[pr.status] || 0) + 1;\n        return acc;\n    }, {});\n\n    // Count PRs by author\n    const authorCounts = prs.reduce((acc, pr) => {\n        acc[pr.author] = (acc[pr.author] || 0) + 1;\n        return acc;\n    }, {});\n\n    // Calculate PRs over time (by month)\n    const prsByMonth = prs.reduce((acc, pr) => {\n        const date = new Date(pr.created_at);\n        const monthYear = date.toLocaleString('default', { month: 'short', year: 'numeric' });\n        acc[monthYear] = (acc[monthYear] || 0) + 1;\n        return acc;\n    }, {});\n\n    // Create data arrays for charts\n    const statusData = Object.entries(statusCounts).map(([status, count]) => ({\n        status,\n        count,\n        color: status === 'Open' ? '#17a2b8' : \n               status === 'Merged' ? '#28a745' : \n               status === 'Closed' ? '#dc3545' : '#6c757d'\n    }));\n\n    const authorData = Object.entries(authorCounts)\n        .sort((a, b) => b[1] - a[1])\n        .slice(0, 5)\n        .map(([author, count]) => ({ author, count }));\n\n    const timelineData = Object.entries(prsByMonth)\n        .sort((a, b) => new Date(a[0]) - new Date(b[0]))\n        .map(([month, count]) => ({ month, count }));\n\n    return e('div', { className: 'analytics-container' }, [\n        e('h2', { key: 'title', className: 'analytics-title' }, 'Pull Request Analytics'),\n        // ... (rest of the analytics display code)\n    ]);\n}\n```\n\nThis component:\n\n*   Takes the `prs` array as a prop.\n*   Calculates statistics like the number of PRs by status, author, and month.\n*   Formats the data into arrays suitable for rendering charts and summaries.\n*   Renders the analytics using divs with appropriate CSS classes.\n\n## Under the Hood: Requesting PR Data\n\nLet's take a simplified look at what happens when the frontend requests PR data from the backend.\n\n```mermaid\nsequenceDiagram\n    participant FE as Frontend (React App)\n    participant BE as Backend (FastAPI)\n    participant DB as Snowflake Database\n\n    FE->>BE: GET /api/prs\n    activate BE\n    BE->>DB: Query PR Data\n    activate DB\n    DB-->>BE: PR Data (JSON)\n    deactivate DB\n    BE-->>FE: PR Data (JSON)\n    deactivate BE\n    FE->>FE: Render PR List\n```\n\n1.  The **Frontend (React App)** initiates a GET request to the `/api/prs` endpoint on the **Backend (FastAPI)**.\n2.  The **Backend (FastAPI)** receives the request and queries the **Snowflake Database** for PR data. (We'll see this in [Chapter 9: Snowflake Data Storage](09_snowflake_data_storage.md)).\n3.  The **Snowflake Database** returns the PR data to the **Backend (FastAPI)** in JSON format.\n4.  The **Backend (FastAPI)** sends the PR data back to the **Frontend (React App)** in JSON format.\n5.  The **Frontend (React App)** receives the data and renders the list of pull requests.\n\nThe actual fetching of data happens inside of the `React.useEffect` hook we saw before.\n\n```javascript\nReact.useEffect(() => {\n    fetch('http://localhost:8000/api/prs')\n        .then(res => res.json())\n        .then(data => {\n            setPrs(data);\n        });\n}, []);\n```\n\nThis hook ensures that the PR data is fetched when the component is first rendered and updates the component's state with the fetched data.\n\n## Conclusion\n\nIn this chapter, we explored the `CodeRoast` frontend, built with React. We learned about components, JSX, state, props, and how they're used to display PR lists, diffs, and analytics. We also took a peek under the hood to see how the frontend interacts with the backend to fetch data.\n\nNow that you have a basic understanding of the frontend, let's move on to the next chapter, where we'll dive into the backend API built with FastAPI: [Chapter 2: FastAPI Backend API](02_fastapi_backend_api.md).\n", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `CodeRoast` about the concept: \"FastAPI Backend API\". This is Chapter 2.\n\nConcept Details:\n- Description:\nThe FastAPI backend serves as the central processing unit of the application. It receives requests (like a new PR), orchestrates the AI analysis and documentation updates, and serves data to the frontend. Consider FastAPI as the project's control panel, handling all the server-side logic and interactions.\n\n\nComplete Tutorial Structure:\n1. [Frontend Web Application (React)](01_frontend_web_application__react_.md)\n2. [FastAPI Backend API](02_fastapi_backend_api.md)\n3. [GitHub Webhook Integration](03_github_webhook_integration.md)\n4. [Pull Request Analysis Pipeline](04_pull_request_analysis_pipeline.md)\n5. [Tree-sitter Based Code Parsing](05_tree_sitter_based_code_parsing.md)\n6. [Groq API Integration](06_groq_api_integration.md)\n7. [RAG (Retrieval-Augmented Generation) Documentation Updates](07_rag__retrieval_augmented_generation__documentation_updates.md)\n8. [PR Review Comment Posting (GitHub)](08_pr_review_comment_posting__github_.md)\n9. [Snowflake Data Storage](09_snowflake_data_storage.md)\n\nContext from previous chapters (summary):\n# Chapter 1: Frontend Web Application (React)\n\nWelcome to CodeRoast! In this chapter, we'll be diving into the heart of the user experience: the frontend web application built with React. Imagine you're a code reviewer, and you want to quickly see all the Pull Requests (PRs) needing your attention, along with AI-powered feedback. Our React frontend makes that happen!\n\nThis chapter will guide you through understanding how this frontend works and how it displays all the amazing insights generated by the rest of the CodeRoast system.\n\n## What is a Frontend, and Why React?\n\nThink of a frontend like the dashboard of a car. You see the speedometer, fuel gauge, and warning lights, but you don't need to know *how* the engine works to drive. Similarly, our frontend lets you interact with CodeRoast without worrying about the complex AI and analysis happening behind the scenes.\n\nReact is a popular JavaScript library for building user interfaces. It helps us create dynamic and interactive web pages. We chose React because it's efficient, organized, and makes it easier to manage complex user interfaces like our PR dashboard.\n\n## Key Concepts\n\nLet's break down the key concepts you'll encounter in our React frontend:\n\n1.  **Components:** React is all about components. Think of them as building blocks for your UI. Each component is responsible for rendering a specific part of the page, like a PR list, a diff view, or an analytics chart.\n\n2.  **JSX:** React uses JSX, which looks like HTML but is actually JavaScript. It allows us to write UI elements directly in our JavaScript code, making it more readable and maintainable.\n\n3.  **State:** State is data that can change over time, and when it changes, React automatically updates the UI. For example, the selected PR, the filter applied to the PR list, or the current theme (light/dark) are all parts of the component's state.\n\n4.  **Props:** Props (short for properties) are how we pass data from a parent component to a child component. Imagine passing a message from one React component to another.\n\n## The CodeRoast Frontend: A Tour\n\nLet's look at how these concepts come together in our `CodeRoast` frontend, using code snippets from `frontend/app.js`.\n\n### Displaying a List of Pull Requests\n\nFirst, let's see how we display the list of PRs. We fetch the PR data from our backend API (more on that in [Chapter 2: FastAPI Backend API](02_fastapi_backend_api.md)) and store it in the component's state.\n\n```javascript\nconst [prs, setPrs] = React.useState([]);\n\nReact.useEffect(() => {\n    fetch('http://localhost:8000/api/prs')\n        .then(res => res.json())\n        .then(data => {\n            setPrs(data);\n        });\n}, []);\n```\n\nThis code does the following:\n\n*   `const [prs, setPrs] = React.useState([]);`: This line initializes a state variable called `prs` to an empty array. `prs` will hold our pull request data. `setPrs` is a function used to update the `prs` data, and React will automatically refresh the user interface whenever you use `setPrs`.\n\n*   `React.useEffect(() => { ... }, []);`: This is a React Hook that runs code when the component is first rendered. The empty array `[]` as the second argument means this effect will only run once when the component mounts.\n\n*   `fetch('http://localhost:8000/api/prs')`: This line makes a request to our backend API to get the list of PRs.\n\n*   `.then(res => res.json())`: This converts the response from the API into a JavaScript object (JSON).\n\n*   `.then(data => { setPrs(data); })`: This updates the `prs` state with the data we received from the API.\n\nNext, we map over the `prs` array to create a list of PR items:\n\n```javascript\nconst prItems = filteredPRs.map(pr => \n    e('div', { \n        key: pr.id, \n        className: selectedPR && selectedPR.id === pr.id ? 'pr-item selected' : 'pr-item', \n        onClick: () => selectPR(pr) \n    }, [\n        e('div', {key: 'pr-header', className: 'pr-header'}, [\n            e('strong', {key: 'id'}, `#${pr.id}`),\n            e('span', {\n                key: 'status',\n                className: `pr-status status-${pr.status}`\n            }, pr.status)\n        ]),\n        e('div', {key: 'pr-title', className: 'pr-title'}, pr.title),\n        e('div', {key: 'pr-meta', className: 'pr-meta'}, [\n            e('span', {key: 'author'}, [\n                e('img', {key: 'avatar', className: 'avatar', src: `https://ui-avatars.com/api/?name=${pr.author}&size=24`, alt: pr.author}),\n                pr.author\n            ]),\n            e('span', {key: 'date', className: 'date'}, 'Updated: ' + (pr.updated_at || 'N/A'))\n        ])\n    ])\n);\n```\n\nThis code takes each `pr` from the `filteredPRs` array and:\n\n*   `e('div', { ... }, [ ... ])`: Creates a `div` element (and nested elements) for each PR, setting its `key`, `className`, and `onClick` properties. The `key` prop is important for React to efficiently update the list. `e` is `React.createElement`, a function that creates React elements. It is used because JSX must be transformed at compile time.\n\n*   `className`: Sets the CSS class for styling.\n\n*   `onClick`: Defines a function that will be called when the PR item is clicked.\n\n*   Within the div, `e(...)` is used to create other html elements to display the PR's `id`, `status`, `title`, `author` and `updated_at`.\n\n### Displaying the Diff\n\nOne of the most important features of `CodeRoast` is displaying the code diff. The `renderDiffSection` function takes the diff string and formats it for display:\n\n```javascript\nfunction renderDiffSection(diff) {\n    const diffLines = parseDiff(diff);\n\n    return React.createElement(\n        'pre',\n        { className: 'diff-content' },\n        diffLines.map((line, index) => {\n            const className =\n                line.type === 'old' ? 'diff-old-code diff-code-line' :\n                line.type === 'new' ? 'diff-new-code diff-code-line' :\n                'diff-context-line';\n\n            return React.createElement('div', { key: index, className }, line.content);\n        })\n    );\n}\n```\n\nHere's what's happening:\n\n*   `const diffLines = parseDiff(diff);`: It calls the `parseDiff` function to split the diff into individual lines and determine their type (old code, new code, or context). See below for details on `parseDiff`.\n\n*   The rest of the function constructs a `<pre>` element which is used to display preformatted text.  Within the `<pre>` tag, we loop through the lines in `diffLines` and create a `div` tag for each line. The `className` will style the background of each line in green, red, or default color, depending on the line type.\n\nThe `parseDiff` function itself is responsible for taking the raw diff output and turning it into an array of objects, each representing a line in the diff:\n\n```javascript\nfunction parseDiff(diff) {\n    const lines = diff.split('\\n');\n    const parsedLines = [];\n\n    let section = null;\n\n    for (let i = 0; i < lines.length; i++) {\n        const line = lines[i];\n\n        if (line.startsWith('File:') || line.startsWith('Function:')) {\n            parsedLines.push({ type: 'context', content: line });\n            section = null;\n        } else if (line.trim() === 'Old Code:') {\n            parsedLines.push({ type: 'context', content: 'Old Code:' });\n            section = 'old';\n\n            // Handle empty old code\n            if (lines[i + 1]?.trim() === 'New Code:') {\n                parsedLines.push({ type: 'old', content: '<NO PREVIOUS CODE>' });\n            }\n        } else if (line.trim() === 'New Code:') {\n            parsedLines.push({ type: 'context', content: 'New Code:' });\n            section = 'new';\n        } else if (section === 'old' || section === 'new') {\n            parsedLines.push({ type: section, content: line });\n        } else {\n            parsedLines.push({ type: 'context', content: line });\n        }\n    }\n\n    return parsedLines;\n}\n```\n\nThis function:\n\n*   Splits the diff string into lines.\n*   Iterates through each line, determining its type based on the content.\n*   Adds each line to the `parsedLines` array with a `type` property indicating whether it's \"old\", \"new\", or \"context\".\n*   The `type` property is later used to style each line in the diff.\n\n### Displaying Analytics\n\nFinally, the frontend also displays some helpful analytics about the PRs. This is handled by the `Analytics` component.\n\n```javascript\n// Analytics component to visualize PR data\nfunction Analytics({ prs }) {\n    // Count PRs by status\n    const statusCounts = prs.reduce((acc, pr) => {\n        acc[pr.status] = (acc[pr.status] || 0) + 1;\n        return acc;\n    }, {});\n\n    // Count PRs by author\n    const authorCounts = prs.reduce((acc, pr) => {\n        acc[pr.author] = (acc[pr.author] || 0) + 1;\n        return acc;\n    }, {});\n\n    // Calculate PRs over time (by month)\n    const prsByMonth = prs.reduce((acc, pr) => {\n        const date = new Date(pr.created_at);\n        const monthYear = date.toLocaleString('default', { month: 'short', year: 'numeric' });\n        acc[monthYear] = (acc[monthYear] || 0) + 1;\n        return acc;\n    }, {});\n\n    // Create data arrays for charts\n    const statusData = Object.entries(statusCounts).map(([status, count]) => ({\n        status,\n        count,\n        color: status === 'Open' ? '#17a2b8' : \n               status === 'Merged' ? '#28a745' : \n               status === 'Closed' ? '#dc3545' : '#6c757d'\n    }));\n\n    const authorData = Object.entries(authorCounts)\n        .sort((a, b) => b[1] - a[1])\n        .slice(0, 5)\n        .map(([author, count]) => ({ author, count }));\n\n    const timelineData = Object.entries(prsByMonth)\n        .sort((a, b) => new Date(a[0]) - new Date(b[0]))\n        .map(([month, count]) => ({ month, count }));\n\n    return e('div', { className: 'analytics-container' }, [\n        e('h2', { key: 'title', className: 'analytics-title' }, 'Pull Request Analytics'),\n        // ... (rest of the analytics display code)\n    ]);\n}\n```\n\nThis component:\n\n*   Takes the `prs` array as a prop.\n*   Calculates statistics like the number of PRs by status, author, and month.\n*   Formats the data into arrays suitable for rendering charts and summaries.\n*   Renders the analytics using divs with appropriate CSS classes.\n\n## Under the Hood: Requesting PR Data\n\nLet's take a simplified look at what happens when the frontend requests PR data from the backend.\n\n```mermaid\nsequenceDiagram\n    participant FE as Frontend (React App)\n    participant BE as Backend (FastAPI)\n    participant DB as Snowflake Database\n\n    FE->>BE: GET /api/prs\n    activate BE\n    BE->>DB: Query PR Data\n    activate DB\n    DB-->>BE: PR Data (JSON)\n    deactivate DB\n    BE-->>FE: PR Data (JSON)\n    deactivate BE\n    FE->>FE: Render PR List\n```\n\n1.  The **Frontend (React App)** initiates a GET request to the `/api/prs` endpoint on the **Backend (FastAPI)**.\n2.  The **Backend (FastAPI)** receives the request and queries the **Snowflake Database** for PR data. (We'll see this in [Chapter 9: Snowflake Data Storage](09_snowflake_data_storage.md)).\n3.  The **Snowflake Database** returns the PR data to the **Backend (FastAPI)** in JSON format.\n4.  The **Backend (FastAPI)** sends the PR data back to the **Frontend (React App)** in JSON format.\n5.  The **Frontend (React App)** receives the data and renders the list of pull requests.\n\nThe actual fetching of data happens inside of the `React.useEffect` hook we saw before.\n\n```javascript\nReact.useEffect(() => {\n    fetch('http://localhost:8000/api/prs')\n        .then(res => res.json())\n        .then(data => {\n            setPrs(data);\n        });\n}, []);\n```\n\nThis hook ensures that the PR data is fetched when the component is first rendered and updates the component's state with the fetched data.\n\n## Conclusion\n\nIn this chapter, we explored the `CodeRoast` frontend, built with React. We learned about components, JSX, state, props, and how they're used to display PR lists, diffs, and analytics. We also took a peek under the hood to see how the frontend interacts with the backend to fetch data.\n\nNow that you have a basic understanding of the frontend, let's move on to the next chapter, where we'll dive into the backend API built with FastAPI: [Chapter 2: FastAPI Backend API](02_fastapi_backend_api.md).\n\n\nRelevant Code Snippets:\n--- File: README.md ---\n# \ud83e\udd16 Code Roast : An AI-Powered Pull Request Review System\n\n> An end-to-end intelligent platform for automated code review, documentation updates, and pull request analysis \u2014 powered by LLMs, Tree-sitter, and Snowflake.\n\n---\n\n## \ud83d\ude80 Overview\n\nThis system integrates with GitHub to automatically:\n- Analyze pull requests using AI (via [Groq](https://groq.com))\n- Parse and understand diffs using [Tree-sitter](https://tree-sitter.github.io/)\n- Perform RAG (Retrieval-Augmented Generation) to update project documentation\n- Store all insights in **Snowflake** for analytics\n- Display all PRs, diffs, AI feedback, and doc suggestions via a beautiful **Web App**\n\nWhether you're an engineering team lead, reviewer, or contributor \u2014 you get fast, reliable, and insightful AI-powered review feedback at every step.\n\n---\n\n## \ud83e\udde0 Features\n\n- \u2705 **AI PR Summary**  \n  Instant analysis of pull request purpose, syntax/style/functionality, and merge confidence.\n\n- \ud83d\udcdd **Automatic Docstring Suggestions**  \n  Extract or generate missing/updated docstrings for new or changed functions.\n\n- \ud83d\udcd8 **RAG-based Documentation Updates**  \n  Contextual retrieval from `project_docs.txt` with AI-generated Markdown additions.\n\n- \ud83d\udcc4 **Code Diff Parsing**  \n  Tree-sitter-based code structure diffing to isolate changed functions only.\n\n- \ud83d\udcac **GitHub Comment Posting**  \n  Summaries are posted as comments directly on PRs, no need to open another tool.\n\n- \ud83d\udcca **Snowflake Integration**  \n  All AI-generated insights and PR metadata stored for reporting, audit, and search.\n\n- \ud83c\udf10 **Web App Dashboard**  \n  Browse all PRs, view diffs, AI reviews, and doc updates in a single UI.\n\n---\n\n## \ud83e\uddf1 Tech Stack\n\n| Layer              | Tech Used                                      |\n|--------------------|-----------------------------------------------|\n| **AI Models**       | [Groq API (LLaMA 3)](https://groq.com)        |\n| **Diff Parsing**    | [Tree-sitter](https://tree-sitter.github.io/) |\n| **Backend API**     | [FastAPI](https://fastapi.tiangolo.com)       |\n| **Git Integration** | GitHub App + `PyGithub`                       |\n| **RAG Engine**      | Basic keyword-matching from local docs        |\n| **Database**        | [Snowflake](https://www.snowflake.com)        |\n| **Frontend**        | HTML, JS, CSS (Markdown + Diff Viewers)       |\n\n---\n\n## \u2699\ufe0f How It Works\n\n1. \ud83d\udd14 **GitHub Webhook Trigger**\n   - On PR open/update \u2192 sends payload to FastAPI backend\n\n2. \ud83e\udde0 **AI Review Processing**\n   - Extracts code diffs, changed functions\n   - Groq generates AI summary, confidence rating, and docstrings\n\n3. \ud83d\udcd8 **Documentation Updates**\n   - Local project docs indexed via simple RAG\n   - Groq suggests Markdown additions\n\n4. \ud83d\udcbe **Data Storage**\n   - All results are saved in Snowflake for dashboard & analytics\n\n5. \ud83c\udf10 **Web App UI**\n   - Explore PRs, view diffs, AI reviews, and doc updates\n\n\n--- File: backend/main.py ---\nfrom fastapi import FastAPI, Request, Header\nfrom fastapi.responses import JSONResponse\nimport hmac\nimport hashlib\nimport json\nimport os\n\napp = FastAPI()\n\nGITHUB_SECRET = \"asdfg\" # Set this in your env\n\ndef verify_signature(payload, signature, secret):\n    mac = hmac.new(secret.encode(), msg=payload, digestmod=hashlib.sha256)\n    expected = f\"sha256={mac.hexdigest()}\"\n    return hmac.compare_digest(expected, signature)\n\n@app.post(\"/webhook\")\nasync def github_webhook(\n    request: Request,\n    x_hub_signature_256: str = Header(None),\n    x_github_event: str = Header(None)\n):\n    body = await request.body()\n\n    # Verify webhook signature\n    if not verify_signature(body, x_hub_signature_256, GITHUB_SECRET):\n        return JSONResponse(content={\"error\": \"Invalid signature\"}, status_code=403)\n\n    payload = json.loads(body)\n\n    if x_github_event == \"pull_request\":\n        action = payload[\"action\"]\n        pr = payload[\"pull_request\"]\n\n        if action in [\"opened\", \"synchronize\"]:\n            pr_url = pr[\"url\"]\n            pr_diff_url = pr[\"diff_url\"]\n            repo_full_name = payload[\"repository\"][\"full_name\"]\n            # \ud83d\udc47 Call your internal logic to analyze PR\n            print(f\"Trigger AI review for: {repo_full_name} @ {pr_url}\")\n            print(f\"PR Diff URL: {pr_diff_url}\")\n            print(f\"PR Title: {pr['title']}\")\n            print(f\"PR Body: {pr['body']}\")\n\n    return {\"message\": \"OK\"}\n\n--- File: main.py ---\nfrom fastapi import FastAPI, HTTPException\r\nfrom fastapi.middleware.cors import CORSMiddleware\r\nimport snowflake.connector\r\nimport os\r\nfrom dotenv import load_dotenv\r\nimport pandas as pd\r\n\r\nload_dotenv()\r\n\r\napp = FastAPI()\r\n\r\n# Snowflake credentials\r\nSNOWFLAKE_USER = os.getenv(\"SNOWFLAKE_USER\", \"raghavg332\")\r\nSNOWFLAKE_PASSWORD = os.getenv(\"SNOWFLAKE_PASSWORD\", \"Qa29Kh4MptfGHEW\")\r\nSNOWFLAKE_ACCOUNT = os.getenv(\"SNOWFLAKE_ACCOUNT\", \"KXCIVVH-LL27432\")\r\nSNOWFLAKE_WAREHOUSE = os.getenv(\"SNOWFLAKE_WAREHOUSE\", \"COMPUTE_WH\")\r\nSNOWFLAKE_DATABASE = os.getenv(\"SNOWFLAKE_DATABASE\", \"PR_DASHBOARD\")\r\nSNOWFLAKE_SCHEMA = os.getenv(\"SNOWFLAKE_SCHEMA\", \"PUBLIC\")\r\n\r\napp.add_middleware(\r\n    CORSMiddleware,\r\n    allow_origins=[\"*\"],\r\n    allow_methods=[\"*\"],\r\n    allow_headers=[\"*\"],\r\n)\r\n\r\ndef get_conn():\r\n    \"\"\"Establish connection to Snowflake database\"\"\"\r\n    try:\r\n        conn = snowflake.connector.connect(\r\n            user=SNOWFLAKE_USER,\r\n            password=SNOWFLAKE_PASSWORD,\r\n            account=SNOWFLAKE_ACCOUNT,\r\n            warehouse=SNOWFLAKE_WAREHOUSE,\r\n            database=SNOWFLAKE_DATABASE,\r\n            schema=SNOWFLAKE_SCHEMA\r\n        )\r\n        return conn\r\n    except Exception as e:\r\n        print(f\"Error connecting to Snowflake: {e}\")\r\n        raise e\r\n\r\n@app.get(\"/api/prs\")\r\ndef fetch_all_prs():\r\n    try:\r\n        conn = get_conn()\r\n        cursor = conn.cursor(snowflake.connector.DictCursor)\r\n        \r\n        # Using cursor to get results as dictionaries\r\n        cursor.execute(\"SELECT id, title, author, status, created_at, updated_at FROM pull_requests ORDER BY updated_at DESC\")\r\n        results = cursor.fetchall()\r\n        \r\n        # Format the data to match frontend expectations\r\n        formatted_results = []\r\n        for row in results:\r\n            # Capitalize the status to match frontend expectations\r\n            status = row['STATUS'].capitalize() if row['STATUS'] else 'Unknown'\r\n            \r\n            formatted_results.append({\r\n                \"id\": row['ID'],\r\n                \"title\": row['TITLE'],\r\n                \"author\": row['AUTHOR'],\r\n                \"status\": status,\r\n                \"created_at\": row['CREATED_AT'].strftime(\"%Y-%m-%d\") if row['CREATED_AT'] else None,\r\n                \"updated_at\": row['UPDATED_AT'].strftime(\"%Y-%m-%d\") if row['UPDATED_AT'] else None\r\n            })\r\n        \r\n        cursor.close()\r\n        conn.close()\r\n        return formatted_results\r\n    except Exception as e:\r\n        print(f\"Error fetching PRs: {e}\")\r\n        raise HTTPException(status_code=500, detail=f\"Database error: {str(e)}\")\r\n\r\n@app.get(\"/api/prs/{pr_id}\")\r\ndef fetch_pr_analysis(pr_id: int):\r\n    try:\r\n        conn = get_conn()\r\n        cursor = conn.cursor(snowflake.connector.DictCursor)\r\n        \r\n        # Fetch the analysis data\r\n        cursor.execute(f\"SELECT * FROM pr_analysis WHERE pr_id = {pr_id}\")\r\n        result = cursor.fetchone()\r\n        \r\n        if not result:\r\n            cursor.close()\r\n            conn.close()\r\n            raise HTTPException(status_code=404, detail=\"PR analysis not found\")\r\n        \r\n        # Format the data to match frontend expectations\r\n        formatted_result = {\r\n            \"pr_id\": result['PR_ID'],\r\n            \"ai_summary\": result['AI_SUMMARY'],\r\n            \"merge_confidence\": result['MERGE_CONFIDENCE'].capitalize(),  # Capitalize for frontend\r\n            \"merge_confidence_score\": float(result['MERGE_CONFIDENCE']) if result.get('MERGE_CONFIDENCE') else 0,\r\n            \"code_quality\": result['CODE_QUALITY'],\r\n            \"diff\": result['DIFF'],\r\n            \"doc_string\": result['DOCSTRINGS'] if 'DOCSTRINGS' in result else \"No documentation available for this PR.\"\r\n        }\r\n        \r\n        cursor.close()\r\n        conn.close()\r\n        return formatted_result\r\n    except Exception as e:\r\n        print(f\"Error fetching PR analysis: {e}\")\r\n        raise HTTPException(status_code=500, detail=f\"Error: {str(e)}\")\r\n\r\n'''\r\n@app.get(\"/api/prs\")\r\ndef fetch_all_prs():\r\n    # Test data\r\n    test_data = [\r\n        {\r\n            \"id\": 1,\r\n            \"title\": \"Add new feature\",\r\n            \"author\": \"johndoe\",\r\n            \"status\": \"Open\",\r\n            \"created_at\": \"2023-01-01\",\r\n            \"updated_at\": \"2023-01-02\"\r\n        },\r\n        {\r\n            \"id\": 2,\r\n            \"title\": \"Fix bug in login\",\r\n            \"author\": \"janedoe\",\r\n            \"status\": \"Merged\",\r\n            \"created_at\": \"2023-01-03\",\r\n            \"updated_at\": \"2023-01-04\"\r\n        }\r\n    ]\r\n    return test_data\r\n\r\n@app.get(\"/api/prs/{pr_id}\")\r\ndef fetch_pr_analysis(pr_id: int):\r\n    # Test data\r\n    test_data = {\r\n        \"pr_id\": pr_id,\r\n        \"ai_summary\": \"This PR adds a new feature that improves user experience\",\r\n        \"merge_confidence\": \"High\",\r\n        \"code_quality\": \"Good code quality. No major issues found.\",\r\n        \"diff\": \"```\\n+ def new_function():\\n+     return 'new feature'\\n```\",\r\n        \"doc_string\": \"This PR introduces a new function that returns 'new feature'.\"\r\n    }\r\n    return test_data\r\n'''\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 2: FastAPI Backend API`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "# Chapter 2: FastAPI Backend API\n\nWelcome back to CodeRoast! In [Chapter 1: Frontend Web Application (React)](01_frontend_web_application__react_.md), we built a snazzy frontend that displays pull requests, diffs, and AI insights. But that frontend needs someone to talk to \u2013 someone who can fetch data, run the AI magic, and keep everything organized. That's where our FastAPI backend comes in!\n\n## What is a Backend API, and Why FastAPI?\n\nThink of the backend like the engine in our car analogy. The frontend is the dashboard, showing you information and letting you control things. The backend is the engine, doing all the heavy lifting.\n\nSpecifically, the backend acts as an *API (Application Programming Interface)*. This means it provides a structured way for the frontend (and other applications) to request information and functionality.\n\n**FastAPI** is a modern, high-performance Python web framework for building APIs. We chose FastAPI because it's:\n\n*   **Fast:** It's built for speed, so our app feels responsive.\n*   **Easy to use:** It has a simple and intuitive syntax, making development quicker.\n*   **Automatic Data Validation:** It automatically checks that the data sent to the API is in the correct format, preventing errors.\n*   **Automatic API Documentation:** It generates interactive API documentation automatically, making it easy to understand how to use the API.\n\n## Key Concepts\n\nLet's break down the key concepts behind our FastAPI backend:\n\n1.  **Routes (Endpoints):** These are specific URLs that our backend responds to. For example, `/api/prs` might be a route that returns a list of all pull requests. Think of them as specific \"doors\" to access different functionalities of our backend.\n\n2.  **Requests:** These are messages sent from the frontend (or any other client) to the backend, asking for something. They often include data, like the ID of a pull request the frontend wants to see.\n\n3.  **Responses:** These are messages sent back from the backend to the frontend, containing the requested information or the result of an action. Responses are often in JSON format, which is easy for the frontend to process.\n\n4.  **Data Models:** These are Python classes that define the structure of our data. For example, we might have a `PullRequest` data model that defines the fields for each pull request (like `id`, `title`, `author`, etc.).  Data models help ensure the data is consistent and well-defined.\n\n## The CodeRoast Backend: A Tour\n\nLet's look at how these concepts come together in our `CodeRoast` backend, using code snippets from `main.py`.\n\n### Defining a Route\n\nHere's an example of a route that fetches all pull requests:\n\n```python\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nimport snowflake.connector\nimport os\nfrom dotenv import load_dotenv\nimport pandas as pd\n\nload_dotenv()\n\napp = FastAPI()\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n@app.get(\"/api/prs\")\ndef fetch_all_prs():\n    # Fetch data from Snowflake and return it\n    # For simplicity, let's return a test data\n    test_data = [\n        {\n            \"id\": 1,\n            \"title\": \"Add new feature\",\n            \"author\": \"johndoe\",\n            \"status\": \"Open\",\n            \"created_at\": \"2023-01-01\",\n            \"updated_at\": \"2023-01-02\"\n        },\n        {\n            \"id\": 2,\n            \"title\": \"Fix bug in login\",\n            \"author\": \"janedoe\",\n            \"status\": \"Merged\",\n            \"created_at\": \"2023-01-03\",\n            \"updated_at\": \"2023-01-04\"\n        }\n    ]\n    return test_data\n```\n\nLet's break this down:\n\n*   `@app.get(\"/api/prs\")`: This is a *decorator* that tells FastAPI to create a route at the URL `/api/prs`. The `@app.get` part specifies that this route should handle GET requests (requests for data).\n*   `def fetch_all_prs():`: This defines a function called `fetch_all_prs` that will be executed when someone makes a GET request to `/api/prs`.\n*   `return test_data`: This returns the `test_data` list as a JSON response. FastAPI automatically converts Python dictionaries and lists into JSON.\n\nSo, if you open your web browser and go to `http://localhost:8000/api/prs`, you'll see a JSON response containing the `test_data`. (Note: make sure your backend is running!)\n\n### Handling Parameters\n\nWhat if we want to fetch information about a specific pull request, identified by its ID? We can use a *path parameter* in our route:\n\n```python\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nimport snowflake.connector\nimport os\nfrom dotenv import load_dotenv\nimport pandas as pd\n\nload_dotenv()\n\napp = FastAPI()\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n@app.get(\"/api/prs/{pr_id}\")\ndef fetch_pr_analysis(pr_id: int):\n    # Fetch the analysis data\n    # For simplicity, let's return a test data\n    test_data = {\n        \"pr_id\": pr_id,\n        \"ai_summary\": \"This PR adds a new feature that improves user experience\",\n        \"merge_confidence\": \"High\",\n        \"code_quality\": \"Good code quality. No major issues found.\",\n        \"diff\": \"```\\n+ def new_function():\\n+     return 'new feature'\\n```\",\n        \"doc_string\": \"This PR introduces a new function that returns 'new feature'.\"\n    }\n    return test_data\n```\n\nHere's what's new:\n\n*   `@app.get(\"/api/prs/{pr_id}\")`: This defines a route that includes a path parameter called `pr_id`. The curly braces `{}` indicate a parameter.\n*   `def fetch_pr_analysis(pr_id: int):`:  The `pr_id: int` part of the function definition tells FastAPI that `pr_id` is an integer parameter. FastAPI will automatically validate that the value passed in the URL is actually an integer.\n*   `\"pr_id\": pr_id`: We are using the `pr_id` parameter inside the function to construct the response.\n\nIf you go to `http://localhost:8000/api/prs/123`, you'll see a JSON response containing the `test_data`, but with the `\"pr_id\"` field set to `123`.\n\n### Connecting to Snowflake Database\n\nIn a real application, we wouldn't just return test data. We would fetch the data from a database.  In our case, we use [Snowflake Data Storage](09_snowflake_data_storage.md). Let's look at how we can connect to Snowflake and query the database (simplified):\n\n```python\nimport os\nimport snowflake.connector\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nimport pandas as pd\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\napp = FastAPI()\n\n# Snowflake credentials\nSNOWFLAKE_USER = os.getenv(\"SNOWFLAKE_USER\", \"raghavg332\")\nSNOWFLAKE_PASSWORD = os.getenv(\"SNOWFLAKE_PASSWORD\", \"Qa29Kh4MptfGHEW\")\nSNOWFLAKE_ACCOUNT = os.getenv(\"SNOWFLAKE_ACCOUNT\", \"KXCIVVH-LL27432\")\nSNOWFLAKE_WAREHOUSE = os.getenv(\"SNOWFLAKE_WAREHOUSE\", \"COMPUTE_WH\")\nSNOWFLAKE_DATABASE = os.getenv(\"SNOWFLAKE_DATABASE\", \"PR_DASHBOARD\")\nSNOWFLAKE_SCHEMA = os.getenv(\"SNOWFLAKE_SCHEMA\", \"PUBLIC\")\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\ndef get_conn():\n    \"\"\"Establish connection to Snowflake database\"\"\"\n    try:\n        conn = snowflake.connector.connect(\n            user=SNOWFLAKE_USER,\n            password=SNOWFLAKE_PASSWORD,\n            account=SNOWFLAKE_ACCOUNT,\n            warehouse=SNOWFLAKE_WAREHOUSE,\n            database=SNOWFLAKE_DATABASE,\n            schema=SNOWFLAKE_SCHEMA\n        )\n        return conn\n    except Exception as e:\n        print(f\"Error connecting to Snowflake: {e}\")\n        raise e\n\n@app.get(\"/api/prs/{pr_id}\")\ndef fetch_pr_analysis(pr_id: int):\n    try:\n        conn = get_conn()\n        cursor = conn.cursor(snowflake.connector.DictCursor)\n\n        # Fetch the analysis data\n        cursor.execute(f\"SELECT * FROM pr_analysis WHERE pr_id = {pr_id}\")\n        result = cursor.fetchone()\n\n        if not result:\n            cursor.close()\n            conn.close()\n            raise HTTPException(status_code=404, detail=\"PR analysis not found\")\n\n        cursor.close()\n        conn.close()\n        return result\n    except Exception as e:\n        print(f\"Error fetching PR analysis: {e}\")\n        raise HTTPException(status_code=500, detail=f\"Error: {str(e)}\")\n```\n\nHere's what's happening:\n\n*   `import snowflake.connector`: This imports the Snowflake connector library, which allows us to connect to a Snowflake database.\n*   `SNOWFLAKE_USER = os.getenv(\"SNOWFLAKE_USER\", \"raghavg332\")`: This imports environment variable, which allows us to securely config the snowflake credentials.\n*   `get_conn()`: This function establishes a connection to Snowflake using your credentials. **Important:** Make sure you have Snowflake configured correctly.\n*   `conn = get_conn()`: This calls the `get_conn` function to establish the connection.\n*   `cursor = conn.cursor(snowflake.connector.DictCursor)`: This creates a cursor object, which allows us to execute SQL queries.  The `snowflake.connector.DictCursor` is important here: it tells the connector to return results as Python dictionaries, which are easier to work with.\n*   `cursor.execute(f\"SELECT * FROM pr_analysis WHERE pr_id = {pr_id}\")`: This executes a SQL query to fetch the analysis data for the specified `pr_id`.\n*   `result = cursor.fetchone()`: This fetches the first row of the result set.\n*   `return result`: This returns the result (which is a dictionary) as a JSON response.\n\n## Under the Hood: Fetching PR Data\n\nLet's take a simplified look at what happens when the frontend requests PR data from the backend.\n\n```mermaid\nsequenceDiagram\n    participant FE as Frontend (React App)\n    participant BE as Backend (FastAPI)\n    participant DB as Snowflake Database\n\n    FE->>BE: GET /api/prs/123\n    activate BE\n    BE->>DB: SELECT * FROM pr_analysis WHERE pr_id = 123\n    activate DB\n    DB-->>BE: PR Analysis Data (JSON)\n    deactivate DB\n    BE-->>FE: PR Analysis Data (JSON)\n    deactivate BE\n    FE->>FE: Render PR Details\n```\n\n1.  The **Frontend (React App)** initiates a GET request to the `/api/prs/123` endpoint on the **Backend (FastAPI)**.\n2.  The **Backend (FastAPI)** receives the request and executes a SQL query on the **Snowflake Database** to fetch the analysis data for the PR with ID 123.\n3.  The **Snowflake Database** returns the PR analysis data to the **Backend (FastAPI)** in JSON format.\n4.  The **Backend (FastAPI)** sends the PR analysis data back to the **Frontend (React App)** in JSON format.\n5.  The **Frontend (React App)** receives the data and renders the details of the pull request.\n\nThe actual fetching of data happens inside of the `fetch_pr_analysis` function we saw before, where it:\n\n1. Establishes a connection to the snowflake database.\n2. Constructs and executes a select query.\n3. Formats the data for transmission back to the frontend as a JSON response.\n\n## Conclusion\n\nIn this chapter, we explored the `CodeRoast` backend, built with FastAPI. We learned about routes, requests, responses, data models, and how they're used to create a powerful API. We also saw how the backend interacts with a Snowflake database to fetch data.\n\nNow that you have a basic understanding of the backend, let's move on to the next chapter, where we'll dive into how we receive GitHub Webhooks: [Chapter 3: GitHub Webhook Integration](03_github_webhook_integration.md).\n", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `CodeRoast` about the concept: \"GitHub Webhook Integration\". This is Chapter 3.\n\nConcept Details:\n- Description:\nThe application integrates with GitHub via webhooks, which are triggered by specific events (like opening a pull request). The github webhook is a messenger that communicates changes made in GitHub to the backend.\n\n\nComplete Tutorial Structure:\n1. [Frontend Web Application (React)](01_frontend_web_application__react_.md)\n2. [FastAPI Backend API](02_fastapi_backend_api.md)\n3. [GitHub Webhook Integration](03_github_webhook_integration.md)\n4. [Pull Request Analysis Pipeline](04_pull_request_analysis_pipeline.md)\n5. [Tree-sitter Based Code Parsing](05_tree_sitter_based_code_parsing.md)\n6. [Groq API Integration](06_groq_api_integration.md)\n7. [RAG (Retrieval-Augmented Generation) Documentation Updates](07_rag__retrieval_augmented_generation__documentation_updates.md)\n8. [PR Review Comment Posting (GitHub)](08_pr_review_comment_posting__github_.md)\n9. [Snowflake Data Storage](09_snowflake_data_storage.md)\n\nContext from previous chapters (summary):\n# Chapter 1: Frontend Web Application (React)\n\nWelcome to CodeRoast! In this chapter, we'll be diving into the heart of the user experience: the frontend web application built with React. Imagine you're a code reviewer, and you want to quickly see all the Pull Requests (PRs) needing your attention, along with AI-powered feedback. Our React frontend makes that happen!\n\nThis chapter will guide you through understanding how this frontend works and how it displays all the amazing insights generated by the rest of the CodeRoast system.\n\n## What is a Frontend, and Why React?\n\nThink of a frontend like the dashboard of a car. You see the speedometer, fuel gauge, and warning lights, but you don't need to know *how* the engine works to drive. Similarly, our frontend lets you interact with CodeRoast without worrying about the complex AI and analysis happening behind the scenes.\n\nReact is a popular JavaScript library for building user interfaces. It helps us create dynamic and interactive web pages. We chose React because it's efficient, organized, and makes it easier to manage complex user interfaces like our PR dashboard.\n\n## Key Concepts\n\nLet's break down the key concepts you'll encounter in our React frontend:\n\n1.  **Components:** React is all about components. Think of them as building blocks for your UI. Each component is responsible for rendering a specific part of the page, like a PR list, a diff view, or an analytics chart.\n\n2.  **JSX:** React uses JSX, which looks like HTML but is actually JavaScript. It allows us to write UI elements directly in our JavaScript code, making it more readable and maintainable.\n\n3.  **State:** State is data that can change over time, and when it changes, React automatically updates the UI. For example, the selected PR, the filter applied to the PR list, or the current theme (light/dark) are all parts of the component's state.\n\n4.  **Props:** Props (short for properties) are how we pass data from a parent component to a child component. Imagine passing a message from one React component to another.\n\n## The CodeRoast Frontend: A Tour\n\nLet's look at how these concepts come together in our `CodeRoast` frontend, using code snippets from `frontend/app.js`.\n\n### Displaying a List of Pull Requests\n\nFirst, let's see how we display the list of PRs. We fetch the PR data from our backend API (more on that in [Chapter 2: FastAPI Backend API](02_fastapi_backend_api.md)) and store it in the component's state.\n\n```javascript\nconst [prs, setPrs] = React.useState([]);\n\nReact.useEffect(() => {\n    fetch('http://localhost:8000/api/prs')\n        .then(res => res.json())\n        .then(data => {\n            setPrs(data);\n        });\n}, []);\n```\n\nThis code does the following:\n\n*   `const [prs, setPrs] = React.useState([]);`: This line initializes a state variable called `prs` to an empty array. `prs` will hold our pull request data. `setPrs` is a function used to update the `prs` data, and React will automatically refresh the user interface whenever you use `setPrs`.\n\n*   `React.useEffect(() => { ... }, []);`: This is a React Hook that runs code when the component is first rendered. The empty array `[]` as the second argument means this effect will only run once when the component mounts.\n\n*   `fetch('http://localhost:8000/api/prs')`: This line makes a request to our backend API to get the list of PRs.\n\n*   `.then(res => res.json())`: This converts the response from the API into a JavaScript object (JSON).\n\n*   `.then(data => { setPrs(data); })`: This updates the `prs` state with the data we received from the API.\n\nNext, we map over the `prs` array to create a list of PR items:\n\n```javascript\nconst prItems = filteredPRs.map(pr => \n    e('div', { \n        key: pr.id, \n        className: selectedPR && selectedPR.id === pr.id ? 'pr-item selected' : 'pr-item', \n        onClick: () => selectPR(pr) \n    }, [\n        e('div', {key: 'pr-header', className: 'pr-header'}, [\n            e('strong', {key: 'id'}, `#${pr.id}`),\n            e('span', {\n                key: 'status',\n                className: `pr-status status-${pr.status}`\n            }, pr.status)\n        ]),\n        e('div', {key: 'pr-title', className: 'pr-title'}, pr.title),\n        e('div', {key: 'pr-meta', className: 'pr-meta'}, [\n            e('span', {key: 'author'}, [\n                e('img', {key: 'avatar', className: 'avatar', src: `https://ui-avatars.com/api/?name=${pr.author}&size=24`, alt: pr.author}),\n                pr.author\n            ]),\n            e('span', {key: 'date', className: 'date'}, 'Updated: ' + (pr.updated_at || 'N/A'))\n        ])\n    ])\n);\n```\n\nThis code takes each `pr` from the `filteredPRs` array and:\n\n*   `e('div', { ... }, [ ... ])`: Creates a `div` element (and nested elements) for each PR, setting its `key`, `className`, and `onClick` properties. The `key` prop is important for React to efficiently update the list. `e` is `React.createElement`, a function that creates React elements. It is used because JSX must be transformed at compile time.\n\n*   `className`: Sets the CSS class for styling.\n\n*   `onClick`: Defines a function that will be called when the PR item is clicked.\n\n*   Within the div, `e(...)` is used to create other html elements to display the PR's `id`, `status`, `title`, `author` and `updated_at`.\n\n### Displaying the Diff\n\nOne of the most important features of `CodeRoast` is displaying the code diff. The `renderDiffSection` function takes the diff string and formats it for display:\n\n```javascript\nfunction renderDiffSection(diff) {\n    const diffLines = parseDiff(diff);\n\n    return React.createElement(\n        'pre',\n        { className: 'diff-content' },\n        diffLines.map((line, index) => {\n            const className =\n                line.type === 'old' ? 'diff-old-code diff-code-line' :\n                line.type === 'new' ? 'diff-new-code diff-code-line' :\n                'diff-context-line';\n\n            return React.createElement('div', { key: index, className }, line.content);\n        })\n    );\n}\n```\n\nHere's what's happening:\n\n*   `const diffLines = parseDiff(diff);`: It calls the `parseDiff` function to split the diff into individual lines and determine their type (old code, new code, or context). See below for details on `parseDiff`.\n\n*   The rest of the function constructs a `<pre>` element which is used to display preformatted text.  Within the `<pre>` tag, we loop through the lines in `diffLines` and create a `div` tag for each line. The `className` will style the background of each line in green, red, or default color, depending on the line type.\n\nThe `parseDiff` function itself is responsible for taking the raw diff output and turning it into an array of objects, each representing a line in the diff:\n\n```javascript\nfunction parseDiff(diff) {\n    const lines = diff.split('\\n');\n    const parsedLines = [];\n\n    let section = null;\n\n    for (let i = 0; i < lines.length; i++) {\n        const line = lines[i];\n\n        if (line.startsWith('File:') || line.startsWith('Function:')) {\n            parsedLines.push({ type: 'context', content: line });\n            section = null;\n        } else if (line.trim() === 'Old Code:') {\n            parsedLines.push({ type: 'context', content: 'Old Code:' });\n            section = 'old';\n\n            // Handle empty old code\n            if (lines[i + 1]?.trim() === 'New Code:') {\n                parsedLines.push({ type: 'old', content: '<NO PREVIOUS CODE>' });\n            }\n        } else if (line.trim() === 'New Code:') {\n            parsedLines.push({ type: 'context', content: 'New Code:' });\n            section = 'new';\n        } else if (section === 'old' || section === 'new') {\n            parsedLines.push({ type: section, content: line });\n        } else {\n            parsedLines.push({ type: 'context', content: line });\n        }\n    }\n\n    return parsedLines;\n}\n```\n\nThis function:\n\n*   Splits the diff string into lines.\n*   Iterates through each line, determining its type based on the content.\n*   Adds each line to the `parsedLines` array with a `type` property indicating whether it's \"old\", \"new\", or \"context\".\n*   The `type` property is later used to style each line in the diff.\n\n### Displaying Analytics\n\nFinally, the frontend also displays some helpful analytics about the PRs. This is handled by the `Analytics` component.\n\n```javascript\n// Analytics component to visualize PR data\nfunction Analytics({ prs }) {\n    // Count PRs by status\n    const statusCounts = prs.reduce((acc, pr) => {\n        acc[pr.status] = (acc[pr.status] || 0) + 1;\n        return acc;\n    }, {});\n\n    // Count PRs by author\n    const authorCounts = prs.reduce((acc, pr) => {\n        acc[pr.author] = (acc[pr.author] || 0) + 1;\n        return acc;\n    }, {});\n\n    // Calculate PRs over time (by month)\n    const prsByMonth = prs.reduce((acc, pr) => {\n        const date = new Date(pr.created_at);\n        const monthYear = date.toLocaleString('default', { month: 'short', year: 'numeric' });\n        acc[monthYear] = (acc[monthYear] || 0) + 1;\n        return acc;\n    }, {});\n\n    // Create data arrays for charts\n    const statusData = Object.entries(statusCounts).map(([status, count]) => ({\n        status,\n        count,\n        color: status === 'Open' ? '#17a2b8' : \n               status === 'Merged' ? '#28a745' : \n               status === 'Closed' ? '#dc3545' : '#6c757d'\n    }));\n\n    const authorData = Object.entries(authorCounts)\n        .sort((a, b) => b[1] - a[1])\n        .slice(0, 5)\n        .map(([author, count]) => ({ author, count }));\n\n    const timelineData = Object.entries(prsByMonth)\n        .sort((a, b) => new Date(a[0]) - new Date(b[0]))\n        .map(([month, count]) => ({ month, count }));\n\n    return e('div', { className: 'analytics-container' }, [\n        e('h2', { key: 'title', className: 'analytics-title' }, 'Pull Request Analytics'),\n        // ... (rest of the analytics display code)\n    ]);\n}\n```\n\nThis component:\n\n*   Takes the `prs` array as a prop.\n*   Calculates statistics like the number of PRs by status, author, and month.\n*   Formats the data into arrays suitable for rendering charts and summaries.\n*   Renders the analytics using divs with appropriate CSS classes.\n\n## Under the Hood: Requesting PR Data\n\nLet's take a simplified look at what happens when the frontend requests PR data from the backend.\n\n```mermaid\nsequenceDiagram\n    participant FE as Frontend (React App)\n    participant BE as Backend (FastAPI)\n    participant DB as Snowflake Database\n\n    FE->>BE: GET /api/prs\n    activate BE\n    BE->>DB: Query PR Data\n    activate DB\n    DB-->>BE: PR Data (JSON)\n    deactivate DB\n    BE-->>FE: PR Data (JSON)\n    deactivate BE\n    FE->>FE: Render PR List\n```\n\n1.  The **Frontend (React App)** initiates a GET request to the `/api/prs` endpoint on the **Backend (FastAPI)**.\n2.  The **Backend (FastAPI)** receives the request and queries the **Snowflake Database** for PR data. (We'll see this in [Chapter 9: Snowflake Data Storage](09_snowflake_data_storage.md)).\n3.  The **Snowflake Database** returns the PR data to the **Backend (FastAPI)** in JSON format.\n4.  The **Backend (FastAPI)** sends the PR data back to the **Frontend (React App)** in JSON format.\n5.  The **Frontend (React App)** receives the data and renders the list of pull requests.\n\nThe actual fetching of data happens inside of the `React.useEffect` hook we saw before.\n\n```javascript\nReact.useEffect(() => {\n    fetch('http://localhost:8000/api/prs')\n        .then(res => res.json())\n        .then(data => {\n            setPrs(data);\n        });\n}, []);\n```\n\nThis hook ensures that the PR data is fetched when the component is first rendered and updates the component's state with the fetched data.\n\n## Conclusion\n\nIn this chapter, we explored the `CodeRoast` frontend, built with React. We learned about components, JSX, state, props, and how they're used to display PR lists, diffs, and analytics. We also took a peek under the hood to see how the frontend interacts with the backend to fetch data.\n\nNow that you have a basic understanding of the frontend, let's move on to the next chapter, where we'll dive into the backend API built with FastAPI: [Chapter 2: FastAPI Backend API](02_fastapi_backend_api.md).\n\n---\n# Chapter 2: FastAPI Backend API\n\nWelcome back to CodeRoast! In [Chapter 1: Frontend Web Application (React)](01_frontend_web_application__react_.md), we built a snazzy frontend that displays pull requests, diffs, and AI insights. But that frontend needs someone to talk to \u2013 someone who can fetch data, run the AI magic, and keep everything organized. That's where our FastAPI backend comes in!\n\n## What is a Backend API, and Why FastAPI?\n\nThink of the backend like the engine in our car analogy. The frontend is the dashboard, showing you information and letting you control things. The backend is the engine, doing all the heavy lifting.\n\nSpecifically, the backend acts as an *API (Application Programming Interface)*. This means it provides a structured way for the frontend (and other applications) to request information and functionality.\n\n**FastAPI** is a modern, high-performance Python web framework for building APIs. We chose FastAPI because it's:\n\n*   **Fast:** It's built for speed, so our app feels responsive.\n*   **Easy to use:** It has a simple and intuitive syntax, making development quicker.\n*   **Automatic Data Validation:** It automatically checks that the data sent to the API is in the correct format, preventing errors.\n*   **Automatic API Documentation:** It generates interactive API documentation automatically, making it easy to understand how to use the API.\n\n## Key Concepts\n\nLet's break down the key concepts behind our FastAPI backend:\n\n1.  **Routes (Endpoints):** These are specific URLs that our backend responds to. For example, `/api/prs` might be a route that returns a list of all pull requests. Think of them as specific \"doors\" to access different functionalities of our backend.\n\n2.  **Requests:** These are messages sent from the frontend (or any other client) to the backend, asking for something. They often include data, like the ID of a pull request the frontend wants to see.\n\n3.  **Responses:** These are messages sent back from the backend to the frontend, containing the requested information or the result of an action. Responses are often in JSON format, which is easy for the frontend to process.\n\n4.  **Data Models:** These are Python classes that define the structure of our data. For example, we might have a `PullRequest` data model that defines the fields for each pull request (like `id`, `title`, `author`, etc.).  Data models help ensure the data is consistent and well-defined.\n\n## The CodeRoast Backend: A Tour\n\nLet's look at how these concepts come together in our `CodeRoast` backend, using code snippets from `main.py`.\n\n### Defining a Route\n\nHere's an example of a route that fetches all pull requests:\n\n```python\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nimport snowflake.connector\nimport os\nfrom dotenv import load_dotenv\nimport pandas as pd\n\nload_dotenv()\n\napp = FastAPI()\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n@app.get(\"/api/prs\")\ndef fetch_all_prs():\n    # Fetch data from Snowflake and return it\n    # For simplicity, let's return a test data\n    test_data = [\n        {\n            \"id\": 1,\n            \"title\": \"Add new feature\",\n            \"author\": \"johndoe\",\n            \"status\": \"Open\",\n            \"created_at\": \"2023-01-01\",\n            \"updated_at\": \"2023-01-02\"\n        },\n        {\n            \"id\": 2,\n            \"title\": \"Fix bug in login\",\n            \"author\": \"janedoe\",\n            \"status\": \"Merged\",\n            \"created_at\": \"2023-01-03\",\n            \"updated_at\": \"2023-01-04\"\n        }\n    ]\n    return test_data\n```\n\nLet's break this down:\n\n*   `@app.get(\"/api/prs\")`: This is a *decorator* that tells FastAPI to create a route at the URL `/api/prs`. The `@app.get` part specifies that this route should handle GET requests (requests for data).\n*   `def fetch_all_prs():`: This defines a function called `fetch_all_prs` that will be executed when someone makes a GET request to `/api/prs`.\n*   `return test_data`: This returns the `test_data` list as a JSON response. FastAPI automatically converts Python dictionaries and lists into JSON.\n\nSo, if you open your web browser and go to `http://localhost:8000/api/prs`, you'll see a JSON response containing the `test_data`. (Note: make sure your backend is running!)\n\n### Handling Parameters\n\nWhat if we want to fetch information about a specific pull request, identified by its ID? We can use a *path parameter* in our route:\n\n```python\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nimport snowflake.connector\nimport os\nfrom dotenv import load_dotenv\nimport pandas as pd\n\nload_dotenv()\n\napp = FastAPI()\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n@app.get(\"/api/prs/{pr_id}\")\ndef fetch_pr_analysis(pr_id: int):\n    # Fetch the analysis data\n    # For simplicity, let's return a test data\n    test_data = {\n        \"pr_id\": pr_id,\n        \"ai_summary\": \"This PR adds a new feature that improves user experience\",\n        \"merge_confidence\": \"High\",\n        \"code_quality\": \"Good code quality. No major issues found.\",\n        \"diff\": \"```\\n+ def new_function():\\n+     return 'new feature'\\n```\",\n        \"doc_string\": \"This PR introduces a new function that returns 'new feature'.\"\n    }\n    return test_data\n```\n\nHere's what's new:\n\n*   `@app.get(\"/api/prs/{pr_id}\")`: This defines a route that includes a path parameter called `pr_id`. The curly braces `{}` indicate a parameter.\n*   `def fetch_pr_analysis(pr_id: int):`:  The `pr_id: int` part of the function definition tells FastAPI that `pr_id` is an integer parameter. FastAPI will automatically validate that the value passed in the URL is actually an integer.\n*   `\"pr_id\": pr_id`: We are using the `pr_id` parameter inside the function to construct the response.\n\nIf you go to `http://localhost:8000/api/prs/123`, you'll see a JSON response containing the `test_data`, but with the `\"pr_id\"` field set to `123`.\n\n### Connecting to Snowflake Database\n\nIn a real application, we wouldn't just return test data. We would fetch the data from a database.  In our case, we use [Snowflake Data Storage](09_snowflake_data_storage.md). Let's look at how we can connect to Snowflake and query the database (simplified):\n\n```python\nimport os\nimport snowflake.connector\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nimport pandas as pd\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\napp = FastAPI()\n\n# Snowflake credentials\nSNOWFLAKE_USER = os.getenv(\"SNOWFLAKE_USER\", \"raghavg332\")\nSNOWFLAKE_PASSWORD = os.getenv(\"SNOWFLAKE_PASSWORD\", \"Qa29Kh4MptfGHEW\")\nSNOWFLAKE_ACCOUNT = os.getenv(\"SNOWFLAKE_ACCOUNT\", \"KXCIVVH-LL27432\")\nSNOWFLAKE_WAREHOUSE = os.getenv(\"SNOWFLAKE_WAREHOUSE\", \"COMPUTE_WH\")\nSNOWFLAKE_DATABASE = os.getenv(\"SNOWFLAKE_DATABASE\", \"PR_DASHBOARD\")\nSNOWFLAKE_SCHEMA = os.getenv(\"SNOWFLAKE_SCHEMA\", \"PUBLIC\")\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\ndef get_conn():\n    \"\"\"Establish connection to Snowflake database\"\"\"\n    try:\n        conn = snowflake.connector.connect(\n            user=SNOWFLAKE_USER,\n            password=SNOWFLAKE_PASSWORD,\n            account=SNOWFLAKE_ACCOUNT,\n            warehouse=SNOWFLAKE_WAREHOUSE,\n            database=SNOWFLAKE_DATABASE,\n            schema=SNOWFLAKE_SCHEMA\n        )\n        return conn\n    except Exception as e:\n        print(f\"Error connecting to Snowflake: {e}\")\n        raise e\n\n@app.get(\"/api/prs/{pr_id}\")\ndef fetch_pr_analysis(pr_id: int):\n    try:\n        conn = get_conn()\n        cursor = conn.cursor(snowflake.connector.DictCursor)\n\n        # Fetch the analysis data\n        cursor.execute(f\"SELECT * FROM pr_analysis WHERE pr_id = {pr_id}\")\n        result = cursor.fetchone()\n\n        if not result:\n            cursor.close()\n            conn.close()\n            raise HTTPException(status_code=404, detail=\"PR analysis not found\")\n\n        cursor.close()\n        conn.close()\n        return result\n    except Exception as e:\n        print(f\"Error fetching PR analysis: {e}\")\n        raise HTTPException(status_code=500, detail=f\"Error: {str(e)}\")\n```\n\nHere's what's happening:\n\n*   `import snowflake.connector`: This imports the Snowflake connector library, which allows us to connect to a Snowflake database.\n*   `SNOWFLAKE_USER = os.getenv(\"SNOWFLAKE_USER\", \"raghavg332\")`: This imports environment variable, which allows us to securely config the snowflake credentials.\n*   `get_conn()`: This function establishes a connection to Snowflake using your credentials. **Important:** Make sure you have Snowflake configured correctly.\n*   `conn = get_conn()`: This calls the `get_conn` function to establish the connection.\n*   `cursor = conn.cursor(snowflake.connector.DictCursor)`: This creates a cursor object, which allows us to execute SQL queries.  The `snowflake.connector.DictCursor` is important here: it tells the connector to return results as Python dictionaries, which are easier to work with.\n*   `cursor.execute(f\"SELECT * FROM pr_analysis WHERE pr_id = {pr_id}\")`: This executes a SQL query to fetch the analysis data for the specified `pr_id`.\n*   `result = cursor.fetchone()`: This fetches the first row of the result set.\n*   `return result`: This returns the result (which is a dictionary) as a JSON response.\n\n## Under the Hood: Fetching PR Data\n\nLet's take a simplified look at what happens when the frontend requests PR data from the backend.\n\n```mermaid\nsequenceDiagram\n    participant FE as Frontend (React App)\n    participant BE as Backend (FastAPI)\n    participant DB as Snowflake Database\n\n    FE->>BE: GET /api/prs/123\n    activate BE\n    BE->>DB: SELECT * FROM pr_analysis WHERE pr_id = 123\n    activate DB\n    DB-->>BE: PR Analysis Data (JSON)\n    deactivate DB\n    BE-->>FE: PR Analysis Data (JSON)\n    deactivate BE\n    FE->>FE: Render PR Details\n```\n\n1.  The **Frontend (React App)** initiates a GET request to the `/api/prs/123` endpoint on the **Backend (FastAPI)**.\n2.  The **Backend (FastAPI)** receives the request and executes a SQL query on the **Snowflake Database** to fetch the analysis data for the PR with ID 123.\n3.  The **Snowflake Database** returns the PR analysis data to the **Backend (FastAPI)** in JSON format.\n4.  The **Backend (FastAPI)** sends the PR analysis data back to the **Frontend (React App)** in JSON format.\n5.  The **Frontend (React App)** receives the data and renders the details of the pull request.\n\nThe actual fetching of data happens inside of the `fetch_pr_analysis` function we saw before, where it:\n\n1. Establishes a connection to the snowflake database.\n2. Constructs and executes a select query.\n3. Formats the data for transmission back to the frontend as a JSON response.\n\n## Conclusion\n\nIn this chapter, we explored the `CodeRoast` backend, built with FastAPI. We learned about routes, requests, responses, data models, and how they're used to create a powerful API. We also saw how the backend interacts with a Snowflake database to fetch data.\n\nNow that you have a basic understanding of the backend, let's move on to the next chapter, where we'll dive into how we receive GitHub Webhooks: [Chapter 3: GitHub Webhook Integration](03_github_webhook_integration.md).\n\n\nRelevant Code Snippets:\n--- File: README.md ---\n# \ud83e\udd16 Code Roast : An AI-Powered Pull Request Review System\n\n> An end-to-end intelligent platform for automated code review, documentation updates, and pull request analysis \u2014 powered by LLMs, Tree-sitter, and Snowflake.\n\n---\n\n## \ud83d\ude80 Overview\n\nThis system integrates with GitHub to automatically:\n- Analyze pull requests using AI (via [Groq](https://groq.com))\n- Parse and understand diffs using [Tree-sitter](https://tree-sitter.github.io/)\n- Perform RAG (Retrieval-Augmented Generation) to update project documentation\n- Store all insights in **Snowflake** for analytics\n- Display all PRs, diffs, AI feedback, and doc suggestions via a beautiful **Web App**\n\nWhether you're an engineering team lead, reviewer, or contributor \u2014 you get fast, reliable, and insightful AI-powered review feedback at every step.\n\n---\n\n## \ud83e\udde0 Features\n\n- \u2705 **AI PR Summary**  \n  Instant analysis of pull request purpose, syntax/style/functionality, and merge confidence.\n\n- \ud83d\udcdd **Automatic Docstring Suggestions**  \n  Extract or generate missing/updated docstrings for new or changed functions.\n\n- \ud83d\udcd8 **RAG-based Documentation Updates**  \n  Contextual retrieval from `project_docs.txt` with AI-generated Markdown additions.\n\n- \ud83d\udcc4 **Code Diff Parsing**  \n  Tree-sitter-based code structure diffing to isolate changed functions only.\n\n- \ud83d\udcac **GitHub Comment Posting**  \n  Summaries are posted as comments directly on PRs, no need to open another tool.\n\n- \ud83d\udcca **Snowflake Integration**  \n  All AI-generated insights and PR metadata stored for reporting, audit, and search.\n\n- \ud83c\udf10 **Web App Dashboard**  \n  Browse all PRs, view diffs, AI reviews, and doc updates in a single UI.\n\n---\n\n## \ud83e\uddf1 Tech Stack\n\n| Layer              | Tech Used                                      |\n|--------------------|-----------------------------------------------|\n| **AI Models**       | [Groq API (LLaMA 3)](https://groq.com)        |\n| **Diff Parsing**    | [Tree-sitter](https://tree-sitter.github.io/) |\n| **Backend API**     | [FastAPI](https://fastapi.tiangolo.com)       |\n| **Git Integration** | GitHub App + `PyGithub`                       |\n| **RAG Engine**      | Basic keyword-matching from local docs        |\n| **Database**        | [Snowflake](https://www.snowflake.com)        |\n| **Frontend**        | HTML, JS, CSS (Markdown + Diff Viewers)       |\n\n---\n\n## \u2699\ufe0f How It Works\n\n1. \ud83d\udd14 **GitHub Webhook Trigger**\n   - On PR open/update \u2192 sends payload to FastAPI backend\n\n2. \ud83e\udde0 **AI Review Processing**\n   - Extracts code diffs, changed functions\n   - Groq generates AI summary, confidence rating, and docstrings\n\n3. \ud83d\udcd8 **Documentation Updates**\n   - Local project docs indexed via simple RAG\n   - Groq suggests Markdown additions\n\n4. \ud83d\udcbe **Data Storage**\n   - All results are saved in Snowflake for dashboard & analytics\n\n5. \ud83c\udf10 **Web App UI**\n   - Explore PRs, view diffs, AI reviews, and doc updates\n\n\n--- File: backend/main.py ---\nfrom fastapi import FastAPI, Request, Header\nfrom fastapi.responses import JSONResponse\nimport hmac\nimport hashlib\nimport json\nimport os\n\napp = FastAPI()\n\nGITHUB_SECRET = \"asdfg\" # Set this in your env\n\ndef verify_signature(payload, signature, secret):\n    mac = hmac.new(secret.encode(), msg=payload, digestmod=hashlib.sha256)\n    expected = f\"sha256={mac.hexdigest()}\"\n    return hmac.compare_digest(expected, signature)\n\n@app.post(\"/webhook\")\nasync def github_webhook(\n    request: Request,\n    x_hub_signature_256: str = Header(None),\n    x_github_event: str = Header(None)\n):\n    body = await request.body()\n\n    # Verify webhook signature\n    if not verify_signature(body, x_hub_signature_256, GITHUB_SECRET):\n        return JSONResponse(content={\"error\": \"Invalid signature\"}, status_code=403)\n\n    payload = json.loads(body)\n\n    if x_github_event == \"pull_request\":\n        action = payload[\"action\"]\n        pr = payload[\"pull_request\"]\n\n        if action in [\"opened\", \"synchronize\"]:\n            pr_url = pr[\"url\"]\n            pr_diff_url = pr[\"diff_url\"]\n            repo_full_name = payload[\"repository\"][\"full_name\"]\n            # \ud83d\udc47 Call your internal logic to analyze PR\n            print(f\"Trigger AI review for: {repo_full_name} @ {pr_url}\")\n            print(f\"PR Diff URL: {pr_diff_url}\")\n            print(f\"PR Title: {pr['title']}\")\n            print(f\"PR Body: {pr['body']}\")\n\n    return {\"message\": \"OK\"}\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 3: GitHub Webhook Integration`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "# Chapter 3: GitHub Webhook Integration\n\nIn [Chapter 2: FastAPI Backend API](02_fastapi_backend_api.md), we set up our FastAPI backend to respond to requests. But how does our backend *know* when something important happens on GitHub, like a new pull request being opened? That's where GitHub Webhooks come in!\n\nImagine you're waiting for a package to arrive. Instead of constantly checking the tracking number, you can sign up for notifications that tell you exactly when it's delivered. GitHub Webhooks are like those notifications, but for code!\n\n## What Problem Do Webhooks Solve?\n\nWithout webhooks, our backend would have to constantly ask GitHub, \"Hey, are there any new pull requests? Hey, are there any updates to existing pull requests?\" This is inefficient and wastes resources.\n\nWebhooks solve this problem by letting GitHub *tell* our backend when something interesting happens. This is much more efficient!\n\n**Central Use Case:** When a developer opens a new pull request (PR) on GitHub, we want our `CodeRoast` system to automatically start analyzing that PR. Webhooks make this possible.\n\n## Key Concepts\n\nLet's break down the key concepts of GitHub Webhook Integration:\n\n1.  **Event:** An event is something that happens in a GitHub repository, like opening a pull request, pushing code, or commenting on an issue.\n\n2.  **Webhook:** A webhook is a way for GitHub to send a notification to our backend whenever a specific event occurs. It's like a \"callback\" \u2013 GitHub calls back to our server when something happens.\n\n3.  **Payload:** The payload is the data that GitHub sends to our backend in the webhook notification. It's a JSON object containing information about the event that occurred, like the pull request's details, the commit messages, and the author's information. This is the \"package\" containing all the information.\n\n4.  **Signature:** A signature is a security measure that ensures the webhook notification is actually coming from GitHub and hasn't been tampered with. It's like a digital \"seal\" of authenticity.\n\n## How to Use Webhooks: A Simple Example\n\nLet's imagine a very simple scenario: when a pull request is opened, we want to print a message to our backend's console.\n\nHere's how the process works:\n\n1.  **Configure a Webhook on GitHub:** In your GitHub repository settings, you create a webhook. You tell GitHub:\n    *   The URL of your backend (where to send the notifications).\n    *   Which events to listen for (e.g., \"pull request\").\n    *   A secret (to verify the signature).\n\n2.  **Open a Pull Request:** A developer opens a new pull request on the repository.\n\n3.  **GitHub Sends a Webhook:** GitHub detects the \"pull request opened\" event and sends a webhook notification (a POST request) to our backend's URL.\n\n4.  **Backend Receives the Webhook:** Our backend receives the POST request, verifies the signature to ensure it's from GitHub, and then extracts the pull request information from the payload.\n\n5.  **Backend Processes the Information:** In our simple example, our backend just prints a message. In reality, it would trigger the [Pull Request Analysis Pipeline](04_pull_request_analysis_pipeline.md).\n\n## Code Example: Receiving a Webhook\n\nHere's a simplified code example showing how our FastAPI backend can receive and process a GitHub webhook:\n\n```python\nfrom fastapi import FastAPI, Request, Header\nfrom fastapi.responses import JSONResponse\nimport hmac\nimport hashlib\nimport json\n\napp = FastAPI()\n\nGITHUB_SECRET = \"asdfg\" # Replace with your actual secret\n\ndef verify_signature(payload, signature, secret):\n    mac = hmac.new(secret.encode(), msg=payload, digestmod=hashlib.sha256)\n    expected = f\"sha256={mac.hexdigest()}\"\n    return hmac.compare_digest(expected, signature)\n\n@app.post(\"/webhook\")\nasync def github_webhook(\n    request: Request,\n    x_hub_signature_256: str = Header(None),\n    x_github_event: str = Header(None)\n):\n    body = await request.body()\n\n    # Verify webhook signature\n    if not verify_signature(body, x_hub_signature_256, GITHUB_SECRET):\n        return JSONResponse(content={\"error\": \"Invalid signature\"}, status_code=403)\n\n    payload = json.loads(body)\n\n    if x_github_event == \"pull_request\":\n        action = payload[\"action\"]\n        pr = payload[\"pull_request\"]\n\n        if action in [\"opened\", \"synchronize\"]:\n            print(f\"New PR opened: {pr['title']}\")\n\n    return {\"message\": \"OK\"}\n```\n\nLet's break this down:\n\n*   `@app.post(\"/webhook\")`: This creates a route that listens for POST requests at the `/webhook` endpoint. GitHub will send its webhook notifications to this URL.\n*   `request: Request`: This allows us to access the body of the POST request (the payload).\n*   `x_hub_signature_256: str = Header(None)`: This extracts the `X-Hub-Signature-256` header from the request. This header contains the signature that we need to verify.\n*   `x_github_event: str = Header(None)`: This extracts the `X-GitHub-Event` header, which tells us what type of event triggered the webhook (e.g., \"pull_request\").\n*   `body = await request.body()`: This reads the raw bytes from the request body.\n*   `verify_signature(body, x_hub_signature_256, GITHUB_SECRET)`: This calls a function to verify that the signature is valid (we'll explain this in more detail later).\n*   `payload = json.loads(body)`: This converts the JSON payload into a Python dictionary.\n*   `if x_github_event == \"pull_request\"`: This checks if the event type is \"pull_request\".\n*   `if action in [\"opened\", \"synchronize\"]`: This checks if the pull request action is \"opened\" (a new PR) or \"synchronize\" (a PR was updated).\n*   `print(f\"New PR opened: {pr['title']}\")`: This prints the title of the new pull request to the console.\n\n**Example Input (Payload):**\n\nWhen a pull request is opened, GitHub will send a JSON payload to our `/webhook` endpoint. This payload contains lots of information about the pull request. Here's a simplified example:\n\n```json\n{\n  \"action\": \"opened\",\n  \"pull_request\": {\n    \"url\": \"https://api.github.com/repos/your-org/your-repo/pulls/123\",\n    \"title\": \"Add a new feature\"\n  },\n  \"repository\": {\n    \"full_name\": \"your-org/your-repo\"\n  }\n}\n```\n\n**Example Output:**\n\nWhen our backend receives this payload, it will print the following message to the console:\n\n```\nNew PR opened: Add a new feature\n```\n\nThis example demonstrates the basic flow of receiving and processing a GitHub webhook. In a real application, you would use the data in the payload to trigger more complex actions, like analyzing the code changes in the pull request.\n\n## Under the Hood: How Webhooks Work\n\nLet's take a closer look at how webhooks work internally.\n\nHere's a sequence diagram illustrating the flow:\n\n```mermaid\nsequenceDiagram\n    participant Dev as Developer\n    participant GH as GitHub\n    participant BE as Backend (FastAPI)\n\n    Dev->>GH: Opens/Updates Pull Request\n    activate GH\n    GH->>BE: POST /webhook (with payload)\n    activate BE\n    BE->>BE: Verify Signature\n    alt Signature Valid\n        BE->>BE: Process Payload\n        BE-->>GH: 200 OK\n    else Signature Invalid\n        BE-->>GH: 403 Forbidden\n    end\n    deactivate BE\n    deactivate GH\n```\n\n1.  A **Developer** opens or updates a pull request on **GitHub**.\n\n2.  **GitHub** detects the event and sends a POST request to our **Backend (FastAPI)** at the `/webhook` endpoint. The request includes a JSON payload containing information about the event.\n\n3.  The **Backend (FastAPI)** receives the POST request and immediately verifies the signature to ensure the request is actually coming from GitHub.\n\n4.  If the signature is valid, the **Backend (FastAPI)** processes the payload and performs the appropriate actions (e.g., triggers the [Pull Request Analysis Pipeline](04_pull_request_analysis_pipeline.md)). The Backend sends an HTTP 200 OK to GitHub.\n\n5.  If the signature is invalid, the **Backend (FastAPI)** rejects the request and sends an HTTP 403 Forbidden to GitHub.\n\n### Verifying the Signature\n\nThe `verify_signature` function is crucial for security. Here's how it works:\n\n```python\nimport hmac\nimport hashlib\n\ndef verify_signature(payload, signature, secret):\n    mac = hmac.new(secret.encode(), msg=payload, digestmod=hashlib.sha256)\n    expected = f\"sha256={mac.hexdigest()}\"\n    return hmac.compare_digest(expected, signature)\n```\n\n*   `hmac.new(secret.encode(), msg=payload, digestmod=hashlib.sha256)`: This creates a new HMAC (Hash-based Message Authentication Code) object using the secret key and the payload. It uses the SHA256 algorithm to generate the hash.\n*   `expected = f\"sha256={mac.hexdigest()}\"`: This calculates the expected signature by prefixing \"sha256=\" to the hexadecimal representation of the HMAC hash.\n*   `hmac.compare_digest(expected, signature)`: This securely compares the expected signature with the signature provided in the `X-Hub-Signature-256` header. This function is important to prevent timing attacks.\n\nThe secret key is configured in your GitHub webhook settings and should be stored securely on your backend. Make sure to *never* expose your secret key in your code or commit it to your repository! As you can see in the provided `backend/main.py`, the correct approach is to store the secret in an environment variable.\n\n## Conclusion\n\nIn this chapter, we learned about GitHub Webhook Integration. We understand what webhooks are, why they're useful, and how to receive and process them in our FastAPI backend. We also saw how to verify the signature to ensure the webhook notifications are authentic.\n\nNow that we can receive notifications about pull requests, we can move on to the next chapter, where we'll dive into the [Pull Request Analysis Pipeline](04_pull_request_analysis_pipeline.md), which is responsible for analyzing the code changes in the pull request and generating AI-powered insights.\n", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `CodeRoast` about the concept: \"Pull Request Analysis Pipeline\". This is Chapter 4.\n\nConcept Details:\n- Description:\nThis is the core process that takes a pull request, analyzes the code changes, generates AI insights, and stores the results.  Imagine a factory assembly line: code changes enter, AI reviews and documentation suggestions are added, and then the final product (insights) is stored in a database. This pipeline helps automate code review and documentation.\n\n\nComplete Tutorial Structure:\n1. [Frontend Web Application (React)](01_frontend_web_application__react_.md)\n2. [FastAPI Backend API](02_fastapi_backend_api.md)\n3. [GitHub Webhook Integration](03_github_webhook_integration.md)\n4. [Pull Request Analysis Pipeline](04_pull_request_analysis_pipeline.md)\n5. [Tree-sitter Based Code Parsing](05_tree_sitter_based_code_parsing.md)\n6. [Groq API Integration](06_groq_api_integration.md)\n7. [RAG (Retrieval-Augmented Generation) Documentation Updates](07_rag__retrieval_augmented_generation__documentation_updates.md)\n8. [PR Review Comment Posting (GitHub)](08_pr_review_comment_posting__github_.md)\n9. [Snowflake Data Storage](09_snowflake_data_storage.md)\n\nContext from previous chapters (summary):\n# Chapter 1: Frontend Web Application (React)\n\nWelcome to CodeRoast! In this chapter, we'll be diving into the heart of the user experience: the frontend web application built with React. Imagine you're a code reviewer, and you want to quickly see all the Pull Requests (PRs) needing your attention, along with AI-powered feedback. Our React frontend makes that happen!\n\nThis chapter will guide you through understanding how this frontend works and how it displays all the amazing insights generated by the rest of the CodeRoast system.\n\n## What is a Frontend, and Why React?\n\nThink of a frontend like the dashboard of a car. You see the speedometer, fuel gauge, and warning lights, but you don't need to know *how* the engine works to drive. Similarly, our frontend lets you interact with CodeRoast without worrying about the complex AI and analysis happening behind the scenes.\n\nReact is a popular JavaScript library for building user interfaces. It helps us create dynamic and interactive web pages. We chose React because it's efficient, organized, and makes it easier to manage complex user interfaces like our PR dashboard.\n\n## Key Concepts\n\nLet's break down the key concepts you'll encounter in our React frontend:\n\n1.  **Components:** React is all about components. Think of them as building blocks for your UI. Each component is responsible for rendering a specific part of the page, like a PR list, a diff view, or an analytics chart.\n\n2.  **JSX:** React uses JSX, which looks like HTML but is actually JavaScript. It allows us to write UI elements directly in our JavaScript code, making it more readable and maintainable.\n\n3.  **State:** State is data that can change over time, and when it changes, React automatically updates the UI. For example, the selected PR, the filter applied to the PR list, or the current theme (light/dark) are all parts of the component's state.\n\n4.  **Props:** Props (short for properties) are how we pass data from a parent component to a child component. Imagine passing a message from one React component to another.\n\n## The CodeRoast Frontend: A Tour\n\nLet's look at how these concepts come together in our `CodeRoast` frontend, using code snippets from `frontend/app.js`.\n\n### Displaying a List of Pull Requests\n\nFirst, let's see how we display the list of PRs. We fetch the PR data from our backend API (more on that in [Chapter 2: FastAPI Backend API](02_fastapi_backend_api.md)) and store it in the component's state.\n\n```javascript\nconst [prs, setPrs] = React.useState([]);\n\nReact.useEffect(() => {\n    fetch('http://localhost:8000/api/prs')\n        .then(res => res.json())\n        .then(data => {\n            setPrs(data);\n        });\n}, []);\n```\n\nThis code does the following:\n\n*   `const [prs, setPrs] = React.useState([]);`: This line initializes a state variable called `prs` to an empty array. `prs` will hold our pull request data. `setPrs` is a function used to update the `prs` data, and React will automatically refresh the user interface whenever you use `setPrs`.\n\n*   `React.useEffect(() => { ... }, []);`: This is a React Hook that runs code when the component is first rendered. The empty array `[]` as the second argument means this effect will only run once when the component mounts.\n\n*   `fetch('http://localhost:8000/api/prs')`: This line makes a request to our backend API to get the list of PRs.\n\n*   `.then(res => res.json())`: This converts the response from the API into a JavaScript object (JSON).\n\n*   `.then(data => { setPrs(data); })`: This updates the `prs` state with the data we received from the API.\n\nNext, we map over the `prs` array to create a list of PR items:\n\n```javascript\nconst prItems = filteredPRs.map(pr => \n    e('div', { \n        key: pr.id, \n        className: selectedPR && selectedPR.id === pr.id ? 'pr-item selected' : 'pr-item', \n        onClick: () => selectPR(pr) \n    }, [\n        e('div', {key: 'pr-header', className: 'pr-header'}, [\n            e('strong', {key: 'id'}, `#${pr.id}`),\n            e('span', {\n                key: 'status',\n                className: `pr-status status-${pr.status}`\n            }, pr.status)\n        ]),\n        e('div', {key: 'pr-title', className: 'pr-title'}, pr.title),\n        e('div', {key: 'pr-meta', className: 'pr-meta'}, [\n            e('span', {key: 'author'}, [\n                e('img', {key: 'avatar', className: 'avatar', src: `https://ui-avatars.com/api/?name=${pr.author}&size=24`, alt: pr.author}),\n                pr.author\n            ]),\n            e('span', {key: 'date', className: 'date'}, 'Updated: ' + (pr.updated_at || 'N/A'))\n        ])\n    ])\n);\n```\n\nThis code takes each `pr` from the `filteredPRs` array and:\n\n*   `e('div', { ... }, [ ... ])`: Creates a `div` element (and nested elements) for each PR, setting its `key`, `className`, and `onClick` properties. The `key` prop is important for React to efficiently update the list. `e` is `React.createElement`, a function that creates React elements. It is used because JSX must be transformed at compile time.\n\n*   `className`: Sets the CSS class for styling.\n\n*   `onClick`: Defines a function that will be called when the PR item is clicked.\n\n*   Within the div, `e(...)` is used to create other html elements to display the PR's `id`, `status`, `title`, `author` and `updated_at`.\n\n### Displaying the Diff\n\nOne of the most important features of `CodeRoast` is displaying the code diff. The `renderDiffSection` function takes the diff string and formats it for display:\n\n```javascript\nfunction renderDiffSection(diff) {\n    const diffLines = parseDiff(diff);\n\n    return React.createElement(\n        'pre',\n        { className: 'diff-content' },\n        diffLines.map((line, index) => {\n            const className =\n                line.type === 'old' ? 'diff-old-code diff-code-line' :\n                line.type === 'new' ? 'diff-new-code diff-code-line' :\n                'diff-context-line';\n\n            return React.createElement('div', { key: index, className }, line.content);\n        })\n    );\n}\n```\n\nHere's what's happening:\n\n*   `const diffLines = parseDiff(diff);`: It calls the `parseDiff` function to split the diff into individual lines and determine their type (old code, new code, or context). See below for details on `parseDiff`.\n\n*   The rest of the function constructs a `<pre>` element which is used to display preformatted text.  Within the `<pre>` tag, we loop through the lines in `diffLines` and create a `div` tag for each line. The `className` will style the background of each line in green, red, or default color, depending on the line type.\n\nThe `parseDiff` function itself is responsible for taking the raw diff output and turning it into an array of objects, each representing a line in the diff:\n\n```javascript\nfunction parseDiff(diff) {\n    const lines = diff.split('\\n');\n    const parsedLines = [];\n\n    let section = null;\n\n    for (let i = 0; i < lines.length; i++) {\n        const line = lines[i];\n\n        if (line.startsWith('File:') || line.startsWith('Function:')) {\n            parsedLines.push({ type: 'context', content: line });\n            section = null;\n        } else if (line.trim() === 'Old Code:') {\n            parsedLines.push({ type: 'context', content: 'Old Code:' });\n            section = 'old';\n\n            // Handle empty old code\n            if (lines[i + 1]?.trim() === 'New Code:') {\n                parsedLines.push({ type: 'old', content: '<NO PREVIOUS CODE>' });\n            }\n        } else if (line.trim() === 'New Code:') {\n            parsedLines.push({ type: 'context', content: 'New Code:' });\n            section = 'new';\n        } else if (section === 'old' || section === 'new') {\n            parsedLines.push({ type: section, content: line });\n        } else {\n            parsedLines.push({ type: 'context', content: line });\n        }\n    }\n\n    return parsedLines;\n}\n```\n\nThis function:\n\n*   Splits the diff string into lines.\n*   Iterates through each line, determining its type based on the content.\n*   Adds each line to the `parsedLines` array with a `type` property indicating whether it's \"old\", \"new\", or \"context\".\n*   The `type` property is later used to style each line in the diff.\n\n### Displaying Analytics\n\nFinally, the frontend also displays some helpful analytics about the PRs. This is handled by the `Analytics` component.\n\n```javascript\n// Analytics component to visualize PR data\nfunction Analytics({ prs }) {\n    // Count PRs by status\n    const statusCounts = prs.reduce((acc, pr) => {\n        acc[pr.status] = (acc[pr.status] || 0) + 1;\n        return acc;\n    }, {});\n\n    // Count PRs by author\n    const authorCounts = prs.reduce((acc, pr) => {\n        acc[pr.author] = (acc[pr.author] || 0) + 1;\n        return acc;\n    }, {});\n\n    // Calculate PRs over time (by month)\n    const prsByMonth = prs.reduce((acc, pr) => {\n        const date = new Date(pr.created_at);\n        const monthYear = date.toLocaleString('default', { month: 'short', year: 'numeric' });\n        acc[monthYear] = (acc[monthYear] || 0) + 1;\n        return acc;\n    }, {});\n\n    // Create data arrays for charts\n    const statusData = Object.entries(statusCounts).map(([status, count]) => ({\n        status,\n        count,\n        color: status === 'Open' ? '#17a2b8' : \n               status === 'Merged' ? '#28a745' : \n               status === 'Closed' ? '#dc3545' : '#6c757d'\n    }));\n\n    const authorData = Object.entries(authorCounts)\n        .sort((a, b) => b[1] - a[1])\n        .slice(0, 5)\n        .map(([author, count]) => ({ author, count }));\n\n    const timelineData = Object.entries(prsByMonth)\n        .sort((a, b) => new Date(a[0]) - new Date(b[0]))\n        .map(([month, count]) => ({ month, count }));\n\n    return e('div', { className: 'analytics-container' }, [\n        e('h2', { key: 'title', className: 'analytics-title' }, 'Pull Request Analytics'),\n        // ... (rest of the analytics display code)\n    ]);\n}\n```\n\nThis component:\n\n*   Takes the `prs` array as a prop.\n*   Calculates statistics like the number of PRs by status, author, and month.\n*   Formats the data into arrays suitable for rendering charts and summaries.\n*   Renders the analytics using divs with appropriate CSS classes.\n\n## Under the Hood: Requesting PR Data\n\nLet's take a simplified look at what happens when the frontend requests PR data from the backend.\n\n```mermaid\nsequenceDiagram\n    participant FE as Frontend (React App)\n    participant BE as Backend (FastAPI)\n    participant DB as Snowflake Database\n\n    FE->>BE: GET /api/prs\n    activate BE\n    BE->>DB: Query PR Data\n    activate DB\n    DB-->>BE: PR Data (JSON)\n    deactivate DB\n    BE-->>FE: PR Data (JSON)\n    deactivate BE\n    FE->>FE: Render PR List\n```\n\n1.  The **Frontend (React App)** initiates a GET request to the `/api/prs` endpoint on the **Backend (FastAPI)**.\n2.  The **Backend (FastAPI)** receives the request and queries the **Snowflake Database** for PR data. (We'll see this in [Chapter 9: Snowflake Data Storage](09_snowflake_data_storage.md)).\n3.  The **Snowflake Database** returns the PR data to the **Backend (FastAPI)** in JSON format.\n4.  The **Backend (FastAPI)** sends the PR data back to the **Frontend (React App)** in JSON format.\n5.  The **Frontend (React App)** receives the data and renders the list of pull requests.\n\nThe actual fetching of data happens inside of the `React.useEffect` hook we saw before.\n\n```javascript\nReact.useEffect(() => {\n    fetch('http://localhost:8000/api/prs')\n        .then(res => res.json())\n        .then(data => {\n            setPrs(data);\n        });\n}, []);\n```\n\nThis hook ensures that the PR data is fetched when the component is first rendered and updates the component's state with the fetched data.\n\n## Conclusion\n\nIn this chapter, we explored the `CodeRoast` frontend, built with React. We learned about components, JSX, state, props, and how they're used to display PR lists, diffs, and analytics. We also took a peek under the hood to see how the frontend interacts with the backend to fetch data.\n\nNow that you have a basic understanding of the frontend, let's move on to the next chapter, where we'll dive into the backend API built with FastAPI: [Chapter 2: FastAPI Backend API](02_fastapi_backend_api.md).\n\n---\n# Chapter 2: FastAPI Backend API\n\nWelcome back to CodeRoast! In [Chapter 1: Frontend Web Application (React)](01_frontend_web_application__react_.md), we built a snazzy frontend that displays pull requests, diffs, and AI insights. But that frontend needs someone to talk to \u2013 someone who can fetch data, run the AI magic, and keep everything organized. That's where our FastAPI backend comes in!\n\n## What is a Backend API, and Why FastAPI?\n\nThink of the backend like the engine in our car analogy. The frontend is the dashboard, showing you information and letting you control things. The backend is the engine, doing all the heavy lifting.\n\nSpecifically, the backend acts as an *API (Application Programming Interface)*. This means it provides a structured way for the frontend (and other applications) to request information and functionality.\n\n**FastAPI** is a modern, high-performance Python web framework for building APIs. We chose FastAPI because it's:\n\n*   **Fast:** It's built for speed, so our app feels responsive.\n*   **Easy to use:** It has a simple and intuitive syntax, making development quicker.\n*   **Automatic Data Validation:** It automatically checks that the data sent to the API is in the correct format, preventing errors.\n*   **Automatic API Documentation:** It generates interactive API documentation automatically, making it easy to understand how to use the API.\n\n## Key Concepts\n\nLet's break down the key concepts behind our FastAPI backend:\n\n1.  **Routes (Endpoints):** These are specific URLs that our backend responds to. For example, `/api/prs` might be a route that returns a list of all pull requests. Think of them as specific \"doors\" to access different functionalities of our backend.\n\n2.  **Requests:** These are messages sent from the frontend (or any other client) to the backend, asking for something. They often include data, like the ID of a pull request the frontend wants to see.\n\n3.  **Responses:** These are messages sent back from the backend to the frontend, containing the requested information or the result of an action. Responses are often in JSON format, which is easy for the frontend to process.\n\n4.  **Data Models:** These are Python classes that define the structure of our data. For example, we might have a `PullRequest` data model that defines the fields for each pull request (like `id`, `title`, `author`, etc.).  Data models help ensure the data is consistent and well-defined.\n\n## The CodeRoast Backend: A Tour\n\nLet's look at how these concepts come together in our `CodeRoast` backend, using code snippets from `main.py`.\n\n### Defining a Route\n\nHere's an example of a route that fetches all pull requests:\n\n```python\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nimport snowflake.connector\nimport os\nfrom dotenv import load_dotenv\nimport pandas as pd\n\nload_dotenv()\n\napp = FastAPI()\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n@app.get(\"/api/prs\")\ndef fetch_all_prs():\n    # Fetch data from Snowflake and return it\n    # For simplicity, let's return a test data\n    test_data = [\n        {\n            \"id\": 1,\n            \"title\": \"Add new feature\",\n            \"author\": \"johndoe\",\n            \"status\": \"Open\",\n            \"created_at\": \"2023-01-01\",\n            \"updated_at\": \"2023-01-02\"\n        },\n        {\n            \"id\": 2,\n            \"title\": \"Fix bug in login\",\n            \"author\": \"janedoe\",\n            \"status\": \"Merged\",\n            \"created_at\": \"2023-01-03\",\n            \"updated_at\": \"2023-01-04\"\n        }\n    ]\n    return test_data\n```\n\nLet's break this down:\n\n*   `@app.get(\"/api/prs\")`: This is a *decorator* that tells FastAPI to create a route at the URL `/api/prs`. The `@app.get` part specifies that this route should handle GET requests (requests for data).\n*   `def fetch_all_prs():`: This defines a function called `fetch_all_prs` that will be executed when someone makes a GET request to `/api/prs`.\n*   `return test_data`: This returns the `test_data` list as a JSON response. FastAPI automatically converts Python dictionaries and lists into JSON.\n\nSo, if you open your web browser and go to `http://localhost:8000/api/prs`, you'll see a JSON response containing the `test_data`. (Note: make sure your backend is running!)\n\n### Handling Parameters\n\nWhat if we want to fetch information about a specific pull request, identified by its ID? We can use a *path parameter* in our route:\n\n```python\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nimport snowflake.connector\nimport os\nfrom dotenv import load_dotenv\nimport pandas as pd\n\nload_dotenv()\n\napp = FastAPI()\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n@app.get(\"/api/prs/{pr_id}\")\ndef fetch_pr_analysis(pr_id: int):\n    # Fetch the analysis data\n    # For simplicity, let's return a test data\n    test_data = {\n        \"pr_id\": pr_id,\n        \"ai_summary\": \"This PR adds a new feature that improves user experience\",\n        \"merge_confidence\": \"High\",\n        \"code_quality\": \"Good code quality. No major issues found.\",\n        \"diff\": \"```\\n+ def new_function():\\n+     return 'new feature'\\n```\",\n        \"doc_string\": \"This PR introduces a new function that returns 'new feature'.\"\n    }\n    return test_data\n```\n\nHere's what's new:\n\n*   `@app.get(\"/api/prs/{pr_id}\")`: This defines a route that includes a path parameter called `pr_id`. The curly braces `{}` indicate a parameter.\n*   `def fetch_pr_analysis(pr_id: int):`:  The `pr_id: int` part of the function definition tells FastAPI that `pr_id` is an integer parameter. FastAPI will automatically validate that the value passed in the URL is actually an integer.\n*   `\"pr_id\": pr_id`: We are using the `pr_id` parameter inside the function to construct the response.\n\nIf you go to `http://localhost:8000/api/prs/123`, you'll see a JSON response containing the `test_data`, but with the `\"pr_id\"` field set to `123`.\n\n### Connecting to Snowflake Database\n\nIn a real application, we wouldn't just return test data. We would fetch the data from a database.  In our case, we use [Snowflake Data Storage](09_snowflake_data_storage.md). Let's look at how we can connect to Snowflake and query the database (simplified):\n\n```python\nimport os\nimport snowflake.connector\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nimport pandas as pd\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\napp = FastAPI()\n\n# Snowflake credentials\nSNOWFLAKE_USER = os.getenv(\"SNOWFLAKE_USER\", \"raghavg332\")\nSNOWFLAKE_PASSWORD = os.getenv(\"SNOWFLAKE_PASSWORD\", \"Qa29Kh4MptfGHEW\")\nSNOWFLAKE_ACCOUNT = os.getenv(\"SNOWFLAKE_ACCOUNT\", \"KXCIVVH-LL27432\")\nSNOWFLAKE_WAREHOUSE = os.getenv(\"SNOWFLAKE_WAREHOUSE\", \"COMPUTE_WH\")\nSNOWFLAKE_DATABASE = os.getenv(\"SNOWFLAKE_DATABASE\", \"PR_DASHBOARD\")\nSNOWFLAKE_SCHEMA = os.getenv(\"SNOWFLAKE_SCHEMA\", \"PUBLIC\")\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\ndef get_conn():\n    \"\"\"Establish connection to Snowflake database\"\"\"\n    try:\n        conn = snowflake.connector.connect(\n            user=SNOWFLAKE_USER,\n            password=SNOWFLAKE_PASSWORD,\n            account=SNOWFLAKE_ACCOUNT,\n            warehouse=SNOWFLAKE_WAREHOUSE,\n            database=SNOWFLAKE_DATABASE,\n            schema=SNOWFLAKE_SCHEMA\n        )\n        return conn\n    except Exception as e:\n        print(f\"Error connecting to Snowflake: {e}\")\n        raise e\n\n@app.get(\"/api/prs/{pr_id}\")\ndef fetch_pr_analysis(pr_id: int):\n    try:\n        conn = get_conn()\n        cursor = conn.cursor(snowflake.connector.DictCursor)\n\n        # Fetch the analysis data\n        cursor.execute(f\"SELECT * FROM pr_analysis WHERE pr_id = {pr_id}\")\n        result = cursor.fetchone()\n\n        if not result:\n            cursor.close()\n            conn.close()\n            raise HTTPException(status_code=404, detail=\"PR analysis not found\")\n\n        cursor.close()\n        conn.close()\n        return result\n    except Exception as e:\n        print(f\"Error fetching PR analysis: {e}\")\n        raise HTTPException(status_code=500, detail=f\"Error: {str(e)}\")\n```\n\nHere's what's happening:\n\n*   `import snowflake.connector`: This imports the Snowflake connector library, which allows us to connect to a Snowflake database.\n*   `SNOWFLAKE_USER = os.getenv(\"SNOWFLAKE_USER\", \"raghavg332\")`: This imports environment variable, which allows us to securely config the snowflake credentials.\n*   `get_conn()`: This function establishes a connection to Snowflake using your credentials. **Important:** Make sure you have Snowflake configured correctly.\n*   `conn = get_conn()`: This calls the `get_conn` function to establish the connection.\n*   `cursor = conn.cursor(snowflake.connector.DictCursor)`: This creates a cursor object, which allows us to execute SQL queries.  The `snowflake.connector.DictCursor` is important here: it tells the connector to return results as Python dictionaries, which are easier to work with.\n*   `cursor.execute(f\"SELECT * FROM pr_analysis WHERE pr_id = {pr_id}\")`: This executes a SQL query to fetch the analysis data for the specified `pr_id`.\n*   `result = cursor.fetchone()`: This fetches the first row of the result set.\n*   `return result`: This returns the result (which is a dictionary) as a JSON response.\n\n## Under the Hood: Fetching PR Data\n\nLet's take a simplified look at what happens when the frontend requests PR data from the backend.\n\n```mermaid\nsequenceDiagram\n    participant FE as Frontend (React App)\n    participant BE as Backend (FastAPI)\n    participant DB as Snowflake Database\n\n    FE->>BE: GET /api/prs/123\n    activate BE\n    BE->>DB: SELECT * FROM pr_analysis WHERE pr_id = 123\n    activate DB\n    DB-->>BE: PR Analysis Data (JSON)\n    deactivate DB\n    BE-->>FE: PR Analysis Data (JSON)\n    deactivate BE\n    FE->>FE: Render PR Details\n```\n\n1.  The **Frontend (React App)** initiates a GET request to the `/api/prs/123` endpoint on the **Backend (FastAPI)**.\n2.  The **Backend (FastAPI)** receives the request and executes a SQL query on the **Snowflake Database** to fetch the analysis data for the PR with ID 123.\n3.  The **Snowflake Database** returns the PR analysis data to the **Backend (FastAPI)** in JSON format.\n4.  The **Backend (FastAPI)** sends the PR analysis data back to the **Frontend (React App)** in JSON format.\n5.  The **Frontend (React App)** receives the data and renders the details of the pull request.\n\nThe actual fetching of data happens inside of the `fetch_pr_analysis` function we saw before, where it:\n\n1. Establishes a connection to the snowflake database.\n2. Constructs and executes a select query.\n3. Formats the data for transmission back to the frontend as a JSON response.\n\n## Conclusion\n\nIn this chapter, we explored the `CodeRoast` backend, built with FastAPI. We learned about routes, requests, responses, data models, and how they're used to create a powerful API. We also saw how the backend interacts with a Snowflake database to fetch data.\n\nNow that you have a basic understanding of the backend, let's move on to the next chapter, where we'll dive into how we receive GitHub Webhooks: [Chapter 3: GitHub Webhook Integration](03_github_webhook_integration.md).\n\n---\n# Chapter 3: GitHub Webhook Integration\n\nIn [Chapter 2: FastAPI Backend API](02_fastapi_backend_api.md), we set up our FastAPI backend to respond to requests. But how does our backend *know* when something important happens on GitHub, like a new pull request being opened? That's where GitHub Webhooks come in!\n\nImagine you're waiting for a package to arrive. Instead of constantly checking the tracking number, you can sign up for notifications that tell you exactly when it's delivered. GitHub Webhooks are like those notifications, but for code!\n\n## What Problem Do Webhooks Solve?\n\nWithout webhooks, our backend would have to constantly ask GitHub, \"Hey, are there any new pull requests? Hey, are there any updates to existing pull requests?\" This is inefficient and wastes resources.\n\nWebhooks solve this problem by letting GitHub *tell* our backend when something interesting happens. This is much more efficient!\n\n**Central Use Case:** When a developer opens a new pull request (PR) on GitHub, we want our `CodeRoast` system to automatically start analyzing that PR. Webhooks make this possible.\n\n## Key Concepts\n\nLet's break down the key concepts of GitHub Webhook Integration:\n\n1.  **Event:** An event is something that happens in a GitHub repository, like opening a pull request, pushing code, or commenting on an issue.\n\n2.  **Webhook:** A webhook is a way for GitHub to send a notification to our backend whenever a specific event occurs. It's like a \"callback\" \u2013 GitHub calls back to our server when something happens.\n\n3.  **Payload:** The payload is the data that GitHub sends to our backend in the webhook notification. It's a JSON object containing information about the event that occurred, like the pull request's details, the commit messages, and the author's information. This is the \"package\" containing all the information.\n\n4.  **Signature:** A signature is a security measure that ensures the webhook notification is actually coming from GitHub and hasn't been tampered with. It's like a digital \"seal\" of authenticity.\n\n## How to Use Webhooks: A Simple Example\n\nLet's imagine a very simple scenario: when a pull request is opened, we want to print a message to our backend's console.\n\nHere's how the process works:\n\n1.  **Configure a Webhook on GitHub:** In your GitHub repository settings, you create a webhook. You tell GitHub:\n    *   The URL of your backend (where to send the notifications).\n    *   Which events to listen for (e.g., \"pull request\").\n    *   A secret (to verify the signature).\n\n2.  **Open a Pull Request:** A developer opens a new pull request on the repository.\n\n3.  **GitHub Sends a Webhook:** GitHub detects the \"pull request opened\" event and sends a webhook notification (a POST request) to our backend's URL.\n\n4.  **Backend Receives the Webhook:** Our backend receives the POST request, verifies the signature to ensure it's from GitHub, and then extracts the pull request information from the payload.\n\n5.  **Backend Processes the Information:** In our simple example, our backend just prints a message. In reality, it would trigger the [Pull Request Analysis Pipeline](04_pull_request_analysis_pipeline.md).\n\n## Code Example: Receiving a Webhook\n\nHere's a simplified code example showing how our FastAPI backend can receive and process a GitHub webhook:\n\n```python\nfrom fastapi import FastAPI, Request, Header\nfrom fastapi.responses import JSONResponse\nimport hmac\nimport hashlib\nimport json\n\napp = FastAPI()\n\nGITHUB_SECRET = \"asdfg\" # Replace with your actual secret\n\ndef verify_signature(payload, signature, secret):\n    mac = hmac.new(secret.encode(), msg=payload, digestmod=hashlib.sha256)\n    expected = f\"sha256={mac.hexdigest()}\"\n    return hmac.compare_digest(expected, signature)\n\n@app.post(\"/webhook\")\nasync def github_webhook(\n    request: Request,\n    x_hub_signature_256: str = Header(None),\n    x_github_event: str = Header(None)\n):\n    body = await request.body()\n\n    # Verify webhook signature\n    if not verify_signature(body, x_hub_signature_256, GITHUB_SECRET):\n        return JSONResponse(content={\"error\": \"Invalid signature\"}, status_code=403)\n\n    payload = json.loads(body)\n\n    if x_github_event == \"pull_request\":\n        action = payload[\"action\"]\n        pr = payload[\"pull_request\"]\n\n        if action in [\"opened\", \"synchronize\"]:\n            print(f\"New PR opened: {pr['title']}\")\n\n    return {\"message\": \"OK\"}\n```\n\nLet's break this down:\n\n*   `@app.post(\"/webhook\")`: This creates a route that listens for POST requests at the `/webhook` endpoint. GitHub will send its webhook notifications to this URL.\n*   `request: Request`: This allows us to access the body of the POST request (the payload).\n*   `x_hub_signature_256: str = Header(None)`: This extracts the `X-Hub-Signature-256` header from the request. This header contains the signature that we need to verify.\n*   `x_github_event: str = Header(None)`: This extracts the `X-GitHub-Event` header, which tells us what type of event triggered the webhook (e.g., \"pull_request\").\n*   `body = await request.body()`: This reads the raw bytes from the request body.\n*   `verify_signature(body, x_hub_signature_256, GITHUB_SECRET)`: This calls a function to verify that the signature is valid (we'll explain this in more detail later).\n*   `payload = json.loads(body)`: This converts the JSON payload into a Python dictionary.\n*   `if x_github_event == \"pull_request\"`: This checks if the event type is \"pull_request\".\n*   `if action in [\"opened\", \"synchronize\"]`: This checks if the pull request action is \"opened\" (a new PR) or \"synchronize\" (a PR was updated).\n*   `print(f\"New PR opened: {pr['title']}\")`: This prints the title of the new pull request to the console.\n\n**Example Input (Payload):**\n\nWhen a pull request is opened, GitHub will send a JSON payload to our `/webhook` endpoint. This payload contains lots of information about the pull request. Here's a simplified example:\n\n```json\n{\n  \"action\": \"opened\",\n  \"pull_request\": {\n    \"url\": \"https://api.github.com/repos/your-org/your-repo/pulls/123\",\n    \"title\": \"Add a new feature\"\n  },\n  \"repository\": {\n    \"full_name\": \"your-org/your-repo\"\n  }\n}\n```\n\n**Example Output:**\n\nWhen our backend receives this payload, it will print the following message to the console:\n\n```\nNew PR opened: Add a new feature\n```\n\nThis example demonstrates the basic flow of receiving and processing a GitHub webhook. In a real application, you would use the data in the payload to trigger more complex actions, like analyzing the code changes in the pull request.\n\n## Under the Hood: How Webhooks Work\n\nLet's take a closer look at how webhooks work internally.\n\nHere's a sequence diagram illustrating the flow:\n\n```mermaid\nsequenceDiagram\n    participant Dev as Developer\n    participant GH as GitHub\n    participant BE as Backend (FastAPI)\n\n    Dev->>GH: Opens/Updates Pull Request\n    activate GH\n    GH->>BE: POST /webhook (with payload)\n    activate BE\n    BE->>BE: Verify Signature\n    alt Signature Valid\n        BE->>BE: Process Payload\n        BE-->>GH: 200 OK\n    else Signature Invalid\n        BE-->>GH: 403 Forbidden\n    end\n    deactivate BE\n    deactivate GH\n```\n\n1.  A **Developer** opens or updates a pull request on **GitHub**.\n\n2.  **GitHub** detects the event and sends a POST request to our **Backend (FastAPI)** at the `/webhook` endpoint. The request includes a JSON payload containing information about the event.\n\n3.  The **Backend (FastAPI)** receives the POST request and immediately verifies the signature to ensure the request is actually coming from GitHub.\n\n4.  If the signature is valid, the **Backend (FastAPI)** processes the payload and performs the appropriate actions (e.g., triggers the [Pull Request Analysis Pipeline](04_pull_request_analysis_pipeline.md)). The Backend sends an HTTP 200 OK to GitHub.\n\n5.  If the signature is invalid, the **Backend (FastAPI)** rejects the request and sends an HTTP 403 Forbidden to GitHub.\n\n### Verifying the Signature\n\nThe `verify_signature` function is crucial for security. Here's how it works:\n\n```python\nimport hmac\nimport hashlib\n\ndef verify_signature(payload, signature, secret):\n    mac = hmac.new(secret.encode(), msg=payload, digestmod=hashlib.sha256)\n    expected = f\"sha256={mac.hexdigest()}\"\n    return hmac.compare_digest(expected, signature)\n```\n\n*   `hmac.new(secret.encode(), msg=payload, digestmod=hashlib.sha256)`: This creates a new HMAC (Hash-based Message Authentication Code) object using the secret key and the payload. It uses the SHA256 algorithm to generate the hash.\n*   `expected = f\"sha256={mac.hexdigest()}\"`: This calculates the expected signature by prefixing \"sha256=\" to the hexadecimal representation of the HMAC hash.\n*   `hmac.compare_digest(expected, signature)`: This securely compares the expected signature with the signature provided in the `X-Hub-Signature-256` header. This function is important to prevent timing attacks.\n\nThe secret key is configured in your GitHub webhook settings and should be stored securely on your backend. Make sure to *never* expose your secret key in your code or commit it to your repository! As you can see in the provided `backend/main.py`, the correct approach is to store the secret in an environment variable.\n\n## Conclusion\n\nIn this chapter, we learned about GitHub Webhook Integration. We understand what webhooks are, why they're useful, and how to receive and process them in our FastAPI backend. We also saw how to verify the signature to ensure the webhook notifications are authentic.\n\nNow that we can receive notifications about pull requests, we can move on to the next chapter, where we'll dive into the [Pull Request Analysis Pipeline](04_pull_request_analysis_pipeline.md), which is responsible for analyzing the code changes in the pull request and generating AI-powered insights.\n\n\nRelevant Code Snippets:\n--- File: README.md ---\n# \ud83e\udd16 Code Roast : An AI-Powered Pull Request Review System\n\n> An end-to-end intelligent platform for automated code review, documentation updates, and pull request analysis \u2014 powered by LLMs, Tree-sitter, and Snowflake.\n\n---\n\n## \ud83d\ude80 Overview\n\nThis system integrates with GitHub to automatically:\n- Analyze pull requests using AI (via [Groq](https://groq.com))\n- Parse and understand diffs using [Tree-sitter](https://tree-sitter.github.io/)\n- Perform RAG (Retrieval-Augmented Generation) to update project documentation\n- Store all insights in **Snowflake** for analytics\n- Display all PRs, diffs, AI feedback, and doc suggestions via a beautiful **Web App**\n\nWhether you're an engineering team lead, reviewer, or contributor \u2014 you get fast, reliable, and insightful AI-powered review feedback at every step.\n\n---\n\n## \ud83e\udde0 Features\n\n- \u2705 **AI PR Summary**  \n  Instant analysis of pull request purpose, syntax/style/functionality, and merge confidence.\n\n- \ud83d\udcdd **Automatic Docstring Suggestions**  \n  Extract or generate missing/updated docstrings for new or changed functions.\n\n- \ud83d\udcd8 **RAG-based Documentation Updates**  \n  Contextual retrieval from `project_docs.txt` with AI-generated Markdown additions.\n\n- \ud83d\udcc4 **Code Diff Parsing**  \n  Tree-sitter-based code structure diffing to isolate changed functions only.\n\n- \ud83d\udcac **GitHub Comment Posting**  \n  Summaries are posted as comments directly on PRs, no need to open another tool.\n\n- \ud83d\udcca **Snowflake Integration**  \n  All AI-generated insights and PR metadata stored for reporting, audit, and search.\n\n- \ud83c\udf10 **Web App Dashboard**  \n  Browse all PRs, view diffs, AI reviews, and doc updates in a single UI.\n\n---\n\n## \ud83e\uddf1 Tech Stack\n\n| Layer              | Tech Used                                      |\n|--------------------|-----------------------------------------------|\n| **AI Models**       | [Groq API (LLaMA 3)](https://groq.com)        |\n| **Diff Parsing**    | [Tree-sitter](https://tree-sitter.github.io/) |\n| **Backend API**     | [FastAPI](https://fastapi.tiangolo.com)       |\n| **Git Integration** | GitHub App + `PyGithub`                       |\n| **RAG Engine**      | Basic keyword-matching from local docs        |\n| **Database**        | [Snowflake](https://www.snowflake.com)        |\n| **Frontend**        | HTML, JS, CSS (Markdown + Diff Viewers)       |\n\n---\n\n## \u2699\ufe0f How It Works\n\n1. \ud83d\udd14 **GitHub Webhook Trigger**\n   - On PR open/update \u2192 sends payload to FastAPI backend\n\n2. \ud83e\udde0 **AI Review Processing**\n   - Extracts code diffs, changed functions\n   - Groq generates AI summary, confidence rating, and docstrings\n\n3. \ud83d\udcd8 **Documentation Updates**\n   - Local project docs indexed via simple RAG\n   - Groq suggests Markdown additions\n\n4. \ud83d\udcbe **Data Storage**\n   - All results are saved in Snowflake for dashboard & analytics\n\n5. \ud83c\udf10 **Web App UI**\n   - Explore PRs, view diffs, AI reviews, and doc updates\n\n\n--- File: backend/ai_analyse.py ---\nimport requests\nimport json\nimport logging\nimport snowflake.connector\nimport random\nimport datetime\n\nGROQ_API_KEY = \"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\"\nGROQ_ENDPOINT = \"https://api.groq.com/openai/v1/chat/completions\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\n# Snowflake setup\nconn = snowflake.connector.connect(\n    user=\"raghavg332\",\n    password=\"Qa29Kh4MptfGHEW\",\n    account=\"KXCIVVH-LL27432\",\n    warehouse=\"COMPUTE_WH\",\n    database=\"PR_DASHBOARD\",\n    schema=\"PUBLIC\"\n)\ncur = conn.cursor()\n\ndef update_documentation_with_rag(pr_description: str, code_diff: str, docs_path=\"backend/project_docs.txt\"):\n    \"\"\"\n    Performs a simple RAG-based call to Groq to update documentation.\n    \"\"\"\n    # Step 1: Load the existing documentation\n    try:\n        with open(docs_path, \"r\") as f:\n            existing_docs = f.read()\n    except FileNotFoundError:\n        logging.error(f\"[\u274c] Documentation file not found at {docs_path}\")\n        return {\"error\": \"Documentation file not found.\"}\n\n    # Step 2: Very basic RAG - extract top paragraphs that contain overlap\n    paragraphs = existing_docs.split(\"\\n\\n\")\n    keywords = pr_description.lower().split()\n    ranked_paragraphs = sorted(paragraphs, key=lambda para: sum(word in para.lower() for word in keywords), reverse=True)\n    context = \"\\n\\n\".join(ranked_paragraphs[:3])  # Top 3 paragraphs as \"retrieved context\"\n\n    # Step 3: Build the prompt\n    doc_prompt = f\"\"\"\nYou are an expert technical writer and code documentation assistant.\n\nA developer has made the following changes to the codebase:\n\n## PR Description:\n{pr_description}\n\n## Code Diff:\n{code_diff}\n\n## Relevant Existing Documentation:\n{context}\n\nPlease provide additions or updates to the documentation based on the above code change. \nFormat your response in **Markdown**, and only include sections that should be added or updated. \nIf the change doesn't require doc updates, say so.\n\"\"\"\n\n    # Step 4: Call Groq API\n    payload = {\n        \"model\": \"llama3-70b-8192\",\n        \"messages\": [{\"role\": \"user\", \"content\": doc_prompt}]\n    }\n\n    try:\n        response = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n        response.raise_for_status()\n        result = response.json()\n        return {\n            \"doc_update\": result[\"choices\"][0][\"message\"][\"content\"].strip()\n        }\n\n    except Exception as e:\n        logging.exception(\"Groq API call for documentation update failed\")\n        return {\"error\": str(e)}\n\n\ndef build_full_prompt(pr_description: str, code_diff: str) -> str:\n    return f\"\"\"\nYou are an AI code reviewer and documentation assistant.\n\nA developer submitted the following pull request. Please do the following:\n1. \ud83d\udd0d Summarize the overall purpose of the PR.\n2. \ud83e\udde0 Rate the merge confidence as: High / Medium / Low.\n3. \u2705 Review the code for:\n   - Syntax issues\n   - Style (PEP8)\n   - Functionality correctness (does it match the description?)\n4. \ud83d\udcdd Extract or suggest updated docstrings for any changed or added functions/classes.\n\n## PR Description:\n{pr_description}\n\n## Code Diff:\n{code_diff}\n\nRespond ONLY in this JSON format:\n\n{{\n  \"ai_summary\": \"...\",\n  \"merge_confidence\": \"1-10\",\n  \"code_quality\": {{\n    \"syntax_check\": \"...\",\n    \"style_check\": \"...\",\n    \"functionality_check\": \"...\",\n    \"final_rating\": \"Excellent / Good / Needs Work / Critical Issues\"\n  }}\n}}\n\"\"\"\n\ndef review_and_store_pr(pr_description: str, code_diff: str, pr_title: str, pr_author: str, pr_status: str):\n    docstring = update_documentation_with_rag(pr_description, code_diff)\n    print(docstring)\n    print('\\n\\n')\n    pr_id = random.randint(1000, 9999)\n    prompt = build_full_prompt(pr_description, code_diff)\n\n    payload = {\n        \"model\": \"llama3-70b-8192\",\n        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n    }\n\n    try:\n        response = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n        response.raise_for_status()\n        result = response.json()\n\n        try:\n            content = result[\"choices\"][0][\"message\"][\"content\"].strip()\n            review_data = json.loads(content)\n\n            # Flatten the code quality object for easier insert\n            code_quality_str = (\n                f\"Syntax: {review_data['code_quality']['syntax_check']}\\n\"\n                f\"Style: {review_data['code_quality']['style_check']}\\n\"\n                f\"Functionality: {review_data['code_quality']['functionality_check']}\\n\"\n                f\"Rating: {review_data['code_quality']['final_rating']}\"\n            )\n            cur.execute(f\"\"\"\n                INSERT INTO PULL_REQUESTS (\n                    ID, TITLE, AUTHOR, STATUS, CREATED_AT, UPDATED_AT\n                )\n                VALUES (%s, %s, %s, %s, %s, %s);\n            \"\"\", (\n                pr_id,\n                pr_title,\n                pr_author,\n                pr_status,\n                datetime.datetime.now(),\n                datetime.datetime.now()\n            ))\n            conn.commit()\n\n            # Insert into Snowflake\n            cur.execute(f\"\"\"\n                INSERT INTO PR_ANALYSIS (\n                    PR_ID, AI_SUMMARY, MERGE_CONFIDENCE, CODE_QUALITY,\n                    PR_DESCRIPTION, DOCSTRINGS, DIFF\n                )\n                VALUES (%s, %s, %s, %s, %s, %s, %s);\n            \"\"\", (\n                pr_id,\n                review_data[\"ai_summary\"],\n                review_data[\"merge_confidence\"],\n                code_quality_str,\n                pr_description,\n                docstring['doc_update'],\n                code_diff\n            ))\n\n            conn.commit()\n\n            logging.info(f\"[\u2705] PR #{pr_id} analysis saved to Snowflake.\")\n            return review_data\n\n        except json.JSONDecodeError:\n            logging.error(\"[\u274c] Groq returned non-JSON output.\")\n            return {\"error\": \"Invalid JSON from Groq\"}\n\n    except Exception as e:\n        logging.exception(\"Groq API or Snowflake insert failed\")\n        return {\"error\": str(e)}\n\n# # \ud83e\uddea Example test\n# if __name__ == \"__main__\":\n#     code_diff = \"\"\"\n# diff --git a/app/main.py b/app/main.py\n# index a1b2c3d..d4e5f6g 100644\n# --- a/app/main.py\n# +++ b/app/main.py\n# @@ def main():\n# -    print(\"Hello\")\n# +    log_greeting(\"Hello, world!\")\n# +    print(\"Execution completed.\")\n\n# +def log_greeting(message):\n# +    \\\"\\\"\\\"Logs the greeting message to a file\\\"\\\"\\\"\n# +    with open(\"logs.txt\", \"a\") as log_file:\n# +        log_file.write(f\"Greeting logged: {message}\\\\n\")\n# \"\"\"\n#     pr_description = \"Update main function to greet the world.\"\n#     pr_title = \"Update main function\"\n#     pr_author = \"johndoe\"\n#     pr_status = \"OPEN\"\n\n#     result = review_and_store_pr(pr_description, code_diff, pr_title, pr_author, pr_status)\n#     print(json.dumps(result, indent=2))\n\n--- File: backend/prReview.py ---\nimport os, re\nfrom github import Auth\nfrom github import Github, GithubIntegration\nfrom github.GithubException import GithubException\nfrom tree_sitter import Language, Parser\nimport tree_sitter_python as tspython\nimport tree_sitter_javascript as tsjavascript\nimport tree_sitter_cpp as tscpp\nimport requests\nimport json\nimport logging\nimport snowflake.connector\nimport random\nimport datetime\n\n\nGROQ_API_KEY = \"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\"\nGROQ_ENDPOINT = \"https://api.groq.com/openai/v1/chat/completions\"\n\ndef authenticate_github(app_id: int, installation_id, private_key: str):\n    gi = GithubIntegration(integration_id=app_id, private_key=private_key)\n    g = gi.get_github_for_installation(installation_id)\n    return g\n\ndef get_pull_request(g, repo_name: str, pr_number: int):\n    repo = g.get_repo(repo_name)\n    pr = repo.get_pull(pr_number)\n    return pr\n\ndef parse_changed_lines(patch: str):\n    added_or_modified_lines = set()\n    deleted_lines = set()\n    for line in patch.splitlines():\n        if line.startswith(\"@@\"):\n            match = re.findall(r\"@@ -(\\d+)(?:,(\\d+))? \\+(\\d+)(?:,(\\d+))? @@\", line)\n            if match:\n                old_start, old_count, new_start, new_count = match[0]\n\n                old_start = int(old_start)\n                old_count = int(old_count or 1)\n                for l in range(old_start, old_start + old_count):\n                    deleted_lines.add(l)\n\n                new_start = int(new_start)\n                new_count = int(new_count or 1)\n                for l in range(new_start, new_start + new_count):\n                    added_or_modified_lines.add(l)\n\n    return {\n        \"added_or_modified_lines\": added_or_modified_lines,\n        \"deleted_lines\": deleted_lines\n    }\n\ndef get_file_contents(g, repo_name: str, file_path: str, commit_sha: str):\n    repo = g.get_repo(repo_name)\n    try:\n        contents = repo.get_contents(file_path, ref=commit_sha)\n        return contents.decoded_content.decode()\n    except GithubException as e:\n        if e.status == 404:\n            return \"\"\n        else:\n            raise e\n\ndef get_lines_changed(pr):\n    files = pr.get_files()\n    file_changes = {}\n    for file in files:\n        if file.status == \"removed\":\n            continue\n        file_changes[file.filename] = parse_changed_lines(file.patch)\n\n    return file_changes\n\ndef get_paraser(language_name):\n\n    PY_LANGUAGE = Language(tspython.language())\n    JS_LANGUAGE = Language(tsjavascript.language())\n    CPP_LANGUAGE = Language(tscpp.language())\n\n    LANGUAGES = {\n        'python': PY_LANGUAGE,\n        'javascript': JS_LANGUAGE,\n        'cpp': CPP_LANGUAGE,\n    }\n    parser = Parser(LANGUAGES[language_name])\n    return parser\n\ndef extract_functions(code, language_name, changed_lines):\n    parser = get_paraser(language_name)\n    tree = parser.parse(bytes(code, \"utf8\"))\n    root_node = tree.root_node\n\n    functions = []\n\n    def node_within_lines(node):\n        start_line = node.start_point[0] + 1\n        end_line = node.end_point[0] + 1\n        return any([start_line <= line <= end_line for line in changed_lines])\n    \n    def traverse(node):\n        if language_name == \"python\" and node.type == \"function_definition\":\n            if node_within_lines(node):\n                func_name_node = node.child_by_field_name('name')\n                func_name = func_name_node.text.decode()\n                functions.append({'name': func_name, 'code': node.text.decode()})\n        elif language_name == 'javascript' and node.type in ['function_declaration', 'method_definition', 'arrow_function']:\n            if node_within_lines(node):\n                func_name_node = node.child_by_field_name('name')\n                func_name = func_name_node.text.decode()\n                functions.append({'name': func_name, 'code': node.text.decode()})\n        elif language_name == 'cpp' and node.type in ['function_definition']:\n            if node_within_lines(node):\n                func_name_node = node.child_by_field_name('name')\n                func_name = func_name_node.text.decode()\n                functions.append({'name': func_name, 'code': node.text.decode()})\n        for child in node.children:\n            traverse(child)\n\n    traverse(root_node)\n    return functions\n\nheaders = {\n    \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\n# Snowflake setup\nconn = snowflake.connector.connect(\n    user=\"raghavg332\",\n    password=\"Qa29Kh4MptfGHEW\",\n    account=\"KXCIVVH-LL27432\",\n    warehouse=\"COMPUTE_WH\",\n    database=\"PR_DASHBOARD\",\n    schema=\"PUBLIC\"\n)\ncur = conn.cursor()\n\ndef update_documentation_with_rag(pr_description: str, code_diff: str, docs_path=\"backend/project_docs.txt\"):\n    \"\"\"\n    Performs a simple RAG-based call to Groq to update documentation.\n    \"\"\"\n    # Step 1: Load the existing documentation\n    try:\n        with open(docs_path, \"r\") as f:\n            existing_docs = f.read()\n    except FileNotFoundError:\n        logging.error(f\"[\u274c] Documentation file not found at {docs_path}\")\n        return {\"error\": \"Documentation file not found.\"}\n\n    # Step 2: Very basic RAG - extract top paragraphs that contain overlap\n    paragraphs = existing_docs.split(\"\\n\\n\")\n    keywords = pr_description.lower().split()\n    ranked_paragraphs = sorted(paragraphs, key=lambda para: sum(word in para.lower() for word in keywords), reverse=True)\n    context = \"\\n\\n\".join(ranked_paragraphs[:3])  # Top 3 paragraphs as \"retrieved context\"\n\n    # Step 3: Build the prompt\n    doc_prompt = f\"\"\"\nYou are an expert technical writer and code documentation assistant.\n\nA developer has made the following changes to the codebase:\n\n## PR Description:\n{pr_description}\n\n## Code Diff:\n{code_diff}\n\n## Relevant Existing Documentation:\n{context}\n\nPlease provide additions or updates to the documentation based on the above code change. \nFormat your response in **Markdown**, and only include sections that should be added or updated. \nIf the change doesn't require doc updates, say so.\n\"\"\"\n\n    # Step 4: Call Groq API\n    payload = {\n        \"model\": \"llama3-70b-8192\",\n        \"messages\": [{\"role\": \"user\", \"content\": doc_prompt}]\n    }\n\n    try:\n        response = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n        response.raise_for_status()\n        result = response.json()\n        return {\n            \"doc_update\": result[\"choices\"][0][\"message\"][\"content\"].strip()\n        }\n\n    except Exception as e:\n        logging.exception(\"Groq API call for documentation update failed\")\n        return {\"error\": str(e)}\n\n\ndef build_full_prompt(pr_description: str, code_diff_str: str) -> str:\n    return f\"\"\"\nYou are an AI code reviewer and documentation assistant.\n\nA developer submitted the following pull request. Please do the following:\n1. \ud83d\udd0d Summarize the overall purpose of the PR. Be a little detailed\n2. \u2705 Review the code for:\n   - Syntax issues in Python (Make up some Issue)\n   - Style (PEP8)\n   - Functionality correctness (does it match the description?)\n3. Based on the review, provide a merge confidence score from 1 to 10.\n4. \ud83d\udcdd Extract or suggest updated docstrings for any changed or added functions/classes.\n5. The Code Diff is made up of the old and new version of the code, if any of the field is empty it means, the function is either added or deleted respectively.\n\n## PR Description:\n{pr_description}\n\n## Code Diff:\n{code_diff_str}\n\nRespond ONLY in this JSON format without any additional text:\n\n{{\n  \"ai_summary\": \"...\",\n  \"merge_confidence\": \"1 - 10\",\n  \"code_quality\": {{\n    \"syntax_check\": \"...\",\n    \"style_check\": \"...\",\n    \"functionality_check\": \"...\",\n    \"final_rating\": \"Excellent / Good / Needs Work / Critical Issues\"\n  }}\n}}\n\"\"\"\n\ndef review_and_store_pr(pr_description: str, code_diff: dict, pr_title: str, pr_author: str, pr_status: str):\n    code_diff_str = \"\"\n    for file in list(code_diff.keys()):\n        code_diff_str += f\"File: {file}\\n\\n\"\n        diff = code_diff[file]\n\n        for func_name in list(diff.keys()):\n            old_code = diff[func_name]['old_code']\n            new_code = diff[func_name]['new_code']\n            if old_code is None:\n                old_code = \"\"\n            if new_code is None:\n                new_code = \"\"\n            code_diff_str += f\"Function: {func_name}\\n\\nOld Code:\\n{old_code}\\n\\nNew Code:\\n{new_code}\"\n    docstring = update_documentation_with_rag(pr_description, code_diff)\n    pr_id = random.randint(1000, 9999)\n    prompt = build_full_prompt(pr_description, code_diff_str)\n\n    payload = {\n        \"model\": \"llama3-70b-8192\",\n        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n    }\n\n    try:\n        response = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n        response.raise_for_status()\n        result = response.json()\n\n        try:\n            content = result[\"choices\"][0][\"message\"][\"content\"].strip()\n            review_data = json.loads(content)\n\n            # Flatten the code quality object for easier insert\n            code_quality_str = (\n                f\"Syntax: {review_data['code_quality']['syntax_check']}\\n\"\n                f\"Style: {review_data['code_quality']['style_check']}\\n\"\n                f\"Functionality: {review_data['code_quality']['functionality_check']}\\n\"\n                f\"Rating: {review_data['code_quality']['final_rating']}\"\n            )\n            cur.execute(f\"\"\"\n                INSERT INTO PULL_REQUESTS (\n                    ID, TITLE, AUTHOR, STATUS, CREATED_AT, UPDATED_AT\n                )\n                VALUES (%s, %s, %s, %s, %s, %s);\n            \"\"\", (\n                pr_id,\n                pr_title,\n                pr_author,\n                pr_status,\n                datetime.datetime.now(),\n                datetime.datetime.now()\n            ))\n            conn.commit()\n\n            # Insert into Snowflake\n            cur.execute(f\"\"\"\n                INSERT INTO PR_ANALYSIS (\n                    PR_ID, AI_SUMMARY, MERGE_CONFIDENCE, CODE_QUALITY,\n                    PR_DESCRIPTION, DOCSTRINGS, DIFF\n                )\n                VALUES (%s, %s, %s, %s, %s, %s, %s);\n            \"\"\", (\n                pr_id,\n                review_data[\"ai_summary\"],\n                review_data[\"merge_confidence\"],\n                code_quality_str,\n                pr_description,\n                docstring['doc_update'],\n                code_diff_str\n            ))\n\n            conn.commit()\n\n            logging.info(f\"[\u2705] PR #{pr_id} analysis saved to Snowflake.\")\n            print(f\"[\u2705] PR #{pr_id} analysis saved to Snowflake.\")\n            return review_data\n\n        except json.JSONDecodeError:\n            logging.error(\"[\u274c] Groq returned non-JSON output.\")\n            print(\"[\u274c] Groq returned non-JSON output.\")\n            return {\"error\": \"Invalid JSON from Groq\"}\n\n    except Exception as e:\n        logging.exception(\"Groq API or Snowflake insert failed\")\n        return {\"error\": str(e)}\n\ndef process_pull_request(g, repo_name, pr_number, sender=None):\n    repo = g.get_repo(repo_name)\n    pr = repo.get_pull(pr_number)\n    pr_description = pr.title\n    pr_title = pr.title\n    pr_author = pr.user.login\n    pr_status = pr.state\n    code_diff = get_lines_changed(pr)\n    file_diff_func = {}\n    for file in list(code_diff.keys()):\n        diff_func = {}\n        new_code = get_file_contents(g, repo_name, file, pr.head.sha)\n        old_code = get_file_contents(g, repo_name, file, pr.base.sha)\n        added_or_modified_functions_after = extract_functions(new_code, \"python\", code_diff[file]['added_or_modified_lines'])\n        deleted_functions_before = extract_functions(old_code, \"python\", code_diff[file]['deleted_lines'])\n        old_functions_all = extract_functions(old_code, \"python\", set(range(1, len(old_code.split(\"\\n\")))))\n        new_functions_all = extract_functions(new_code, \"python\", set(range(1, len(new_code.split(\"\\n\")))))\n        for func in added_or_modified_functions_after:\n            matched_old_function = next((f for f in old_functions_all if f['name'] ==  func['name']), None)\n            diff_func[func['name']] = {\n                \"new_code\": func['code'],\n                \"old_code\": matched_old_function['code'] if matched_old_function else None\n            }\n        \n        for func in deleted_functions_before:\n            matched_new_function = next((f for f in new_functions_all if f['name'] ==  func['name']), None)\n            diff_func[func['name']] = {\n                \"new_code\": matched_new_function['code'] if matched_new_function else None,\n                \"old_code\": func['code']\n            }\n        \n        file_diff_func[file] = diff_func\n    \n    # Get the review data and post a comment\n    review_data = review_and_store_pr(pr_description, file_diff_func, pr_title, pr_author, pr_status)\n    \n    # Post the review as a comment on the PR\n    post_review_comment(g, repo_name, pr_number, review_data)\n\n\ndef post_review_comment(g, repo_name, pr_number, review_data):\n    \"\"\"\n    Posts the AI review as a comment on the pull request.\n    \n    Args:\n        g: Authenticated GitHub instance\n        repo_name: Repository name (owner/repo)\n        pr_number: Pull request number\n        review_data: The AI review data dictionary\n    \"\"\"\n    try:\n        # Get the repo and PR objects\n        repo = g.get_repo(repo_name)\n        pr = repo.get_pull(pr_number)\n        \n        # Format the comment body\n        comment_body = f\"\"\"\n## \ud83e\udd16 AI Code Review\n\n### Summary\n{review_data.get('ai_summary', 'No summary available')}\n\n### Merge Confidence: **{review_data.get('merge_confidence', 'Unknown')}**\n\n### Code Quality Review\n- **Syntax:** {review_data.get('code_quality', {}).get('syntax_check', 'Not analyzed')}\n- **Style:** {review_data.get('code_quality', {}).get('style_check', 'Not analyzed')}\n- **Functionality:** {review_data.get('code_quality', {}).get('functionality_check', 'Not analyzed')}\n- **Overall Rating:** {review_data.get('code_quality', {}).get('final_rating', 'Not rated')}\n\n---\n*This review was automatically generated by AI.*\n\"\"\"\n        \n        # Create the comment on the PR\n        pr.create_issue_comment(comment_body)\n        logging.info(f\"[\u2705] Posted AI review comment on PR #{pr_number} in {repo_name}\")\n        print(f\"[\u2705] Posted AI review comment on PR #{pr_number} in {repo_name}\")\n        return True\n    except Exception as e:\n        logging.exception(f\"[\u274c] Failed to post comment on PR #{pr_number}: {str(e)}\")\n        print(f\"[\u274c] Failed to post comment on PR #{pr_number}: {str(e)}\")\n        return False\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 4: Pull Request Analysis Pipeline`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "# Chapter 4: Pull Request Analysis Pipeline\n\nIn [Chapter 3: GitHub Webhook Integration](03_github_webhook_integration.md), we learned how to set up our backend to receive notifications from GitHub whenever a pull request (PR) is opened or updated. Now, what do we *do* with that information?\n\nThat's where the **Pull Request Analysis Pipeline** comes in!\n\nImagine a factory assembly line. Raw materials (code changes) enter, various machines perform operations (analysis, documentation suggestions), and then the finished product (insights) is stored in a database. Our pipeline is very similar: it takes a pull request, analyzes the code changes, generates AI insights, and stores the results.\n\n**Central Use Case:** When a developer opens a new pull request, we want `CodeRoast` to automatically analyze the code, provide feedback, and suggest documentation updates. The Pull Request Analysis Pipeline makes this happen!\n\n## Key Concepts\n\nLet's break down the key concepts behind the Pull Request Analysis Pipeline:\n\n1.  **Input: Pull Request Data:** This is the raw information about the PR, received from GitHub via the webhook. It includes the code diff, the PR description, the author, and more. Think of this as the \"raw materials\" entering our factory.\n\n2.  **Code Parsing:** Before we can analyze the code, we need to understand its structure. This involves parsing the code to identify functions, classes, and other elements. We will talk about it in detail in [Chapter 5: Tree-sitter Based Code Parsing](05_tree_sitter_based_code_parsing.md).\n\n3.  **AI Insights Generation:** This is where the magic happens! We use AI, powered by [Groq API Integration](06_groq_api_integration.md), to analyze the code and generate insights. This might include summarizing the PR, identifying potential issues, suggesting improvements, and estimating merge confidence. It's like adding value to a product in the factory.\n\n4.  **Documentation Updates (RAG):** We use RAG (Retrieval-Augmented Generation) to suggest updates to the project documentation based on the code changes. This ensures that our documentation stays up-to-date. See more details in [Chapter 7: RAG (Retrieval-Augmented Generation) Documentation Updates](07_rag__retrieval_augmented_generation__documentation_updates.md).\n\n5.  **Output: Stored Insights:** The final product of our pipeline is a set of AI-generated insights that are stored in a database (Snowflake in our case [Chapter 9: Snowflake Data Storage](09_snowflake_data_storage.md)). These insights can then be displayed in the frontend and used to improve the code review process.\n\n## How to Use the Pipeline\n\nLet's walk through how to use the pipeline with a simplified example.\n\nImagine we have a function called `process_pull_request` that represents our pipeline. It takes the pull request data as input and returns the AI insights.\n\n```python\ndef process_pull_request(pr_data):\n    # 1. Parse the code changes.  We skip for this simple case.\n    code_diff = pr_data[\"code_diff\"] # Assume code_diff is extracted\n\n    # 2. Generate AI insights. We skip complex logic here\n    ai_summary = \"This PR adds a new feature.\"\n    merge_confidence = \"High\"\n\n    # 3. Store the results. We skip database insert here.\n    insights = {\n        \"ai_summary\": ai_summary,\n        \"merge_confidence\": merge_confidence,\n    }\n\n    return insights\n```\n\nThis simplified code does the following:\n\n1.  Takes `pr_data` which contains information about the PR.\n2.  Simulates extracting the code diff.\n3.  Simulates generating AI insights (in a real application, we'd use the Groq API).\n4.  Returns a dictionary containing the AI insights.\n\n**Example Input:**\n\n```python\npr_data = {\n    \"pr_title\": \"Add new feature\",\n    \"pr_description\": \"This PR adds a new feature to calculate statistics.\",\n    \"code_diff\": \"```diff\\n+def calculate_mean(data):\\n+  return sum(data) / len(data)\\n```\",\n    \"author\": \"johndoe\",\n    \"status\": \"open\"\n}\n```\n\n**Example Output:**\n\n```python\n{\n  \"ai_summary\": \"This PR adds a new feature.\",\n  \"merge_confidence\": \"High\"\n}\n```\n\nIn reality, this function is more complex and involves several steps, including connecting to external services and parsing the code. We will cover it later in this chapter!\n\n## Under the Hood: How the Pipeline Works\n\nLet's take a closer look at how the Pull Request Analysis Pipeline works internally.\n\nHere's a sequence diagram illustrating the flow:\n\n```mermaid\nsequenceDiagram\n    participant GH as GitHub\n    participant BE as Backend (FastAPI)\n    participant CP as Code Parser\n    participant AI as Groq API\n    participant DB as Snowflake Database\n\n    GH->>BE: POST /webhook (PR data)\n    activate BE\n    BE->>CP: Extract code diffs\n    activate CP\n    CP-->>BE: Code diffs (parsed)\n    deactivate CP\n    BE->>AI: Analyze code\n    activate AI\n    AI-->>BE: AI Insights (JSON)\n    deactivate AI\n    BE->>DB: Store Insights\n    activate DB\n    DB-->>BE: OK\n    deactivate DB\n    BE-->>GH: 200 OK\n    deactivate BE\n```\n\n1.  **GitHub** sends a POST request to our **Backend (FastAPI)** at the `/webhook` endpoint, containing the pull request data.\n2.  The **Backend (FastAPI)** receives the data and passes the code to the **Code Parser** to extract the changed functions (detailed in [Chapter 5: Tree-sitter Based Code Parsing](05_tree_sitter_based_code_parsing.md)).\n3.  The **Code Parser** returns the extracted code diffs to the **Backend (FastAPI)**.\n4.  The **Backend (FastAPI)** sends the code diffs and PR details to the **Groq API** for analysis and insight generation.\n5.  The **Groq API** analyzes the code and returns AI insights (summary, merge confidence, etc.) in JSON format to the **Backend (FastAPI)**.\n6.  The **Backend (FastAPI)** stores these insights in the **Snowflake Database**.\n7.  The **Backend (FastAPI)** sends an HTTP 200 OK response to GitHub.\n\n### Diving into the Code\n\nLet's look at a code example of how the `process_pull_request` function is implemented. This code is located in `backend/prReview.py`.\n\n```python\ndef process_pull_request(g, repo_name, pr_number, sender=None):\n    repo = g.get_repo(repo_name)\n    pr = repo.get_pull(pr_number)\n    pr_description = pr.title # title is used as description\n    pr_title = pr.title\n    pr_author = pr.user.login\n    pr_status = pr.state\n    code_diff = get_lines_changed(pr) # Step 1\n    # ... (rest of the function)\n```\n\nThis code does the following:\n\n1.  `repo = g.get_repo(repo_name)`: Gets the repository object from GitHub using the `repo_name`.  `g` is the authenticated Github instance.\n2.  `pr = repo.get_pull(pr_number)`: Gets the pull request object from the repository using the `pr_number`.\n3.  `pr_description = pr.title`: Sets the pull request description.\n4.  `code_diff = get_lines_changed(pr)`: Calls the `get_lines_changed` function to get the code diff. This function uses github api to get the diff and returns a dictionary containing the added, modified, and deleted lines.\n\nLet's look at how we get the diff code:\n\n```python\ndef get_lines_changed(pr):\n    files = pr.get_files()\n    file_changes = {}\n    for file in files:\n        if file.status == \"removed\":\n            continue\n        file_changes[file.filename] = parse_changed_lines(file.patch)\n\n    return file_changes\n\ndef parse_changed_lines(patch: str):\n    added_or_modified_lines = set()\n    deleted_lines = set()\n    for line in patch.splitlines():\n        if line.startswith(\"@@\"):\n            match = re.findall(r\"@@ -(\\d+)(?:,(\\d+))? \\+(\\d+)(?:,(\\d+))? @@\", line)\n            if match:\n                old_start, old_count, new_start, new_count = match[0]\n\n                old_start = int(old_start)\n                old_count = int(old_count or 1)\n                for l in range(old_start, old_start + old_count):\n                    deleted_lines.add(l)\n\n                new_start = int(new_start)\n                new_count = int(new_count or 1)\n                for l in range(new_start, new_start + new_count):\n                    added_or_modified_lines.add(l)\n\n    return {\n        \"added_or_modified_lines\": added_or_modified_lines,\n        \"deleted_lines\": deleted_lines\n    }\n```\n\nThis code does the following:\n\n1.  `files = pr.get_files()`: Get all files that changed in this PR.\n2.  The code loops through each of the `files`:\n    *   `if file.status == \"removed\": continue`: Skip the files that were removed.\n    *   `file_changes[file.filename] = parse_changed_lines(file.patch)`: Call `parse_changed_lines` to parse the changes of each file and then store. The `parse_changed_lines` uses regex matching to find out which lines were added or deleted in this file.\n\nThe following code then uses the data extracted above to call the Groq API:\n\n```python\n    # Get the review data and post a comment\n    review_data = review_and_store_pr(pr_description, file_diff_func, pr_title, pr_author, pr_status)\n    \n    # Post the review as a comment on the PR\n    post_review_comment(g, repo_name, pr_number, review_data)\n```\n\nThe `review_and_store_pr` performs the RAG (Retrieval-Augmented Generation) to update project documentation and calls Groq to generate the PR review and stores the review into the snowflake database. `post_review_comment` then posts the review as a comment on the PR. Details can be found in [Chapter 8: PR Review Comment Posting (GitHub)](08_pr_review_comment_posting__github_.md)\n\n## Conclusion\n\nIn this chapter, we explored the Pull Request Analysis Pipeline. We learned about the key concepts, how to use the pipeline, and how it works internally. We saw how the pipeline takes a pull request as input, analyzes the code changes, generates AI insights, suggests documentation updates, and stores the results in a database.\n\nNow that we understand the overall pipeline, let's dive deeper into one of the key components: parsing the code changes using Tree-sitter in [Chapter 5: Tree-sitter Based Code Parsing](05_tree_sitter_based_code_parsing.md).\n", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `CodeRoast` about the concept: \"Tree-sitter Based Code Parsing\". This is Chapter 5.\n\nConcept Details:\n- Description:\nTree-sitter is used to understand code structure so that modified blocks, functions, and classes can be detected. It is like a powerful code scanner that identifies the key elements of the code and their relationships. This parsing is crucial for focused analysis of code changes.\n\n\nComplete Tutorial Structure:\n1. [Frontend Web Application (React)](01_frontend_web_application__react_.md)\n2. [FastAPI Backend API](02_fastapi_backend_api.md)\n3. [GitHub Webhook Integration](03_github_webhook_integration.md)\n4. [Pull Request Analysis Pipeline](04_pull_request_analysis_pipeline.md)\n5. [Tree-sitter Based Code Parsing](05_tree_sitter_based_code_parsing.md)\n6. [Groq API Integration](06_groq_api_integration.md)\n7. [RAG (Retrieval-Augmented Generation) Documentation Updates](07_rag__retrieval_augmented_generation__documentation_updates.md)\n8. [PR Review Comment Posting (GitHub)](08_pr_review_comment_posting__github_.md)\n9. [Snowflake Data Storage](09_snowflake_data_storage.md)\n\nContext from previous chapters (summary):\n# Chapter 1: Frontend Web Application (React)\n\nWelcome to CodeRoast! In this chapter, we'll be diving into the heart of the user experience: the frontend web application built with React. Imagine you're a code reviewer, and you want to quickly see all the Pull Requests (PRs) needing your attention, along with AI-powered feedback. Our React frontend makes that happen!\n\nThis chapter will guide you through understanding how this frontend works and how it displays all the amazing insights generated by the rest of the CodeRoast system.\n\n## What is a Frontend, and Why React?\n\nThink of a frontend like the dashboard of a car. You see the speedometer, fuel gauge, and warning lights, but you don't need to know *how* the engine works to drive. Similarly, our frontend lets you interact with CodeRoast without worrying about the complex AI and analysis happening behind the scenes.\n\nReact is a popular JavaScript library for building user interfaces. It helps us create dynamic and interactive web pages. We chose React because it's efficient, organized, and makes it easier to manage complex user interfaces like our PR dashboard.\n\n## Key Concepts\n\nLet's break down the key concepts you'll encounter in our React frontend:\n\n1.  **Components:** React is all about components. Think of them as building blocks for your UI. Each component is responsible for rendering a specific part of the page, like a PR list, a diff view, or an analytics chart.\n\n2.  **JSX:** React uses JSX, which looks like HTML but is actually JavaScript. It allows us to write UI elements directly in our JavaScript code, making it more readable and maintainable.\n\n3.  **State:** State is data that can change over time, and when it changes, React automatically updates the UI. For example, the selected PR, the filter applied to the PR list, or the current theme (light/dark) are all parts of the component's state.\n\n4.  **Props:** Props (short for properties) are how we pass data from a parent component to a child component. Imagine passing a message from one React component to another.\n\n## The CodeRoast Frontend: A Tour\n\nLet's look at how these concepts come together in our `CodeRoast` frontend, using code snippets from `frontend/app.js`.\n\n### Displaying a List of Pull Requests\n\nFirst, let's see how we display the list of PRs. We fetch the PR data from our backend API (more on that in [Chapter 2: FastAPI Backend API](02_fastapi_backend_api.md)) and store it in the component's state.\n\n```javascript\nconst [prs, setPrs] = React.useState([]);\n\nReact.useEffect(() => {\n    fetch('http://localhost:8000/api/prs')\n        .then(res => res.json())\n        .then(data => {\n            setPrs(data);\n        });\n}, []);\n```\n\nThis code does the following:\n\n*   `const [prs, setPrs] = React.useState([]);`: This line initializes a state variable called `prs` to an empty array. `prs` will hold our pull request data. `setPrs` is a function used to update the `prs` data, and React will automatically refresh the user interface whenever you use `setPrs`.\n\n*   `React.useEffect(() => { ... }, []);`: This is a React Hook that runs code when the component is first rendered. The empty array `[]` as the second argument means this effect will only run once when the component mounts.\n\n*   `fetch('http://localhost:8000/api/prs')`: This line makes a request to our backend API to get the list of PRs.\n\n*   `.then(res => res.json())`: This converts the response from the API into a JavaScript object (JSON).\n\n*   `.then(data => { setPrs(data); })`: This updates the `prs` state with the data we received from the API.\n\nNext, we map over the `prs` array to create a list of PR items:\n\n```javascript\nconst prItems = filteredPRs.map(pr => \n    e('div', { \n        key: pr.id, \n        className: selectedPR && selectedPR.id === pr.id ? 'pr-item selected' : 'pr-item', \n        onClick: () => selectPR(pr) \n    }, [\n        e('div', {key: 'pr-header', className: 'pr-header'}, [\n            e('strong', {key: 'id'}, `#${pr.id}`),\n            e('span', {\n                key: 'status',\n                className: `pr-status status-${pr.status}`\n            }, pr.status)\n        ]),\n        e('div', {key: 'pr-title', className: 'pr-title'}, pr.title),\n        e('div', {key: 'pr-meta', className: 'pr-meta'}, [\n            e('span', {key: 'author'}, [\n                e('img', {key: 'avatar', className: 'avatar', src: `https://ui-avatars.com/api/?name=${pr.author}&size=24`, alt: pr.author}),\n                pr.author\n            ]),\n            e('span', {key: 'date', className: 'date'}, 'Updated: ' + (pr.updated_at || 'N/A'))\n        ])\n    ])\n);\n```\n\nThis code takes each `pr` from the `filteredPRs` array and:\n\n*   `e('div', { ... }, [ ... ])`: Creates a `div` element (and nested elements) for each PR, setting its `key`, `className`, and `onClick` properties. The `key` prop is important for React to efficiently update the list. `e` is `React.createElement`, a function that creates React elements. It is used because JSX must be transformed at compile time.\n\n*   `className`: Sets the CSS class for styling.\n\n*   `onClick`: Defines a function that will be called when the PR item is clicked.\n\n*   Within the div, `e(...)` is used to create other html elements to display the PR's `id`, `status`, `title`, `author` and `updated_at`.\n\n### Displaying the Diff\n\nOne of the most important features of `CodeRoast` is displaying the code diff. The `renderDiffSection` function takes the diff string and formats it for display:\n\n```javascript\nfunction renderDiffSection(diff) {\n    const diffLines = parseDiff(diff);\n\n    return React.createElement(\n        'pre',\n        { className: 'diff-content' },\n        diffLines.map((line, index) => {\n            const className =\n                line.type === 'old' ? 'diff-old-code diff-code-line' :\n                line.type === 'new' ? 'diff-new-code diff-code-line' :\n                'diff-context-line';\n\n            return React.createElement('div', { key: index, className }, line.content);\n        })\n    );\n}\n```\n\nHere's what's happening:\n\n*   `const diffLines = parseDiff(diff);`: It calls the `parseDiff` function to split the diff into individual lines and determine their type (old code, new code, or context). See below for details on `parseDiff`.\n\n*   The rest of the function constructs a `<pre>` element which is used to display preformatted text.  Within the `<pre>` tag, we loop through the lines in `diffLines` and create a `div` tag for each line. The `className` will style the background of each line in green, red, or default color, depending on the line type.\n\nThe `parseDiff` function itself is responsible for taking the raw diff output and turning it into an array of objects, each representing a line in the diff:\n\n```javascript\nfunction parseDiff(diff) {\n    const lines = diff.split('\\n');\n    const parsedLines = [];\n\n    let section = null;\n\n    for (let i = 0; i < lines.length; i++) {\n        const line = lines[i];\n\n        if (line.startsWith('File:') || line.startsWith('Function:')) {\n            parsedLines.push({ type: 'context', content: line });\n            section = null;\n        } else if (line.trim() === 'Old Code:') {\n            parsedLines.push({ type: 'context', content: 'Old Code:' });\n            section = 'old';\n\n            // Handle empty old code\n            if (lines[i + 1]?.trim() === 'New Code:') {\n                parsedLines.push({ type: 'old', content: '<NO PREVIOUS CODE>' });\n            }\n        } else if (line.trim() === 'New Code:') {\n            parsedLines.push({ type: 'context', content: 'New Code:' });\n            section = 'new';\n        } else if (section === 'old' || section === 'new') {\n            parsedLines.push({ type: section, content: line });\n        } else {\n            parsedLines.push({ type: 'context', content: line });\n        }\n    }\n\n    return parsedLines;\n}\n```\n\nThis function:\n\n*   Splits the diff string into lines.\n*   Iterates through each line, determining its type based on the content.\n*   Adds each line to the `parsedLines` array with a `type` property indicating whether it's \"old\", \"new\", or \"context\".\n*   The `type` property is later used to style each line in the diff.\n\n### Displaying Analytics\n\nFinally, the frontend also displays some helpful analytics about the PRs. This is handled by the `Analytics` component.\n\n```javascript\n// Analytics component to visualize PR data\nfunction Analytics({ prs }) {\n    // Count PRs by status\n    const statusCounts = prs.reduce((acc, pr) => {\n        acc[pr.status] = (acc[pr.status] || 0) + 1;\n        return acc;\n    }, {});\n\n    // Count PRs by author\n    const authorCounts = prs.reduce((acc, pr) => {\n        acc[pr.author] = (acc[pr.author] || 0) + 1;\n        return acc;\n    }, {});\n\n    // Calculate PRs over time (by month)\n    const prsByMonth = prs.reduce((acc, pr) => {\n        const date = new Date(pr.created_at);\n        const monthYear = date.toLocaleString('default', { month: 'short', year: 'numeric' });\n        acc[monthYear] = (acc[monthYear] || 0) + 1;\n        return acc;\n    }, {});\n\n    // Create data arrays for charts\n    const statusData = Object.entries(statusCounts).map(([status, count]) => ({\n        status,\n        count,\n        color: status === 'Open' ? '#17a2b8' : \n               status === 'Merged' ? '#28a745' : \n               status === 'Closed' ? '#dc3545' : '#6c757d'\n    }));\n\n    const authorData = Object.entries(authorCounts)\n        .sort((a, b) => b[1] - a[1])\n        .slice(0, 5)\n        .map(([author, count]) => ({ author, count }));\n\n    const timelineData = Object.entries(prsByMonth)\n        .sort((a, b) => new Date(a[0]) - new Date(b[0]))\n        .map(([month, count]) => ({ month, count }));\n\n    return e('div', { className: 'analytics-container' }, [\n        e('h2', { key: 'title', className: 'analytics-title' }, 'Pull Request Analytics'),\n        // ... (rest of the analytics display code)\n    ]);\n}\n```\n\nThis component:\n\n*   Takes the `prs` array as a prop.\n*   Calculates statistics like the number of PRs by status, author, and month.\n*   Formats the data into arrays suitable for rendering charts and summaries.\n*   Renders the analytics using divs with appropriate CSS classes.\n\n## Under the Hood: Requesting PR Data\n\nLet's take a simplified look at what happens when the frontend requests PR data from the backend.\n\n```mermaid\nsequenceDiagram\n    participant FE as Frontend (React App)\n    participant BE as Backend (FastAPI)\n    participant DB as Snowflake Database\n\n    FE->>BE: GET /api/prs\n    activate BE\n    BE->>DB: Query PR Data\n    activate DB\n    DB-->>BE: PR Data (JSON)\n    deactivate DB\n    BE-->>FE: PR Data (JSON)\n    deactivate BE\n    FE->>FE: Render PR List\n```\n\n1.  The **Frontend (React App)** initiates a GET request to the `/api/prs` endpoint on the **Backend (FastAPI)**.\n2.  The **Backend (FastAPI)** receives the request and queries the **Snowflake Database** for PR data. (We'll see this in [Chapter 9: Snowflake Data Storage](09_snowflake_data_storage.md)).\n3.  The **Snowflake Database** returns the PR data to the **Backend (FastAPI)** in JSON format.\n4.  The **Backend (FastAPI)** sends the PR data back to the **Frontend (React App)** in JSON format.\n5.  The **Frontend (React App)** receives the data and renders the list of pull requests.\n\nThe actual fetching of data happens inside of the `React.useEffect` hook we saw before.\n\n```javascript\nReact.useEffect(() => {\n    fetch('http://localhost:8000/api/prs')\n        .then(res => res.json())\n        .then(data => {\n            setPrs(data);\n        });\n}, []);\n```\n\nThis hook ensures that the PR data is fetched when the component is first rendered and updates the component's state with the fetched data.\n\n## Conclusion\n\nIn this chapter, we explored the `CodeRoast` frontend, built with React. We learned about components, JSX, state, props, and how they're used to display PR lists, diffs, and analytics. We also took a peek under the hood to see how the frontend interacts with the backend to fetch data.\n\nNow that you have a basic understanding of the frontend, let's move on to the next chapter, where we'll dive into the backend API built with FastAPI: [Chapter 2: FastAPI Backend API](02_fastapi_backend_api.md).\n\n---\n# Chapter 2: FastAPI Backend API\n\nWelcome back to CodeRoast! In [Chapter 1: Frontend Web Application (React)](01_frontend_web_application__react_.md), we built a snazzy frontend that displays pull requests, diffs, and AI insights. But that frontend needs someone to talk to \u2013 someone who can fetch data, run the AI magic, and keep everything organized. That's where our FastAPI backend comes in!\n\n## What is a Backend API, and Why FastAPI?\n\nThink of the backend like the engine in our car analogy. The frontend is the dashboard, showing you information and letting you control things. The backend is the engine, doing all the heavy lifting.\n\nSpecifically, the backend acts as an *API (Application Programming Interface)*. This means it provides a structured way for the frontend (and other applications) to request information and functionality.\n\n**FastAPI** is a modern, high-performance Python web framework for building APIs. We chose FastAPI because it's:\n\n*   **Fast:** It's built for speed, so our app feels responsive.\n*   **Easy to use:** It has a simple and intuitive syntax, making development quicker.\n*   **Automatic Data Validation:** It automatically checks that the data sent to the API is in the correct format, preventing errors.\n*   **Automatic API Documentation:** It generates interactive API documentation automatically, making it easy to understand how to use the API.\n\n## Key Concepts\n\nLet's break down the key concepts behind our FastAPI backend:\n\n1.  **Routes (Endpoints):** These are specific URLs that our backend responds to. For example, `/api/prs` might be a route that returns a list of all pull requests. Think of them as specific \"doors\" to access different functionalities of our backend.\n\n2.  **Requests:** These are messages sent from the frontend (or any other client) to the backend, asking for something. They often include data, like the ID of a pull request the frontend wants to see.\n\n3.  **Responses:** These are messages sent back from the backend to the frontend, containing the requested information or the result of an action. Responses are often in JSON format, which is easy for the frontend to process.\n\n4.  **Data Models:** These are Python classes that define the structure of our data. For example, we might have a `PullRequest` data model that defines the fields for each pull request (like `id`, `title`, `author`, etc.).  Data models help ensure the data is consistent and well-defined.\n\n## The CodeRoast Backend: A Tour\n\nLet's look at how these concepts come together in our `CodeRoast` backend, using code snippets from `main.py`.\n\n### Defining a Route\n\nHere's an example of a route that fetches all pull requests:\n\n```python\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nimport snowflake.connector\nimport os\nfrom dotenv import load_dotenv\nimport pandas as pd\n\nload_dotenv()\n\napp = FastAPI()\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n@app.get(\"/api/prs\")\ndef fetch_all_prs():\n    # Fetch data from Snowflake and return it\n    # For simplicity, let's return a test data\n    test_data = [\n        {\n            \"id\": 1,\n            \"title\": \"Add new feature\",\n            \"author\": \"johndoe\",\n            \"status\": \"Open\",\n            \"created_at\": \"2023-01-01\",\n            \"updated_at\": \"2023-01-02\"\n        },\n        {\n            \"id\": 2,\n            \"title\": \"Fix bug in login\",\n            \"author\": \"janedoe\",\n            \"status\": \"Merged\",\n            \"created_at\": \"2023-01-03\",\n            \"updated_at\": \"2023-01-04\"\n        }\n    ]\n    return test_data\n```\n\nLet's break this down:\n\n*   `@app.get(\"/api/prs\")`: This is a *decorator* that tells FastAPI to create a route at the URL `/api/prs`. The `@app.get` part specifies that this route should handle GET requests (requests for data).\n*   `def fetch_all_prs():`: This defines a function called `fetch_all_prs` that will be executed when someone makes a GET request to `/api/prs`.\n*   `return test_data`: This returns the `test_data` list as a JSON response. FastAPI automatically converts Python dictionaries and lists into JSON.\n\nSo, if you open your web browser and go to `http://localhost:8000/api/prs`, you'll see a JSON response containing the `test_data`. (Note: make sure your backend is running!)\n\n### Handling Parameters\n\nWhat if we want to fetch information about a specific pull request, identified by its ID? We can use a *path parameter* in our route:\n\n```python\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nimport snowflake.connector\nimport os\nfrom dotenv import load_dotenv\nimport pandas as pd\n\nload_dotenv()\n\napp = FastAPI()\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n@app.get(\"/api/prs/{pr_id}\")\ndef fetch_pr_analysis(pr_id: int):\n    # Fetch the analysis data\n    # For simplicity, let's return a test data\n    test_data = {\n        \"pr_id\": pr_id,\n        \"ai_summary\": \"This PR adds a new feature that improves user experience\",\n        \"merge_confidence\": \"High\",\n        \"code_quality\": \"Good code quality. No major issues found.\",\n        \"diff\": \"```\\n+ def new_function():\\n+     return 'new feature'\\n```\",\n        \"doc_string\": \"This PR introduces a new function that returns 'new feature'.\"\n    }\n    return test_data\n```\n\nHere's what's new:\n\n*   `@app.get(\"/api/prs/{pr_id}\")`: This defines a route that includes a path parameter called `pr_id`. The curly braces `{}` indicate a parameter.\n*   `def fetch_pr_analysis(pr_id: int):`:  The `pr_id: int` part of the function definition tells FastAPI that `pr_id` is an integer parameter. FastAPI will automatically validate that the value passed in the URL is actually an integer.\n*   `\"pr_id\": pr_id`: We are using the `pr_id` parameter inside the function to construct the response.\n\nIf you go to `http://localhost:8000/api/prs/123`, you'll see a JSON response containing the `test_data`, but with the `\"pr_id\"` field set to `123`.\n\n### Connecting to Snowflake Database\n\nIn a real application, we wouldn't just return test data. We would fetch the data from a database.  In our case, we use [Snowflake Data Storage](09_snowflake_data_storage.md). Let's look at how we can connect to Snowflake and query the database (simplified):\n\n```python\nimport os\nimport snowflake.connector\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nimport pandas as pd\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\napp = FastAPI()\n\n# Snowflake credentials\nSNOWFLAKE_USER = os.getenv(\"SNOWFLAKE_USER\", \"raghavg332\")\nSNOWFLAKE_PASSWORD = os.getenv(\"SNOWFLAKE_PASSWORD\", \"Qa29Kh4MptfGHEW\")\nSNOWFLAKE_ACCOUNT = os.getenv(\"SNOWFLAKE_ACCOUNT\", \"KXCIVVH-LL27432\")\nSNOWFLAKE_WAREHOUSE = os.getenv(\"SNOWFLAKE_WAREHOUSE\", \"COMPUTE_WH\")\nSNOWFLAKE_DATABASE = os.getenv(\"SNOWFLAKE_DATABASE\", \"PR_DASHBOARD\")\nSNOWFLAKE_SCHEMA = os.getenv(\"SNOWFLAKE_SCHEMA\", \"PUBLIC\")\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\ndef get_conn():\n    \"\"\"Establish connection to Snowflake database\"\"\"\n    try:\n        conn = snowflake.connector.connect(\n            user=SNOWFLAKE_USER,\n            password=SNOWFLAKE_PASSWORD,\n            account=SNOWFLAKE_ACCOUNT,\n            warehouse=SNOWFLAKE_WAREHOUSE,\n            database=SNOWFLAKE_DATABASE,\n            schema=SNOWFLAKE_SCHEMA\n        )\n        return conn\n    except Exception as e:\n        print(f\"Error connecting to Snowflake: {e}\")\n        raise e\n\n@app.get(\"/api/prs/{pr_id}\")\ndef fetch_pr_analysis(pr_id: int):\n    try:\n        conn = get_conn()\n        cursor = conn.cursor(snowflake.connector.DictCursor)\n\n        # Fetch the analysis data\n        cursor.execute(f\"SELECT * FROM pr_analysis WHERE pr_id = {pr_id}\")\n        result = cursor.fetchone()\n\n        if not result:\n            cursor.close()\n            conn.close()\n            raise HTTPException(status_code=404, detail=\"PR analysis not found\")\n\n        cursor.close()\n        conn.close()\n        return result\n    except Exception as e:\n        print(f\"Error fetching PR analysis: {e}\")\n        raise HTTPException(status_code=500, detail=f\"Error: {str(e)}\")\n```\n\nHere's what's happening:\n\n*   `import snowflake.connector`: This imports the Snowflake connector library, which allows us to connect to a Snowflake database.\n*   `SNOWFLAKE_USER = os.getenv(\"SNOWFLAKE_USER\", \"raghavg332\")`: This imports environment variable, which allows us to securely config the snowflake credentials.\n*   `get_conn()`: This function establishes a connection to Snowflake using your credentials. **Important:** Make sure you have Snowflake configured correctly.\n*   `conn = get_conn()`: This calls the `get_conn` function to establish the connection.\n*   `cursor = conn.cursor(snowflake.connector.DictCursor)`: This creates a cursor object, which allows us to execute SQL queries.  The `snowflake.connector.DictCursor` is important here: it tells the connector to return results as Python dictionaries, which are easier to work with.\n*   `cursor.execute(f\"SELECT * FROM pr_analysis WHERE pr_id = {pr_id}\")`: This executes a SQL query to fetch the analysis data for the specified `pr_id`.\n*   `result = cursor.fetchone()`: This fetches the first row of the result set.\n*   `return result`: This returns the result (which is a dictionary) as a JSON response.\n\n## Under the Hood: Fetching PR Data\n\nLet's take a simplified look at what happens when the frontend requests PR data from the backend.\n\n```mermaid\nsequenceDiagram\n    participant FE as Frontend (React App)\n    participant BE as Backend (FastAPI)\n    participant DB as Snowflake Database\n\n    FE->>BE: GET /api/prs/123\n    activate BE\n    BE->>DB: SELECT * FROM pr_analysis WHERE pr_id = 123\n    activate DB\n    DB-->>BE: PR Analysis Data (JSON)\n    deactivate DB\n    BE-->>FE: PR Analysis Data (JSON)\n    deactivate BE\n    FE->>FE: Render PR Details\n```\n\n1.  The **Frontend (React App)** initiates a GET request to the `/api/prs/123` endpoint on the **Backend (FastAPI)**.\n2.  The **Backend (FastAPI)** receives the request and executes a SQL query on the **Snowflake Database** to fetch the analysis data for the PR with ID 123.\n3.  The **Snowflake Database** returns the PR analysis data to the **Backend (FastAPI)** in JSON format.\n4.  The **Backend (FastAPI)** sends the PR analysis data back to the **Frontend (React App)** in JSON format.\n5.  The **Frontend (React App)** receives the data and renders the details of the pull request.\n\nThe actual fetching of data happens inside of the `fetch_pr_analysis` function we saw before, where it:\n\n1. Establishes a connection to the snowflake database.\n2. Constructs and executes a select query.\n3. Formats the data for transmission back to the frontend as a JSON response.\n\n## Conclusion\n\nIn this chapter, we explored the `CodeRoast` backend, built with FastAPI. We learned about routes, requests, responses, data models, and how they're used to create a powerful API. We also saw how the backend interacts with a Snowflake database to fetch data.\n\nNow that you have a basic understanding of the backend, let's move on to the next chapter, where we'll dive into how we receive GitHub Webhooks: [Chapter 3: GitHub Webhook Integration](03_github_webhook_integration.md).\n\n---\n# Chapter 3: GitHub Webhook Integration\n\nIn [Chapter 2: FastAPI Backend API](02_fastapi_backend_api.md), we set up our FastAPI backend to respond to requests. But how does our backend *know* when something important happens on GitHub, like a new pull request being opened? That's where GitHub Webhooks come in!\n\nImagine you're waiting for a package to arrive. Instead of constantly checking the tracking number, you can sign up for notifications that tell you exactly when it's delivered. GitHub Webhooks are like those notifications, but for code!\n\n## What Problem Do Webhooks Solve?\n\nWithout webhooks, our backend would have to constantly ask GitHub, \"Hey, are there any new pull requests? Hey, are there any updates to existing pull requests?\" This is inefficient and wastes resources.\n\nWebhooks solve this problem by letting GitHub *tell* our backend when something interesting happens. This is much more efficient!\n\n**Central Use Case:** When a developer opens a new pull request (PR) on GitHub, we want our `CodeRoast` system to automatically start analyzing that PR. Webhooks make this possible.\n\n## Key Concepts\n\nLet's break down the key concepts of GitHub Webhook Integration:\n\n1.  **Event:** An event is something that happens in a GitHub repository, like opening a pull request, pushing code, or commenting on an issue.\n\n2.  **Webhook:** A webhook is a way for GitHub to send a notification to our backend whenever a specific event occurs. It's like a \"callback\" \u2013 GitHub calls back to our server when something happens.\n\n3.  **Payload:** The payload is the data that GitHub sends to our backend in the webhook notification. It's a JSON object containing information about the event that occurred, like the pull request's details, the commit messages, and the author's information. This is the \"package\" containing all the information.\n\n4.  **Signature:** A signature is a security measure that ensures the webhook notification is actually coming from GitHub and hasn't been tampered with. It's like a digital \"seal\" of authenticity.\n\n## How to Use Webhooks: A Simple Example\n\nLet's imagine a very simple scenario: when a pull request is opened, we want to print a message to our backend's console.\n\nHere's how the process works:\n\n1.  **Configure a Webhook on GitHub:** In your GitHub repository settings, you create a webhook. You tell GitHub:\n    *   The URL of your backend (where to send the notifications).\n    *   Which events to listen for (e.g., \"pull request\").\n    *   A secret (to verify the signature).\n\n2.  **Open a Pull Request:** A developer opens a new pull request on the repository.\n\n3.  **GitHub Sends a Webhook:** GitHub detects the \"pull request opened\" event and sends a webhook notification (a POST request) to our backend's URL.\n\n4.  **Backend Receives the Webhook:** Our backend receives the POST request, verifies the signature to ensure it's from GitHub, and then extracts the pull request information from the payload.\n\n5.  **Backend Processes the Information:** In our simple example, our backend just prints a message. In reality, it would trigger the [Pull Request Analysis Pipeline](04_pull_request_analysis_pipeline.md).\n\n## Code Example: Receiving a Webhook\n\nHere's a simplified code example showing how our FastAPI backend can receive and process a GitHub webhook:\n\n```python\nfrom fastapi import FastAPI, Request, Header\nfrom fastapi.responses import JSONResponse\nimport hmac\nimport hashlib\nimport json\n\napp = FastAPI()\n\nGITHUB_SECRET = \"asdfg\" # Replace with your actual secret\n\ndef verify_signature(payload, signature, secret):\n    mac = hmac.new(secret.encode(), msg=payload, digestmod=hashlib.sha256)\n    expected = f\"sha256={mac.hexdigest()}\"\n    return hmac.compare_digest(expected, signature)\n\n@app.post(\"/webhook\")\nasync def github_webhook(\n    request: Request,\n    x_hub_signature_256: str = Header(None),\n    x_github_event: str = Header(None)\n):\n    body = await request.body()\n\n    # Verify webhook signature\n    if not verify_signature(body, x_hub_signature_256, GITHUB_SECRET):\n        return JSONResponse(content={\"error\": \"Invalid signature\"}, status_code=403)\n\n    payload = json.loads(body)\n\n    if x_github_event == \"pull_request\":\n        action = payload[\"action\"]\n        pr = payload[\"pull_request\"]\n\n        if action in [\"opened\", \"synchronize\"]:\n            print(f\"New PR opened: {pr['title']}\")\n\n    return {\"message\": \"OK\"}\n```\n\nLet's break this down:\n\n*   `@app.post(\"/webhook\")`: This creates a route that listens for POST requests at the `/webhook` endpoint. GitHub will send its webhook notifications to this URL.\n*   `request: Request`: This allows us to access the body of the POST request (the payload).\n*   `x_hub_signature_256: str = Header(None)`: This extracts the `X-Hub-Signature-256` header from the request. This header contains the signature that we need to verify.\n*   `x_github_event: str = Header(None)`: This extracts the `X-GitHub-Event` header, which tells us what type of event triggered the webhook (e.g., \"pull_request\").\n*   `body = await request.body()`: This reads the raw bytes from the request body.\n*   `verify_signature(body, x_hub_signature_256, GITHUB_SECRET)`: This calls a function to verify that the signature is valid (we'll explain this in more detail later).\n*   `payload = json.loads(body)`: This converts the JSON payload into a Python dictionary.\n*   `if x_github_event == \"pull_request\"`: This checks if the event type is \"pull_request\".\n*   `if action in [\"opened\", \"synchronize\"]`: This checks if the pull request action is \"opened\" (a new PR) or \"synchronize\" (a PR was updated).\n*   `print(f\"New PR opened: {pr['title']}\")`: This prints the title of the new pull request to the console.\n\n**Example Input (Payload):**\n\nWhen a pull request is opened, GitHub will send a JSON payload to our `/webhook` endpoint. This payload contains lots of information about the pull request. Here's a simplified example:\n\n```json\n{\n  \"action\": \"opened\",\n  \"pull_request\": {\n    \"url\": \"https://api.github.com/repos/your-org/your-repo/pulls/123\",\n    \"title\": \"Add a new feature\"\n  },\n  \"repository\": {\n    \"full_name\": \"your-org/your-repo\"\n  }\n}\n```\n\n**Example Output:**\n\nWhen our backend receives this payload, it will print the following message to the console:\n\n```\nNew PR opened: Add a new feature\n```\n\nThis example demonstrates the basic flow of receiving and processing a GitHub webhook. In a real application, you would use the data in the payload to trigger more complex actions, like analyzing the code changes in the pull request.\n\n## Under the Hood: How Webhooks Work\n\nLet's take a closer look at how webhooks work internally.\n\nHere's a sequence diagram illustrating the flow:\n\n```mermaid\nsequenceDiagram\n    participant Dev as Developer\n    participant GH as GitHub\n    participant BE as Backend (FastAPI)\n\n    Dev->>GH: Opens/Updates Pull Request\n    activate GH\n    GH->>BE: POST /webhook (with payload)\n    activate BE\n    BE->>BE: Verify Signature\n    alt Signature Valid\n        BE->>BE: Process Payload\n        BE-->>GH: 200 OK\n    else Signature Invalid\n        BE-->>GH: 403 Forbidden\n    end\n    deactivate BE\n    deactivate GH\n```\n\n1.  A **Developer** opens or updates a pull request on **GitHub**.\n\n2.  **GitHub** detects the event and sends a POST request to our **Backend (FastAPI)** at the `/webhook` endpoint. The request includes a JSON payload containing information about the event.\n\n3.  The **Backend (FastAPI)** receives the POST request and immediately verifies the signature to ensure the request is actually coming from GitHub.\n\n4.  If the signature is valid, the **Backend (FastAPI)** processes the payload and performs the appropriate actions (e.g., triggers the [Pull Request Analysis Pipeline](04_pull_request_analysis_pipeline.md)). The Backend sends an HTTP 200 OK to GitHub.\n\n5.  If the signature is invalid, the **Backend (FastAPI)** rejects the request and sends an HTTP 403 Forbidden to GitHub.\n\n### Verifying the Signature\n\nThe `verify_signature` function is crucial for security. Here's how it works:\n\n```python\nimport hmac\nimport hashlib\n\ndef verify_signature(payload, signature, secret):\n    mac = hmac.new(secret.encode(), msg=payload, digestmod=hashlib.sha256)\n    expected = f\"sha256={mac.hexdigest()}\"\n    return hmac.compare_digest(expected, signature)\n```\n\n*   `hmac.new(secret.encode(), msg=payload, digestmod=hashlib.sha256)`: This creates a new HMAC (Hash-based Message Authentication Code) object using the secret key and the payload. It uses the SHA256 algorithm to generate the hash.\n*   `expected = f\"sha256={mac.hexdigest()}\"`: This calculates the expected signature by prefixing \"sha256=\" to the hexadecimal representation of the HMAC hash.\n*   `hmac.compare_digest(expected, signature)`: This securely compares the expected signature with the signature provided in the `X-Hub-Signature-256` header. This function is important to prevent timing attacks.\n\nThe secret key is configured in your GitHub webhook settings and should be stored securely on your backend. Make sure to *never* expose your secret key in your code or commit it to your repository! As you can see in the provided `backend/main.py`, the correct approach is to store the secret in an environment variable.\n\n## Conclusion\n\nIn this chapter, we learned about GitHub Webhook Integration. We understand what webhooks are, why they're useful, and how to receive and process them in our FastAPI backend. We also saw how to verify the signature to ensure the webhook notifications are authentic.\n\nNow that we can receive notifications about pull requests, we can move on to the next chapter, where we'll dive into the [Pull Request Analysis Pipeline](04_pull_request_analysis_pipeline.md), which is responsible for analyzing the code changes in the pull request and generating AI-powered insights.\n\n---\n# Chapter 4: Pull Request Analysis Pipeline\n\nIn [Chapter 3: GitHub Webhook Integration](03_github_webhook_integration.md), we learned how to set up our backend to receive notifications from GitHub whenever a pull request (PR) is opened or updated. Now, what do we *do* with that information?\n\nThat's where the **Pull Request Analysis Pipeline** comes in!\n\nImagine a factory assembly line. Raw materials (code changes) enter, various machines perform operations (analysis, documentation suggestions), and then the finished product (insights) is stored in a database. Our pipeline is very similar: it takes a pull request, analyzes the code changes, generates AI insights, and stores the results.\n\n**Central Use Case:** When a developer opens a new pull request, we want `CodeRoast` to automatically analyze the code, provide feedback, and suggest documentation updates. The Pull Request Analysis Pipeline makes this happen!\n\n## Key Concepts\n\nLet's break down the key concepts behind the Pull Request Analysis Pipeline:\n\n1.  **Input: Pull Request Data:** This is the raw information about the PR, received from GitHub via the webhook. It includes the code diff, the PR description, the author, and more. Think of this as the \"raw materials\" entering our factory.\n\n2.  **Code Parsing:** Before we can analyze the code, we need to understand its structure. This involves parsing the code to identify functions, classes, and other elements. We will talk about it in detail in [Chapter 5: Tree-sitter Based Code Parsing](05_tree_sitter_based_code_parsing.md).\n\n3.  **AI Insights Generation:** This is where the magic happens! We use AI, powered by [Groq API Integration](06_groq_api_integration.md), to analyze the code and generate insights. This might include summarizing the PR, identifying potential issues, suggesting improvements, and estimating merge confidence. It's like adding value to a product in the factory.\n\n4.  **Documentation Updates (RAG):** We use RAG (Retrieval-Augmented Generation) to suggest updates to the project documentation based on the code changes. This ensures that our documentation stays up-to-date. See more details in [Chapter 7: RAG (Retrieval-Augmented Generation) Documentation Updates](07_rag__retrieval_augmented_generation__documentation_updates.md).\n\n5.  **Output: Stored Insights:** The final product of our pipeline is a set of AI-generated insights that are stored in a database (Snowflake in our case [Chapter 9: Snowflake Data Storage](09_snowflake_data_storage.md)). These insights can then be displayed in the frontend and used to improve the code review process.\n\n## How to Use the Pipeline\n\nLet's walk through how to use the pipeline with a simplified example.\n\nImagine we have a function called `process_pull_request` that represents our pipeline. It takes the pull request data as input and returns the AI insights.\n\n```python\ndef process_pull_request(pr_data):\n    # 1. Parse the code changes.  We skip for this simple case.\n    code_diff = pr_data[\"code_diff\"] # Assume code_diff is extracted\n\n    # 2. Generate AI insights. We skip complex logic here\n    ai_summary = \"This PR adds a new feature.\"\n    merge_confidence = \"High\"\n\n    # 3. Store the results. We skip database insert here.\n    insights = {\n        \"ai_summary\": ai_summary,\n        \"merge_confidence\": merge_confidence,\n    }\n\n    return insights\n```\n\nThis simplified code does the following:\n\n1.  Takes `pr_data` which contains information about the PR.\n2.  Simulates extracting the code diff.\n3.  Simulates generating AI insights (in a real application, we'd use the Groq API).\n4.  Returns a dictionary containing the AI insights.\n\n**Example Input:**\n\n```python\npr_data = {\n    \"pr_title\": \"Add new feature\",\n    \"pr_description\": \"This PR adds a new feature to calculate statistics.\",\n    \"code_diff\": \"```diff\\n+def calculate_mean(data):\\n+  return sum(data) / len(data)\\n```\",\n    \"author\": \"johndoe\",\n    \"status\": \"open\"\n}\n```\n\n**Example Output:**\n\n```python\n{\n  \"ai_summary\": \"This PR adds a new feature.\",\n  \"merge_confidence\": \"High\"\n}\n```\n\nIn reality, this function is more complex and involves several steps, including connecting to external services and parsing the code. We will cover it later in this chapter!\n\n## Under the Hood: How the Pipeline Works\n\nLet's take a closer look at how the Pull Request Analysis Pipeline works internally.\n\nHere's a sequence diagram illustrating the flow:\n\n```mermaid\nsequenceDiagram\n    participant GH as GitHub\n    participant BE as Backend (FastAPI)\n    participant CP as Code Parser\n    participant AI as Groq API\n    participant DB as Snowflake Database\n\n    GH->>BE: POST /webhook (PR data)\n    activate BE\n    BE->>CP: Extract code diffs\n    activate CP\n    CP-->>BE: Code diffs (parsed)\n    deactivate CP\n    BE->>AI: Analyze code\n    activate AI\n    AI-->>BE: AI Insights (JSON)\n    deactivate AI\n    BE->>DB: Store Insights\n    activate DB\n    DB-->>BE: OK\n    deactivate DB\n    BE-->>GH: 200 OK\n    deactivate BE\n```\n\n1.  **GitHub** sends a POST request to our **Backend (FastAPI)** at the `/webhook` endpoint, containing the pull request data.\n2.  The **Backend (FastAPI)** receives the data and passes the code to the **Code Parser** to extract the changed functions (detailed in [Chapter 5: Tree-sitter Based Code Parsing](05_tree_sitter_based_code_parsing.md)).\n3.  The **Code Parser** returns the extracted code diffs to the **Backend (FastAPI)**.\n4.  The **Backend (FastAPI)** sends the code diffs and PR details to the **Groq API** for analysis and insight generation.\n5.  The **Groq API** analyzes the code and returns AI insights (summary, merge confidence, etc.) in JSON format to the **Backend (FastAPI)**.\n6.  The **Backend (FastAPI)** stores these insights in the **Snowflake Database**.\n7.  The **Backend (FastAPI)** sends an HTTP 200 OK response to GitHub.\n\n### Diving into the Code\n\nLet's look at a code example of how the `process_pull_request` function is implemented. This code is located in `backend/prReview.py`.\n\n```python\ndef process_pull_request(g, repo_name, pr_number, sender=None):\n    repo = g.get_repo(repo_name)\n    pr = repo.get_pull(pr_number)\n    pr_description = pr.title # title is used as description\n    pr_title = pr.title\n    pr_author = pr.user.login\n    pr_status = pr.state\n    code_diff = get_lines_changed(pr) # Step 1\n    # ... (rest of the function)\n```\n\nThis code does the following:\n\n1.  `repo = g.get_repo(repo_name)`: Gets the repository object from GitHub using the `repo_name`.  `g` is the authenticated Github instance.\n2.  `pr = repo.get_pull(pr_number)`: Gets the pull request object from the repository using the `pr_number`.\n3.  `pr_description = pr.title`: Sets the pull request description.\n4.  `code_diff = get_lines_changed(pr)`: Calls the `get_lines_changed` function to get the code diff. This function uses github api to get the diff and returns a dictionary containing the added, modified, and deleted lines.\n\nLet's look at how we get the diff code:\n\n```python\ndef get_lines_changed(pr):\n    files = pr.get_files()\n    file_changes = {}\n    for file in files:\n        if file.status == \"removed\":\n            continue\n        file_changes[file.filename] = parse_changed_lines(file.patch)\n\n    return file_changes\n\ndef parse_changed_lines(patch: str):\n    added_or_modified_lines = set()\n    deleted_lines = set()\n    for line in patch.splitlines():\n        if line.startswith(\"@@\"):\n            match = re.findall(r\"@@ -(\\d+)(?:,(\\d+))? \\+(\\d+)(?:,(\\d+))? @@\", line)\n            if match:\n                old_start, old_count, new_start, new_count = match[0]\n\n                old_start = int(old_start)\n                old_count = int(old_count or 1)\n                for l in range(old_start, old_start + old_count):\n                    deleted_lines.add(l)\n\n                new_start = int(new_start)\n                new_count = int(new_count or 1)\n                for l in range(new_start, new_start + new_count):\n                    added_or_modified_lines.add(l)\n\n    return {\n        \"added_or_modified_lines\": added_or_modified_lines,\n        \"deleted_lines\": deleted_lines\n    }\n```\n\nThis code does the following:\n\n1.  `files = pr.get_files()`: Get all files that changed in this PR.\n2.  The code loops through each of the `files`:\n    *   `if file.status == \"removed\": continue`: Skip the files that were removed.\n    *   `file_changes[file.filename] = parse_changed_lines(file.patch)`: Call `parse_changed_lines` to parse the changes of each file and then store. The `parse_changed_lines` uses regex matching to find out which lines were added or deleted in this file.\n\nThe following code then uses the data extracted above to call the Groq API:\n\n```python\n    # Get the review data and post a comment\n    review_data = review_and_store_pr(pr_description, file_diff_func, pr_title, pr_author, pr_status)\n    \n    # Post the review as a comment on the PR\n    post_review_comment(g, repo_name, pr_number, review_data)\n```\n\nThe `review_and_store_pr` performs the RAG (Retrieval-Augmented Generation) to update project documentation and calls Groq to generate the PR review and stores the review into the snowflake database. `post_review_comment` then posts the review as a comment on the PR. Details can be found in [Chapter 8: PR Review Comment Posting (GitHub)](08_pr_review_comment_posting__github_.md)\n\n## Conclusion\n\nIn this chapter, we explored the Pull Request Analysis Pipeline. We learned about the key concepts, how to use the pipeline, and how it works internally. We saw how the pipeline takes a pull request as input, analyzes the code changes, generates AI insights, suggests documentation updates, and stores the results in a database.\n\nNow that we understand the overall pipeline, let's dive deeper into one of the key components: parsing the code changes using Tree-sitter in [Chapter 5: Tree-sitter Based Code Parsing](05_tree_sitter_based_code_parsing.md).\n\n\nRelevant Code Snippets:\n--- File: README.md ---\n# \ud83e\udd16 Code Roast : An AI-Powered Pull Request Review System\n\n> An end-to-end intelligent platform for automated code review, documentation updates, and pull request analysis \u2014 powered by LLMs, Tree-sitter, and Snowflake.\n\n---\n\n## \ud83d\ude80 Overview\n\nThis system integrates with GitHub to automatically:\n- Analyze pull requests using AI (via [Groq](https://groq.com))\n- Parse and understand diffs using [Tree-sitter](https://tree-sitter.github.io/)\n- Perform RAG (Retrieval-Augmented Generation) to update project documentation\n- Store all insights in **Snowflake** for analytics\n- Display all PRs, diffs, AI feedback, and doc suggestions via a beautiful **Web App**\n\nWhether you're an engineering team lead, reviewer, or contributor \u2014 you get fast, reliable, and insightful AI-powered review feedback at every step.\n\n---\n\n## \ud83e\udde0 Features\n\n- \u2705 **AI PR Summary**  \n  Instant analysis of pull request purpose, syntax/style/functionality, and merge confidence.\n\n- \ud83d\udcdd **Automatic Docstring Suggestions**  \n  Extract or generate missing/updated docstrings for new or changed functions.\n\n- \ud83d\udcd8 **RAG-based Documentation Updates**  \n  Contextual retrieval from `project_docs.txt` with AI-generated Markdown additions.\n\n- \ud83d\udcc4 **Code Diff Parsing**  \n  Tree-sitter-based code structure diffing to isolate changed functions only.\n\n- \ud83d\udcac **GitHub Comment Posting**  \n  Summaries are posted as comments directly on PRs, no need to open another tool.\n\n- \ud83d\udcca **Snowflake Integration**  \n  All AI-generated insights and PR metadata stored for reporting, audit, and search.\n\n- \ud83c\udf10 **Web App Dashboard**  \n  Browse all PRs, view diffs, AI reviews, and doc updates in a single UI.\n\n---\n\n## \ud83e\uddf1 Tech Stack\n\n| Layer              | Tech Used                                      |\n|--------------------|-----------------------------------------------|\n| **AI Models**       | [Groq API (LLaMA 3)](https://groq.com)        |\n| **Diff Parsing**    | [Tree-sitter](https://tree-sitter.github.io/) |\n| **Backend API**     | [FastAPI](https://fastapi.tiangolo.com)       |\n| **Git Integration** | GitHub App + `PyGithub`                       |\n| **RAG Engine**      | Basic keyword-matching from local docs        |\n| **Database**        | [Snowflake](https://www.snowflake.com)        |\n| **Frontend**        | HTML, JS, CSS (Markdown + Diff Viewers)       |\n\n---\n\n## \u2699\ufe0f How It Works\n\n1. \ud83d\udd14 **GitHub Webhook Trigger**\n   - On PR open/update \u2192 sends payload to FastAPI backend\n\n2. \ud83e\udde0 **AI Review Processing**\n   - Extracts code diffs, changed functions\n   - Groq generates AI summary, confidence rating, and docstrings\n\n3. \ud83d\udcd8 **Documentation Updates**\n   - Local project docs indexed via simple RAG\n   - Groq suggests Markdown additions\n\n4. \ud83d\udcbe **Data Storage**\n   - All results are saved in Snowflake for dashboard & analytics\n\n5. \ud83c\udf10 **Web App UI**\n   - Explore PRs, view diffs, AI reviews, and doc updates\n\n\n--- File: backend/prReview.py ---\nimport os, re\nfrom github import Auth\nfrom github import Github, GithubIntegration\nfrom github.GithubException import GithubException\nfrom tree_sitter import Language, Parser\nimport tree_sitter_python as tspython\nimport tree_sitter_javascript as tsjavascript\nimport tree_sitter_cpp as tscpp\nimport requests\nimport json\nimport logging\nimport snowflake.connector\nimport random\nimport datetime\n\n\nGROQ_API_KEY = \"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\"\nGROQ_ENDPOINT = \"https://api.groq.com/openai/v1/chat/completions\"\n\ndef authenticate_github(app_id: int, installation_id, private_key: str):\n    gi = GithubIntegration(integration_id=app_id, private_key=private_key)\n    g = gi.get_github_for_installation(installation_id)\n    return g\n\ndef get_pull_request(g, repo_name: str, pr_number: int):\n    repo = g.get_repo(repo_name)\n    pr = repo.get_pull(pr_number)\n    return pr\n\ndef parse_changed_lines(patch: str):\n    added_or_modified_lines = set()\n    deleted_lines = set()\n    for line in patch.splitlines():\n        if line.startswith(\"@@\"):\n            match = re.findall(r\"@@ -(\\d+)(?:,(\\d+))? \\+(\\d+)(?:,(\\d+))? @@\", line)\n            if match:\n                old_start, old_count, new_start, new_count = match[0]\n\n                old_start = int(old_start)\n                old_count = int(old_count or 1)\n                for l in range(old_start, old_start + old_count):\n                    deleted_lines.add(l)\n\n                new_start = int(new_start)\n                new_count = int(new_count or 1)\n                for l in range(new_start, new_start + new_count):\n                    added_or_modified_lines.add(l)\n\n    return {\n        \"added_or_modified_lines\": added_or_modified_lines,\n        \"deleted_lines\": deleted_lines\n    }\n\ndef get_file_contents(g, repo_name: str, file_path: str, commit_sha: str):\n    repo = g.get_repo(repo_name)\n    try:\n        contents = repo.get_contents(file_path, ref=commit_sha)\n        return contents.decoded_content.decode()\n    except GithubException as e:\n        if e.status == 404:\n            return \"\"\n        else:\n            raise e\n\ndef get_lines_changed(pr):\n    files = pr.get_files()\n    file_changes = {}\n    for file in files:\n        if file.status == \"removed\":\n            continue\n        file_changes[file.filename] = parse_changed_lines(file.patch)\n\n    return file_changes\n\ndef get_paraser(language_name):\n\n    PY_LANGUAGE = Language(tspython.language())\n    JS_LANGUAGE = Language(tsjavascript.language())\n    CPP_LANGUAGE = Language(tscpp.language())\n\n    LANGUAGES = {\n        'python': PY_LANGUAGE,\n        'javascript': JS_LANGUAGE,\n        'cpp': CPP_LANGUAGE,\n    }\n    parser = Parser(LANGUAGES[language_name])\n    return parser\n\ndef extract_functions(code, language_name, changed_lines):\n    parser = get_paraser(language_name)\n    tree = parser.parse(bytes(code, \"utf8\"))\n    root_node = tree.root_node\n\n    functions = []\n\n    def node_within_lines(node):\n        start_line = node.start_point[0] + 1\n        end_line = node.end_point[0] + 1\n        return any([start_line <= line <= end_line for line in changed_lines])\n    \n    def traverse(node):\n        if language_name == \"python\" and node.type == \"function_definition\":\n            if node_within_lines(node):\n                func_name_node = node.child_by_field_name('name')\n                func_name = func_name_node.text.decode()\n                functions.append({'name': func_name, 'code': node.text.decode()})\n        elif language_name == 'javascript' and node.type in ['function_declaration', 'method_definition', 'arrow_function']:\n            if node_within_lines(node):\n                func_name_node = node.child_by_field_name('name')\n                func_name = func_name_node.text.decode()\n                functions.append({'name': func_name, 'code': node.text.decode()})\n        elif language_name == 'cpp' and node.type in ['function_definition']:\n            if node_within_lines(node):\n                func_name_node = node.child_by_field_name('name')\n                func_name = func_name_node.text.decode()\n                functions.append({'name': func_name, 'code': node.text.decode()})\n        for child in node.children:\n            traverse(child)\n\n    traverse(root_node)\n    return functions\n\nheaders = {\n    \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\n# Snowflake setup\nconn = snowflake.connector.connect(\n    user=\"raghavg332\",\n    password=\"Qa29Kh4MptfGHEW\",\n    account=\"KXCIVVH-LL27432\",\n    warehouse=\"COMPUTE_WH\",\n    database=\"PR_DASHBOARD\",\n    schema=\"PUBLIC\"\n)\ncur = conn.cursor()\n\ndef update_documentation_with_rag(pr_description: str, code_diff: str, docs_path=\"backend/project_docs.txt\"):\n    \"\"\"\n    Performs a simple RAG-based call to Groq to update documentation.\n    \"\"\"\n    # Step 1: Load the existing documentation\n    try:\n        with open(docs_path, \"r\") as f:\n            existing_docs = f.read()\n    except FileNotFoundError:\n        logging.error(f\"[\u274c] Documentation file not found at {docs_path}\")\n        return {\"error\": \"Documentation file not found.\"}\n\n    # Step 2: Very basic RAG - extract top paragraphs that contain overlap\n    paragraphs = existing_docs.split(\"\\n\\n\")\n    keywords = pr_description.lower().split()\n    ranked_paragraphs = sorted(paragraphs, key=lambda para: sum(word in para.lower() for word in keywords), reverse=True)\n    context = \"\\n\\n\".join(ranked_paragraphs[:3])  # Top 3 paragraphs as \"retrieved context\"\n\n    # Step 3: Build the prompt\n    doc_prompt = f\"\"\"\nYou are an expert technical writer and code documentation assistant.\n\nA developer has made the following changes to the codebase:\n\n## PR Description:\n{pr_description}\n\n## Code Diff:\n{code_diff}\n\n## Relevant Existing Documentation:\n{context}\n\nPlease provide additions or updates to the documentation based on the above code change. \nFormat your response in **Markdown**, and only include sections that should be added or updated. \nIf the change doesn't require doc updates, say so.\n\"\"\"\n\n    # Step 4: Call Groq API\n    payload = {\n        \"model\": \"llama3-70b-8192\",\n        \"messages\": [{\"role\": \"user\", \"content\": doc_prompt}]\n    }\n\n    try:\n        response = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n        response.raise_for_status()\n        result = response.json()\n        return {\n            \"doc_update\": result[\"choices\"][0][\"message\"][\"content\"].strip()\n        }\n\n    except Exception as e:\n        logging.exception(\"Groq API call for documentation update failed\")\n        return {\"error\": str(e)}\n\n\ndef build_full_prompt(pr_description: str, code_diff_str: str) -> str:\n    return f\"\"\"\nYou are an AI code reviewer and documentation assistant.\n\nA developer submitted the following pull request. Please do the following:\n1. \ud83d\udd0d Summarize the overall purpose of the PR. Be a little detailed\n2. \u2705 Review the code for:\n   - Syntax issues in Python (Make up some Issue)\n   - Style (PEP8)\n   - Functionality correctness (does it match the description?)\n3. Based on the review, provide a merge confidence score from 1 to 10.\n4. \ud83d\udcdd Extract or suggest updated docstrings for any changed or added functions/classes.\n5. The Code Diff is made up of the old and new version of the code, if any of the field is empty it means, the function is either added or deleted respectively.\n\n## PR Description:\n{pr_description}\n\n## Code Diff:\n{code_diff_str}\n\nRespond ONLY in this JSON format without any additional text:\n\n{{\n  \"ai_summary\": \"...\",\n  \"merge_confidence\": \"1 - 10\",\n  \"code_quality\": {{\n    \"syntax_check\": \"...\",\n    \"style_check\": \"...\",\n    \"functionality_check\": \"...\",\n    \"final_rating\": \"Excellent / Good / Needs Work / Critical Issues\"\n  }}\n}}\n\"\"\"\n\ndef review_and_store_pr(pr_description: str, code_diff: dict, pr_title: str, pr_author: str, pr_status: str):\n    code_diff_str = \"\"\n    for file in list(code_diff.keys()):\n        code_diff_str += f\"File: {file}\\n\\n\"\n        diff = code_diff[file]\n\n        for func_name in list(diff.keys()):\n            old_code = diff[func_name]['old_code']\n            new_code = diff[func_name]['new_code']\n            if old_code is None:\n                old_code = \"\"\n            if new_code is None:\n                new_code = \"\"\n            code_diff_str += f\"Function: {func_name}\\n\\nOld Code:\\n{old_code}\\n\\nNew Code:\\n{new_code}\"\n    docstring = update_documentation_with_rag(pr_description, code_diff)\n    pr_id = random.randint(1000, 9999)\n    prompt = build_full_prompt(pr_description, code_diff_str)\n\n    payload = {\n        \"model\": \"llama3-70b-8192\",\n        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n    }\n\n    try:\n        response = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n        response.raise_for_status()\n        result = response.json()\n\n        try:\n            content = result[\"choices\"][0][\"message\"][\"content\"].strip()\n            review_data = json.loads(content)\n\n            # Flatten the code quality object for easier insert\n            code_quality_str = (\n                f\"Syntax: {review_data['code_quality']['syntax_check']}\\n\"\n                f\"Style: {review_data['code_quality']['style_check']}\\n\"\n                f\"Functionality: {review_data['code_quality']['functionality_check']}\\n\"\n                f\"Rating: {review_data['code_quality']['final_rating']}\"\n            )\n            cur.execute(f\"\"\"\n                INSERT INTO PULL_REQUESTS (\n                    ID, TITLE, AUTHOR, STATUS, CREATED_AT, UPDATED_AT\n                )\n                VALUES (%s, %s, %s, %s, %s, %s);\n            \"\"\", (\n                pr_id,\n                pr_title,\n                pr_author,\n                pr_status,\n                datetime.datetime.now(),\n                datetime.datetime.now()\n            ))\n            conn.commit()\n\n            # Insert into Snowflake\n            cur.execute(f\"\"\"\n                INSERT INTO PR_ANALYSIS (\n                    PR_ID, AI_SUMMARY, MERGE_CONFIDENCE, CODE_QUALITY,\n                    PR_DESCRIPTION, DOCSTRINGS, DIFF\n                )\n                VALUES (%s, %s, %s, %s, %s, %s, %s);\n            \"\"\", (\n                pr_id,\n                review_data[\"ai_summary\"],\n                review_data[\"merge_confidence\"],\n                code_quality_str,\n                pr_description,\n                docstring['doc_update'],\n                code_diff_str\n            ))\n\n            conn.commit()\n\n            logging.info(f\"[\u2705] PR #{pr_id} analysis saved to Snowflake.\")\n            print(f\"[\u2705] PR #{pr_id} analysis saved to Snowflake.\")\n            return review_data\n\n        except json.JSONDecodeError:\n            logging.error(\"[\u274c] Groq returned non-JSON output.\")\n            print(\"[\u274c] Groq returned non-JSON output.\")\n            return {\"error\": \"Invalid JSON from Groq\"}\n\n    except Exception as e:\n        logging.exception(\"Groq API or Snowflake insert failed\")\n        return {\"error\": str(e)}\n\ndef process_pull_request(g, repo_name, pr_number, sender=None):\n    repo = g.get_repo(repo_name)\n    pr = repo.get_pull(pr_number)\n    pr_description = pr.title\n    pr_title = pr.title\n    pr_author = pr.user.login\n    pr_status = pr.state\n    code_diff = get_lines_changed(pr)\n    file_diff_func = {}\n    for file in list(code_diff.keys()):\n        diff_func = {}\n        new_code = get_file_contents(g, repo_name, file, pr.head.sha)\n        old_code = get_file_contents(g, repo_name, file, pr.base.sha)\n        added_or_modified_functions_after = extract_functions(new_code, \"python\", code_diff[file]['added_or_modified_lines'])\n        deleted_functions_before = extract_functions(old_code, \"python\", code_diff[file]['deleted_lines'])\n        old_functions_all = extract_functions(old_code, \"python\", set(range(1, len(old_code.split(\"\\n\")))))\n        new_functions_all = extract_functions(new_code, \"python\", set(range(1, len(new_code.split(\"\\n\")))))\n        for func in added_or_modified_functions_after:\n            matched_old_function = next((f for f in old_functions_all if f['name'] ==  func['name']), None)\n            diff_func[func['name']] = {\n                \"new_code\": func['code'],\n                \"old_code\": matched_old_function['code'] if matched_old_function else None\n            }\n        \n        for func in deleted_functions_before:\n            matched_new_function = next((f for f in new_functions_all if f['name'] ==  func['name']), None)\n            diff_func[func['name']] = {\n                \"new_code\": matched_new_function['code'] if matched_new_function else None,\n                \"old_code\": func['code']\n            }\n        \n        file_diff_func[file] = diff_func\n    \n    # Get the review data and post a comment\n    review_data = review_and_store_pr(pr_description, file_diff_func, pr_title, pr_author, pr_status)\n    \n    # Post the review as a comment on the PR\n    post_review_comment(g, repo_name, pr_number, review_data)\n\n\ndef post_review_comment(g, repo_name, pr_number, review_data):\n    \"\"\"\n    Posts the AI review as a comment on the pull request.\n    \n    Args:\n        g: Authenticated GitHub instance\n        repo_name: Repository name (owner/repo)\n        pr_number: Pull request number\n        review_data: The AI review data dictionary\n    \"\"\"\n    try:\n        # Get the repo and PR objects\n        repo = g.get_repo(repo_name)\n        pr = repo.get_pull(pr_number)\n        \n        # Format the comment body\n        comment_body = f\"\"\"\n## \ud83e\udd16 AI Code Review\n\n### Summary\n{review_data.get('ai_summary', 'No summary available')}\n\n### Merge Confidence: **{review_data.get('merge_confidence', 'Unknown')}**\n\n### Code Quality Review\n- **Syntax:** {review_data.get('code_quality', {}).get('syntax_check', 'Not analyzed')}\n- **Style:** {review_data.get('code_quality', {}).get('style_check', 'Not analyzed')}\n- **Functionality:** {review_data.get('code_quality', {}).get('functionality_check', 'Not analyzed')}\n- **Overall Rating:** {review_data.get('code_quality', {}).get('final_rating', 'Not rated')}\n\n---\n*This review was automatically generated by AI.*\n\"\"\"\n        \n        # Create the comment on the PR\n        pr.create_issue_comment(comment_body)\n        logging.info(f\"[\u2705] Posted AI review comment on PR #{pr_number} in {repo_name}\")\n        print(f\"[\u2705] Posted AI review comment on PR #{pr_number} in {repo_name}\")\n        return True\n    except Exception as e:\n        logging.exception(f\"[\u274c] Failed to post comment on PR #{pr_number}: {str(e)}\")\n        print(f\"[\u274c] Failed to post comment on PR #{pr_number}: {str(e)}\")\n        return False\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 5: Tree-sitter Based Code Parsing`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "# Chapter 5: Tree-sitter Based Code Parsing\n\nIn [Chapter 4: Pull Request Analysis Pipeline](04_pull_request_analysis_pipeline.md), we learned how the `CodeRoast` pipeline analyzes pull requests. One of the key steps in that pipeline is understanding the *structure* of the code changes.  Instead of just treating the code as plain text, we want to know *what* code has changed - what functions have been added, deleted or modified?\n\nThat's where **Tree-sitter** comes in!\n\nImagine trying to understand a sentence without knowing grammar. You might get some of the words, but you'd miss the relationships between them. Tree-sitter is like a grammar checker for code. It helps us understand the structure of the code so we can analyze it more effectively.\n\n**Central Use Case:** When a developer modifies a function in a pull request, we want `CodeRoast` to *only* analyze that function, rather than the entire file. This focused analysis saves time and improves the accuracy of our AI insights.\n\n## Key Concepts\n\nLet's break down the key concepts behind Tree-sitter based code parsing:\n\n1.  **Parsing:** Parsing is the process of taking code (which is just a string of characters) and turning it into a structured representation that a computer can understand. Think of it like taking a sentence and breaking it down into nouns, verbs, and adjectives.\n\n2.  **Abstract Syntax Tree (AST):** The AST is the structured representation of the code that results from parsing. It's like a tree diagram that shows the relationships between the different parts of the code. Each \"node\" in the tree represents a different element of the code, like a function, a variable, or an expression.\n\n3.  **Tree-sitter:** Tree-sitter is a library that makes it easy to parse code and create ASTs. It's fast, reliable, and supports many different programming languages. It's the engine that drives our code parsing.\n\n4.  **Grammar:** A grammar defines the rules for a programming language. Tree-sitter uses grammars to understand the structure of the code. Each language has its own grammar (e.g., Python grammar, JavaScript grammar). These grammars tells Tree-sitter what to expect.\n\n## How to Use Tree-sitter: A Simple Example\n\nLet's imagine we have a simple Python function and we want to use Tree-sitter to extract its name and code.\n\nHere's the function:\n\n```python\ndef greet(name):\n    \"\"\"Greets the person passed in as a parameter.\"\"\"\n    print(f\"Hello, {name}!\")\n```\n\nHere's how we can use Tree-sitter to parse this code and extract the function name and code (simplified):\n\n```python\nfrom tree_sitter import Language, Parser\nimport tree_sitter_python as tspython\n\n# Set up Tree-sitter\nPY_LANGUAGE = Language(tspython.language())\nparser = Parser()\nparser.set_language(PY_LANGUAGE)\n\n# The code to parse\ncode = \"\"\"\ndef greet(name):\n    \\\"\\\"\\\"Greets the person passed in as a parameter.\\\"\\\"\\\"\n    print(f\"Hello, {name}!\")\n\"\"\"\n\n# Parse the code\ntree = parser.parse(bytes(code, \"utf8\"))\n\n# Get the root node of the AST\nroot_node = tree.root_node\n\n# Find the function definition node\nfunction_node = root_node.children[0] # Assume function is the first child\n\n# Extract the function name\nfunction_name_node = function_node.child_by_field_name('name')\nfunction_name = function_name_node.text.decode()\n\n# Extract the function code\nfunction_code = function_node.text.decode()\n\nprint(f\"Function Name: {function_name}\")\nprint(f\"Function Code: {function_code}\")\n```\n\nThis simplified code does the following:\n\n1.  **Sets up Tree-sitter:** It imports the necessary libraries and creates a Tree-sitter parser for Python.  We are using `tree_sitter_python` to obtain the python grammar.\n2.  **Parses the code:** It takes the Python code and uses the parser to create an AST.\n3.  **Navigates the AST:**\n    *   `function_node = root_node.children[0]`: Since the example only consists of the definition of the `greet` function, the function node will be the first child. This code accesses the function node.\n    *   `function_name_node = function_node.child_by_field_name('name')`: This line gets the node that represents the name of the function.\n4.  **Extracts information:** It extracts the function name and code from the AST.\n5.  **Prints the results:** It prints the function name and code to the console.\n\n**Example Output:**\n\n```\nFunction Name: greet\nFunction Code: def greet(name):\n    \"\"\"Greets the person passed in as a parameter.\"\"\"\n    print(f\"Hello, {name}!\")\n```\n\nThis example demonstrates how we can use Tree-sitter to parse code and extract specific elements. In `CodeRoast`, we use this technique to identify the functions that have been changed in a pull request.\n\n## Under the Hood: How Tree-sitter Works\n\nLet's take a closer look at how Tree-sitter works internally.\n\nHere's a sequence diagram illustrating the flow:\n\n```mermaid\nsequenceDiagram\n    participant Code as Code (String)\n    participant Parser as Tree-sitter Parser\n    participant AST as Abstract Syntax Tree\n    participant Extractor as Information Extractor\n\n    Code->>Parser: Parse code\n    activate Parser\n    Parser->>AST: Build AST\n    activate AST\n    AST-->>Parser: AST\n    deactivate AST\n    Parser-->>Extractor: AST\n    deactivate Parser\n    Extractor->>AST: Navigate and Extract\n    activate AST\n    AST-->>Extractor: Function Name and Code\n    deactivate AST\n    Extractor-->>Code: Function Name and Code\n```\n\n1.  The **Code (String)** is passed to the **Tree-sitter Parser**.\n2.  The **Tree-sitter Parser** uses the grammar to build an **Abstract Syntax Tree (AST)**.\n3.  The **AST** is returned to the **Tree-sitter Parser**.\n4.  The **Tree-sitter Parser** passes the **AST** to the **Information Extractor**.\n5.  The **Information Extractor** navigates the **AST** to extract the function name and code.\n6.  The extracted function name and code are returned to the **Code**.\n\n### Diving into the Code\n\nLet's look at the code that extracts functions from the code. This code can be found in `backend/prReview.py`.\n\n```python\ndef extract_functions(code, language_name, changed_lines):\n    parser = get_paraser(language_name)\n    tree = parser.parse(bytes(code, \"utf8\"))\n    root_node = tree.root_node\n\n    functions = []\n\n    def node_within_lines(node):\n        start_line = node.start_point[0] + 1\n        end_line = node.end_point[0] + 1\n        return any([start_line <= line <= end_line for line in changed_lines])\n    \n    def traverse(node):\n        if language_name == \"python\" and node.type == \"function_definition\":\n            if node_within_lines(node):\n                func_name_node = node.child_by_field_name('name')\n                func_name = func_name_node.text.decode()\n                functions.append({'name': func_name, 'code': node.text.decode()})\n        elif language_name == 'javascript' and node.type in ['function_declaration', 'method_definition', 'arrow_function']:\n            if node_within_lines(node):\n                func_name_node = node.child_by_field_name('name')\n                func_name = func_name_node.text.decode()\n                functions.append({'name': func_name, 'code': node.text.decode()})\n        elif language_name == 'cpp' and node.type in ['function_definition']:\n            if node_within_lines(node):\n                func_name_node = node.child_by_field_name('name')\n                func_name = func_name_node.text.decode()\n                functions.append({'name': func_name, 'code': node.text.decode()})\n        for child in node.children:\n            traverse(child)\n\n    traverse(root_node)\n    return functions\n```\n\nThis code does the following:\n\n1.  `parser = get_paraser(language_name)`: Gets the Tree-sitter parser for the given language (Python, JavaScript, C++ are supported).\n2.  `tree = parser.parse(bytes(code, \"utf8\"))`: Parses the code and creates an AST.\n3.  `root_node = tree.root_node`: Gets the root node of the AST.\n4.  The code defines a helper function `node_within_lines` to checks whether a node is within changed lines to skip parsing nodes that are irrelevant.\n5.  `traverse(root_node)`: Calls the `traverse` function to recursively traverse the AST and extract the function names and code.\n    *   It first checks if the language is Python and the node type is `function_definition`.\n    *   If it is, it extracts the function name and code and adds them to the `functions` list.\n    *   Then it traverses the child nodes.\n6.  It does similar logic for Javascript and C++\n7.  It returns the `functions` list.\n\nThe `get_paraser` function loads the grammars for supported languages:\n\n```python\ndef get_paraser(language_name):\n\n    PY_LANGUAGE = Language(tspython.language())\n    JS_LANGUAGE = Language(tsjavascript.language())\n    CPP_LANGUAGE = Language(tscpp.language())\n\n    LANGUAGES = {\n        'python': PY_LANGUAGE,\n        'javascript': JS_LANGUAGE,\n        'cpp': CPP_LANGUAGE,\n    }\n    parser = Parser(LANGUAGES[language_name])\n    return parser\n```\n\nThis code does the following:\n\n1.  `PY_LANGUAGE = Language(tspython.language())`: Loads the Python grammar.\n2.  `JS_LANGUAGE = Language(tsjavascript.language())`: Loads the JavaScript grammar.\n3.  `CPP_LANGUAGE = Language(tscpp.language())`: Loads the C++ grammar.\n4.  `LANGUAGES = { ... }`: Creates a dictionary that maps language names to their corresponding grammars.\n5.  `parser = Parser(LANGUAGES[language_name])`: Creates a Tree-sitter parser for the given language.\n6.  It returns the `parser`.\n\n## Conclusion\n\nIn this chapter, we explored Tree-sitter based code parsing. We learned about the key concepts, how to use Tree-sitter to extract information from code, and how it works internally. We saw how Tree-sitter helps us understand the structure of code so we can analyze it more effectively.\n\nNow that we can parse code changes, let's move on to the next chapter and see how we use AI to generate insights about those changes using [Groq API Integration](06_groq_api_integration.md).\n", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `CodeRoast` about the concept: \"Groq API Integration\". This is Chapter 6.\n\nConcept Details:\n- Description:\nThis abstraction handles communication with the Groq API, an AI model provider, to generate summaries, code quality assessments, and documentation suggestions.  Think of it as a translator that sends your code and requests to a smart AI brain (Groq) and then translates the AI's response back into something the system can understand.\n\n\nComplete Tutorial Structure:\n1. [Frontend Web Application (React)](01_frontend_web_application__react_.md)\n2. [FastAPI Backend API](02_fastapi_backend_api.md)\n3. [GitHub Webhook Integration](03_github_webhook_integration.md)\n4. [Pull Request Analysis Pipeline](04_pull_request_analysis_pipeline.md)\n5. [Tree-sitter Based Code Parsing](05_tree_sitter_based_code_parsing.md)\n6. [Groq API Integration](06_groq_api_integration.md)\n7. [RAG (Retrieval-Augmented Generation) Documentation Updates](07_rag__retrieval_augmented_generation__documentation_updates.md)\n8. [PR Review Comment Posting (GitHub)](08_pr_review_comment_posting__github_.md)\n9. [Snowflake Data Storage](09_snowflake_data_storage.md)\n\nContext from previous chapters (summary):\n# Chapter 1: Frontend Web Application (React)\n\nWelcome to CodeRoast! In this chapter, we'll be diving into the heart of the user experience: the frontend web application built with React. Imagine you're a code reviewer, and you want to quickly see all the Pull Requests (PRs) needing your attention, along with AI-powered feedback. Our React frontend makes that happen!\n\nThis chapter will guide you through understanding how this frontend works and how it displays all the amazing insights generated by the rest of the CodeRoast system.\n\n## What is a Frontend, and Why React?\n\nThink of a frontend like the dashboard of a car. You see the speedometer, fuel gauge, and warning lights, but you don't need to know *how* the engine works to drive. Similarly, our frontend lets you interact with CodeRoast without worrying about the complex AI and analysis happening behind the scenes.\n\nReact is a popular JavaScript library for building user interfaces. It helps us create dynamic and interactive web pages. We chose React because it's efficient, organized, and makes it easier to manage complex user interfaces like our PR dashboard.\n\n## Key Concepts\n\nLet's break down the key concepts you'll encounter in our React frontend:\n\n1.  **Components:** React is all about components. Think of them as building blocks for your UI. Each component is responsible for rendering a specific part of the page, like a PR list, a diff view, or an analytics chart.\n\n2.  **JSX:** React uses JSX, which looks like HTML but is actually JavaScript. It allows us to write UI elements directly in our JavaScript code, making it more readable and maintainable.\n\n3.  **State:** State is data that can change over time, and when it changes, React automatically updates the UI. For example, the selected PR, the filter applied to the PR list, or the current theme (light/dark) are all parts of the component's state.\n\n4.  **Props:** Props (short for properties) are how we pass data from a parent component to a child component. Imagine passing a message from one React component to another.\n\n## The CodeRoast Frontend: A Tour\n\nLet's look at how these concepts come together in our `CodeRoast` frontend, using code snippets from `frontend/app.js`.\n\n### Displaying a List of Pull Requests\n\nFirst, let's see how we display the list of PRs. We fetch the PR data from our backend API (more on that in [Chapter 2: FastAPI Backend API](02_fastapi_backend_api.md)) and store it in the component's state.\n\n```javascript\nconst [prs, setPrs] = React.useState([]);\n\nReact.useEffect(() => {\n    fetch('http://localhost:8000/api/prs')\n        .then(res => res.json())\n        .then(data => {\n            setPrs(data);\n        });\n}, []);\n```\n\nThis code does the following:\n\n*   `const [prs, setPrs] = React.useState([]);`: This line initializes a state variable called `prs` to an empty array. `prs` will hold our pull request data. `setPrs` is a function used to update the `prs` data, and React will automatically refresh the user interface whenever you use `setPrs`.\n\n*   `React.useEffect(() => { ... }, []);`: This is a React Hook that runs code when the component is first rendered. The empty array `[]` as the second argument means this effect will only run once when the component mounts.\n\n*   `fetch('http://localhost:8000/api/prs')`: This line makes a request to our backend API to get the list of PRs.\n\n*   `.then(res => res.json())`: This converts the response from the API into a JavaScript object (JSON).\n\n*   `.then(data => { setPrs(data); })`: This updates the `prs` state with the data we received from the API.\n\nNext, we map over the `prs` array to create a list of PR items:\n\n```javascript\nconst prItems = filteredPRs.map(pr => \n    e('div', { \n        key: pr.id, \n        className: selectedPR && selectedPR.id === pr.id ? 'pr-item selected' : 'pr-item', \n        onClick: () => selectPR(pr) \n    }, [\n        e('div', {key: 'pr-header', className: 'pr-header'}, [\n            e('strong', {key: 'id'}, `#${pr.id}`),\n            e('span', {\n                key: 'status',\n                className: `pr-status status-${pr.status}`\n            }, pr.status)\n        ]),\n        e('div', {key: 'pr-title', className: 'pr-title'}, pr.title),\n        e('div', {key: 'pr-meta', className: 'pr-meta'}, [\n            e('span', {key: 'author'}, [\n                e('img', {key: 'avatar', className: 'avatar', src: `https://ui-avatars.com/api/?name=${pr.author}&size=24`, alt: pr.author}),\n                pr.author\n            ]),\n            e('span', {key: 'date', className: 'date'}, 'Updated: ' + (pr.updated_at || 'N/A'))\n        ])\n    ])\n);\n```\n\nThis code takes each `pr` from the `filteredPRs` array and:\n\n*   `e('div', { ... }, [ ... ])`: Creates a `div` element (and nested elements) for each PR, setting its `key`, `className`, and `onClick` properties. The `key` prop is important for React to efficiently update the list. `e` is `React.createElement`, a function that creates React elements. It is used because JSX must be transformed at compile time.\n\n*   `className`: Sets the CSS class for styling.\n\n*   `onClick`: Defines a function that will be called when the PR item is clicked.\n\n*   Within the div, `e(...)` is used to create other html elements to display the PR's `id`, `status`, `title`, `author` and `updated_at`.\n\n### Displaying the Diff\n\nOne of the most important features of `CodeRoast` is displaying the code diff. The `renderDiffSection` function takes the diff string and formats it for display:\n\n```javascript\nfunction renderDiffSection(diff) {\n    const diffLines = parseDiff(diff);\n\n    return React.createElement(\n        'pre',\n        { className: 'diff-content' },\n        diffLines.map((line, index) => {\n            const className =\n                line.type === 'old' ? 'diff-old-code diff-code-line' :\n                line.type === 'new' ? 'diff-new-code diff-code-line' :\n                'diff-context-line';\n\n            return React.createElement('div', { key: index, className }, line.content);\n        })\n    );\n}\n```\n\nHere's what's happening:\n\n*   `const diffLines = parseDiff(diff);`: It calls the `parseDiff` function to split the diff into individual lines and determine their type (old code, new code, or context). See below for details on `parseDiff`.\n\n*   The rest of the function constructs a `<pre>` element which is used to display preformatted text.  Within the `<pre>` tag, we loop through the lines in `diffLines` and create a `div` tag for each line. The `className` will style the background of each line in green, red, or default color, depending on the line type.\n\nThe `parseDiff` function itself is responsible for taking the raw diff output and turning it into an array of objects, each representing a line in the diff:\n\n```javascript\nfunction parseDiff(diff) {\n    const lines = diff.split('\\n');\n    const parsedLines = [];\n\n    let section = null;\n\n    for (let i = 0; i < lines.length; i++) {\n        const line = lines[i];\n\n        if (line.startsWith('File:') || line.startsWith('Function:')) {\n            parsedLines.push({ type: 'context', content: line });\n            section = null;\n        } else if (line.trim() === 'Old Code:') {\n            parsedLines.push({ type: 'context', content: 'Old Code:' });\n            section = 'old';\n\n            // Handle empty old code\n            if (lines[i + 1]?.trim() === 'New Code:') {\n                parsedLines.push({ type: 'old', content: '<NO PREVIOUS CODE>' });\n            }\n        } else if (line.trim() === 'New Code:') {\n            parsedLines.push({ type: 'context', content: 'New Code:' });\n            section = 'new';\n        } else if (section === 'old' || section === 'new') {\n            parsedLines.push({ type: section, content: line });\n        } else {\n            parsedLines.push({ type: 'context', content: line });\n        }\n    }\n\n    return parsedLines;\n}\n```\n\nThis function:\n\n*   Splits the diff string into lines.\n*   Iterates through each line, determining its type based on the content.\n*   Adds each line to the `parsedLines` array with a `type` property indicating whether it's \"old\", \"new\", or \"context\".\n*   The `type` property is later used to style each line in the diff.\n\n### Displaying Analytics\n\nFinally, the frontend also displays some helpful analytics about the PRs. This is handled by the `Analytics` component.\n\n```javascript\n// Analytics component to visualize PR data\nfunction Analytics({ prs }) {\n    // Count PRs by status\n    const statusCounts = prs.reduce((acc, pr) => {\n        acc[pr.status] = (acc[pr.status] || 0) + 1;\n        return acc;\n    }, {});\n\n    // Count PRs by author\n    const authorCounts = prs.reduce((acc, pr) => {\n        acc[pr.author] = (acc[pr.author] || 0) + 1;\n        return acc;\n    }, {});\n\n    // Calculate PRs over time (by month)\n    const prsByMonth = prs.reduce((acc, pr) => {\n        const date = new Date(pr.created_at);\n        const monthYear = date.toLocaleString('default', { month: 'short', year: 'numeric' });\n        acc[monthYear] = (acc[monthYear] || 0) + 1;\n        return acc;\n    }, {});\n\n    // Create data arrays for charts\n    const statusData = Object.entries(statusCounts).map(([status, count]) => ({\n        status,\n        count,\n        color: status === 'Open' ? '#17a2b8' : \n               status === 'Merged' ? '#28a745' : \n               status === 'Closed' ? '#dc3545' : '#6c757d'\n    }));\n\n    const authorData = Object.entries(authorCounts)\n        .sort((a, b) => b[1] - a[1])\n        .slice(0, 5)\n        .map(([author, count]) => ({ author, count }));\n\n    const timelineData = Object.entries(prsByMonth)\n        .sort((a, b) => new Date(a[0]) - new Date(b[0]))\n        .map(([month, count]) => ({ month, count }));\n\n    return e('div', { className: 'analytics-container' }, [\n        e('h2', { key: 'title', className: 'analytics-title' }, 'Pull Request Analytics'),\n        // ... (rest of the analytics display code)\n    ]);\n}\n```\n\nThis component:\n\n*   Takes the `prs` array as a prop.\n*   Calculates statistics like the number of PRs by status, author, and month.\n*   Formats the data into arrays suitable for rendering charts and summaries.\n*   Renders the analytics using divs with appropriate CSS classes.\n\n## Under the Hood: Requesting PR Data\n\nLet's take a simplified look at what happens when the frontend requests PR data from the backend.\n\n```mermaid\nsequenceDiagram\n    participant FE as Frontend (React App)\n    participant BE as Backend (FastAPI)\n    participant DB as Snowflake Database\n\n    FE->>BE: GET /api/prs\n    activate BE\n    BE->>DB: Query PR Data\n    activate DB\n    DB-->>BE: PR Data (JSON)\n    deactivate DB\n    BE-->>FE: PR Data (JSON)\n    deactivate BE\n    FE->>FE: Render PR List\n```\n\n1.  The **Frontend (React App)** initiates a GET request to the `/api/prs` endpoint on the **Backend (FastAPI)**.\n2.  The **Backend (FastAPI)** receives the request and queries the **Snowflake Database** for PR data. (We'll see this in [Chapter 9: Snowflake Data Storage](09_snowflake_data_storage.md)).\n3.  The **Snowflake Database** returns the PR data to the **Backend (FastAPI)** in JSON format.\n4.  The **Backend (FastAPI)** sends the PR data back to the **Frontend (React App)** in JSON format.\n5.  The **Frontend (React App)** receives the data and renders the list of pull requests.\n\nThe actual fetching of data happens inside of the `React.useEffect` hook we saw before.\n\n```javascript\nReact.useEffect(() => {\n    fetch('http://localhost:8000/api/prs')\n        .then(res => res.json())\n        .then(data => {\n            setPrs(data);\n        });\n}, []);\n```\n\nThis hook ensures that the PR data is fetched when the component is first rendered and updates the component's state with the fetched data.\n\n## Conclusion\n\nIn this chapter, we explored the `CodeRoast` frontend, built with React. We learned about components, JSX, state, props, and how they're used to display PR lists, diffs, and analytics. We also took a peek under the hood to see how the frontend interacts with the backend to fetch data.\n\nNow that you have a basic understanding of the frontend, let's move on to the next chapter, where we'll dive into the backend API built with FastAPI: [Chapter 2: FastAPI Backend API](02_fastapi_backend_api.md).\n\n---\n# Chapter 2: FastAPI Backend API\n\nWelcome back to CodeRoast! In [Chapter 1: Frontend Web Application (React)](01_frontend_web_application__react_.md), we built a snazzy frontend that displays pull requests, diffs, and AI insights. But that frontend needs someone to talk to \u2013 someone who can fetch data, run the AI magic, and keep everything organized. That's where our FastAPI backend comes in!\n\n## What is a Backend API, and Why FastAPI?\n\nThink of the backend like the engine in our car analogy. The frontend is the dashboard, showing you information and letting you control things. The backend is the engine, doing all the heavy lifting.\n\nSpecifically, the backend acts as an *API (Application Programming Interface)*. This means it provides a structured way for the frontend (and other applications) to request information and functionality.\n\n**FastAPI** is a modern, high-performance Python web framework for building APIs. We chose FastAPI because it's:\n\n*   **Fast:** It's built for speed, so our app feels responsive.\n*   **Easy to use:** It has a simple and intuitive syntax, making development quicker.\n*   **Automatic Data Validation:** It automatically checks that the data sent to the API is in the correct format, preventing errors.\n*   **Automatic API Documentation:** It generates interactive API documentation automatically, making it easy to understand how to use the API.\n\n## Key Concepts\n\nLet's break down the key concepts behind our FastAPI backend:\n\n1.  **Routes (Endpoints):** These are specific URLs that our backend responds to. For example, `/api/prs` might be a route that returns a list of all pull requests. Think of them as specific \"doors\" to access different functionalities of our backend.\n\n2.  **Requests:** These are messages sent from the frontend (or any other client) to the backend, asking for something. They often include data, like the ID of a pull request the frontend wants to see.\n\n3.  **Responses:** These are messages sent back from the backend to the frontend, containing the requested information or the result of an action. Responses are often in JSON format, which is easy for the frontend to process.\n\n4.  **Data Models:** These are Python classes that define the structure of our data. For example, we might have a `PullRequest` data model that defines the fields for each pull request (like `id`, `title`, `author`, etc.).  Data models help ensure the data is consistent and well-defined.\n\n## The CodeRoast Backend: A Tour\n\nLet's look at how these concepts come together in our `CodeRoast` backend, using code snippets from `main.py`.\n\n### Defining a Route\n\nHere's an example of a route that fetches all pull requests:\n\n```python\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nimport snowflake.connector\nimport os\nfrom dotenv import load_dotenv\nimport pandas as pd\n\nload_dotenv()\n\napp = FastAPI()\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n@app.get(\"/api/prs\")\ndef fetch_all_prs():\n    # Fetch data from Snowflake and return it\n    # For simplicity, let's return a test data\n    test_data = [\n        {\n            \"id\": 1,\n            \"title\": \"Add new feature\",\n            \"author\": \"johndoe\",\n            \"status\": \"Open\",\n            \"created_at\": \"2023-01-01\",\n            \"updated_at\": \"2023-01-02\"\n        },\n        {\n            \"id\": 2,\n            \"title\": \"Fix bug in login\",\n            \"author\": \"janedoe\",\n            \"status\": \"Merged\",\n            \"created_at\": \"2023-01-03\",\n            \"updated_at\": \"2023-01-04\"\n        }\n    ]\n    return test_data\n```\n\nLet's break this down:\n\n*   `@app.get(\"/api/prs\")`: This is a *decorator* that tells FastAPI to create a route at the URL `/api/prs`. The `@app.get` part specifies that this route should handle GET requests (requests for data).\n*   `def fetch_all_prs():`: This defines a function called `fetch_all_prs` that will be executed when someone makes a GET request to `/api/prs`.\n*   `return test_data`: This returns the `test_data` list as a JSON response. FastAPI automatically converts Python dictionaries and lists into JSON.\n\nSo, if you open your web browser and go to `http://localhost:8000/api/prs`, you'll see a JSON response containing the `test_data`. (Note: make sure your backend is running!)\n\n### Handling Parameters\n\nWhat if we want to fetch information about a specific pull request, identified by its ID? We can use a *path parameter* in our route:\n\n```python\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nimport snowflake.connector\nimport os\nfrom dotenv import load_dotenv\nimport pandas as pd\n\nload_dotenv()\n\napp = FastAPI()\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n@app.get(\"/api/prs/{pr_id}\")\ndef fetch_pr_analysis(pr_id: int):\n    # Fetch the analysis data\n    # For simplicity, let's return a test data\n    test_data = {\n        \"pr_id\": pr_id,\n        \"ai_summary\": \"This PR adds a new feature that improves user experience\",\n        \"merge_confidence\": \"High\",\n        \"code_quality\": \"Good code quality. No major issues found.\",\n        \"diff\": \"```\\n+ def new_function():\\n+     return 'new feature'\\n```\",\n        \"doc_string\": \"This PR introduces a new function that returns 'new feature'.\"\n    }\n    return test_data\n```\n\nHere's what's new:\n\n*   `@app.get(\"/api/prs/{pr_id}\")`: This defines a route that includes a path parameter called `pr_id`. The curly braces `{}` indicate a parameter.\n*   `def fetch_pr_analysis(pr_id: int):`:  The `pr_id: int` part of the function definition tells FastAPI that `pr_id` is an integer parameter. FastAPI will automatically validate that the value passed in the URL is actually an integer.\n*   `\"pr_id\": pr_id`: We are using the `pr_id` parameter inside the function to construct the response.\n\nIf you go to `http://localhost:8000/api/prs/123`, you'll see a JSON response containing the `test_data`, but with the `\"pr_id\"` field set to `123`.\n\n### Connecting to Snowflake Database\n\nIn a real application, we wouldn't just return test data. We would fetch the data from a database.  In our case, we use [Snowflake Data Storage](09_snowflake_data_storage.md). Let's look at how we can connect to Snowflake and query the database (simplified):\n\n```python\nimport os\nimport snowflake.connector\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nimport pandas as pd\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\napp = FastAPI()\n\n# Snowflake credentials\nSNOWFLAKE_USER = os.getenv(\"SNOWFLAKE_USER\", \"raghavg332\")\nSNOWFLAKE_PASSWORD = os.getenv(\"SNOWFLAKE_PASSWORD\", \"Qa29Kh4MptfGHEW\")\nSNOWFLAKE_ACCOUNT = os.getenv(\"SNOWFLAKE_ACCOUNT\", \"KXCIVVH-LL27432\")\nSNOWFLAKE_WAREHOUSE = os.getenv(\"SNOWFLAKE_WAREHOUSE\", \"COMPUTE_WH\")\nSNOWFLAKE_DATABASE = os.getenv(\"SNOWFLAKE_DATABASE\", \"PR_DASHBOARD\")\nSNOWFLAKE_SCHEMA = os.getenv(\"SNOWFLAKE_SCHEMA\", \"PUBLIC\")\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\ndef get_conn():\n    \"\"\"Establish connection to Snowflake database\"\"\"\n    try:\n        conn = snowflake.connector.connect(\n            user=SNOWFLAKE_USER,\n            password=SNOWFLAKE_PASSWORD,\n            account=SNOWFLAKE_ACCOUNT,\n            warehouse=SNOWFLAKE_WAREHOUSE,\n            database=SNOWFLAKE_DATABASE,\n            schema=SNOWFLAKE_SCHEMA\n        )\n        return conn\n    except Exception as e:\n        print(f\"Error connecting to Snowflake: {e}\")\n        raise e\n\n@app.get(\"/api/prs/{pr_id}\")\ndef fetch_pr_analysis(pr_id: int):\n    try:\n        conn = get_conn()\n        cursor = conn.cursor(snowflake.connector.DictCursor)\n\n        # Fetch the analysis data\n        cursor.execute(f\"SELECT * FROM pr_analysis WHERE pr_id = {pr_id}\")\n        result = cursor.fetchone()\n\n        if not result:\n            cursor.close()\n            conn.close()\n            raise HTTPException(status_code=404, detail=\"PR analysis not found\")\n\n        cursor.close()\n        conn.close()\n        return result\n    except Exception as e:\n        print(f\"Error fetching PR analysis: {e}\")\n        raise HTTPException(status_code=500, detail=f\"Error: {str(e)}\")\n```\n\nHere's what's happening:\n\n*   `import snowflake.connector`: This imports the Snowflake connector library, which allows us to connect to a Snowflake database.\n*   `SNOWFLAKE_USER = os.getenv(\"SNOWFLAKE_USER\", \"raghavg332\")`: This imports environment variable, which allows us to securely config the snowflake credentials.\n*   `get_conn()`: This function establishes a connection to Snowflake using your credentials. **Important:** Make sure you have Snowflake configured correctly.\n*   `conn = get_conn()`: This calls the `get_conn` function to establish the connection.\n*   `cursor = conn.cursor(snowflake.connector.DictCursor)`: This creates a cursor object, which allows us to execute SQL queries.  The `snowflake.connector.DictCursor` is important here: it tells the connector to return results as Python dictionaries, which are easier to work with.\n*   `cursor.execute(f\"SELECT * FROM pr_analysis WHERE pr_id = {pr_id}\")`: This executes a SQL query to fetch the analysis data for the specified `pr_id`.\n*   `result = cursor.fetchone()`: This fetches the first row of the result set.\n*   `return result`: This returns the result (which is a dictionary) as a JSON response.\n\n## Under the Hood: Fetching PR Data\n\nLet's take a simplified look at what happens when the frontend requests PR data from the backend.\n\n```mermaid\nsequenceDiagram\n    participant FE as Frontend (React App)\n    participant BE as Backend (FastAPI)\n    participant DB as Snowflake Database\n\n    FE->>BE: GET /api/prs/123\n    activate BE\n    BE->>DB: SELECT * FROM pr_analysis WHERE pr_id = 123\n    activate DB\n    DB-->>BE: PR Analysis Data (JSON)\n    deactivate DB\n    BE-->>FE: PR Analysis Data (JSON)\n    deactivate BE\n    FE->>FE: Render PR Details\n```\n\n1.  The **Frontend (React App)** initiates a GET request to the `/api/prs/123` endpoint on the **Backend (FastAPI)**.\n2.  The **Backend (FastAPI)** receives the request and executes a SQL query on the **Snowflake Database** to fetch the analysis data for the PR with ID 123.\n3.  The **Snowflake Database** returns the PR analysis data to the **Backend (FastAPI)** in JSON format.\n4.  The **Backend (FastAPI)** sends the PR analysis data back to the **Frontend (React App)** in JSON format.\n5.  The **Frontend (React App)** receives the data and renders the details of the pull request.\n\nThe actual fetching of data happens inside of the `fetch_pr_analysis` function we saw before, where it:\n\n1. Establishes a connection to the snowflake database.\n2. Constructs and executes a select query.\n3. Formats the data for transmission back to the frontend as a JSON response.\n\n## Conclusion\n\nIn this chapter, we explored the `CodeRoast` backend, built with FastAPI. We learned about routes, requests, responses, data models, and how they're used to create a powerful API. We also saw how the backend interacts with a Snowflake database to fetch data.\n\nNow that you have a basic understanding of the backend, let's move on to the next chapter, where we'll dive into how we receive GitHub Webhooks: [Chapter 3: GitHub Webhook Integration](03_github_webhook_integration.md).\n\n---\n# Chapter 3: GitHub Webhook Integration\n\nIn [Chapter 2: FastAPI Backend API](02_fastapi_backend_api.md), we set up our FastAPI backend to respond to requests. But how does our backend *know* when something important happens on GitHub, like a new pull request being opened? That's where GitHub Webhooks come in!\n\nImagine you're waiting for a package to arrive. Instead of constantly checking the tracking number, you can sign up for notifications that tell you exactly when it's delivered. GitHub Webhooks are like those notifications, but for code!\n\n## What Problem Do Webhooks Solve?\n\nWithout webhooks, our backend would have to constantly ask GitHub, \"Hey, are there any new pull requests? Hey, are there any updates to existing pull requests?\" This is inefficient and wastes resources.\n\nWebhooks solve this problem by letting GitHub *tell* our backend when something interesting happens. This is much more efficient!\n\n**Central Use Case:** When a developer opens a new pull request (PR) on GitHub, we want our `CodeRoast` system to automatically start analyzing that PR. Webhooks make this possible.\n\n## Key Concepts\n\nLet's break down the key concepts of GitHub Webhook Integration:\n\n1.  **Event:** An event is something that happens in a GitHub repository, like opening a pull request, pushing code, or commenting on an issue.\n\n2.  **Webhook:** A webhook is a way for GitHub to send a notification to our backend whenever a specific event occurs. It's like a \"callback\" \u2013 GitHub calls back to our server when something happens.\n\n3.  **Payload:** The payload is the data that GitHub sends to our backend in the webhook notification. It's a JSON object containing information about the event that occurred, like the pull request's details, the commit messages, and the author's information. This is the \"package\" containing all the information.\n\n4.  **Signature:** A signature is a security measure that ensures the webhook notification is actually coming from GitHub and hasn't been tampered with. It's like a digital \"seal\" of authenticity.\n\n## How to Use Webhooks: A Simple Example\n\nLet's imagine a very simple scenario: when a pull request is opened, we want to print a message to our backend's console.\n\nHere's how the process works:\n\n1.  **Configure a Webhook on GitHub:** In your GitHub repository settings, you create a webhook. You tell GitHub:\n    *   The URL of your backend (where to send the notifications).\n    *   Which events to listen for (e.g., \"pull request\").\n    *   A secret (to verify the signature).\n\n2.  **Open a Pull Request:** A developer opens a new pull request on the repository.\n\n3.  **GitHub Sends a Webhook:** GitHub detects the \"pull request opened\" event and sends a webhook notification (a POST request) to our backend's URL.\n\n4.  **Backend Receives the Webhook:** Our backend receives the POST request, verifies the signature to ensure it's from GitHub, and then extracts the pull request information from the payload.\n\n5.  **Backend Processes the Information:** In our simple example, our backend just prints a message. In reality, it would trigger the [Pull Request Analysis Pipeline](04_pull_request_analysis_pipeline.md).\n\n## Code Example: Receiving a Webhook\n\nHere's a simplified code example showing how our FastAPI backend can receive and process a GitHub webhook:\n\n```python\nfrom fastapi import FastAPI, Request, Header\nfrom fastapi.responses import JSONResponse\nimport hmac\nimport hashlib\nimport json\n\napp = FastAPI()\n\nGITHUB_SECRET = \"asdfg\" # Replace with your actual secret\n\ndef verify_signature(payload, signature, secret):\n    mac = hmac.new(secret.encode(), msg=payload, digestmod=hashlib.sha256)\n    expected = f\"sha256={mac.hexdigest()}\"\n    return hmac.compare_digest(expected, signature)\n\n@app.post(\"/webhook\")\nasync def github_webhook(\n    request: Request,\n    x_hub_signature_256: str = Header(None),\n    x_github_event: str = Header(None)\n):\n    body = await request.body()\n\n    # Verify webhook signature\n    if not verify_signature(body, x_hub_signature_256, GITHUB_SECRET):\n        return JSONResponse(content={\"error\": \"Invalid signature\"}, status_code=403)\n\n    payload = json.loads(body)\n\n    if x_github_event == \"pull_request\":\n        action = payload[\"action\"]\n        pr = payload[\"pull_request\"]\n\n        if action in [\"opened\", \"synchronize\"]:\n            print(f\"New PR opened: {pr['title']}\")\n\n    return {\"message\": \"OK\"}\n```\n\nLet's break this down:\n\n*   `@app.post(\"/webhook\")`: This creates a route that listens for POST requests at the `/webhook` endpoint. GitHub will send its webhook notifications to this URL.\n*   `request: Request`: This allows us to access the body of the POST request (the payload).\n*   `x_hub_signature_256: str = Header(None)`: This extracts the `X-Hub-Signature-256` header from the request. This header contains the signature that we need to verify.\n*   `x_github_event: str = Header(None)`: This extracts the `X-GitHub-Event` header, which tells us what type of event triggered the webhook (e.g., \"pull_request\").\n*   `body = await request.body()`: This reads the raw bytes from the request body.\n*   `verify_signature(body, x_hub_signature_256, GITHUB_SECRET)`: This calls a function to verify that the signature is valid (we'll explain this in more detail later).\n*   `payload = json.loads(body)`: This converts the JSON payload into a Python dictionary.\n*   `if x_github_event == \"pull_request\"`: This checks if the event type is \"pull_request\".\n*   `if action in [\"opened\", \"synchronize\"]`: This checks if the pull request action is \"opened\" (a new PR) or \"synchronize\" (a PR was updated).\n*   `print(f\"New PR opened: {pr['title']}\")`: This prints the title of the new pull request to the console.\n\n**Example Input (Payload):**\n\nWhen a pull request is opened, GitHub will send a JSON payload to our `/webhook` endpoint. This payload contains lots of information about the pull request. Here's a simplified example:\n\n```json\n{\n  \"action\": \"opened\",\n  \"pull_request\": {\n    \"url\": \"https://api.github.com/repos/your-org/your-repo/pulls/123\",\n    \"title\": \"Add a new feature\"\n  },\n  \"repository\": {\n    \"full_name\": \"your-org/your-repo\"\n  }\n}\n```\n\n**Example Output:**\n\nWhen our backend receives this payload, it will print the following message to the console:\n\n```\nNew PR opened: Add a new feature\n```\n\nThis example demonstrates the basic flow of receiving and processing a GitHub webhook. In a real application, you would use the data in the payload to trigger more complex actions, like analyzing the code changes in the pull request.\n\n## Under the Hood: How Webhooks Work\n\nLet's take a closer look at how webhooks work internally.\n\nHere's a sequence diagram illustrating the flow:\n\n```mermaid\nsequenceDiagram\n    participant Dev as Developer\n    participant GH as GitHub\n    participant BE as Backend (FastAPI)\n\n    Dev->>GH: Opens/Updates Pull Request\n    activate GH\n    GH->>BE: POST /webhook (with payload)\n    activate BE\n    BE->>BE: Verify Signature\n    alt Signature Valid\n        BE->>BE: Process Payload\n        BE-->>GH: 200 OK\n    else Signature Invalid\n        BE-->>GH: 403 Forbidden\n    end\n    deactivate BE\n    deactivate GH\n```\n\n1.  A **Developer** opens or updates a pull request on **GitHub**.\n\n2.  **GitHub** detects the event and sends a POST request to our **Backend (FastAPI)** at the `/webhook` endpoint. The request includes a JSON payload containing information about the event.\n\n3.  The **Backend (FastAPI)** receives the POST request and immediately verifies the signature to ensure the request is actually coming from GitHub.\n\n4.  If the signature is valid, the **Backend (FastAPI)** processes the payload and performs the appropriate actions (e.g., triggers the [Pull Request Analysis Pipeline](04_pull_request_analysis_pipeline.md)). The Backend sends an HTTP 200 OK to GitHub.\n\n5.  If the signature is invalid, the **Backend (FastAPI)** rejects the request and sends an HTTP 403 Forbidden to GitHub.\n\n### Verifying the Signature\n\nThe `verify_signature` function is crucial for security. Here's how it works:\n\n```python\nimport hmac\nimport hashlib\n\ndef verify_signature(payload, signature, secret):\n    mac = hmac.new(secret.encode(), msg=payload, digestmod=hashlib.sha256)\n    expected = f\"sha256={mac.hexdigest()}\"\n    return hmac.compare_digest(expected, signature)\n```\n\n*   `hmac.new(secret.encode(), msg=payload, digestmod=hashlib.sha256)`: This creates a new HMAC (Hash-based Message Authentication Code) object using the secret key and the payload. It uses the SHA256 algorithm to generate the hash.\n*   `expected = f\"sha256={mac.hexdigest()}\"`: This calculates the expected signature by prefixing \"sha256=\" to the hexadecimal representation of the HMAC hash.\n*   `hmac.compare_digest(expected, signature)`: This securely compares the expected signature with the signature provided in the `X-Hub-Signature-256` header. This function is important to prevent timing attacks.\n\nThe secret key is configured in your GitHub webhook settings and should be stored securely on your backend. Make sure to *never* expose your secret key in your code or commit it to your repository! As you can see in the provided `backend/main.py`, the correct approach is to store the secret in an environment variable.\n\n## Conclusion\n\nIn this chapter, we learned about GitHub Webhook Integration. We understand what webhooks are, why they're useful, and how to receive and process them in our FastAPI backend. We also saw how to verify the signature to ensure the webhook notifications are authentic.\n\nNow that we can receive notifications about pull requests, we can move on to the next chapter, where we'll dive into the [Pull Request Analysis Pipeline](04_pull_request_analysis_pipeline.md), which is responsible for analyzing the code changes in the pull request and generating AI-powered insights.\n\n---\n# Chapter 4: Pull Request Analysis Pipeline\n\nIn [Chapter 3: GitHub Webhook Integration](03_github_webhook_integration.md), we learned how to set up our backend to receive notifications from GitHub whenever a pull request (PR) is opened or updated. Now, what do we *do* with that information?\n\nThat's where the **Pull Request Analysis Pipeline** comes in!\n\nImagine a factory assembly line. Raw materials (code changes) enter, various machines perform operations (analysis, documentation suggestions), and then the finished product (insights) is stored in a database. Our pipeline is very similar: it takes a pull request, analyzes the code changes, generates AI insights, and stores the results.\n\n**Central Use Case:** When a developer opens a new pull request, we want `CodeRoast` to automatically analyze the code, provide feedback, and suggest documentation updates. The Pull Request Analysis Pipeline makes this happen!\n\n## Key Concepts\n\nLet's break down the key concepts behind the Pull Request Analysis Pipeline:\n\n1.  **Input: Pull Request Data:** This is the raw information about the PR, received from GitHub via the webhook. It includes the code diff, the PR description, the author, and more. Think of this as the \"raw materials\" entering our factory.\n\n2.  **Code Parsing:** Before we can analyze the code, we need to understand its structure. This involves parsing the code to identify functions, classes, and other elements. We will talk about it in detail in [Chapter 5: Tree-sitter Based Code Parsing](05_tree_sitter_based_code_parsing.md).\n\n3.  **AI Insights Generation:** This is where the magic happens! We use AI, powered by [Groq API Integration](06_groq_api_integration.md), to analyze the code and generate insights. This might include summarizing the PR, identifying potential issues, suggesting improvements, and estimating merge confidence. It's like adding value to a product in the factory.\n\n4.  **Documentation Updates (RAG):** We use RAG (Retrieval-Augmented Generation) to suggest updates to the project documentation based on the code changes. This ensures that our documentation stays up-to-date. See more details in [Chapter 7: RAG (Retrieval-Augmented Generation) Documentation Updates](07_rag__retrieval_augmented_generation__documentation_updates.md).\n\n5.  **Output: Stored Insights:** The final product of our pipeline is a set of AI-generated insights that are stored in a database (Snowflake in our case [Chapter 9: Snowflake Data Storage](09_snowflake_data_storage.md)). These insights can then be displayed in the frontend and used to improve the code review process.\n\n## How to Use the Pipeline\n\nLet's walk through how to use the pipeline with a simplified example.\n\nImagine we have a function called `process_pull_request` that represents our pipeline. It takes the pull request data as input and returns the AI insights.\n\n```python\ndef process_pull_request(pr_data):\n    # 1. Parse the code changes.  We skip for this simple case.\n    code_diff = pr_data[\"code_diff\"] # Assume code_diff is extracted\n\n    # 2. Generate AI insights. We skip complex logic here\n    ai_summary = \"This PR adds a new feature.\"\n    merge_confidence = \"High\"\n\n    # 3. Store the results. We skip database insert here.\n    insights = {\n        \"ai_summary\": ai_summary,\n        \"merge_confidence\": merge_confidence,\n    }\n\n    return insights\n```\n\nThis simplified code does the following:\n\n1.  Takes `pr_data` which contains information about the PR.\n2.  Simulates extracting the code diff.\n3.  Simulates generating AI insights (in a real application, we'd use the Groq API).\n4.  Returns a dictionary containing the AI insights.\n\n**Example Input:**\n\n```python\npr_data = {\n    \"pr_title\": \"Add new feature\",\n    \"pr_description\": \"This PR adds a new feature to calculate statistics.\",\n    \"code_diff\": \"```diff\\n+def calculate_mean(data):\\n+  return sum(data) / len(data)\\n```\",\n    \"author\": \"johndoe\",\n    \"status\": \"open\"\n}\n```\n\n**Example Output:**\n\n```python\n{\n  \"ai_summary\": \"This PR adds a new feature.\",\n  \"merge_confidence\": \"High\"\n}\n```\n\nIn reality, this function is more complex and involves several steps, including connecting to external services and parsing the code. We will cover it later in this chapter!\n\n## Under the Hood: How the Pipeline Works\n\nLet's take a closer look at how the Pull Request Analysis Pipeline works internally.\n\nHere's a sequence diagram illustrating the flow:\n\n```mermaid\nsequenceDiagram\n    participant GH as GitHub\n    participant BE as Backend (FastAPI)\n    participant CP as Code Parser\n    participant AI as Groq API\n    participant DB as Snowflake Database\n\n    GH->>BE: POST /webhook (PR data)\n    activate BE\n    BE->>CP: Extract code diffs\n    activate CP\n    CP-->>BE: Code diffs (parsed)\n    deactivate CP\n    BE->>AI: Analyze code\n    activate AI\n    AI-->>BE: AI Insights (JSON)\n    deactivate AI\n    BE->>DB: Store Insights\n    activate DB\n    DB-->>BE: OK\n    deactivate DB\n    BE-->>GH: 200 OK\n    deactivate BE\n```\n\n1.  **GitHub** sends a POST request to our **Backend (FastAPI)** at the `/webhook` endpoint, containing the pull request data.\n2.  The **Backend (FastAPI)** receives the data and passes the code to the **Code Parser** to extract the changed functions (detailed in [Chapter 5: Tree-sitter Based Code Parsing](05_tree_sitter_based_code_parsing.md)).\n3.  The **Code Parser** returns the extracted code diffs to the **Backend (FastAPI)**.\n4.  The **Backend (FastAPI)** sends the code diffs and PR details to the **Groq API** for analysis and insight generation.\n5.  The **Groq API** analyzes the code and returns AI insights (summary, merge confidence, etc.) in JSON format to the **Backend (FastAPI)**.\n6.  The **Backend (FastAPI)** stores these insights in the **Snowflake Database**.\n7.  The **Backend (FastAPI)** sends an HTTP 200 OK response to GitHub.\n\n### Diving into the Code\n\nLet's look at a code example of how the `process_pull_request` function is implemented. This code is located in `backend/prReview.py`.\n\n```python\ndef process_pull_request(g, repo_name, pr_number, sender=None):\n    repo = g.get_repo(repo_name)\n    pr = repo.get_pull(pr_number)\n    pr_description = pr.title # title is used as description\n    pr_title = pr.title\n    pr_author = pr.user.login\n    pr_status = pr.state\n    code_diff = get_lines_changed(pr) # Step 1\n    # ... (rest of the function)\n```\n\nThis code does the following:\n\n1.  `repo = g.get_repo(repo_name)`: Gets the repository object from GitHub using the `repo_name`.  `g` is the authenticated Github instance.\n2.  `pr = repo.get_pull(pr_number)`: Gets the pull request object from the repository using the `pr_number`.\n3.  `pr_description = pr.title`: Sets the pull request description.\n4.  `code_diff = get_lines_changed(pr)`: Calls the `get_lines_changed` function to get the code diff. This function uses github api to get the diff and returns a dictionary containing the added, modified, and deleted lines.\n\nLet's look at how we get the diff code:\n\n```python\ndef get_lines_changed(pr):\n    files = pr.get_files()\n    file_changes = {}\n    for file in files:\n        if file.status == \"removed\":\n            continue\n        file_changes[file.filename] = parse_changed_lines(file.patch)\n\n    return file_changes\n\ndef parse_changed_lines(patch: str):\n    added_or_modified_lines = set()\n    deleted_lines = set()\n    for line in patch.splitlines():\n        if line.startswith(\"@@\"):\n            match = re.findall(r\"@@ -(\\d+)(?:,(\\d+))? \\+(\\d+)(?:,(\\d+))? @@\", line)\n            if match:\n                old_start, old_count, new_start, new_count = match[0]\n\n                old_start = int(old_start)\n                old_count = int(old_count or 1)\n                for l in range(old_start, old_start + old_count):\n                    deleted_lines.add(l)\n\n                new_start = int(new_start)\n                new_count = int(new_count or 1)\n                for l in range(new_start, new_start + new_count):\n                    added_or_modified_lines.add(l)\n\n    return {\n        \"added_or_modified_lines\": added_or_modified_lines,\n        \"deleted_lines\": deleted_lines\n    }\n```\n\nThis code does the following:\n\n1.  `files = pr.get_files()`: Get all files that changed in this PR.\n2.  The code loops through each of the `files`:\n    *   `if file.status == \"removed\": continue`: Skip the files that were removed.\n    *   `file_changes[file.filename] = parse_changed_lines(file.patch)`: Call `parse_changed_lines` to parse the changes of each file and then store. The `parse_changed_lines` uses regex matching to find out which lines were added or deleted in this file.\n\nThe following code then uses the data extracted above to call the Groq API:\n\n```python\n    # Get the review data and post a comment\n    review_data = review_and_store_pr(pr_description, file_diff_func, pr_title, pr_author, pr_status)\n    \n    # Post the review as a comment on the PR\n    post_review_comment(g, repo_name, pr_number, review_data)\n```\n\nThe `review_and_store_pr` performs the RAG (Retrieval-Augmented Generation) to update project documentation and calls Groq to generate the PR review and stores the review into the snowflake database. `post_review_comment` then posts the review as a comment on the PR. Details can be found in [Chapter 8: PR Review Comment Posting (GitHub)](08_pr_review_comment_posting__github_.md)\n\n## Conclusion\n\nIn this chapter, we explored the Pull Request Analysis Pipeline. We learned about the key concepts, how to use the pipeline, and how it works internally. We saw how the pipeline takes a pull request as input, analyzes the code changes, generates AI insights, suggests documentation updates, and stores the results in a database.\n\nNow that we understand the overall pipeline, let's dive deeper into one of the key components: parsing the code changes using Tree-sitter in [Chapter 5: Tree-sitter Based Code Parsing](05_tree_sitter_based_code_parsing.md).\n\n---\n# Chapter 5: Tree-sitter Based Code Parsing\n\nIn [Chapter 4: Pull Request Analysis Pipeline](04_pull_request_analysis_pipeline.md), we learned how the `CodeRoast` pipeline analyzes pull requests. One of the key steps in that pipeline is understanding the *structure* of the code changes.  Instead of just treating the code as plain text, we want to know *what* code has changed - what functions have been added, deleted or modified?\n\nThat's where **Tree-sitter** comes in!\n\nImagine trying to understand a sentence without knowing grammar. You might get some of the words, but you'd miss the relationships between them. Tree-sitter is like a grammar checker for code. It helps us understand the structure of the code so we can analyze it more effectively.\n\n**Central Use Case:** When a developer modifies a function in a pull request, we want `CodeRoast` to *only* analyze that function, rather than the entire file. This focused analysis saves time and improves the accuracy of our AI insights.\n\n## Key Concepts\n\nLet's break down the key concepts behind Tree-sitter based code parsing:\n\n1.  **Parsing:** Parsing is the process of taking code (which is just a string of characters) and turning it into a structured representation that a computer can understand. Think of it like taking a sentence and breaking it down into nouns, verbs, and adjectives.\n\n2.  **Abstract Syntax Tree (AST):** The AST is the structured representation of the code that results from parsing. It's like a tree diagram that shows the relationships between the different parts of the code. Each \"node\" in the tree represents a different element of the code, like a function, a variable, or an expression.\n\n3.  **Tree-sitter:** Tree-sitter is a library that makes it easy to parse code and create ASTs. It's fast, reliable, and supports many different programming languages. It's the engine that drives our code parsing.\n\n4.  **Grammar:** A grammar defines the rules for a programming language. Tree-sitter uses grammars to understand the structure of the code. Each language has its own grammar (e.g., Python grammar, JavaScript grammar). These grammars tells Tree-sitter what to expect.\n\n## How to Use Tree-sitter: A Simple Example\n\nLet's imagine we have a simple Python function and we want to use Tree-sitter to extract its name and code.\n\nHere's the function:\n\n```python\ndef greet(name):\n    \"\"\"Greets the person passed in as a parameter.\"\"\"\n    print(f\"Hello, {name}!\")\n```\n\nHere's how we can use Tree-sitter to parse this code and extract the function name and code (simplified):\n\n```python\nfrom tree_sitter import Language, Parser\nimport tree_sitter_python as tspython\n\n# Set up Tree-sitter\nPY_LANGUAGE = Language(tspython.language())\nparser = Parser()\nparser.set_language(PY_LANGUAGE)\n\n# The code to parse\ncode = \"\"\"\ndef greet(name):\n    \\\"\\\"\\\"Greets the person passed in as a parameter.\\\"\\\"\\\"\n    print(f\"Hello, {name}!\")\n\"\"\"\n\n# Parse the code\ntree = parser.parse(bytes(code, \"utf8\"))\n\n# Get the root node of the AST\nroot_node = tree.root_node\n\n# Find the function definition node\nfunction_node = root_node.children[0] # Assume function is the first child\n\n# Extract the function name\nfunction_name_node = function_node.child_by_field_name('name')\nfunction_name = function_name_node.text.decode()\n\n# Extract the function code\nfunction_code = function_node.text.decode()\n\nprint(f\"Function Name: {function_name}\")\nprint(f\"Function Code: {function_code}\")\n```\n\nThis simplified code does the following:\n\n1.  **Sets up Tree-sitter:** It imports the necessary libraries and creates a Tree-sitter parser for Python.  We are using `tree_sitter_python` to obtain the python grammar.\n2.  **Parses the code:** It takes the Python code and uses the parser to create an AST.\n3.  **Navigates the AST:**\n    *   `function_node = root_node.children[0]`: Since the example only consists of the definition of the `greet` function, the function node will be the first child. This code accesses the function node.\n    *   `function_name_node = function_node.child_by_field_name('name')`: This line gets the node that represents the name of the function.\n4.  **Extracts information:** It extracts the function name and code from the AST.\n5.  **Prints the results:** It prints the function name and code to the console.\n\n**Example Output:**\n\n```\nFunction Name: greet\nFunction Code: def greet(name):\n    \"\"\"Greets the person passed in as a parameter.\"\"\"\n    print(f\"Hello, {name}!\")\n```\n\nThis example demonstrates how we can use Tree-sitter to parse code and extract specific elements. In `CodeRoast`, we use this technique to identify the functions that have been changed in a pull request.\n\n## Under the Hood: How Tree-sitter Works\n\nLet's take a closer look at how Tree-sitter works internally.\n\nHere's a sequence diagram illustrating the flow:\n\n```mermaid\nsequenceDiagram\n    participant Code as Code (String)\n    participant Parser as Tree-sitter Parser\n    participant AST as Abstract Syntax Tree\n    participant Extractor as Information Extractor\n\n    Code->>Parser: Parse code\n    activate Parser\n    Parser->>AST: Build AST\n    activate AST\n    AST-->>Parser: AST\n    deactivate AST\n    Parser-->>Extractor: AST\n    deactivate Parser\n    Extractor->>AST: Navigate and Extract\n    activate AST\n    AST-->>Extractor: Function Name and Code\n    deactivate AST\n    Extractor-->>Code: Function Name and Code\n```\n\n1.  The **Code (String)** is passed to the **Tree-sitter Parser**.\n2.  The **Tree-sitter Parser** uses the grammar to build an **Abstract Syntax Tree (AST)**.\n3.  The **AST** is returned to the **Tree-sitter Parser**.\n4.  The **Tree-sitter Parser** passes the **AST** to the **Information Extractor**.\n5.  The **Information Extractor** navigates the **AST** to extract the function name and code.\n6.  The extracted function name and code are returned to the **Code**.\n\n### Diving into the Code\n\nLet's look at the code that extracts functions from the code. This code can be found in `backend/prReview.py`.\n\n```python\ndef extract_functions(code, language_name, changed_lines):\n    parser = get_paraser(language_name)\n    tree = parser.parse(bytes(code, \"utf8\"))\n    root_node = tree.root_node\n\n    functions = []\n\n    def node_within_lines(node):\n        start_line = node.start_point[0] + 1\n        end_line = node.end_point[0] + 1\n        return any([start_line <= line <= end_line for line in changed_lines])\n    \n    def traverse(node):\n        if language_name == \"python\" and node.type == \"function_definition\":\n            if node_within_lines(node):\n                func_name_node = node.child_by_field_name('name')\n                func_name = func_name_node.text.decode()\n                functions.append({'name': func_name, 'code': node.text.decode()})\n        elif language_name == 'javascript' and node.type in ['function_declaration', 'method_definition', 'arrow_function']:\n            if node_within_lines(node):\n                func_name_node = node.child_by_field_name('name')\n                func_name = func_name_node.text.decode()\n                functions.append({'name': func_name, 'code': node.text.decode()})\n        elif language_name == 'cpp' and node.type in ['function_definition']:\n            if node_within_lines(node):\n                func_name_node = node.child_by_field_name('name')\n                func_name = func_name_node.text.decode()\n                functions.append({'name': func_name, 'code': node.text.decode()})\n        for child in node.children:\n            traverse(child)\n\n    traverse(root_node)\n    return functions\n```\n\nThis code does the following:\n\n1.  `parser = get_paraser(language_name)`: Gets the Tree-sitter parser for the given language (Python, JavaScript, C++ are supported).\n2.  `tree = parser.parse(bytes(code, \"utf8\"))`: Parses the code and creates an AST.\n3.  `root_node = tree.root_node`: Gets the root node of the AST.\n4.  The code defines a helper function `node_within_lines` to checks whether a node is within changed lines to skip parsing nodes that are irrelevant.\n5.  `traverse(root_node)`: Calls the `traverse` function to recursively traverse the AST and extract the function names and code.\n    *   It first checks if the language is Python and the node type is `function_definition`.\n    *   If it is, it extracts the function name and code and adds them to the `functions` list.\n    *   Then it traverses the child nodes.\n6.  It does similar logic for Javascript and C++\n7.  It returns the `functions` list.\n\nThe `get_paraser` function loads the grammars for supported languages:\n\n```python\ndef get_paraser(language_name):\n\n    PY_LANGUAGE = Language(tspython.language())\n    JS_LANGUAGE = Language(tsjavascript.language())\n    CPP_LANGUAGE = Language(tscpp.language())\n\n    LANGUAGES = {\n        'python': PY_LANGUAGE,\n        'javascript': JS_LANGUAGE,\n        'cpp': CPP_LANGUAGE,\n    }\n    parser = Parser(LANGUAGES[language_name])\n    return parser\n```\n\nThis code does the following:\n\n1.  `PY_LANGUAGE = Language(tspython.language())`: Loads the Python grammar.\n2.  `JS_LANGUAGE = Language(tsjavascript.language())`: Loads the JavaScript grammar.\n3.  `CPP_LANGUAGE = Language(tscpp.language())`: Loads the C++ grammar.\n4.  `LANGUAGES = { ... }`: Creates a dictionary that maps language names to their corresponding grammars.\n5.  `parser = Parser(LANGUAGES[language_name])`: Creates a Tree-sitter parser for the given language.\n6.  It returns the `parser`.\n\n## Conclusion\n\nIn this chapter, we explored Tree-sitter based code parsing. We learned about the key concepts, how to use Tree-sitter to extract information from code, and how it works internally. We saw how Tree-sitter helps us understand the structure of code so we can analyze it more effectively.\n\nNow that we can parse code changes, let's move on to the next chapter and see how we use AI to generate insights about those changes using [Groq API Integration](06_groq_api_integration.md).\n\n\nRelevant Code Snippets:\n--- File: backend/ai_analyse.py ---\nimport requests\nimport json\nimport logging\nimport snowflake.connector\nimport random\nimport datetime\n\nGROQ_API_KEY = \"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\"\nGROQ_ENDPOINT = \"https://api.groq.com/openai/v1/chat/completions\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\n# Snowflake setup\nconn = snowflake.connector.connect(\n    user=\"raghavg332\",\n    password=\"Qa29Kh4MptfGHEW\",\n    account=\"KXCIVVH-LL27432\",\n    warehouse=\"COMPUTE_WH\",\n    database=\"PR_DASHBOARD\",\n    schema=\"PUBLIC\"\n)\ncur = conn.cursor()\n\ndef update_documentation_with_rag(pr_description: str, code_diff: str, docs_path=\"backend/project_docs.txt\"):\n    \"\"\"\n    Performs a simple RAG-based call to Groq to update documentation.\n    \"\"\"\n    # Step 1: Load the existing documentation\n    try:\n        with open(docs_path, \"r\") as f:\n            existing_docs = f.read()\n    except FileNotFoundError:\n        logging.error(f\"[\u274c] Documentation file not found at {docs_path}\")\n        return {\"error\": \"Documentation file not found.\"}\n\n    # Step 2: Very basic RAG - extract top paragraphs that contain overlap\n    paragraphs = existing_docs.split(\"\\n\\n\")\n    keywords = pr_description.lower().split()\n    ranked_paragraphs = sorted(paragraphs, key=lambda para: sum(word in para.lower() for word in keywords), reverse=True)\n    context = \"\\n\\n\".join(ranked_paragraphs[:3])  # Top 3 paragraphs as \"retrieved context\"\n\n    # Step 3: Build the prompt\n    doc_prompt = f\"\"\"\nYou are an expert technical writer and code documentation assistant.\n\nA developer has made the following changes to the codebase:\n\n## PR Description:\n{pr_description}\n\n## Code Diff:\n{code_diff}\n\n## Relevant Existing Documentation:\n{context}\n\nPlease provide additions or updates to the documentation based on the above code change. \nFormat your response in **Markdown**, and only include sections that should be added or updated. \nIf the change doesn't require doc updates, say so.\n\"\"\"\n\n    # Step 4: Call Groq API\n    payload = {\n        \"model\": \"llama3-70b-8192\",\n        \"messages\": [{\"role\": \"user\", \"content\": doc_prompt}]\n    }\n\n    try:\n        response = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n        response.raise_for_status()\n        result = response.json()\n        return {\n            \"doc_update\": result[\"choices\"][0][\"message\"][\"content\"].strip()\n        }\n\n    except Exception as e:\n        logging.exception(\"Groq API call for documentation update failed\")\n        return {\"error\": str(e)}\n\n\ndef build_full_prompt(pr_description: str, code_diff: str) -> str:\n    return f\"\"\"\nYou are an AI code reviewer and documentation assistant.\n\nA developer submitted the following pull request. Please do the following:\n1. \ud83d\udd0d Summarize the overall purpose of the PR.\n2. \ud83e\udde0 Rate the merge confidence as: High / Medium / Low.\n3. \u2705 Review the code for:\n   - Syntax issues\n   - Style (PEP8)\n   - Functionality correctness (does it match the description?)\n4. \ud83d\udcdd Extract or suggest updated docstrings for any changed or added functions/classes.\n\n## PR Description:\n{pr_description}\n\n## Code Diff:\n{code_diff}\n\nRespond ONLY in this JSON format:\n\n{{\n  \"ai_summary\": \"...\",\n  \"merge_confidence\": \"1-10\",\n  \"code_quality\": {{\n    \"syntax_check\": \"...\",\n    \"style_check\": \"...\",\n    \"functionality_check\": \"...\",\n    \"final_rating\": \"Excellent / Good / Needs Work / Critical Issues\"\n  }}\n}}\n\"\"\"\n\ndef review_and_store_pr(pr_description: str, code_diff: str, pr_title: str, pr_author: str, pr_status: str):\n    docstring = update_documentation_with_rag(pr_description, code_diff)\n    print(docstring)\n    print('\\n\\n')\n    pr_id = random.randint(1000, 9999)\n    prompt = build_full_prompt(pr_description, code_diff)\n\n    payload = {\n        \"model\": \"llama3-70b-8192\",\n        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n    }\n\n    try:\n        response = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n        response.raise_for_status()\n        result = response.json()\n\n        try:\n            content = result[\"choices\"][0][\"message\"][\"content\"].strip()\n            review_data = json.loads(content)\n\n            # Flatten the code quality object for easier insert\n            code_quality_str = (\n                f\"Syntax: {review_data['code_quality']['syntax_check']}\\n\"\n                f\"Style: {review_data['code_quality']['style_check']}\\n\"\n                f\"Functionality: {review_data['code_quality']['functionality_check']}\\n\"\n                f\"Rating: {review_data['code_quality']['final_rating']}\"\n            )\n            cur.execute(f\"\"\"\n                INSERT INTO PULL_REQUESTS (\n                    ID, TITLE, AUTHOR, STATUS, CREATED_AT, UPDATED_AT\n                )\n                VALUES (%s, %s, %s, %s, %s, %s);\n            \"\"\", (\n                pr_id,\n                pr_title,\n                pr_author,\n                pr_status,\n                datetime.datetime.now(),\n                datetime.datetime.now()\n            ))\n            conn.commit()\n\n            # Insert into Snowflake\n            cur.execute(f\"\"\"\n                INSERT INTO PR_ANALYSIS (\n                    PR_ID, AI_SUMMARY, MERGE_CONFIDENCE, CODE_QUALITY,\n                    PR_DESCRIPTION, DOCSTRINGS, DIFF\n                )\n                VALUES (%s, %s, %s, %s, %s, %s, %s);\n            \"\"\", (\n                pr_id,\n                review_data[\"ai_summary\"],\n                review_data[\"merge_confidence\"],\n                code_quality_str,\n                pr_description,\n                docstring['doc_update'],\n                code_diff\n            ))\n\n            conn.commit()\n\n            logging.info(f\"[\u2705] PR #{pr_id} analysis saved to Snowflake.\")\n            return review_data\n\n        except json.JSONDecodeError:\n            logging.error(\"[\u274c] Groq returned non-JSON output.\")\n            return {\"error\": \"Invalid JSON from Groq\"}\n\n    except Exception as e:\n        logging.exception(\"Groq API or Snowflake insert failed\")\n        return {\"error\": str(e)}\n\n# # \ud83e\uddea Example test\n# if __name__ == \"__main__\":\n#     code_diff = \"\"\"\n# diff --git a/app/main.py b/app/main.py\n# index a1b2c3d..d4e5f6g 100644\n# --- a/app/main.py\n# +++ b/app/main.py\n# @@ def main():\n# -    print(\"Hello\")\n# +    log_greeting(\"Hello, world!\")\n# +    print(\"Execution completed.\")\n\n# +def log_greeting(message):\n# +    \\\"\\\"\\\"Logs the greeting message to a file\\\"\\\"\\\"\n# +    with open(\"logs.txt\", \"a\") as log_file:\n# +        log_file.write(f\"Greeting logged: {message}\\\\n\")\n# \"\"\"\n#     pr_description = \"Update main function to greet the world.\"\n#     pr_title = \"Update main function\"\n#     pr_author = \"johndoe\"\n#     pr_status = \"OPEN\"\n\n#     result = review_and_store_pr(pr_description, code_diff, pr_title, pr_author, pr_status)\n#     print(json.dumps(result, indent=2))\n\n--- File: backend/prReview.py ---\nimport os, re\nfrom github import Auth\nfrom github import Github, GithubIntegration\nfrom github.GithubException import GithubException\nfrom tree_sitter import Language, Parser\nimport tree_sitter_python as tspython\nimport tree_sitter_javascript as tsjavascript\nimport tree_sitter_cpp as tscpp\nimport requests\nimport json\nimport logging\nimport snowflake.connector\nimport random\nimport datetime\n\n\nGROQ_API_KEY = \"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\"\nGROQ_ENDPOINT = \"https://api.groq.com/openai/v1/chat/completions\"\n\ndef authenticate_github(app_id: int, installation_id, private_key: str):\n    gi = GithubIntegration(integration_id=app_id, private_key=private_key)\n    g = gi.get_github_for_installation(installation_id)\n    return g\n\ndef get_pull_request(g, repo_name: str, pr_number: int):\n    repo = g.get_repo(repo_name)\n    pr = repo.get_pull(pr_number)\n    return pr\n\ndef parse_changed_lines(patch: str):\n    added_or_modified_lines = set()\n    deleted_lines = set()\n    for line in patch.splitlines():\n        if line.startswith(\"@@\"):\n            match = re.findall(r\"@@ -(\\d+)(?:,(\\d+))? \\+(\\d+)(?:,(\\d+))? @@\", line)\n            if match:\n                old_start, old_count, new_start, new_count = match[0]\n\n                old_start = int(old_start)\n                old_count = int(old_count or 1)\n                for l in range(old_start, old_start + old_count):\n                    deleted_lines.add(l)\n\n                new_start = int(new_start)\n                new_count = int(new_count or 1)\n                for l in range(new_start, new_start + new_count):\n                    added_or_modified_lines.add(l)\n\n    return {\n        \"added_or_modified_lines\": added_or_modified_lines,\n        \"deleted_lines\": deleted_lines\n    }\n\ndef get_file_contents(g, repo_name: str, file_path: str, commit_sha: str):\n    repo = g.get_repo(repo_name)\n    try:\n        contents = repo.get_contents(file_path, ref=commit_sha)\n        return contents.decoded_content.decode()\n    except GithubException as e:\n        if e.status == 404:\n            return \"\"\n        else:\n            raise e\n\ndef get_lines_changed(pr):\n    files = pr.get_files()\n    file_changes = {}\n    for file in files:\n        if file.status == \"removed\":\n            continue\n        file_changes[file.filename] = parse_changed_lines(file.patch)\n\n    return file_changes\n\ndef get_paraser(language_name):\n\n    PY_LANGUAGE = Language(tspython.language())\n    JS_LANGUAGE = Language(tsjavascript.language())\n    CPP_LANGUAGE = Language(tscpp.language())\n\n    LANGUAGES = {\n        'python': PY_LANGUAGE,\n        'javascript': JS_LANGUAGE,\n        'cpp': CPP_LANGUAGE,\n    }\n    parser = Parser(LANGUAGES[language_name])\n    return parser\n\ndef extract_functions(code, language_name, changed_lines):\n    parser = get_paraser(language_name)\n    tree = parser.parse(bytes(code, \"utf8\"))\n    root_node = tree.root_node\n\n    functions = []\n\n    def node_within_lines(node):\n        start_line = node.start_point[0] + 1\n        end_line = node.end_point[0] + 1\n        return any([start_line <= line <= end_line for line in changed_lines])\n    \n    def traverse(node):\n        if language_name == \"python\" and node.type == \"function_definition\":\n            if node_within_lines(node):\n                func_name_node = node.child_by_field_name('name')\n                func_name = func_name_node.text.decode()\n                functions.append({'name': func_name, 'code': node.text.decode()})\n        elif language_name == 'javascript' and node.type in ['function_declaration', 'method_definition', 'arrow_function']:\n            if node_within_lines(node):\n                func_name_node = node.child_by_field_name('name')\n                func_name = func_name_node.text.decode()\n                functions.append({'name': func_name, 'code': node.text.decode()})\n        elif language_name == 'cpp' and node.type in ['function_definition']:\n            if node_within_lines(node):\n                func_name_node = node.child_by_field_name('name')\n                func_name = func_name_node.text.decode()\n                functions.append({'name': func_name, 'code': node.text.decode()})\n        for child in node.children:\n            traverse(child)\n\n    traverse(root_node)\n    return functions\n\nheaders = {\n    \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\n# Snowflake setup\nconn = snowflake.connector.connect(\n    user=\"raghavg332\",\n    password=\"Qa29Kh4MptfGHEW\",\n    account=\"KXCIVVH-LL27432\",\n    warehouse=\"COMPUTE_WH\",\n    database=\"PR_DASHBOARD\",\n    schema=\"PUBLIC\"\n)\ncur = conn.cursor()\n\ndef update_documentation_with_rag(pr_description: str, code_diff: str, docs_path=\"backend/project_docs.txt\"):\n    \"\"\"\n    Performs a simple RAG-based call to Groq to update documentation.\n    \"\"\"\n    # Step 1: Load the existing documentation\n    try:\n        with open(docs_path, \"r\") as f:\n            existing_docs = f.read()\n    except FileNotFoundError:\n        logging.error(f\"[\u274c] Documentation file not found at {docs_path}\")\n        return {\"error\": \"Documentation file not found.\"}\n\n    # Step 2: Very basic RAG - extract top paragraphs that contain overlap\n    paragraphs = existing_docs.split(\"\\n\\n\")\n    keywords = pr_description.lower().split()\n    ranked_paragraphs = sorted(paragraphs, key=lambda para: sum(word in para.lower() for word in keywords), reverse=True)\n    context = \"\\n\\n\".join(ranked_paragraphs[:3])  # Top 3 paragraphs as \"retrieved context\"\n\n    # Step 3: Build the prompt\n    doc_prompt = f\"\"\"\nYou are an expert technical writer and code documentation assistant.\n\nA developer has made the following changes to the codebase:\n\n## PR Description:\n{pr_description}\n\n## Code Diff:\n{code_diff}\n\n## Relevant Existing Documentation:\n{context}\n\nPlease provide additions or updates to the documentation based on the above code change. \nFormat your response in **Markdown**, and only include sections that should be added or updated. \nIf the change doesn't require doc updates, say so.\n\"\"\"\n\n    # Step 4: Call Groq API\n    payload = {\n        \"model\": \"llama3-70b-8192\",\n        \"messages\": [{\"role\": \"user\", \"content\": doc_prompt}]\n    }\n\n    try:\n        response = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n        response.raise_for_status()\n        result = response.json()\n        return {\n            \"doc_update\": result[\"choices\"][0][\"message\"][\"content\"].strip()\n        }\n\n    except Exception as e:\n        logging.exception(\"Groq API call for documentation update failed\")\n        return {\"error\": str(e)}\n\n\ndef build_full_prompt(pr_description: str, code_diff_str: str) -> str:\n    return f\"\"\"\nYou are an AI code reviewer and documentation assistant.\n\nA developer submitted the following pull request. Please do the following:\n1. \ud83d\udd0d Summarize the overall purpose of the PR. Be a little detailed\n2. \u2705 Review the code for:\n   - Syntax issues in Python (Make up some Issue)\n   - Style (PEP8)\n   - Functionality correctness (does it match the description?)\n3. Based on the review, provide a merge confidence score from 1 to 10.\n4. \ud83d\udcdd Extract or suggest updated docstrings for any changed or added functions/classes.\n5. The Code Diff is made up of the old and new version of the code, if any of the field is empty it means, the function is either added or deleted respectively.\n\n## PR Description:\n{pr_description}\n\n## Code Diff:\n{code_diff_str}\n\nRespond ONLY in this JSON format without any additional text:\n\n{{\n  \"ai_summary\": \"...\",\n  \"merge_confidence\": \"1 - 10\",\n  \"code_quality\": {{\n    \"syntax_check\": \"...\",\n    \"style_check\": \"...\",\n    \"functionality_check\": \"...\",\n    \"final_rating\": \"Excellent / Good / Needs Work / Critical Issues\"\n  }}\n}}\n\"\"\"\n\ndef review_and_store_pr(pr_description: str, code_diff: dict, pr_title: str, pr_author: str, pr_status: str):\n    code_diff_str = \"\"\n    for file in list(code_diff.keys()):\n        code_diff_str += f\"File: {file}\\n\\n\"\n        diff = code_diff[file]\n\n        for func_name in list(diff.keys()):\n            old_code = diff[func_name]['old_code']\n            new_code = diff[func_name]['new_code']\n            if old_code is None:\n                old_code = \"\"\n            if new_code is None:\n                new_code = \"\"\n            code_diff_str += f\"Function: {func_name}\\n\\nOld Code:\\n{old_code}\\n\\nNew Code:\\n{new_code}\"\n    docstring = update_documentation_with_rag(pr_description, code_diff)\n    pr_id = random.randint(1000, 9999)\n    prompt = build_full_prompt(pr_description, code_diff_str)\n\n    payload = {\n        \"model\": \"llama3-70b-8192\",\n        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n    }\n\n    try:\n        response = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n        response.raise_for_status()\n        result = response.json()\n\n        try:\n            content = result[\"choices\"][0][\"message\"][\"content\"].strip()\n            review_data = json.loads(content)\n\n            # Flatten the code quality object for easier insert\n            code_quality_str = (\n                f\"Syntax: {review_data['code_quality']['syntax_check']}\\n\"\n                f\"Style: {review_data['code_quality']['style_check']}\\n\"\n                f\"Functionality: {review_data['code_quality']['functionality_check']}\\n\"\n                f\"Rating: {review_data['code_quality']['final_rating']}\"\n            )\n            cur.execute(f\"\"\"\n                INSERT INTO PULL_REQUESTS (\n                    ID, TITLE, AUTHOR, STATUS, CREATED_AT, UPDATED_AT\n                )\n                VALUES (%s, %s, %s, %s, %s, %s);\n            \"\"\", (\n                pr_id,\n                pr_title,\n                pr_author,\n                pr_status,\n                datetime.datetime.now(),\n                datetime.datetime.now()\n            ))\n            conn.commit()\n\n            # Insert into Snowflake\n            cur.execute(f\"\"\"\n                INSERT INTO PR_ANALYSIS (\n                    PR_ID, AI_SUMMARY, MERGE_CONFIDENCE, CODE_QUALITY,\n                    PR_DESCRIPTION, DOCSTRINGS, DIFF\n                )\n                VALUES (%s, %s, %s, %s, %s, %s, %s);\n            \"\"\", (\n                pr_id,\n                review_data[\"ai_summary\"],\n                review_data[\"merge_confidence\"],\n                code_quality_str,\n                pr_description,\n                docstring['doc_update'],\n                code_diff_str\n            ))\n\n            conn.commit()\n\n            logging.info(f\"[\u2705] PR #{pr_id} analysis saved to Snowflake.\")\n            print(f\"[\u2705] PR #{pr_id} analysis saved to Snowflake.\")\n            return review_data\n\n        except json.JSONDecodeError:\n            logging.error(\"[\u274c] Groq returned non-JSON output.\")\n            print(\"[\u274c] Groq returned non-JSON output.\")\n            return {\"error\": \"Invalid JSON from Groq\"}\n\n    except Exception as e:\n        logging.exception(\"Groq API or Snowflake insert failed\")\n        return {\"error\": str(e)}\n\ndef process_pull_request(g, repo_name, pr_number, sender=None):\n    repo = g.get_repo(repo_name)\n    pr = repo.get_pull(pr_number)\n    pr_description = pr.title\n    pr_title = pr.title\n    pr_author = pr.user.login\n    pr_status = pr.state\n    code_diff = get_lines_changed(pr)\n    file_diff_func = {}\n    for file in list(code_diff.keys()):\n        diff_func = {}\n        new_code = get_file_contents(g, repo_name, file, pr.head.sha)\n        old_code = get_file_contents(g, repo_name, file, pr.base.sha)\n        added_or_modified_functions_after = extract_functions(new_code, \"python\", code_diff[file]['added_or_modified_lines'])\n        deleted_functions_before = extract_functions(old_code, \"python\", code_diff[file]['deleted_lines'])\n        old_functions_all = extract_functions(old_code, \"python\", set(range(1, len(old_code.split(\"\\n\")))))\n        new_functions_all = extract_functions(new_code, \"python\", set(range(1, len(new_code.split(\"\\n\")))))\n        for func in added_or_modified_functions_after:\n            matched_old_function = next((f for f in old_functions_all if f['name'] ==  func['name']), None)\n            diff_func[func['name']] = {\n                \"new_code\": func['code'],\n                \"old_code\": matched_old_function['code'] if matched_old_function else None\n            }\n        \n        for func in deleted_functions_before:\n            matched_new_function = next((f for f in new_functions_all if f['name'] ==  func['name']), None)\n            diff_func[func['name']] = {\n                \"new_code\": matched_new_function['code'] if matched_new_function else None,\n                \"old_code\": func['code']\n            }\n        \n        file_diff_func[file] = diff_func\n    \n    # Get the review data and post a comment\n    review_data = review_and_store_pr(pr_description, file_diff_func, pr_title, pr_author, pr_status)\n    \n    # Post the review as a comment on the PR\n    post_review_comment(g, repo_name, pr_number, review_data)\n\n\ndef post_review_comment(g, repo_name, pr_number, review_data):\n    \"\"\"\n    Posts the AI review as a comment on the pull request.\n    \n    Args:\n        g: Authenticated GitHub instance\n        repo_name: Repository name (owner/repo)\n        pr_number: Pull request number\n        review_data: The AI review data dictionary\n    \"\"\"\n    try:\n        # Get the repo and PR objects\n        repo = g.get_repo(repo_name)\n        pr = repo.get_pull(pr_number)\n        \n        # Format the comment body\n        comment_body = f\"\"\"\n## \ud83e\udd16 AI Code Review\n\n### Summary\n{review_data.get('ai_summary', 'No summary available')}\n\n### Merge Confidence: **{review_data.get('merge_confidence', 'Unknown')}**\n\n### Code Quality Review\n- **Syntax:** {review_data.get('code_quality', {}).get('syntax_check', 'Not analyzed')}\n- **Style:** {review_data.get('code_quality', {}).get('style_check', 'Not analyzed')}\n- **Functionality:** {review_data.get('code_quality', {}).get('functionality_check', 'Not analyzed')}\n- **Overall Rating:** {review_data.get('code_quality', {}).get('final_rating', 'Not rated')}\n\n---\n*This review was automatically generated by AI.*\n\"\"\"\n        \n        # Create the comment on the PR\n        pr.create_issue_comment(comment_body)\n        logging.info(f\"[\u2705] Posted AI review comment on PR #{pr_number} in {repo_name}\")\n        print(f\"[\u2705] Posted AI review comment on PR #{pr_number} in {repo_name}\")\n        return True\n    except Exception as e:\n        logging.exception(f\"[\u274c] Failed to post comment on PR #{pr_number}: {str(e)}\")\n        print(f\"[\u274c] Failed to post comment on PR #{pr_number}: {str(e)}\")\n        return False\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 6: Groq API Integration`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "# Chapter 6: Groq API Integration\n\nIn [Chapter 5: Tree-sitter Based Code Parsing](05_tree_sitter_based_code_parsing.md), we learned how to parse the code changes in a pull request. Now, how do we *understand* what those changes mean and generate useful insights?\n\nThat's where the **Groq API Integration** comes in!\n\nThink of the Groq API as a super-smart AI assistant. We send it the code and a description of the pull request, and it sends back a summary of the changes, a code quality assessment, and even suggestions for improving the documentation.\n\n**Central Use Case:** When a developer submits a pull request, we want `CodeRoast` to automatically generate a helpful summary of the changes and identify potential problems in the code. The Groq API integration makes this possible.\n\n## Key Concepts\n\nLet's break down the key concepts behind the Groq API Integration:\n\n1.  **API (Application Programming Interface):**  An API is a way for different software systems to talk to each other.  The Groq API lets us send requests to Groq's AI models and get responses back. It is like ordering from a restaurant; we send the order (request), and the restaurant delivers the food (response).\n\n2.  **Groq API Key:** This is a secret code that identifies us to the Groq API. Think of it like a password that allows us to access the Groq API. You'll need to obtain your own API key from Groq to use their service.\n\n3.  **Prompt:** A prompt is the message we send to the Groq API, telling it what we want it to do.  For example, our prompt might include the code changes, the pull request description, and a request to summarize the changes. It's like writing instructions to your super-smart AI assistant.\n\n4.  **AI Model:** An AI model is a pre-trained algorithm that can perform a specific task, like summarizing text or assessing code quality. Groq offers a variety of AI models that we can use.\n\n5.  **JSON (JavaScript Object Notation):** JSON is a way to format data that's easy for computers to read and write. We send data to the Groq API in JSON format, and it sends data back to us in JSON format.\n\n## How to Use the Groq API: A Simple Example\n\nLet's imagine we want to use the Groq API to summarize a simple code change.\n\nHere's the code change:\n\n```python\ndef add(x, y):\n  \"\"\"Adds two numbers together.\"\"\"\n  return x + y\n```\n\nHere's how we can use the Groq API to summarize this code (simplified):\n\n```python\nimport requests\nimport json\n\n# Your Groq API key\nGROQ_API_KEY = \"YOUR_GROQ_API_KEY\" # Replace with your actual key\n\n# The API endpoint\nGROQ_ENDPOINT = \"https://api.groq.com/openai/v1/chat/completions\"\n\n# The prompt\nprompt = \"\"\"\nSummarize the following Python code:\n\ndef add(x, y):\n  \\\"\\\"\\\"Adds two numbers together.\\\"\\\"\\\"\n  return x + y\n\"\"\"\n\n# Construct the payload\npayload = {\n    \"model\": \"llama3-70b-8192\", # You should check with Groq for the latest model\n    \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n}\n\n# Set the headers\nheaders = {\n    \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\n# Make the request\nresponse = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n\n# Parse the response\nresult = response.json()\n\n# Extract the summary\nsummary = result[\"choices\"][0][\"message\"][\"content\"].strip()\n\n# Print the summary\nprint(summary)\n```\n\nThis simplified code does the following:\n\n1.  **Sets up the API key and endpoint:** It defines the `GROQ_API_KEY` and `GROQ_ENDPOINT` variables.  **Important:** Replace `\"YOUR_GROQ_API_KEY\"` with your actual Groq API key.\n2.  **Constructs the prompt:** It defines the `prompt` variable, which tells the Groq API what we want it to do.\n3.  **Constructs the payload:** It creates a dictionary containing the prompt and the AI model to use.\n    *   `\"model\": \"llama3-70b-8192\"`:  This tells the Groq API to use the \"llama3-70b-8192\" AI model. Please check Groq documentation for the latest models and their API names.\n4.  **Sets the headers:** It defines the `headers` variable, which includes our Groq API key and tells the Groq API that we're sending data in JSON format.\n5.  **Makes the request:** It uses the `requests.post` function to send the prompt to the Groq API.\n6.  **Parses the response:** It uses the `response.json()` function to convert the JSON response from the Groq API into a Python dictionary.\n7.  **Extracts the summary:** It extracts the summary from the response and prints it to the console.\n\n**Example Output:**\n\n```\nThis code defines a function called \"add\" that takes two arguments, x and y, and returns their sum. The docstring indicates that the function adds two numbers together.\n```\n\nThis example demonstrates how we can use the Groq API to summarize code. In `CodeRoast`, we use this technique to generate summaries of pull requests, assess code quality, and suggest documentation updates.\n\n## Under the Hood: How the Groq API Integration Works\n\nLet's take a closer look at how the Groq API Integration works internally.\n\nHere's a sequence diagram illustrating the flow:\n\n```mermaid\nsequenceDiagram\n    participant BE as Backend (FastAPI)\n    participant Groq as Groq API\n\n    BE->>Groq: POST /openai/v1/chat/completions (Prompt + Code)\n    activate Groq\n    Groq->>Groq: AI Model Analysis\n    Groq-->>BE: JSON (Summary, Quality, etc.)\n    deactivate Groq\n    BE->>BE: Process Results\n```\n\n1.  The **Backend (FastAPI)** constructs a prompt containing the code changes and sends a POST request to the **Groq API** at the `/openai/v1/chat/completions` endpoint.\n\n2.  The **Groq API** receives the prompt and uses its AI models to analyze the code.\n\n3.  The **Groq API** returns a JSON response containing the summary, code quality assessment, and documentation suggestions to the **Backend (FastAPI)**.\n\n4.  The **Backend (FastAPI)** processes the results and stores them in the database or displays them in the frontend.\n\n### Diving into the Code\n\nLet's look at the code that calls the Groq API and processes the response. This code can be found in `backend/ai_analyse.py` and `backend/prReview.py`. First, let's look at the `build_full_prompt` function:\n\n```python\ndef build_full_prompt(pr_description: str, code_diff_str: str) -> str:\n    return f\"\"\"\nYou are an AI code reviewer and documentation assistant.\n\nA developer submitted the following pull request. Please do the following:\n1. \ud83d\udd0d Summarize the overall purpose of the PR. Be a little detailed\n2. \u2705 Review the code for:\n   - Syntax issues in Python (Make up some Issue)\n   - Style (PEP8)\n   - Functionality correctness (does it match the description?)\n3. Based on the review, provide a merge confidence score from 1 to 10.\n4. \ud83d\udcdd Extract or suggest updated docstrings for any changed or added functions/classes.\n5. The Code Diff is made up of the old and new version of the code, if any of the field is empty it means, the function is either added or deleted respectively.\n\n## PR Description:\n{pr_description}\n\n## Code Diff:\n{code_diff_str}\n\nRespond ONLY in this JSON format without any additional text:\n\n{{\n  \"ai_summary\": \"...\",\n  \"merge_confidence\": \"1 - 10\",\n  \"code_quality\": {{\n    \"syntax_check\": \"...\",\n    \"style_check\": \"...\",\n    \"functionality_check\": \"...\",\n    \"final_rating\": \"Excellent / Good / Needs Work / Critical Issues\"\n  }}\n}}\n\"\"\"\n```\n\nThis code constructs the prompt that we send to the Groq API. It includes:\n\n*   A description of the role the AI should play (\"AI code reviewer and documentation assistant\").\n*   Instructions on what the AI should do (summarize the PR, review the code, suggest documentation updates).\n*   The pull request description and code diff.\n*   **Important:** It tells the AI to respond in JSON format. This makes it easy for us to parse the response.\n\nThe `review_and_store_pr` function calls the Groq API and process the response.\n\n```python\nimport requests\nimport json\n\ndef review_and_store_pr(pr_description: str, code_diff: dict, pr_title: str, pr_author: str, pr_status: str):\n    code_diff_str = \"\"\n    for file in list(code_diff.keys()):\n        code_diff_str += f\"File: {file}\\n\\n\"\n        diff = code_diff[file]\n\n        for func_name in list(diff.keys()):\n            old_code = diff[func_name]['old_code']\n            new_code = diff[func_name]['new_code']\n            if old_code is None:\n                old_code = \"\"\n            if new_code is None:\n                new_code = \"\"\n            code_diff_str += f\"Function: {func_name}\\n\\nOld Code:\\n{old_code}\\n\\nNew Code:\\n{new_code}\"\n    docstring = update_documentation_with_rag(pr_description, code_diff)\n    pr_id = random.randint(1000, 9999)\n    prompt = build_full_prompt(pr_description, code_diff_str)\n\n    payload = {\n        \"model\": \"llama3-70b-8192\",\n        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n    }\n\n    try:\n        response = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n        response.raise_for_status()\n        result = response.json()\n\n        try:\n            content = result[\"choices\"][0][\"message\"][\"content\"].strip()\n            review_data = json.loads(content)\n\n            # Flatten the code quality object for easier insert\n            code_quality_str = (\n                f\"Syntax: {review_data['code_quality']['syntax_check']}\\n\"\n                f\"Style: {review_data['code_quality']['style_check']}\\n\"\n                f\"Functionality: {review_data['code_quality']['functionality_check']}\\n\"\n                f\"Rating: {review_data['code_quality']['final_rating']}\"\n            )\n            cur.execute(f\"\"\"\n                INSERT INTO PULL_REQUESTS (\n                    ID, TITLE, AUTHOR, STATUS, CREATED_AT, UPDATED_AT\n                )\n                VALUES (%s, %s, %s, %s, %s, %s);\n            \"\"\", (\n                pr_id,\n                pr_title,\n                pr_author,\n                pr_status,\n                datetime.datetime.now(),\n                datetime.datetime.now()\n            ))\n            conn.commit()\n\n            # Insert into Snowflake\n            cur.execute(f\"\"\"\n                INSERT INTO PR_ANALYSIS (\n                    PR_ID, AI_SUMMARY, MERGE_CONFIDENCE, CODE_QUALITY,\n                    PR_DESCRIPTION, DOCSTRINGS, DIFF\n                )\n                VALUES (%s, %s, %s, %s, %s, %s, %s);\n            \"\"\", (\n                pr_id,\n                review_data[\"ai_summary\"],\n                review_data[\"merge_confidence\"],\n                code_quality_str,\n                pr_description,\n                docstring['doc_update'],\n                code_diff_str\n            ))\n\n            conn.commit()\n\n            logging.info(f\"[\u2705] PR #{pr_id} analysis saved to Snowflake.\")\n            print(f\"[\u2705] PR #{pr_id} analysis saved to Snowflake.\")\n            return review_data\n\n        except json.JSONDecodeError:\n            logging.error(\"[\u274c] Groq returned non-JSON output.\")\n            print(\"[\u274c] Groq returned non-JSON output.\")\n            return {\"error\": \"Invalid JSON from Groq\"}\n\n    except Exception as e:\n        logging.exception(\"Groq API or Snowflake insert failed\")\n        return {\"error\": str(e)}\n```\n\nThis code does the following:\n\n1.  First loops through the `code_diff` to construct the `code_diff_str`\n2.  Calls the `build_full_prompt` function to construct the prompt.\n3.  Constructs the `payload`, setting the ai model to be used.\n4.  Calls the `requests.post` function to send the request to the Groq API.\n5.  Parses the JSON response using `response.json()`.\n6.  Extracts the AI insights from the response and stores them in the database ([Chapter 9: Snowflake Data Storage](09_snowflake_data_storage.md)).\n7.  Handles potential errors, like invalid JSON responses.\n\n## Conclusion\n\nIn this chapter, we explored the Groq API Integration. We learned about the key concepts, how to use the Groq API to generate AI insights, and how it works internally. We saw how the Groq API helps us understand code changes and generate useful summaries, code quality assessments, and documentation suggestions.\n\nNow that we can use AI to generate insights, let's move on to the next chapter and see how we use RAG (Retrieval-Augmented Generation) to suggest updates to the project documentation in [Chapter 7: RAG (Retrieval-Augmented Generation) Documentation Updates](07_rag__retrieval_augmented_generation__documentation_updates.md).\n", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `CodeRoast` about the concept: \"RAG (Retrieval-Augmented Generation) Documentation Updates\". This is Chapter 7.\n\nConcept Details:\n- Description:\nThis component automatically updates project documentation by using RAG. It finds relevant sections in existing documentation, and uses an AI model to generate suggested updates. It's like having a helpful assistant who automatically keeps your documentation up-to-date based on new code changes.\n\n\nComplete Tutorial Structure:\n1. [Frontend Web Application (React)](01_frontend_web_application__react_.md)\n2. [FastAPI Backend API](02_fastapi_backend_api.md)\n3. [GitHub Webhook Integration](03_github_webhook_integration.md)\n4. [Pull Request Analysis Pipeline](04_pull_request_analysis_pipeline.md)\n5. [Tree-sitter Based Code Parsing](05_tree_sitter_based_code_parsing.md)\n6. [Groq API Integration](06_groq_api_integration.md)\n7. [RAG (Retrieval-Augmented Generation) Documentation Updates](07_rag__retrieval_augmented_generation__documentation_updates.md)\n8. [PR Review Comment Posting (GitHub)](08_pr_review_comment_posting__github_.md)\n9. [Snowflake Data Storage](09_snowflake_data_storage.md)\n\nContext from previous chapters (summary):\n# Chapter 1: Frontend Web Application (React)\n\nWelcome to CodeRoast! In this chapter, we'll be diving into the heart of the user experience: the frontend web application built with React. Imagine you're a code reviewer, and you want to quickly see all the Pull Requests (PRs) needing your attention, along with AI-powered feedback. Our React frontend makes that happen!\n\nThis chapter will guide you through understanding how this frontend works and how it displays all the amazing insights generated by the rest of the CodeRoast system.\n\n## What is a Frontend, and Why React?\n\nThink of a frontend like the dashboard of a car. You see the speedometer, fuel gauge, and warning lights, but you don't need to know *how* the engine works to drive. Similarly, our frontend lets you interact with CodeRoast without worrying about the complex AI and analysis happening behind the scenes.\n\nReact is a popular JavaScript library for building user interfaces. It helps us create dynamic and interactive web pages. We chose React because it's efficient, organized, and makes it easier to manage complex user interfaces like our PR dashboard.\n\n## Key Concepts\n\nLet's break down the key concepts you'll encounter in our React frontend:\n\n1.  **Components:** React is all about components. Think of them as building blocks for your UI. Each component is responsible for rendering a specific part of the page, like a PR list, a diff view, or an analytics chart.\n\n2.  **JSX:** React uses JSX, which looks like HTML but is actually JavaScript. It allows us to write UI elements directly in our JavaScript code, making it more readable and maintainable.\n\n3.  **State:** State is data that can change over time, and when it changes, React automatically updates the UI. For example, the selected PR, the filter applied to the PR list, or the current theme (light/dark) are all parts of the component's state.\n\n4.  **Props:** Props (short for properties) are how we pass data from a parent component to a child component. Imagine passing a message from one React component to another.\n\n## The CodeRoast Frontend: A Tour\n\nLet's look at how these concepts come together in our `CodeRoast` frontend, using code snippets from `frontend/app.js`.\n\n### Displaying a List of Pull Requests\n\nFirst, let's see how we display the list of PRs. We fetch the PR data from our backend API (more on that in [Chapter 2: FastAPI Backend API](02_fastapi_backend_api.md)) and store it in the component's state.\n\n```javascript\nconst [prs, setPrs] = React.useState([]);\n\nReact.useEffect(() => {\n    fetch('http://localhost:8000/api/prs')\n        .then(res => res.json())\n        .then(data => {\n            setPrs(data);\n        });\n}, []);\n```\n\nThis code does the following:\n\n*   `const [prs, setPrs] = React.useState([]);`: This line initializes a state variable called `prs` to an empty array. `prs` will hold our pull request data. `setPrs` is a function used to update the `prs` data, and React will automatically refresh the user interface whenever you use `setPrs`.\n\n*   `React.useEffect(() => { ... }, []);`: This is a React Hook that runs code when the component is first rendered. The empty array `[]` as the second argument means this effect will only run once when the component mounts.\n\n*   `fetch('http://localhost:8000/api/prs')`: This line makes a request to our backend API to get the list of PRs.\n\n*   `.then(res => res.json())`: This converts the response from the API into a JavaScript object (JSON).\n\n*   `.then(data => { setPrs(data); })`: This updates the `prs` state with the data we received from the API.\n\nNext, we map over the `prs` array to create a list of PR items:\n\n```javascript\nconst prItems = filteredPRs.map(pr => \n    e('div', { \n        key: pr.id, \n        className: selectedPR && selectedPR.id === pr.id ? 'pr-item selected' : 'pr-item', \n        onClick: () => selectPR(pr) \n    }, [\n        e('div', {key: 'pr-header', className: 'pr-header'}, [\n            e('strong', {key: 'id'}, `#${pr.id}`),\n            e('span', {\n                key: 'status',\n                className: `pr-status status-${pr.status}`\n            }, pr.status)\n        ]),\n        e('div', {key: 'pr-title', className: 'pr-title'}, pr.title),\n        e('div', {key: 'pr-meta', className: 'pr-meta'}, [\n            e('span', {key: 'author'}, [\n                e('img', {key: 'avatar', className: 'avatar', src: `https://ui-avatars.com/api/?name=${pr.author}&size=24`, alt: pr.author}),\n                pr.author\n            ]),\n            e('span', {key: 'date', className: 'date'}, 'Updated: ' + (pr.updated_at || 'N/A'))\n        ])\n    ])\n);\n```\n\nThis code takes each `pr` from the `filteredPRs` array and:\n\n*   `e('div', { ... }, [ ... ])`: Creates a `div` element (and nested elements) for each PR, setting its `key`, `className`, and `onClick` properties. The `key` prop is important for React to efficiently update the list. `e` is `React.createElement`, a function that creates React elements. It is used because JSX must be transformed at compile time.\n\n*   `className`: Sets the CSS class for styling.\n\n*   `onClick`: Defines a function that will be called when the PR item is clicked.\n\n*   Within the div, `e(...)` is used to create other html elements to display the PR's `id`, `status`, `title`, `author` and `updated_at`.\n\n### Displaying the Diff\n\nOne of the most important features of `CodeRoast` is displaying the code diff. The `renderDiffSection` function takes the diff string and formats it for display:\n\n```javascript\nfunction renderDiffSection(diff) {\n    const diffLines = parseDiff(diff);\n\n    return React.createElement(\n        'pre',\n        { className: 'diff-content' },\n        diffLines.map((line, index) => {\n            const className =\n                line.type === 'old' ? 'diff-old-code diff-code-line' :\n                line.type === 'new' ? 'diff-new-code diff-code-line' :\n                'diff-context-line';\n\n            return React.createElement('div', { key: index, className }, line.content);\n        })\n    );\n}\n```\n\nHere's what's happening:\n\n*   `const diffLines = parseDiff(diff);`: It calls the `parseDiff` function to split the diff into individual lines and determine their type (old code, new code, or context). See below for details on `parseDiff`.\n\n*   The rest of the function constructs a `<pre>` element which is used to display preformatted text.  Within the `<pre>` tag, we loop through the lines in `diffLines` and create a `div` tag for each line. The `className` will style the background of each line in green, red, or default color, depending on the line type.\n\nThe `parseDiff` function itself is responsible for taking the raw diff output and turning it into an array of objects, each representing a line in the diff:\n\n```javascript\nfunction parseDiff(diff) {\n    const lines = diff.split('\\n');\n    const parsedLines = [];\n\n    let section = null;\n\n    for (let i = 0; i < lines.length; i++) {\n        const line = lines[i];\n\n        if (line.startsWith('File:') || line.startsWith('Function:')) {\n            parsedLines.push({ type: 'context', content: line });\n            section = null;\n        } else if (line.trim() === 'Old Code:') {\n            parsedLines.push({ type: 'context', content: 'Old Code:' });\n            section = 'old';\n\n            // Handle empty old code\n            if (lines[i + 1]?.trim() === 'New Code:') {\n                parsedLines.push({ type: 'old', content: '<NO PREVIOUS CODE>' });\n            }\n        } else if (line.trim() === 'New Code:') {\n            parsedLines.push({ type: 'context', content: 'New Code:' });\n            section = 'new';\n        } else if (section === 'old' || section === 'new') {\n            parsedLines.push({ type: section, content: line });\n        } else {\n            parsedLines.push({ type: 'context', content: line });\n        }\n    }\n\n    return parsedLines;\n}\n```\n\nThis function:\n\n*   Splits the diff string into lines.\n*   Iterates through each line, determining its type based on the content.\n*   Adds each line to the `parsedLines` array with a `type` property indicating whether it's \"old\", \"new\", or \"context\".\n*   The `type` property is later used to style each line in the diff.\n\n### Displaying Analytics\n\nFinally, the frontend also displays some helpful analytics about the PRs. This is handled by the `Analytics` component.\n\n```javascript\n// Analytics component to visualize PR data\nfunction Analytics({ prs }) {\n    // Count PRs by status\n    const statusCounts = prs.reduce((acc, pr) => {\n        acc[pr.status] = (acc[pr.status] || 0) + 1;\n        return acc;\n    }, {});\n\n    // Count PRs by author\n    const authorCounts = prs.reduce((acc, pr) => {\n        acc[pr.author] = (acc[pr.author] || 0) + 1;\n        return acc;\n    }, {});\n\n    // Calculate PRs over time (by month)\n    const prsByMonth = prs.reduce((acc, pr) => {\n        const date = new Date(pr.created_at);\n        const monthYear = date.toLocaleString('default', { month: 'short', year: 'numeric' });\n        acc[monthYear] = (acc[monthYear] || 0) + 1;\n        return acc;\n    }, {});\n\n    // Create data arrays for charts\n    const statusData = Object.entries(statusCounts).map(([status, count]) => ({\n        status,\n        count,\n        color: status === 'Open' ? '#17a2b8' : \n               status === 'Merged' ? '#28a745' : \n               status === 'Closed' ? '#dc3545' : '#6c757d'\n    }));\n\n    const authorData = Object.entries(authorCounts)\n        .sort((a, b) => b[1] - a[1])\n        .slice(0, 5)\n        .map(([author, count]) => ({ author, count }));\n\n    const timelineData = Object.entries(prsByMonth)\n        .sort((a, b) => new Date(a[0]) - new Date(b[0]))\n        .map(([month, count]) => ({ month, count }));\n\n    return e('div', { className: 'analytics-container' }, [\n        e('h2', { key: 'title', className: 'analytics-title' }, 'Pull Request Analytics'),\n        // ... (rest of the analytics display code)\n    ]);\n}\n```\n\nThis component:\n\n*   Takes the `prs` array as a prop.\n*   Calculates statistics like the number of PRs by status, author, and month.\n*   Formats the data into arrays suitable for rendering charts and summaries.\n*   Renders the analytics using divs with appropriate CSS classes.\n\n## Under the Hood: Requesting PR Data\n\nLet's take a simplified look at what happens when the frontend requests PR data from the backend.\n\n```mermaid\nsequenceDiagram\n    participant FE as Frontend (React App)\n    participant BE as Backend (FastAPI)\n    participant DB as Snowflake Database\n\n    FE->>BE: GET /api/prs\n    activate BE\n    BE->>DB: Query PR Data\n    activate DB\n    DB-->>BE: PR Data (JSON)\n    deactivate DB\n    BE-->>FE: PR Data (JSON)\n    deactivate BE\n    FE->>FE: Render PR List\n```\n\n1.  The **Frontend (React App)** initiates a GET request to the `/api/prs` endpoint on the **Backend (FastAPI)**.\n2.  The **Backend (FastAPI)** receives the request and queries the **Snowflake Database** for PR data. (We'll see this in [Chapter 9: Snowflake Data Storage](09_snowflake_data_storage.md)).\n3.  The **Snowflake Database** returns the PR data to the **Backend (FastAPI)** in JSON format.\n4.  The **Backend (FastAPI)** sends the PR data back to the **Frontend (React App)** in JSON format.\n5.  The **Frontend (React App)** receives the data and renders the list of pull requests.\n\nThe actual fetching of data happens inside of the `React.useEffect` hook we saw before.\n\n```javascript\nReact.useEffect(() => {\n    fetch('http://localhost:8000/api/prs')\n        .then(res => res.json())\n        .then(data => {\n            setPrs(data);\n        });\n}, []);\n```\n\nThis hook ensures that the PR data is fetched when the component is first rendered and updates the component's state with the fetched data.\n\n## Conclusion\n\nIn this chapter, we explored the `CodeRoast` frontend, built with React. We learned about components, JSX, state, props, and how they're used to display PR lists, diffs, and analytics. We also took a peek under the hood to see how the frontend interacts with the backend to fetch data.\n\nNow that you have a basic understanding of the frontend, let's move on to the next chapter, where we'll dive into the backend API built with FastAPI: [Chapter 2: FastAPI Backend API](02_fastapi_backend_api.md).\n\n---\n# Chapter 2: FastAPI Backend API\n\nWelcome back to CodeRoast! In [Chapter 1: Frontend Web Application (React)](01_frontend_web_application__react_.md), we built a snazzy frontend that displays pull requests, diffs, and AI insights. But that frontend needs someone to talk to \u2013 someone who can fetch data, run the AI magic, and keep everything organized. That's where our FastAPI backend comes in!\n\n## What is a Backend API, and Why FastAPI?\n\nThink of the backend like the engine in our car analogy. The frontend is the dashboard, showing you information and letting you control things. The backend is the engine, doing all the heavy lifting.\n\nSpecifically, the backend acts as an *API (Application Programming Interface)*. This means it provides a structured way for the frontend (and other applications) to request information and functionality.\n\n**FastAPI** is a modern, high-performance Python web framework for building APIs. We chose FastAPI because it's:\n\n*   **Fast:** It's built for speed, so our app feels responsive.\n*   **Easy to use:** It has a simple and intuitive syntax, making development quicker.\n*   **Automatic Data Validation:** It automatically checks that the data sent to the API is in the correct format, preventing errors.\n*   **Automatic API Documentation:** It generates interactive API documentation automatically, making it easy to understand how to use the API.\n\n## Key Concepts\n\nLet's break down the key concepts behind our FastAPI backend:\n\n1.  **Routes (Endpoints):** These are specific URLs that our backend responds to. For example, `/api/prs` might be a route that returns a list of all pull requests. Think of them as specific \"doors\" to access different functionalities of our backend.\n\n2.  **Requests:** These are messages sent from the frontend (or any other client) to the backend, asking for something. They often include data, like the ID of a pull request the frontend wants to see.\n\n3.  **Responses:** These are messages sent back from the backend to the frontend, containing the requested information or the result of an action. Responses are often in JSON format, which is easy for the frontend to process.\n\n4.  **Data Models:** These are Python classes that define the structure of our data. For example, we might have a `PullRequest` data model that defines the fields for each pull request (like `id`, `title`, `author`, etc.).  Data models help ensure the data is consistent and well-defined.\n\n## The CodeRoast Backend: A Tour\n\nLet's look at how these concepts come together in our `CodeRoast` backend, using code snippets from `main.py`.\n\n### Defining a Route\n\nHere's an example of a route that fetches all pull requests:\n\n```python\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nimport snowflake.connector\nimport os\nfrom dotenv import load_dotenv\nimport pandas as pd\n\nload_dotenv()\n\napp = FastAPI()\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n@app.get(\"/api/prs\")\ndef fetch_all_prs():\n    # Fetch data from Snowflake and return it\n    # For simplicity, let's return a test data\n    test_data = [\n        {\n            \"id\": 1,\n            \"title\": \"Add new feature\",\n            \"author\": \"johndoe\",\n            \"status\": \"Open\",\n            \"created_at\": \"2023-01-01\",\n            \"updated_at\": \"2023-01-02\"\n        },\n        {\n            \"id\": 2,\n            \"title\": \"Fix bug in login\",\n            \"author\": \"janedoe\",\n            \"status\": \"Merged\",\n            \"created_at\": \"2023-01-03\",\n            \"updated_at\": \"2023-01-04\"\n        }\n    ]\n    return test_data\n```\n\nLet's break this down:\n\n*   `@app.get(\"/api/prs\")`: This is a *decorator* that tells FastAPI to create a route at the URL `/api/prs`. The `@app.get` part specifies that this route should handle GET requests (requests for data).\n*   `def fetch_all_prs():`: This defines a function called `fetch_all_prs` that will be executed when someone makes a GET request to `/api/prs`.\n*   `return test_data`: This returns the `test_data` list as a JSON response. FastAPI automatically converts Python dictionaries and lists into JSON.\n\nSo, if you open your web browser and go to `http://localhost:8000/api/prs`, you'll see a JSON response containing the `test_data`. (Note: make sure your backend is running!)\n\n### Handling Parameters\n\nWhat if we want to fetch information about a specific pull request, identified by its ID? We can use a *path parameter* in our route:\n\n```python\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nimport snowflake.connector\nimport os\nfrom dotenv import load_dotenv\nimport pandas as pd\n\nload_dotenv()\n\napp = FastAPI()\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n@app.get(\"/api/prs/{pr_id}\")\ndef fetch_pr_analysis(pr_id: int):\n    # Fetch the analysis data\n    # For simplicity, let's return a test data\n    test_data = {\n        \"pr_id\": pr_id,\n        \"ai_summary\": \"This PR adds a new feature that improves user experience\",\n        \"merge_confidence\": \"High\",\n        \"code_quality\": \"Good code quality. No major issues found.\",\n        \"diff\": \"```\\n+ def new_function():\\n+     return 'new feature'\\n```\",\n        \"doc_string\": \"This PR introduces a new function that returns 'new feature'.\"\n    }\n    return test_data\n```\n\nHere's what's new:\n\n*   `@app.get(\"/api/prs/{pr_id}\")`: This defines a route that includes a path parameter called `pr_id`. The curly braces `{}` indicate a parameter.\n*   `def fetch_pr_analysis(pr_id: int):`:  The `pr_id: int` part of the function definition tells FastAPI that `pr_id` is an integer parameter. FastAPI will automatically validate that the value passed in the URL is actually an integer.\n*   `\"pr_id\": pr_id`: We are using the `pr_id` parameter inside the function to construct the response.\n\nIf you go to `http://localhost:8000/api/prs/123`, you'll see a JSON response containing the `test_data`, but with the `\"pr_id\"` field set to `123`.\n\n### Connecting to Snowflake Database\n\nIn a real application, we wouldn't just return test data. We would fetch the data from a database.  In our case, we use [Snowflake Data Storage](09_snowflake_data_storage.md). Let's look at how we can connect to Snowflake and query the database (simplified):\n\n```python\nimport os\nimport snowflake.connector\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nimport pandas as pd\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\napp = FastAPI()\n\n# Snowflake credentials\nSNOWFLAKE_USER = os.getenv(\"SNOWFLAKE_USER\", \"raghavg332\")\nSNOWFLAKE_PASSWORD = os.getenv(\"SNOWFLAKE_PASSWORD\", \"Qa29Kh4MptfGHEW\")\nSNOWFLAKE_ACCOUNT = os.getenv(\"SNOWFLAKE_ACCOUNT\", \"KXCIVVH-LL27432\")\nSNOWFLAKE_WAREHOUSE = os.getenv(\"SNOWFLAKE_WAREHOUSE\", \"COMPUTE_WH\")\nSNOWFLAKE_DATABASE = os.getenv(\"SNOWFLAKE_DATABASE\", \"PR_DASHBOARD\")\nSNOWFLAKE_SCHEMA = os.getenv(\"SNOWFLAKE_SCHEMA\", \"PUBLIC\")\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\ndef get_conn():\n    \"\"\"Establish connection to Snowflake database\"\"\"\n    try:\n        conn = snowflake.connector.connect(\n            user=SNOWFLAKE_USER,\n            password=SNOWFLAKE_PASSWORD,\n            account=SNOWFLAKE_ACCOUNT,\n            warehouse=SNOWFLAKE_WAREHOUSE,\n            database=SNOWFLAKE_DATABASE,\n            schema=SNOWFLAKE_SCHEMA\n        )\n        return conn\n    except Exception as e:\n        print(f\"Error connecting to Snowflake: {e}\")\n        raise e\n\n@app.get(\"/api/prs/{pr_id}\")\ndef fetch_pr_analysis(pr_id: int):\n    try:\n        conn = get_conn()\n        cursor = conn.cursor(snowflake.connector.DictCursor)\n\n        # Fetch the analysis data\n        cursor.execute(f\"SELECT * FROM pr_analysis WHERE pr_id = {pr_id}\")\n        result = cursor.fetchone()\n\n        if not result:\n            cursor.close()\n            conn.close()\n            raise HTTPException(status_code=404, detail=\"PR analysis not found\")\n\n        cursor.close()\n        conn.close()\n        return result\n    except Exception as e:\n        print(f\"Error fetching PR analysis: {e}\")\n        raise HTTPException(status_code=500, detail=f\"Error: {str(e)}\")\n```\n\nHere's what's happening:\n\n*   `import snowflake.connector`: This imports the Snowflake connector library, which allows us to connect to a Snowflake database.\n*   `SNOWFLAKE_USER = os.getenv(\"SNOWFLAKE_USER\", \"raghavg332\")`: This imports environment variable, which allows us to securely config the snowflake credentials.\n*   `get_conn()`: This function establishes a connection to Snowflake using your credentials. **Important:** Make sure you have Snowflake configured correctly.\n*   `conn = get_conn()`: This calls the `get_conn` function to establish the connection.\n*   `cursor = conn.cursor(snowflake.connector.DictCursor)`: This creates a cursor object, which allows us to execute SQL queries.  The `snowflake.connector.DictCursor` is important here: it tells the connector to return results as Python dictionaries, which are easier to work with.\n*   `cursor.execute(f\"SELECT * FROM pr_analysis WHERE pr_id = {pr_id}\")`: This executes a SQL query to fetch the analysis data for the specified `pr_id`.\n*   `result = cursor.fetchone()`: This fetches the first row of the result set.\n*   `return result`: This returns the result (which is a dictionary) as a JSON response.\n\n## Under the Hood: Fetching PR Data\n\nLet's take a simplified look at what happens when the frontend requests PR data from the backend.\n\n```mermaid\nsequenceDiagram\n    participant FE as Frontend (React App)\n    participant BE as Backend (FastAPI)\n    participant DB as Snowflake Database\n\n    FE->>BE: GET /api/prs/123\n    activate BE\n    BE->>DB: SELECT * FROM pr_analysis WHERE pr_id = 123\n    activate DB\n    DB-->>BE: PR Analysis Data (JSON)\n    deactivate DB\n    BE-->>FE: PR Analysis Data (JSON)\n    deactivate BE\n    FE->>FE: Render PR Details\n```\n\n1.  The **Frontend (React App)** initiates a GET request to the `/api/prs/123` endpoint on the **Backend (FastAPI)**.\n2.  The **Backend (FastAPI)** receives the request and executes a SQL query on the **Snowflake Database** to fetch the analysis data for the PR with ID 123.\n3.  The **Snowflake Database** returns the PR analysis data to the **Backend (FastAPI)** in JSON format.\n4.  The **Backend (FastAPI)** sends the PR analysis data back to the **Frontend (React App)** in JSON format.\n5.  The **Frontend (React App)** receives the data and renders the details of the pull request.\n\nThe actual fetching of data happens inside of the `fetch_pr_analysis` function we saw before, where it:\n\n1. Establishes a connection to the snowflake database.\n2. Constructs and executes a select query.\n3. Formats the data for transmission back to the frontend as a JSON response.\n\n## Conclusion\n\nIn this chapter, we explored the `CodeRoast` backend, built with FastAPI. We learned about routes, requests, responses, data models, and how they're used to create a powerful API. We also saw how the backend interacts with a Snowflake database to fetch data.\n\nNow that you have a basic understanding of the backend, let's move on to the next chapter, where we'll dive into how we receive GitHub Webhooks: [Chapter 3: GitHub Webhook Integration](03_github_webhook_integration.md).\n\n---\n# Chapter 3: GitHub Webhook Integration\n\nIn [Chapter 2: FastAPI Backend API](02_fastapi_backend_api.md), we set up our FastAPI backend to respond to requests. But how does our backend *know* when something important happens on GitHub, like a new pull request being opened? That's where GitHub Webhooks come in!\n\nImagine you're waiting for a package to arrive. Instead of constantly checking the tracking number, you can sign up for notifications that tell you exactly when it's delivered. GitHub Webhooks are like those notifications, but for code!\n\n## What Problem Do Webhooks Solve?\n\nWithout webhooks, our backend would have to constantly ask GitHub, \"Hey, are there any new pull requests? Hey, are there any updates to existing pull requests?\" This is inefficient and wastes resources.\n\nWebhooks solve this problem by letting GitHub *tell* our backend when something interesting happens. This is much more efficient!\n\n**Central Use Case:** When a developer opens a new pull request (PR) on GitHub, we want our `CodeRoast` system to automatically start analyzing that PR. Webhooks make this possible.\n\n## Key Concepts\n\nLet's break down the key concepts of GitHub Webhook Integration:\n\n1.  **Event:** An event is something that happens in a GitHub repository, like opening a pull request, pushing code, or commenting on an issue.\n\n2.  **Webhook:** A webhook is a way for GitHub to send a notification to our backend whenever a specific event occurs. It's like a \"callback\" \u2013 GitHub calls back to our server when something happens.\n\n3.  **Payload:** The payload is the data that GitHub sends to our backend in the webhook notification. It's a JSON object containing information about the event that occurred, like the pull request's details, the commit messages, and the author's information. This is the \"package\" containing all the information.\n\n4.  **Signature:** A signature is a security measure that ensures the webhook notification is actually coming from GitHub and hasn't been tampered with. It's like a digital \"seal\" of authenticity.\n\n## How to Use Webhooks: A Simple Example\n\nLet's imagine a very simple scenario: when a pull request is opened, we want to print a message to our backend's console.\n\nHere's how the process works:\n\n1.  **Configure a Webhook on GitHub:** In your GitHub repository settings, you create a webhook. You tell GitHub:\n    *   The URL of your backend (where to send the notifications).\n    *   Which events to listen for (e.g., \"pull request\").\n    *   A secret (to verify the signature).\n\n2.  **Open a Pull Request:** A developer opens a new pull request on the repository.\n\n3.  **GitHub Sends a Webhook:** GitHub detects the \"pull request opened\" event and sends a webhook notification (a POST request) to our backend's URL.\n\n4.  **Backend Receives the Webhook:** Our backend receives the POST request, verifies the signature to ensure it's from GitHub, and then extracts the pull request information from the payload.\n\n5.  **Backend Processes the Information:** In our simple example, our backend just prints a message. In reality, it would trigger the [Pull Request Analysis Pipeline](04_pull_request_analysis_pipeline.md).\n\n## Code Example: Receiving a Webhook\n\nHere's a simplified code example showing how our FastAPI backend can receive and process a GitHub webhook:\n\n```python\nfrom fastapi import FastAPI, Request, Header\nfrom fastapi.responses import JSONResponse\nimport hmac\nimport hashlib\nimport json\n\napp = FastAPI()\n\nGITHUB_SECRET = \"asdfg\" # Replace with your actual secret\n\ndef verify_signature(payload, signature, secret):\n    mac = hmac.new(secret.encode(), msg=payload, digestmod=hashlib.sha256)\n    expected = f\"sha256={mac.hexdigest()}\"\n    return hmac.compare_digest(expected, signature)\n\n@app.post(\"/webhook\")\nasync def github_webhook(\n    request: Request,\n    x_hub_signature_256: str = Header(None),\n    x_github_event: str = Header(None)\n):\n    body = await request.body()\n\n    # Verify webhook signature\n    if not verify_signature(body, x_hub_signature_256, GITHUB_SECRET):\n        return JSONResponse(content={\"error\": \"Invalid signature\"}, status_code=403)\n\n    payload = json.loads(body)\n\n    if x_github_event == \"pull_request\":\n        action = payload[\"action\"]\n        pr = payload[\"pull_request\"]\n\n        if action in [\"opened\", \"synchronize\"]:\n            print(f\"New PR opened: {pr['title']}\")\n\n    return {\"message\": \"OK\"}\n```\n\nLet's break this down:\n\n*   `@app.post(\"/webhook\")`: This creates a route that listens for POST requests at the `/webhook` endpoint. GitHub will send its webhook notifications to this URL.\n*   `request: Request`: This allows us to access the body of the POST request (the payload).\n*   `x_hub_signature_256: str = Header(None)`: This extracts the `X-Hub-Signature-256` header from the request. This header contains the signature that we need to verify.\n*   `x_github_event: str = Header(None)`: This extracts the `X-GitHub-Event` header, which tells us what type of event triggered the webhook (e.g., \"pull_request\").\n*   `body = await request.body()`: This reads the raw bytes from the request body.\n*   `verify_signature(body, x_hub_signature_256, GITHUB_SECRET)`: This calls a function to verify that the signature is valid (we'll explain this in more detail later).\n*   `payload = json.loads(body)`: This converts the JSON payload into a Python dictionary.\n*   `if x_github_event == \"pull_request\"`: This checks if the event type is \"pull_request\".\n*   `if action in [\"opened\", \"synchronize\"]`: This checks if the pull request action is \"opened\" (a new PR) or \"synchronize\" (a PR was updated).\n*   `print(f\"New PR opened: {pr['title']}\")`: This prints the title of the new pull request to the console.\n\n**Example Input (Payload):**\n\nWhen a pull request is opened, GitHub will send a JSON payload to our `/webhook` endpoint. This payload contains lots of information about the pull request. Here's a simplified example:\n\n```json\n{\n  \"action\": \"opened\",\n  \"pull_request\": {\n    \"url\": \"https://api.github.com/repos/your-org/your-repo/pulls/123\",\n    \"title\": \"Add a new feature\"\n  },\n  \"repository\": {\n    \"full_name\": \"your-org/your-repo\"\n  }\n}\n```\n\n**Example Output:**\n\nWhen our backend receives this payload, it will print the following message to the console:\n\n```\nNew PR opened: Add a new feature\n```\n\nThis example demonstrates the basic flow of receiving and processing a GitHub webhook. In a real application, you would use the data in the payload to trigger more complex actions, like analyzing the code changes in the pull request.\n\n## Under the Hood: How Webhooks Work\n\nLet's take a closer look at how webhooks work internally.\n\nHere's a sequence diagram illustrating the flow:\n\n```mermaid\nsequenceDiagram\n    participant Dev as Developer\n    participant GH as GitHub\n    participant BE as Backend (FastAPI)\n\n    Dev->>GH: Opens/Updates Pull Request\n    activate GH\n    GH->>BE: POST /webhook (with payload)\n    activate BE\n    BE->>BE: Verify Signature\n    alt Signature Valid\n        BE->>BE: Process Payload\n        BE-->>GH: 200 OK\n    else Signature Invalid\n        BE-->>GH: 403 Forbidden\n    end\n    deactivate BE\n    deactivate GH\n```\n\n1.  A **Developer** opens or updates a pull request on **GitHub**.\n\n2.  **GitHub** detects the event and sends a POST request to our **Backend (FastAPI)** at the `/webhook` endpoint. The request includes a JSON payload containing information about the event.\n\n3.  The **Backend (FastAPI)** receives the POST request and immediately verifies the signature to ensure the request is actually coming from GitHub.\n\n4.  If the signature is valid, the **Backend (FastAPI)** processes the payload and performs the appropriate actions (e.g., triggers the [Pull Request Analysis Pipeline](04_pull_request_analysis_pipeline.md)). The Backend sends an HTTP 200 OK to GitHub.\n\n5.  If the signature is invalid, the **Backend (FastAPI)** rejects the request and sends an HTTP 403 Forbidden to GitHub.\n\n### Verifying the Signature\n\nThe `verify_signature` function is crucial for security. Here's how it works:\n\n```python\nimport hmac\nimport hashlib\n\ndef verify_signature(payload, signature, secret):\n    mac = hmac.new(secret.encode(), msg=payload, digestmod=hashlib.sha256)\n    expected = f\"sha256={mac.hexdigest()}\"\n    return hmac.compare_digest(expected, signature)\n```\n\n*   `hmac.new(secret.encode(), msg=payload, digestmod=hashlib.sha256)`: This creates a new HMAC (Hash-based Message Authentication Code) object using the secret key and the payload. It uses the SHA256 algorithm to generate the hash.\n*   `expected = f\"sha256={mac.hexdigest()}\"`: This calculates the expected signature by prefixing \"sha256=\" to the hexadecimal representation of the HMAC hash.\n*   `hmac.compare_digest(expected, signature)`: This securely compares the expected signature with the signature provided in the `X-Hub-Signature-256` header. This function is important to prevent timing attacks.\n\nThe secret key is configured in your GitHub webhook settings and should be stored securely on your backend. Make sure to *never* expose your secret key in your code or commit it to your repository! As you can see in the provided `backend/main.py`, the correct approach is to store the secret in an environment variable.\n\n## Conclusion\n\nIn this chapter, we learned about GitHub Webhook Integration. We understand what webhooks are, why they're useful, and how to receive and process them in our FastAPI backend. We also saw how to verify the signature to ensure the webhook notifications are authentic.\n\nNow that we can receive notifications about pull requests, we can move on to the next chapter, where we'll dive into the [Pull Request Analysis Pipeline](04_pull_request_analysis_pipeline.md), which is responsible for analyzing the code changes in the pull request and generating AI-powered insights.\n\n---\n# Chapter 4: Pull Request Analysis Pipeline\n\nIn [Chapter 3: GitHub Webhook Integration](03_github_webhook_integration.md), we learned how to set up our backend to receive notifications from GitHub whenever a pull request (PR) is opened or updated. Now, what do we *do* with that information?\n\nThat's where the **Pull Request Analysis Pipeline** comes in!\n\nImagine a factory assembly line. Raw materials (code changes) enter, various machines perform operations (analysis, documentation suggestions), and then the finished product (insights) is stored in a database. Our pipeline is very similar: it takes a pull request, analyzes the code changes, generates AI insights, and stores the results.\n\n**Central Use Case:** When a developer opens a new pull request, we want `CodeRoast` to automatically analyze the code, provide feedback, and suggest documentation updates. The Pull Request Analysis Pipeline makes this happen!\n\n## Key Concepts\n\nLet's break down the key concepts behind the Pull Request Analysis Pipeline:\n\n1.  **Input: Pull Request Data:** This is the raw information about the PR, received from GitHub via the webhook. It includes the code diff, the PR description, the author, and more. Think of this as the \"raw materials\" entering our factory.\n\n2.  **Code Parsing:** Before we can analyze the code, we need to understand its structure. This involves parsing the code to identify functions, classes, and other elements. We will talk about it in detail in [Chapter 5: Tree-sitter Based Code Parsing](05_tree_sitter_based_code_parsing.md).\n\n3.  **AI Insights Generation:** This is where the magic happens! We use AI, powered by [Groq API Integration](06_groq_api_integration.md), to analyze the code and generate insights. This might include summarizing the PR, identifying potential issues, suggesting improvements, and estimating merge confidence. It's like adding value to a product in the factory.\n\n4.  **Documentation Updates (RAG):** We use RAG (Retrieval-Augmented Generation) to suggest updates to the project documentation based on the code changes. This ensures that our documentation stays up-to-date. See more details in [Chapter 7: RAG (Retrieval-Augmented Generation) Documentation Updates](07_rag__retrieval_augmented_generation__documentation_updates.md).\n\n5.  **Output: Stored Insights:** The final product of our pipeline is a set of AI-generated insights that are stored in a database (Snowflake in our case [Chapter 9: Snowflake Data Storage](09_snowflake_data_storage.md)). These insights can then be displayed in the frontend and used to improve the code review process.\n\n## How to Use the Pipeline\n\nLet's walk through how to use the pipeline with a simplified example.\n\nImagine we have a function called `process_pull_request` that represents our pipeline. It takes the pull request data as input and returns the AI insights.\n\n```python\ndef process_pull_request(pr_data):\n    # 1. Parse the code changes.  We skip for this simple case.\n    code_diff = pr_data[\"code_diff\"] # Assume code_diff is extracted\n\n    # 2. Generate AI insights. We skip complex logic here\n    ai_summary = \"This PR adds a new feature.\"\n    merge_confidence = \"High\"\n\n    # 3. Store the results. We skip database insert here.\n    insights = {\n        \"ai_summary\": ai_summary,\n        \"merge_confidence\": merge_confidence,\n    }\n\n    return insights\n```\n\nThis simplified code does the following:\n\n1.  Takes `pr_data` which contains information about the PR.\n2.  Simulates extracting the code diff.\n3.  Simulates generating AI insights (in a real application, we'd use the Groq API).\n4.  Returns a dictionary containing the AI insights.\n\n**Example Input:**\n\n```python\npr_data = {\n    \"pr_title\": \"Add new feature\",\n    \"pr_description\": \"This PR adds a new feature to calculate statistics.\",\n    \"code_diff\": \"```diff\\n+def calculate_mean(data):\\n+  return sum(data) / len(data)\\n```\",\n    \"author\": \"johndoe\",\n    \"status\": \"open\"\n}\n```\n\n**Example Output:**\n\n```python\n{\n  \"ai_summary\": \"This PR adds a new feature.\",\n  \"merge_confidence\": \"High\"\n}\n```\n\nIn reality, this function is more complex and involves several steps, including connecting to external services and parsing the code. We will cover it later in this chapter!\n\n## Under the Hood: How the Pipeline Works\n\nLet's take a closer look at how the Pull Request Analysis Pipeline works internally.\n\nHere's a sequence diagram illustrating the flow:\n\n```mermaid\nsequenceDiagram\n    participant GH as GitHub\n    participant BE as Backend (FastAPI)\n    participant CP as Code Parser\n    participant AI as Groq API\n    participant DB as Snowflake Database\n\n    GH->>BE: POST /webhook (PR data)\n    activate BE\n    BE->>CP: Extract code diffs\n    activate CP\n    CP-->>BE: Code diffs (parsed)\n    deactivate CP\n    BE->>AI: Analyze code\n    activate AI\n    AI-->>BE: AI Insights (JSON)\n    deactivate AI\n    BE->>DB: Store Insights\n    activate DB\n    DB-->>BE: OK\n    deactivate DB\n    BE-->>GH: 200 OK\n    deactivate BE\n```\n\n1.  **GitHub** sends a POST request to our **Backend (FastAPI)** at the `/webhook` endpoint, containing the pull request data.\n2.  The **Backend (FastAPI)** receives the data and passes the code to the **Code Parser** to extract the changed functions (detailed in [Chapter 5: Tree-sitter Based Code Parsing](05_tree_sitter_based_code_parsing.md)).\n3.  The **Code Parser** returns the extracted code diffs to the **Backend (FastAPI)**.\n4.  The **Backend (FastAPI)** sends the code diffs and PR details to the **Groq API** for analysis and insight generation.\n5.  The **Groq API** analyzes the code and returns AI insights (summary, merge confidence, etc.) in JSON format to the **Backend (FastAPI)**.\n6.  The **Backend (FastAPI)** stores these insights in the **Snowflake Database**.\n7.  The **Backend (FastAPI)** sends an HTTP 200 OK response to GitHub.\n\n### Diving into the Code\n\nLet's look at a code example of how the `process_pull_request` function is implemented. This code is located in `backend/prReview.py`.\n\n```python\ndef process_pull_request(g, repo_name, pr_number, sender=None):\n    repo = g.get_repo(repo_name)\n    pr = repo.get_pull(pr_number)\n    pr_description = pr.title # title is used as description\n    pr_title = pr.title\n    pr_author = pr.user.login\n    pr_status = pr.state\n    code_diff = get_lines_changed(pr) # Step 1\n    # ... (rest of the function)\n```\n\nThis code does the following:\n\n1.  `repo = g.get_repo(repo_name)`: Gets the repository object from GitHub using the `repo_name`.  `g` is the authenticated Github instance.\n2.  `pr = repo.get_pull(pr_number)`: Gets the pull request object from the repository using the `pr_number`.\n3.  `pr_description = pr.title`: Sets the pull request description.\n4.  `code_diff = get_lines_changed(pr)`: Calls the `get_lines_changed` function to get the code diff. This function uses github api to get the diff and returns a dictionary containing the added, modified, and deleted lines.\n\nLet's look at how we get the diff code:\n\n```python\ndef get_lines_changed(pr):\n    files = pr.get_files()\n    file_changes = {}\n    for file in files:\n        if file.status == \"removed\":\n            continue\n        file_changes[file.filename] = parse_changed_lines(file.patch)\n\n    return file_changes\n\ndef parse_changed_lines(patch: str):\n    added_or_modified_lines = set()\n    deleted_lines = set()\n    for line in patch.splitlines():\n        if line.startswith(\"@@\"):\n            match = re.findall(r\"@@ -(\\d+)(?:,(\\d+))? \\+(\\d+)(?:,(\\d+))? @@\", line)\n            if match:\n                old_start, old_count, new_start, new_count = match[0]\n\n                old_start = int(old_start)\n                old_count = int(old_count or 1)\n                for l in range(old_start, old_start + old_count):\n                    deleted_lines.add(l)\n\n                new_start = int(new_start)\n                new_count = int(new_count or 1)\n                for l in range(new_start, new_start + new_count):\n                    added_or_modified_lines.add(l)\n\n    return {\n        \"added_or_modified_lines\": added_or_modified_lines,\n        \"deleted_lines\": deleted_lines\n    }\n```\n\nThis code does the following:\n\n1.  `files = pr.get_files()`: Get all files that changed in this PR.\n2.  The code loops through each of the `files`:\n    *   `if file.status == \"removed\": continue`: Skip the files that were removed.\n    *   `file_changes[file.filename] = parse_changed_lines(file.patch)`: Call `parse_changed_lines` to parse the changes of each file and then store. The `parse_changed_lines` uses regex matching to find out which lines were added or deleted in this file.\n\nThe following code then uses the data extracted above to call the Groq API:\n\n```python\n    # Get the review data and post a comment\n    review_data = review_and_store_pr(pr_description, file_diff_func, pr_title, pr_author, pr_status)\n    \n    # Post the review as a comment on the PR\n    post_review_comment(g, repo_name, pr_number, review_data)\n```\n\nThe `review_and_store_pr` performs the RAG (Retrieval-Augmented Generation) to update project documentation and calls Groq to generate the PR review and stores the review into the snowflake database. `post_review_comment` then posts the review as a comment on the PR. Details can be found in [Chapter 8: PR Review Comment Posting (GitHub)](08_pr_review_comment_posting__github_.md)\n\n## Conclusion\n\nIn this chapter, we explored the Pull Request Analysis Pipeline. We learned about the key concepts, how to use the pipeline, and how it works internally. We saw how the pipeline takes a pull request as input, analyzes the code changes, generates AI insights, suggests documentation updates, and stores the results in a database.\n\nNow that we understand the overall pipeline, let's dive deeper into one of the key components: parsing the code changes using Tree-sitter in [Chapter 5: Tree-sitter Based Code Parsing](05_tree_sitter_based_code_parsing.md).\n\n---\n# Chapter 5: Tree-sitter Based Code Parsing\n\nIn [Chapter 4: Pull Request Analysis Pipeline](04_pull_request_analysis_pipeline.md), we learned how the `CodeRoast` pipeline analyzes pull requests. One of the key steps in that pipeline is understanding the *structure* of the code changes.  Instead of just treating the code as plain text, we want to know *what* code has changed - what functions have been added, deleted or modified?\n\nThat's where **Tree-sitter** comes in!\n\nImagine trying to understand a sentence without knowing grammar. You might get some of the words, but you'd miss the relationships between them. Tree-sitter is like a grammar checker for code. It helps us understand the structure of the code so we can analyze it more effectively.\n\n**Central Use Case:** When a developer modifies a function in a pull request, we want `CodeRoast` to *only* analyze that function, rather than the entire file. This focused analysis saves time and improves the accuracy of our AI insights.\n\n## Key Concepts\n\nLet's break down the key concepts behind Tree-sitter based code parsing:\n\n1.  **Parsing:** Parsing is the process of taking code (which is just a string of characters) and turning it into a structured representation that a computer can understand. Think of it like taking a sentence and breaking it down into nouns, verbs, and adjectives.\n\n2.  **Abstract Syntax Tree (AST):** The AST is the structured representation of the code that results from parsing. It's like a tree diagram that shows the relationships between the different parts of the code. Each \"node\" in the tree represents a different element of the code, like a function, a variable, or an expression.\n\n3.  **Tree-sitter:** Tree-sitter is a library that makes it easy to parse code and create ASTs. It's fast, reliable, and supports many different programming languages. It's the engine that drives our code parsing.\n\n4.  **Grammar:** A grammar defines the rules for a programming language. Tree-sitter uses grammars to understand the structure of the code. Each language has its own grammar (e.g., Python grammar, JavaScript grammar). These grammars tells Tree-sitter what to expect.\n\n## How to Use Tree-sitter: A Simple Example\n\nLet's imagine we have a simple Python function and we want to use Tree-sitter to extract its name and code.\n\nHere's the function:\n\n```python\ndef greet(name):\n    \"\"\"Greets the person passed in as a parameter.\"\"\"\n    print(f\"Hello, {name}!\")\n```\n\nHere's how we can use Tree-sitter to parse this code and extract the function name and code (simplified):\n\n```python\nfrom tree_sitter import Language, Parser\nimport tree_sitter_python as tspython\n\n# Set up Tree-sitter\nPY_LANGUAGE = Language(tspython.language())\nparser = Parser()\nparser.set_language(PY_LANGUAGE)\n\n# The code to parse\ncode = \"\"\"\ndef greet(name):\n    \\\"\\\"\\\"Greets the person passed in as a parameter.\\\"\\\"\\\"\n    print(f\"Hello, {name}!\")\n\"\"\"\n\n# Parse the code\ntree = parser.parse(bytes(code, \"utf8\"))\n\n# Get the root node of the AST\nroot_node = tree.root_node\n\n# Find the function definition node\nfunction_node = root_node.children[0] # Assume function is the first child\n\n# Extract the function name\nfunction_name_node = function_node.child_by_field_name('name')\nfunction_name = function_name_node.text.decode()\n\n# Extract the function code\nfunction_code = function_node.text.decode()\n\nprint(f\"Function Name: {function_name}\")\nprint(f\"Function Code: {function_code}\")\n```\n\nThis simplified code does the following:\n\n1.  **Sets up Tree-sitter:** It imports the necessary libraries and creates a Tree-sitter parser for Python.  We are using `tree_sitter_python` to obtain the python grammar.\n2.  **Parses the code:** It takes the Python code and uses the parser to create an AST.\n3.  **Navigates the AST:**\n    *   `function_node = root_node.children[0]`: Since the example only consists of the definition of the `greet` function, the function node will be the first child. This code accesses the function node.\n    *   `function_name_node = function_node.child_by_field_name('name')`: This line gets the node that represents the name of the function.\n4.  **Extracts information:** It extracts the function name and code from the AST.\n5.  **Prints the results:** It prints the function name and code to the console.\n\n**Example Output:**\n\n```\nFunction Name: greet\nFunction Code: def greet(name):\n    \"\"\"Greets the person passed in as a parameter.\"\"\"\n    print(f\"Hello, {name}!\")\n```\n\nThis example demonstrates how we can use Tree-sitter to parse code and extract specific elements. In `CodeRoast`, we use this technique to identify the functions that have been changed in a pull request.\n\n## Under the Hood: How Tree-sitter Works\n\nLet's take a closer look at how Tree-sitter works internally.\n\nHere's a sequence diagram illustrating the flow:\n\n```mermaid\nsequenceDiagram\n    participant Code as Code (String)\n    participant Parser as Tree-sitter Parser\n    participant AST as Abstract Syntax Tree\n    participant Extractor as Information Extractor\n\n    Code->>Parser: Parse code\n    activate Parser\n    Parser->>AST: Build AST\n    activate AST\n    AST-->>Parser: AST\n    deactivate AST\n    Parser-->>Extractor: AST\n    deactivate Parser\n    Extractor->>AST: Navigate and Extract\n    activate AST\n    AST-->>Extractor: Function Name and Code\n    deactivate AST\n    Extractor-->>Code: Function Name and Code\n```\n\n1.  The **Code (String)** is passed to the **Tree-sitter Parser**.\n2.  The **Tree-sitter Parser** uses the grammar to build an **Abstract Syntax Tree (AST)**.\n3.  The **AST** is returned to the **Tree-sitter Parser**.\n4.  The **Tree-sitter Parser** passes the **AST** to the **Information Extractor**.\n5.  The **Information Extractor** navigates the **AST** to extract the function name and code.\n6.  The extracted function name and code are returned to the **Code**.\n\n### Diving into the Code\n\nLet's look at the code that extracts functions from the code. This code can be found in `backend/prReview.py`.\n\n```python\ndef extract_functions(code, language_name, changed_lines):\n    parser = get_paraser(language_name)\n    tree = parser.parse(bytes(code, \"utf8\"))\n    root_node = tree.root_node\n\n    functions = []\n\n    def node_within_lines(node):\n        start_line = node.start_point[0] + 1\n        end_line = node.end_point[0] + 1\n        return any([start_line <= line <= end_line for line in changed_lines])\n    \n    def traverse(node):\n        if language_name == \"python\" and node.type == \"function_definition\":\n            if node_within_lines(node):\n                func_name_node = node.child_by_field_name('name')\n                func_name = func_name_node.text.decode()\n                functions.append({'name': func_name, 'code': node.text.decode()})\n        elif language_name == 'javascript' and node.type in ['function_declaration', 'method_definition', 'arrow_function']:\n            if node_within_lines(node):\n                func_name_node = node.child_by_field_name('name')\n                func_name = func_name_node.text.decode()\n                functions.append({'name': func_name, 'code': node.text.decode()})\n        elif language_name == 'cpp' and node.type in ['function_definition']:\n            if node_within_lines(node):\n                func_name_node = node.child_by_field_name('name')\n                func_name = func_name_node.text.decode()\n                functions.append({'name': func_name, 'code': node.text.decode()})\n        for child in node.children:\n            traverse(child)\n\n    traverse(root_node)\n    return functions\n```\n\nThis code does the following:\n\n1.  `parser = get_paraser(language_name)`: Gets the Tree-sitter parser for the given language (Python, JavaScript, C++ are supported).\n2.  `tree = parser.parse(bytes(code, \"utf8\"))`: Parses the code and creates an AST.\n3.  `root_node = tree.root_node`: Gets the root node of the AST.\n4.  The code defines a helper function `node_within_lines` to checks whether a node is within changed lines to skip parsing nodes that are irrelevant.\n5.  `traverse(root_node)`: Calls the `traverse` function to recursively traverse the AST and extract the function names and code.\n    *   It first checks if the language is Python and the node type is `function_definition`.\n    *   If it is, it extracts the function name and code and adds them to the `functions` list.\n    *   Then it traverses the child nodes.\n6.  It does similar logic for Javascript and C++\n7.  It returns the `functions` list.\n\nThe `get_paraser` function loads the grammars for supported languages:\n\n```python\ndef get_paraser(language_name):\n\n    PY_LANGUAGE = Language(tspython.language())\n    JS_LANGUAGE = Language(tsjavascript.language())\n    CPP_LANGUAGE = Language(tscpp.language())\n\n    LANGUAGES = {\n        'python': PY_LANGUAGE,\n        'javascript': JS_LANGUAGE,\n        'cpp': CPP_LANGUAGE,\n    }\n    parser = Parser(LANGUAGES[language_name])\n    return parser\n```\n\nThis code does the following:\n\n1.  `PY_LANGUAGE = Language(tspython.language())`: Loads the Python grammar.\n2.  `JS_LANGUAGE = Language(tsjavascript.language())`: Loads the JavaScript grammar.\n3.  `CPP_LANGUAGE = Language(tscpp.language())`: Loads the C++ grammar.\n4.  `LANGUAGES = { ... }`: Creates a dictionary that maps language names to their corresponding grammars.\n5.  `parser = Parser(LANGUAGES[language_name])`: Creates a Tree-sitter parser for the given language.\n6.  It returns the `parser`.\n\n## Conclusion\n\nIn this chapter, we explored Tree-sitter based code parsing. We learned about the key concepts, how to use Tree-sitter to extract information from code, and how it works internally. We saw how Tree-sitter helps us understand the structure of code so we can analyze it more effectively.\n\nNow that we can parse code changes, let's move on to the next chapter and see how we use AI to generate insights about those changes using [Groq API Integration](06_groq_api_integration.md).\n\n---\n# Chapter 6: Groq API Integration\n\nIn [Chapter 5: Tree-sitter Based Code Parsing](05_tree_sitter_based_code_parsing.md), we learned how to parse the code changes in a pull request. Now, how do we *understand* what those changes mean and generate useful insights?\n\nThat's where the **Groq API Integration** comes in!\n\nThink of the Groq API as a super-smart AI assistant. We send it the code and a description of the pull request, and it sends back a summary of the changes, a code quality assessment, and even suggestions for improving the documentation.\n\n**Central Use Case:** When a developer submits a pull request, we want `CodeRoast` to automatically generate a helpful summary of the changes and identify potential problems in the code. The Groq API integration makes this possible.\n\n## Key Concepts\n\nLet's break down the key concepts behind the Groq API Integration:\n\n1.  **API (Application Programming Interface):**  An API is a way for different software systems to talk to each other.  The Groq API lets us send requests to Groq's AI models and get responses back. It is like ordering from a restaurant; we send the order (request), and the restaurant delivers the food (response).\n\n2.  **Groq API Key:** This is a secret code that identifies us to the Groq API. Think of it like a password that allows us to access the Groq API. You'll need to obtain your own API key from Groq to use their service.\n\n3.  **Prompt:** A prompt is the message we send to the Groq API, telling it what we want it to do.  For example, our prompt might include the code changes, the pull request description, and a request to summarize the changes. It's like writing instructions to your super-smart AI assistant.\n\n4.  **AI Model:** An AI model is a pre-trained algorithm that can perform a specific task, like summarizing text or assessing code quality. Groq offers a variety of AI models that we can use.\n\n5.  **JSON (JavaScript Object Notation):** JSON is a way to format data that's easy for computers to read and write. We send data to the Groq API in JSON format, and it sends data back to us in JSON format.\n\n## How to Use the Groq API: A Simple Example\n\nLet's imagine we want to use the Groq API to summarize a simple code change.\n\nHere's the code change:\n\n```python\ndef add(x, y):\n  \"\"\"Adds two numbers together.\"\"\"\n  return x + y\n```\n\nHere's how we can use the Groq API to summarize this code (simplified):\n\n```python\nimport requests\nimport json\n\n# Your Groq API key\nGROQ_API_KEY = \"YOUR_GROQ_API_KEY\" # Replace with your actual key\n\n# The API endpoint\nGROQ_ENDPOINT = \"https://api.groq.com/openai/v1/chat/completions\"\n\n# The prompt\nprompt = \"\"\"\nSummarize the following Python code:\n\ndef add(x, y):\n  \\\"\\\"\\\"Adds two numbers together.\\\"\\\"\\\"\n  return x + y\n\"\"\"\n\n# Construct the payload\npayload = {\n    \"model\": \"llama3-70b-8192\", # You should check with Groq for the latest model\n    \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n}\n\n# Set the headers\nheaders = {\n    \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\n# Make the request\nresponse = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n\n# Parse the response\nresult = response.json()\n\n# Extract the summary\nsummary = result[\"choices\"][0][\"message\"][\"content\"].strip()\n\n# Print the summary\nprint(summary)\n```\n\nThis simplified code does the following:\n\n1.  **Sets up the API key and endpoint:** It defines the `GROQ_API_KEY` and `GROQ_ENDPOINT` variables.  **Important:** Replace `\"YOUR_GROQ_API_KEY\"` with your actual Groq API key.\n2.  **Constructs the prompt:** It defines the `prompt` variable, which tells the Groq API what we want it to do.\n3.  **Constructs the payload:** It creates a dictionary containing the prompt and the AI model to use.\n    *   `\"model\": \"llama3-70b-8192\"`:  This tells the Groq API to use the \"llama3-70b-8192\" AI model. Please check Groq documentation for the latest models and their API names.\n4.  **Sets the headers:** It defines the `headers` variable, which includes our Groq API key and tells the Groq API that we're sending data in JSON format.\n5.  **Makes the request:** It uses the `requests.post` function to send the prompt to the Groq API.\n6.  **Parses the response:** It uses the `response.json()` function to convert the JSON response from the Groq API into a Python dictionary.\n7.  **Extracts the summary:** It extracts the summary from the response and prints it to the console.\n\n**Example Output:**\n\n```\nThis code defines a function called \"add\" that takes two arguments, x and y, and returns their sum. The docstring indicates that the function adds two numbers together.\n```\n\nThis example demonstrates how we can use the Groq API to summarize code. In `CodeRoast`, we use this technique to generate summaries of pull requests, assess code quality, and suggest documentation updates.\n\n## Under the Hood: How the Groq API Integration Works\n\nLet's take a closer look at how the Groq API Integration works internally.\n\nHere's a sequence diagram illustrating the flow:\n\n```mermaid\nsequenceDiagram\n    participant BE as Backend (FastAPI)\n    participant Groq as Groq API\n\n    BE->>Groq: POST /openai/v1/chat/completions (Prompt + Code)\n    activate Groq\n    Groq->>Groq: AI Model Analysis\n    Groq-->>BE: JSON (Summary, Quality, etc.)\n    deactivate Groq\n    BE->>BE: Process Results\n```\n\n1.  The **Backend (FastAPI)** constructs a prompt containing the code changes and sends a POST request to the **Groq API** at the `/openai/v1/chat/completions` endpoint.\n\n2.  The **Groq API** receives the prompt and uses its AI models to analyze the code.\n\n3.  The **Groq API** returns a JSON response containing the summary, code quality assessment, and documentation suggestions to the **Backend (FastAPI)**.\n\n4.  The **Backend (FastAPI)** processes the results and stores them in the database or displays them in the frontend.\n\n### Diving into the Code\n\nLet's look at the code that calls the Groq API and processes the response. This code can be found in `backend/ai_analyse.py` and `backend/prReview.py`. First, let's look at the `build_full_prompt` function:\n\n```python\ndef build_full_prompt(pr_description: str, code_diff_str: str) -> str:\n    return f\"\"\"\nYou are an AI code reviewer and documentation assistant.\n\nA developer submitted the following pull request. Please do the following:\n1. \ud83d\udd0d Summarize the overall purpose of the PR. Be a little detailed\n2. \u2705 Review the code for:\n   - Syntax issues in Python (Make up some Issue)\n   - Style (PEP8)\n   - Functionality correctness (does it match the description?)\n3. Based on the review, provide a merge confidence score from 1 to 10.\n4. \ud83d\udcdd Extract or suggest updated docstrings for any changed or added functions/classes.\n5. The Code Diff is made up of the old and new version of the code, if any of the field is empty it means, the function is either added or deleted respectively.\n\n## PR Description:\n{pr_description}\n\n## Code Diff:\n{code_diff_str}\n\nRespond ONLY in this JSON format without any additional text:\n\n{{\n  \"ai_summary\": \"...\",\n  \"merge_confidence\": \"1 - 10\",\n  \"code_quality\": {{\n    \"syntax_check\": \"...\",\n    \"style_check\": \"...\",\n    \"functionality_check\": \"...\",\n    \"final_rating\": \"Excellent / Good / Needs Work / Critical Issues\"\n  }}\n}}\n\"\"\"\n```\n\nThis code constructs the prompt that we send to the Groq API. It includes:\n\n*   A description of the role the AI should play (\"AI code reviewer and documentation assistant\").\n*   Instructions on what the AI should do (summarize the PR, review the code, suggest documentation updates).\n*   The pull request description and code diff.\n*   **Important:** It tells the AI to respond in JSON format. This makes it easy for us to parse the response.\n\nThe `review_and_store_pr` function calls the Groq API and process the response.\n\n```python\nimport requests\nimport json\n\ndef review_and_store_pr(pr_description: str, code_diff: dict, pr_title: str, pr_author: str, pr_status: str):\n    code_diff_str = \"\"\n    for file in list(code_diff.keys()):\n        code_diff_str += f\"File: {file}\\n\\n\"\n        diff = code_diff[file]\n\n        for func_name in list(diff.keys()):\n            old_code = diff[func_name]['old_code']\n            new_code = diff[func_name]['new_code']\n            if old_code is None:\n                old_code = \"\"\n            if new_code is None:\n                new_code = \"\"\n            code_diff_str += f\"Function: {func_name}\\n\\nOld Code:\\n{old_code}\\n\\nNew Code:\\n{new_code}\"\n    docstring = update_documentation_with_rag(pr_description, code_diff)\n    pr_id = random.randint(1000, 9999)\n    prompt = build_full_prompt(pr_description, code_diff_str)\n\n    payload = {\n        \"model\": \"llama3-70b-8192\",\n        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n    }\n\n    try:\n        response = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n        response.raise_for_status()\n        result = response.json()\n\n        try:\n            content = result[\"choices\"][0][\"message\"][\"content\"].strip()\n            review_data = json.loads(content)\n\n            # Flatten the code quality object for easier insert\n            code_quality_str = (\n                f\"Syntax: {review_data['code_quality']['syntax_check']}\\n\"\n                f\"Style: {review_data['code_quality']['style_check']}\\n\"\n                f\"Functionality: {review_data['code_quality']['functionality_check']}\\n\"\n                f\"Rating: {review_data['code_quality']['final_rating']}\"\n            )\n            cur.execute(f\"\"\"\n                INSERT INTO PULL_REQUESTS (\n                    ID, TITLE, AUTHOR, STATUS, CREATED_AT, UPDATED_AT\n                )\n                VALUES (%s, %s, %s, %s, %s, %s);\n            \"\"\", (\n                pr_id,\n                pr_title,\n                pr_author,\n                pr_status,\n                datetime.datetime.now(),\n                datetime.datetime.now()\n            ))\n            conn.commit()\n\n            # Insert into Snowflake\n            cur.execute(f\"\"\"\n                INSERT INTO PR_ANALYSIS (\n                    PR_ID, AI_SUMMARY, MERGE_CONFIDENCE, CODE_QUALITY,\n                    PR_DESCRIPTION, DOCSTRINGS, DIFF\n                )\n                VALUES (%s, %s, %s, %s, %s, %s, %s);\n            \"\"\", (\n                pr_id,\n                review_data[\"ai_summary\"],\n                review_data[\"merge_confidence\"],\n                code_quality_str,\n                pr_description,\n                docstring['doc_update'],\n                code_diff_str\n            ))\n\n            conn.commit()\n\n            logging.info(f\"[\u2705] PR #{pr_id} analysis saved to Snowflake.\")\n            print(f\"[\u2705] PR #{pr_id} analysis saved to Snowflake.\")\n            return review_data\n\n        except json.JSONDecodeError:\n            logging.error(\"[\u274c] Groq returned non-JSON output.\")\n            print(\"[\u274c] Groq returned non-JSON output.\")\n            return {\"error\": \"Invalid JSON from Groq\"}\n\n    except Exception as e:\n        logging.exception(\"Groq API or Snowflake insert failed\")\n        return {\"error\": str(e)}\n```\n\nThis code does the following:\n\n1.  First loops through the `code_diff` to construct the `code_diff_str`\n2.  Calls the `build_full_prompt` function to construct the prompt.\n3.  Constructs the `payload`, setting the ai model to be used.\n4.  Calls the `requests.post` function to send the request to the Groq API.\n5.  Parses the JSON response using `response.json()`.\n6.  Extracts the AI insights from the response and stores them in the database ([Chapter 9: Snowflake Data Storage](09_snowflake_data_storage.md)).\n7.  Handles potential errors, like invalid JSON responses.\n\n## Conclusion\n\nIn this chapter, we explored the Groq API Integration. We learned about the key concepts, how to use the Groq API to generate AI insights, and how it works internally. We saw how the Groq API helps us understand code changes and generate useful summaries, code quality assessments, and documentation suggestions.\n\nNow that we can use AI to generate insights, let's move on to the next chapter and see how we use RAG (Retrieval-Augmented Generation) to suggest updates to the project documentation in [Chapter 7: RAG (Retrieval-Augmented Generation) Documentation Updates](07_rag__retrieval_augmented_generation__documentation_updates.md).\n\n\nRelevant Code Snippets:\n--- File: README.md ---\n# \ud83e\udd16 Code Roast : An AI-Powered Pull Request Review System\n\n> An end-to-end intelligent platform for automated code review, documentation updates, and pull request analysis \u2014 powered by LLMs, Tree-sitter, and Snowflake.\n\n---\n\n## \ud83d\ude80 Overview\n\nThis system integrates with GitHub to automatically:\n- Analyze pull requests using AI (via [Groq](https://groq.com))\n- Parse and understand diffs using [Tree-sitter](https://tree-sitter.github.io/)\n- Perform RAG (Retrieval-Augmented Generation) to update project documentation\n- Store all insights in **Snowflake** for analytics\n- Display all PRs, diffs, AI feedback, and doc suggestions via a beautiful **Web App**\n\nWhether you're an engineering team lead, reviewer, or contributor \u2014 you get fast, reliable, and insightful AI-powered review feedback at every step.\n\n---\n\n## \ud83e\udde0 Features\n\n- \u2705 **AI PR Summary**  \n  Instant analysis of pull request purpose, syntax/style/functionality, and merge confidence.\n\n- \ud83d\udcdd **Automatic Docstring Suggestions**  \n  Extract or generate missing/updated docstrings for new or changed functions.\n\n- \ud83d\udcd8 **RAG-based Documentation Updates**  \n  Contextual retrieval from `project_docs.txt` with AI-generated Markdown additions.\n\n- \ud83d\udcc4 **Code Diff Parsing**  \n  Tree-sitter-based code structure diffing to isolate changed functions only.\n\n- \ud83d\udcac **GitHub Comment Posting**  \n  Summaries are posted as comments directly on PRs, no need to open another tool.\n\n- \ud83d\udcca **Snowflake Integration**  \n  All AI-generated insights and PR metadata stored for reporting, audit, and search.\n\n- \ud83c\udf10 **Web App Dashboard**  \n  Browse all PRs, view diffs, AI reviews, and doc updates in a single UI.\n\n---\n\n## \ud83e\uddf1 Tech Stack\n\n| Layer              | Tech Used                                      |\n|--------------------|-----------------------------------------------|\n| **AI Models**       | [Groq API (LLaMA 3)](https://groq.com)        |\n| **Diff Parsing**    | [Tree-sitter](https://tree-sitter.github.io/) |\n| **Backend API**     | [FastAPI](https://fastapi.tiangolo.com)       |\n| **Git Integration** | GitHub App + `PyGithub`                       |\n| **RAG Engine**      | Basic keyword-matching from local docs        |\n| **Database**        | [Snowflake](https://www.snowflake.com)        |\n| **Frontend**        | HTML, JS, CSS (Markdown + Diff Viewers)       |\n\n---\n\n## \u2699\ufe0f How It Works\n\n1. \ud83d\udd14 **GitHub Webhook Trigger**\n   - On PR open/update \u2192 sends payload to FastAPI backend\n\n2. \ud83e\udde0 **AI Review Processing**\n   - Extracts code diffs, changed functions\n   - Groq generates AI summary, confidence rating, and docstrings\n\n3. \ud83d\udcd8 **Documentation Updates**\n   - Local project docs indexed via simple RAG\n   - Groq suggests Markdown additions\n\n4. \ud83d\udcbe **Data Storage**\n   - All results are saved in Snowflake for dashboard & analytics\n\n5. \ud83c\udf10 **Web App UI**\n   - Explore PRs, view diffs, AI reviews, and doc updates\n\n\n--- File: backend/ai_analyse.py ---\nimport requests\nimport json\nimport logging\nimport snowflake.connector\nimport random\nimport datetime\n\nGROQ_API_KEY = \"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\"\nGROQ_ENDPOINT = \"https://api.groq.com/openai/v1/chat/completions\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\n# Snowflake setup\nconn = snowflake.connector.connect(\n    user=\"raghavg332\",\n    password=\"Qa29Kh4MptfGHEW\",\n    account=\"KXCIVVH-LL27432\",\n    warehouse=\"COMPUTE_WH\",\n    database=\"PR_DASHBOARD\",\n    schema=\"PUBLIC\"\n)\ncur = conn.cursor()\n\ndef update_documentation_with_rag(pr_description: str, code_diff: str, docs_path=\"backend/project_docs.txt\"):\n    \"\"\"\n    Performs a simple RAG-based call to Groq to update documentation.\n    \"\"\"\n    # Step 1: Load the existing documentation\n    try:\n        with open(docs_path, \"r\") as f:\n            existing_docs = f.read()\n    except FileNotFoundError:\n        logging.error(f\"[\u274c] Documentation file not found at {docs_path}\")\n        return {\"error\": \"Documentation file not found.\"}\n\n    # Step 2: Very basic RAG - extract top paragraphs that contain overlap\n    paragraphs = existing_docs.split(\"\\n\\n\")\n    keywords = pr_description.lower().split()\n    ranked_paragraphs = sorted(paragraphs, key=lambda para: sum(word in para.lower() for word in keywords), reverse=True)\n    context = \"\\n\\n\".join(ranked_paragraphs[:3])  # Top 3 paragraphs as \"retrieved context\"\n\n    # Step 3: Build the prompt\n    doc_prompt = f\"\"\"\nYou are an expert technical writer and code documentation assistant.\n\nA developer has made the following changes to the codebase:\n\n## PR Description:\n{pr_description}\n\n## Code Diff:\n{code_diff}\n\n## Relevant Existing Documentation:\n{context}\n\nPlease provide additions or updates to the documentation based on the above code change. \nFormat your response in **Markdown**, and only include sections that should be added or updated. \nIf the change doesn't require doc updates, say so.\n\"\"\"\n\n    # Step 4: Call Groq API\n    payload = {\n        \"model\": \"llama3-70b-8192\",\n        \"messages\": [{\"role\": \"user\", \"content\": doc_prompt}]\n    }\n\n    try:\n        response = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n        response.raise_for_status()\n        result = response.json()\n        return {\n            \"doc_update\": result[\"choices\"][0][\"message\"][\"content\"].strip()\n        }\n\n    except Exception as e:\n        logging.exception(\"Groq API call for documentation update failed\")\n        return {\"error\": str(e)}\n\n\ndef build_full_prompt(pr_description: str, code_diff: str) -> str:\n    return f\"\"\"\nYou are an AI code reviewer and documentation assistant.\n\nA developer submitted the following pull request. Please do the following:\n1. \ud83d\udd0d Summarize the overall purpose of the PR.\n2. \ud83e\udde0 Rate the merge confidence as: High / Medium / Low.\n3. \u2705 Review the code for:\n   - Syntax issues\n   - Style (PEP8)\n   - Functionality correctness (does it match the description?)\n4. \ud83d\udcdd Extract or suggest updated docstrings for any changed or added functions/classes.\n\n## PR Description:\n{pr_description}\n\n## Code Diff:\n{code_diff}\n\nRespond ONLY in this JSON format:\n\n{{\n  \"ai_summary\": \"...\",\n  \"merge_confidence\": \"1-10\",\n  \"code_quality\": {{\n    \"syntax_check\": \"...\",\n    \"style_check\": \"...\",\n    \"functionality_check\": \"...\",\n    \"final_rating\": \"Excellent / Good / Needs Work / Critical Issues\"\n  }}\n}}\n\"\"\"\n\ndef review_and_store_pr(pr_description: str, code_diff: str, pr_title: str, pr_author: str, pr_status: str):\n    docstring = update_documentation_with_rag(pr_description, code_diff)\n    print(docstring)\n    print('\\n\\n')\n    pr_id = random.randint(1000, 9999)\n    prompt = build_full_prompt(pr_description, code_diff)\n\n    payload = {\n        \"model\": \"llama3-70b-8192\",\n        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n    }\n\n    try:\n        response = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n        response.raise_for_status()\n        result = response.json()\n\n        try:\n            content = result[\"choices\"][0][\"message\"][\"content\"].strip()\n            review_data = json.loads(content)\n\n            # Flatten the code quality object for easier insert\n            code_quality_str = (\n                f\"Syntax: {review_data['code_quality']['syntax_check']}\\n\"\n                f\"Style: {review_data['code_quality']['style_check']}\\n\"\n                f\"Functionality: {review_data['code_quality']['functionality_check']}\\n\"\n                f\"Rating: {review_data['code_quality']['final_rating']}\"\n            )\n            cur.execute(f\"\"\"\n                INSERT INTO PULL_REQUESTS (\n                    ID, TITLE, AUTHOR, STATUS, CREATED_AT, UPDATED_AT\n                )\n                VALUES (%s, %s, %s, %s, %s, %s);\n            \"\"\", (\n                pr_id,\n                pr_title,\n                pr_author,\n                pr_status,\n                datetime.datetime.now(),\n                datetime.datetime.now()\n            ))\n            conn.commit()\n\n            # Insert into Snowflake\n            cur.execute(f\"\"\"\n                INSERT INTO PR_ANALYSIS (\n                    PR_ID, AI_SUMMARY, MERGE_CONFIDENCE, CODE_QUALITY,\n                    PR_DESCRIPTION, DOCSTRINGS, DIFF\n                )\n                VALUES (%s, %s, %s, %s, %s, %s, %s);\n            \"\"\", (\n                pr_id,\n                review_data[\"ai_summary\"],\n                review_data[\"merge_confidence\"],\n                code_quality_str,\n                pr_description,\n                docstring['doc_update'],\n                code_diff\n            ))\n\n            conn.commit()\n\n            logging.info(f\"[\u2705] PR #{pr_id} analysis saved to Snowflake.\")\n            return review_data\n\n        except json.JSONDecodeError:\n            logging.error(\"[\u274c] Groq returned non-JSON output.\")\n            return {\"error\": \"Invalid JSON from Groq\"}\n\n    except Exception as e:\n        logging.exception(\"Groq API or Snowflake insert failed\")\n        return {\"error\": str(e)}\n\n# # \ud83e\uddea Example test\n# if __name__ == \"__main__\":\n#     code_diff = \"\"\"\n# diff --git a/app/main.py b/app/main.py\n# index a1b2c3d..d4e5f6g 100644\n# --- a/app/main.py\n# +++ b/app/main.py\n# @@ def main():\n# -    print(\"Hello\")\n# +    log_greeting(\"Hello, world!\")\n# +    print(\"Execution completed.\")\n\n# +def log_greeting(message):\n# +    \\\"\\\"\\\"Logs the greeting message to a file\\\"\\\"\\\"\n# +    with open(\"logs.txt\", \"a\") as log_file:\n# +        log_file.write(f\"Greeting logged: {message}\\\\n\")\n# \"\"\"\n#     pr_description = \"Update main function to greet the world.\"\n#     pr_title = \"Update main function\"\n#     pr_author = \"johndoe\"\n#     pr_status = \"OPEN\"\n\n#     result = review_and_store_pr(pr_description, code_diff, pr_title, pr_author, pr_status)\n#     print(json.dumps(result, indent=2))\n\n--- File: backend/prReview.py ---\nimport os, re\nfrom github import Auth\nfrom github import Github, GithubIntegration\nfrom github.GithubException import GithubException\nfrom tree_sitter import Language, Parser\nimport tree_sitter_python as tspython\nimport tree_sitter_javascript as tsjavascript\nimport tree_sitter_cpp as tscpp\nimport requests\nimport json\nimport logging\nimport snowflake.connector\nimport random\nimport datetime\n\n\nGROQ_API_KEY = \"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\"\nGROQ_ENDPOINT = \"https://api.groq.com/openai/v1/chat/completions\"\n\ndef authenticate_github(app_id: int, installation_id, private_key: str):\n    gi = GithubIntegration(integration_id=app_id, private_key=private_key)\n    g = gi.get_github_for_installation(installation_id)\n    return g\n\ndef get_pull_request(g, repo_name: str, pr_number: int):\n    repo = g.get_repo(repo_name)\n    pr = repo.get_pull(pr_number)\n    return pr\n\ndef parse_changed_lines(patch: str):\n    added_or_modified_lines = set()\n    deleted_lines = set()\n    for line in patch.splitlines():\n        if line.startswith(\"@@\"):\n            match = re.findall(r\"@@ -(\\d+)(?:,(\\d+))? \\+(\\d+)(?:,(\\d+))? @@\", line)\n            if match:\n                old_start, old_count, new_start, new_count = match[0]\n\n                old_start = int(old_start)\n                old_count = int(old_count or 1)\n                for l in range(old_start, old_start + old_count):\n                    deleted_lines.add(l)\n\n                new_start = int(new_start)\n                new_count = int(new_count or 1)\n                for l in range(new_start, new_start + new_count):\n                    added_or_modified_lines.add(l)\n\n    return {\n        \"added_or_modified_lines\": added_or_modified_lines,\n        \"deleted_lines\": deleted_lines\n    }\n\ndef get_file_contents(g, repo_name: str, file_path: str, commit_sha: str):\n    repo = g.get_repo(repo_name)\n    try:\n        contents = repo.get_contents(file_path, ref=commit_sha)\n        return contents.decoded_content.decode()\n    except GithubException as e:\n        if e.status == 404:\n            return \"\"\n        else:\n            raise e\n\ndef get_lines_changed(pr):\n    files = pr.get_files()\n    file_changes = {}\n    for file in files:\n        if file.status == \"removed\":\n            continue\n        file_changes[file.filename] = parse_changed_lines(file.patch)\n\n    return file_changes\n\ndef get_paraser(language_name):\n\n    PY_LANGUAGE = Language(tspython.language())\n    JS_LANGUAGE = Language(tsjavascript.language())\n    CPP_LANGUAGE = Language(tscpp.language())\n\n    LANGUAGES = {\n        'python': PY_LANGUAGE,\n        'javascript': JS_LANGUAGE,\n        'cpp': CPP_LANGUAGE,\n    }\n    parser = Parser(LANGUAGES[language_name])\n    return parser\n\ndef extract_functions(code, language_name, changed_lines):\n    parser = get_paraser(language_name)\n    tree = parser.parse(bytes(code, \"utf8\"))\n    root_node = tree.root_node\n\n    functions = []\n\n    def node_within_lines(node):\n        start_line = node.start_point[0] + 1\n        end_line = node.end_point[0] + 1\n        return any([start_line <= line <= end_line for line in changed_lines])\n    \n    def traverse(node):\n        if language_name == \"python\" and node.type == \"function_definition\":\n            if node_within_lines(node):\n                func_name_node = node.child_by_field_name('name')\n                func_name = func_name_node.text.decode()\n                functions.append({'name': func_name, 'code': node.text.decode()})\n        elif language_name == 'javascript' and node.type in ['function_declaration', 'method_definition', 'arrow_function']:\n            if node_within_lines(node):\n                func_name_node = node.child_by_field_name('name')\n                func_name = func_name_node.text.decode()\n                functions.append({'name': func_name, 'code': node.text.decode()})\n        elif language_name == 'cpp' and node.type in ['function_definition']:\n            if node_within_lines(node):\n                func_name_node = node.child_by_field_name('name')\n                func_name = func_name_node.text.decode()\n                functions.append({'name': func_name, 'code': node.text.decode()})\n        for child in node.children:\n            traverse(child)\n\n    traverse(root_node)\n    return functions\n\nheaders = {\n    \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\n# Snowflake setup\nconn = snowflake.connector.connect(\n    user=\"raghavg332\",\n    password=\"Qa29Kh4MptfGHEW\",\n    account=\"KXCIVVH-LL27432\",\n    warehouse=\"COMPUTE_WH\",\n    database=\"PR_DASHBOARD\",\n    schema=\"PUBLIC\"\n)\ncur = conn.cursor()\n\ndef update_documentation_with_rag(pr_description: str, code_diff: str, docs_path=\"backend/project_docs.txt\"):\n    \"\"\"\n    Performs a simple RAG-based call to Groq to update documentation.\n    \"\"\"\n    # Step 1: Load the existing documentation\n    try:\n        with open(docs_path, \"r\") as f:\n            existing_docs = f.read()\n    except FileNotFoundError:\n        logging.error(f\"[\u274c] Documentation file not found at {docs_path}\")\n        return {\"error\": \"Documentation file not found.\"}\n\n    # Step 2: Very basic RAG - extract top paragraphs that contain overlap\n    paragraphs = existing_docs.split(\"\\n\\n\")\n    keywords = pr_description.lower().split()\n    ranked_paragraphs = sorted(paragraphs, key=lambda para: sum(word in para.lower() for word in keywords), reverse=True)\n    context = \"\\n\\n\".join(ranked_paragraphs[:3])  # Top 3 paragraphs as \"retrieved context\"\n\n    # Step 3: Build the prompt\n    doc_prompt = f\"\"\"\nYou are an expert technical writer and code documentation assistant.\n\nA developer has made the following changes to the codebase:\n\n## PR Description:\n{pr_description}\n\n## Code Diff:\n{code_diff}\n\n## Relevant Existing Documentation:\n{context}\n\nPlease provide additions or updates to the documentation based on the above code change. \nFormat your response in **Markdown**, and only include sections that should be added or updated. \nIf the change doesn't require doc updates, say so.\n\"\"\"\n\n    # Step 4: Call Groq API\n    payload = {\n        \"model\": \"llama3-70b-8192\",\n        \"messages\": [{\"role\": \"user\", \"content\": doc_prompt}]\n    }\n\n    try:\n        response = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n        response.raise_for_status()\n        result = response.json()\n        return {\n            \"doc_update\": result[\"choices\"][0][\"message\"][\"content\"].strip()\n        }\n\n    except Exception as e:\n        logging.exception(\"Groq API call for documentation update failed\")\n        return {\"error\": str(e)}\n\n\ndef build_full_prompt(pr_description: str, code_diff_str: str) -> str:\n    return f\"\"\"\nYou are an AI code reviewer and documentation assistant.\n\nA developer submitted the following pull request. Please do the following:\n1. \ud83d\udd0d Summarize the overall purpose of the PR. Be a little detailed\n2. \u2705 Review the code for:\n   - Syntax issues in Python (Make up some Issue)\n   - Style (PEP8)\n   - Functionality correctness (does it match the description?)\n3. Based on the review, provide a merge confidence score from 1 to 10.\n4. \ud83d\udcdd Extract or suggest updated docstrings for any changed or added functions/classes.\n5. The Code Diff is made up of the old and new version of the code, if any of the field is empty it means, the function is either added or deleted respectively.\n\n## PR Description:\n{pr_description}\n\n## Code Diff:\n{code_diff_str}\n\nRespond ONLY in this JSON format without any additional text:\n\n{{\n  \"ai_summary\": \"...\",\n  \"merge_confidence\": \"1 - 10\",\n  \"code_quality\": {{\n    \"syntax_check\": \"...\",\n    \"style_check\": \"...\",\n    \"functionality_check\": \"...\",\n    \"final_rating\": \"Excellent / Good / Needs Work / Critical Issues\"\n  }}\n}}\n\"\"\"\n\ndef review_and_store_pr(pr_description: str, code_diff: dict, pr_title: str, pr_author: str, pr_status: str):\n    code_diff_str = \"\"\n    for file in list(code_diff.keys()):\n        code_diff_str += f\"File: {file}\\n\\n\"\n        diff = code_diff[file]\n\n        for func_name in list(diff.keys()):\n            old_code = diff[func_name]['old_code']\n            new_code = diff[func_name]['new_code']\n            if old_code is None:\n                old_code = \"\"\n            if new_code is None:\n                new_code = \"\"\n            code_diff_str += f\"Function: {func_name}\\n\\nOld Code:\\n{old_code}\\n\\nNew Code:\\n{new_code}\"\n    docstring = update_documentation_with_rag(pr_description, code_diff)\n    pr_id = random.randint(1000, 9999)\n    prompt = build_full_prompt(pr_description, code_diff_str)\n\n    payload = {\n        \"model\": \"llama3-70b-8192\",\n        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n    }\n\n    try:\n        response = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n        response.raise_for_status()\n        result = response.json()\n\n        try:\n            content = result[\"choices\"][0][\"message\"][\"content\"].strip()\n            review_data = json.loads(content)\n\n            # Flatten the code quality object for easier insert\n            code_quality_str = (\n                f\"Syntax: {review_data['code_quality']['syntax_check']}\\n\"\n                f\"Style: {review_data['code_quality']['style_check']}\\n\"\n                f\"Functionality: {review_data['code_quality']['functionality_check']}\\n\"\n                f\"Rating: {review_data['code_quality']['final_rating']}\"\n            )\n            cur.execute(f\"\"\"\n                INSERT INTO PULL_REQUESTS (\n                    ID, TITLE, AUTHOR, STATUS, CREATED_AT, UPDATED_AT\n                )\n                VALUES (%s, %s, %s, %s, %s, %s);\n            \"\"\", (\n                pr_id,\n                pr_title,\n                pr_author,\n                pr_status,\n                datetime.datetime.now(),\n                datetime.datetime.now()\n            ))\n            conn.commit()\n\n            # Insert into Snowflake\n            cur.execute(f\"\"\"\n                INSERT INTO PR_ANALYSIS (\n                    PR_ID, AI_SUMMARY, MERGE_CONFIDENCE, CODE_QUALITY,\n                    PR_DESCRIPTION, DOCSTRINGS, DIFF\n                )\n                VALUES (%s, %s, %s, %s, %s, %s, %s);\n            \"\"\", (\n                pr_id,\n                review_data[\"ai_summary\"],\n                review_data[\"merge_confidence\"],\n                code_quality_str,\n                pr_description,\n                docstring['doc_update'],\n                code_diff_str\n            ))\n\n            conn.commit()\n\n            logging.info(f\"[\u2705] PR #{pr_id} analysis saved to Snowflake.\")\n            print(f\"[\u2705] PR #{pr_id} analysis saved to Snowflake.\")\n            return review_data\n\n        except json.JSONDecodeError:\n            logging.error(\"[\u274c] Groq returned non-JSON output.\")\n            print(\"[\u274c] Groq returned non-JSON output.\")\n            return {\"error\": \"Invalid JSON from Groq\"}\n\n    except Exception as e:\n        logging.exception(\"Groq API or Snowflake insert failed\")\n        return {\"error\": str(e)}\n\ndef process_pull_request(g, repo_name, pr_number, sender=None):\n    repo = g.get_repo(repo_name)\n    pr = repo.get_pull(pr_number)\n    pr_description = pr.title\n    pr_title = pr.title\n    pr_author = pr.user.login\n    pr_status = pr.state\n    code_diff = get_lines_changed(pr)\n    file_diff_func = {}\n    for file in list(code_diff.keys()):\n        diff_func = {}\n        new_code = get_file_contents(g, repo_name, file, pr.head.sha)\n        old_code = get_file_contents(g, repo_name, file, pr.base.sha)\n        added_or_modified_functions_after = extract_functions(new_code, \"python\", code_diff[file]['added_or_modified_lines'])\n        deleted_functions_before = extract_functions(old_code, \"python\", code_diff[file]['deleted_lines'])\n        old_functions_all = extract_functions(old_code, \"python\", set(range(1, len(old_code.split(\"\\n\")))))\n        new_functions_all = extract_functions(new_code, \"python\", set(range(1, len(new_code.split(\"\\n\")))))\n        for func in added_or_modified_functions_after:\n            matched_old_function = next((f for f in old_functions_all if f['name'] ==  func['name']), None)\n            diff_func[func['name']] = {\n                \"new_code\": func['code'],\n                \"old_code\": matched_old_function['code'] if matched_old_function else None\n            }\n        \n        for func in deleted_functions_before:\n            matched_new_function = next((f for f in new_functions_all if f['name'] ==  func['name']), None)\n            diff_func[func['name']] = {\n                \"new_code\": matched_new_function['code'] if matched_new_function else None,\n                \"old_code\": func['code']\n            }\n        \n        file_diff_func[file] = diff_func\n    \n    # Get the review data and post a comment\n    review_data = review_and_store_pr(pr_description, file_diff_func, pr_title, pr_author, pr_status)\n    \n    # Post the review as a comment on the PR\n    post_review_comment(g, repo_name, pr_number, review_data)\n\n\ndef post_review_comment(g, repo_name, pr_number, review_data):\n    \"\"\"\n    Posts the AI review as a comment on the pull request.\n    \n    Args:\n        g: Authenticated GitHub instance\n        repo_name: Repository name (owner/repo)\n        pr_number: Pull request number\n        review_data: The AI review data dictionary\n    \"\"\"\n    try:\n        # Get the repo and PR objects\n        repo = g.get_repo(repo_name)\n        pr = repo.get_pull(pr_number)\n        \n        # Format the comment body\n        comment_body = f\"\"\"\n## \ud83e\udd16 AI Code Review\n\n### Summary\n{review_data.get('ai_summary', 'No summary available')}\n\n### Merge Confidence: **{review_data.get('merge_confidence', 'Unknown')}**\n\n### Code Quality Review\n- **Syntax:** {review_data.get('code_quality', {}).get('syntax_check', 'Not analyzed')}\n- **Style:** {review_data.get('code_quality', {}).get('style_check', 'Not analyzed')}\n- **Functionality:** {review_data.get('code_quality', {}).get('functionality_check', 'Not analyzed')}\n- **Overall Rating:** {review_data.get('code_quality', {}).get('final_rating', 'Not rated')}\n\n---\n*This review was automatically generated by AI.*\n\"\"\"\n        \n        # Create the comment on the PR\n        pr.create_issue_comment(comment_body)\n        logging.info(f\"[\u2705] Posted AI review comment on PR #{pr_number} in {repo_name}\")\n        print(f\"[\u2705] Posted AI review comment on PR #{pr_number} in {repo_name}\")\n        return True\n    except Exception as e:\n        logging.exception(f\"[\u274c] Failed to post comment on PR #{pr_number}: {str(e)}\")\n        print(f\"[\u274c] Failed to post comment on PR #{pr_number}: {str(e)}\")\n        return False\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 7: RAG (Retrieval-Augmented Generation) Documentation Updates`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "# Chapter 7: RAG (Retrieval-Augmented Generation) Documentation Updates\n\nIn [Chapter 6: Groq API Integration](06_groq_api_integration.md), we learned how to use the Groq API to generate insights about code changes. But what about the *documentation*? How do we keep it up-to-date when code changes are made?\n\nThat's where **RAG (Retrieval-Augmented Generation) Documentation Updates** comes in!\n\nImagine you're reading a project's documentation, and it's completely out of sync with the current code. Frustrating, right? RAG helps solve this. It's like having a helpful assistant who automatically finds the relevant sections in your existing documentation and suggests updates based on new code changes.\n\n**Central Use Case:** When a developer adds a new function, we want `CodeRoast` to automatically find the relevant part of the project documentation and suggest adding a description of the new function. This keeps our documentation fresh and useful!\n\n## Key Concepts\n\nLet's break down the key concepts behind RAG Documentation Updates:\n\n1.  **Retrieval:** This is the process of finding the relevant parts of the existing documentation that are related to the code changes. Think of it like searching a library for books that are related to a specific topic. In CodeRoast, we load a `project_docs.txt` and then retrieve the most relevant parts.\n\n2.  **Augmentation:** This is the process of combining the retrieved documentation with the code changes to create a prompt for the AI model. It's like giving the AI model the context it needs to understand the code changes and suggest appropriate documentation updates.\n\n3.  **Generation:** This is the process of using the AI model to generate the suggested documentation updates. It's like asking the AI model to write a new section for the documentation based on the code changes and the existing documentation. We call the Groq API to do the actual writing.\n\n4.  **Markdown:** Markdown is a simple way to format text. It's used to write the project documentation. It's like using a simple set of rules to make the documentation look nice.\n\n## How to Use RAG: A Simple Example\n\nLet's imagine we have a project with a simple `project_docs.txt` file:\n\n```\n# My Project Documentation\n\nThis project is all about doing cool stuff.\n\n## Functions\n\n### add(x, y)\nAdds two numbers together.\n```\n\nAnd we add a new function:\n\n```python\ndef subtract(x, y):\n  \"\"\"Subtracts two numbers.\"\"\"\n  return x - y\n```\n\nHere's how we can use RAG to suggest adding documentation for this new function (simplified):\n\n```python\nimport os\nimport requests\nimport json\n\n# Your Groq API key\nGROQ_API_KEY = \"YOUR_GROQ_API_KEY\" # Replace with your actual key\nGROQ_ENDPOINT = \"https://api.groq.com/openai/v1/chat/completions\"\n\n# The existing documentation\nexisting_docs = \"\"\"\n# My Project Documentation\n\nThis project is all about doing cool stuff.\n\n## Functions\n\n### add(x, y)\nAdds two numbers together.\n\"\"\"\n\n# The new function\nnew_function = \"\"\"\ndef subtract(x, y):\n  \\\"\\\"\\\"Subtracts two numbers.\\\"\\\"\\\"\n  return x - y\n\"\"\"\n\n# The prompt\nprompt = f\"\"\"\nYou are an expert technical writer. Update the following documentation to include this new function:\n\n{new_function}\n\nHere is the existing documentation:\n\n{existing_docs}\n\nRespond in Markdown format.\n\"\"\"\n\n# Construct the payload\npayload = {\n    \"model\": \"llama3-70b-8192\", # You should check with Groq for the latest model\n    \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n}\n\n# Set the headers\nheaders = {\n    \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\n# Make the request\nresponse = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n\n# Parse the response\nresult = response.json()\n\n# Extract the summary\ndocumentation_update = result[\"choices\"][0][\"message\"][\"content\"].strip()\n\n# Print the summary\nprint(documentation_update)\n```\n\nThis simplified code does the following:\n\n1.  **Sets up the API key and endpoint:** It defines the `GROQ_API_KEY` and `GROQ_ENDPOINT` variables.  **Important:** Replace `\"YOUR_GROQ_API_KEY\"` with your actual Groq API key.\n2.  **Defines the existing documentation and the new function:** It defines the `existing_docs` and `new_function` variables.\n3.  **Constructs the prompt:** It defines the `prompt` variable, which tells the Groq API what we want it to do.\n4.  **Calls the Groq API:** It makes a request to the Groq API to generate the documentation update.\n5.  **Prints the documentation update:** It prints the documentation update to the console.\n\n**Example Output:**\n\n```markdown\n### subtract(x, y)\nSubtracts two numbers.\n```\n\nThis output suggests adding a new section to the `project_docs.txt` file for the `subtract` function.\n\n## Under the Hood: How RAG Works\n\nLet's take a closer look at how RAG works internally.\n\nHere's a sequence diagram illustrating the flow:\n\n```mermaid\nsequenceDiagram\n    participant BE as Backend (FastAPI)\n    participant Docs as Existing Documentation\n    participant Code as Code Changes\n    participant Groq as Groq API\n\n    BE->>Docs: Retrieve Relevant Sections\n    BE->>Code: Augment with Code Changes\n    BE->>Groq: POST /openai/v1/chat/completions (Prompt + Context)\n    activate Groq\n    Groq->>Groq: AI Model Generation\n    Groq-->>BE: Markdown (Documentation Update)\n    deactivate Groq\n    BE->>BE: Process Results\n```\n\n1.  The **Backend (FastAPI)** retrieves the relevant sections from the **Existing Documentation** (`project_docs.txt`). In our basic example, we just load the whole document.\n2.  The **Backend (FastAPI)** augments the retrieved sections with the **Code Changes** to create a prompt.\n3.  The **Backend (FastAPI)** sends a POST request to the **Groq API** with the prompt.\n4.  The **Groq API** uses its AI models to generate the documentation update.\n5.  The **Groq API** returns the documentation update in Markdown format to the **Backend (FastAPI)**.\n6.  The **Backend (FastAPI)** processes the results and stores them in the database or displays them in the frontend.\n\n### Diving into the Code\n\nLet's look at the code that implements the RAG documentation update. This code can be found in `backend/ai_analyse.py` and `backend/prReview.py` inside the `update_documentation_with_rag` function.\n\n```python\ndef update_documentation_with_rag(pr_description: str, code_diff: str, docs_path=\"backend/project_docs.txt\"):\n    \"\"\"\n    Performs a simple RAG-based call to Groq to update documentation.\n    \"\"\"\n    # Step 1: Load the existing documentation\n    try:\n        with open(docs_path, \"r\") as f:\n            existing_docs = f.read()\n    except FileNotFoundError:\n        logging.error(f\"[\u274c] Documentation file not found at {docs_path}\")\n        return {\"error\": \"Documentation file not found.\"}\n\n    # Step 2: Very basic RAG - extract top paragraphs that contain overlap\n    paragraphs = existing_docs.split(\"\\n\\n\")\n    keywords = pr_description.lower().split()\n    ranked_paragraphs = sorted(paragraphs, key=lambda para: sum(word in para.lower() for word in keywords), reverse=True)\n    context = \"\\n\\n\".join(ranked_paragraphs[:3])  # Top 3 paragraphs as \"retrieved context\"\n\n    # Step 3: Build the prompt\n    doc_prompt = f\"\"\"\nYou are an expert technical writer and code documentation assistant.\n\nA developer has made the following changes to the codebase:\n\n## PR Description:\n{pr_description}\n\n## Code Diff:\n{code_diff}\n\n## Relevant Existing Documentation:\n{context}\n\nPlease provide additions or updates to the documentation based on the above code change. \nFormat your response in **Markdown**, and only include sections that should be added or updated. \nIf the change doesn't require doc updates, say so.\n\"\"\"\n\n    # Step 4: Call Groq API\n    payload = {\n        \"model\": \"llama3-70b-8192\",\n        \"messages\": [{\"role\": \"user\", \"content\": doc_prompt}]\n    }\n\n    try:\n        # The Groq API code from before\n        response = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n        response.raise_for_status()\n        result = response.json()\n        return {\n            \"doc_update\": result[\"choices\"][0][\"message\"][\"content\"].strip()\n        }\n\n    except Exception as e:\n        logging.exception(\"Groq API call for documentation update failed\")\n        return {\"error\": str(e)}\n```\n\nThis code does the following:\n\n1.  **Loads the existing documentation:**\n    *   `with open(docs_path, \"r\") as f: existing_docs = f.read()`: This code reads the contents of the `project_docs.txt` file into the `existing_docs` variable.  It also has error handling in case the documentation is not found.\n2.  **Retrieves relevant sections:**\n    *   It splits the documentation into paragraphs and ranks them based on how many keywords from the pull request description they contain.\n    *   This is a simplified approach to RAG. More sophisticated techniques could involve vector embeddings and similarity search.\n    *   `context = \"\\n\\n\".join(ranked_paragraphs[:3])`: This joins the top 3 paragraphs to get the context.\n3.  **Builds the prompt:**\n    *   It constructs a prompt that includes the pull request description, the code diff, and the relevant sections from the existing documentation.\n    *   It tells the AI model to provide additions or updates to the documentation in Markdown format.\n4.  **Calls the Groq API:**\n    *   This part is the same as in the [Groq API Integration](06_groq_api_integration.md) chapter.  It sends the prompt to the Groq API and gets the documentation update.\n\n## Conclusion\n\nIn this chapter, we explored RAG (Retrieval-Augmented Generation) Documentation Updates. We learned about the key concepts, how to use RAG to suggest documentation updates, and how it works internally. We saw how RAG helps keep our project documentation up-to-date with the latest code changes.\n\nNow that we can generate documentation updates, let's move on to the next chapter and see how we post those updates (and other AI insights) as comments on GitHub pull requests in [Chapter 8: PR Review Comment Posting (GitHub)](08_pr_review_comment_posting__github_.md).\n", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `CodeRoast` about the concept: \"PR Review Comment Posting (GitHub)\". This is Chapter 8.\n\nConcept Details:\n- Description:\nOnce the AI analysis is complete, this component posts the generated summary as a comment directly on the pull request in GitHub. It's like an automated messenger that delivers the AI's review directly to the developer where they are already working.\n\n\nComplete Tutorial Structure:\n1. [Frontend Web Application (React)](01_frontend_web_application__react_.md)\n2. [FastAPI Backend API](02_fastapi_backend_api.md)\n3. [GitHub Webhook Integration](03_github_webhook_integration.md)\n4. [Pull Request Analysis Pipeline](04_pull_request_analysis_pipeline.md)\n5. [Tree-sitter Based Code Parsing](05_tree_sitter_based_code_parsing.md)\n6. [Groq API Integration](06_groq_api_integration.md)\n7. [RAG (Retrieval-Augmented Generation) Documentation Updates](07_rag__retrieval_augmented_generation__documentation_updates.md)\n8. [PR Review Comment Posting (GitHub)](08_pr_review_comment_posting__github_.md)\n9. [Snowflake Data Storage](09_snowflake_data_storage.md)\n\nContext from previous chapters (summary):\n# Chapter 1: Frontend Web Application (React)\n\nWelcome to CodeRoast! In this chapter, we'll be diving into the heart of the user experience: the frontend web application built with React. Imagine you're a code reviewer, and you want to quickly see all the Pull Requests (PRs) needing your attention, along with AI-powered feedback. Our React frontend makes that happen!\n\nThis chapter will guide you through understanding how this frontend works and how it displays all the amazing insights generated by the rest of the CodeRoast system.\n\n## What is a Frontend, and Why React?\n\nThink of a frontend like the dashboard of a car. You see the speedometer, fuel gauge, and warning lights, but you don't need to know *how* the engine works to drive. Similarly, our frontend lets you interact with CodeRoast without worrying about the complex AI and analysis happening behind the scenes.\n\nReact is a popular JavaScript library for building user interfaces. It helps us create dynamic and interactive web pages. We chose React because it's efficient, organized, and makes it easier to manage complex user interfaces like our PR dashboard.\n\n## Key Concepts\n\nLet's break down the key concepts you'll encounter in our React frontend:\n\n1.  **Components:** React is all about components. Think of them as building blocks for your UI. Each component is responsible for rendering a specific part of the page, like a PR list, a diff view, or an analytics chart.\n\n2.  **JSX:** React uses JSX, which looks like HTML but is actually JavaScript. It allows us to write UI elements directly in our JavaScript code, making it more readable and maintainable.\n\n3.  **State:** State is data that can change over time, and when it changes, React automatically updates the UI. For example, the selected PR, the filter applied to the PR list, or the current theme (light/dark) are all parts of the component's state.\n\n4.  **Props:** Props (short for properties) are how we pass data from a parent component to a child component. Imagine passing a message from one React component to another.\n\n## The CodeRoast Frontend: A Tour\n\nLet's look at how these concepts come together in our `CodeRoast` frontend, using code snippets from `frontend/app.js`.\n\n### Displaying a List of Pull Requests\n\nFirst, let's see how we display the list of PRs. We fetch the PR data from our backend API (more on that in [Chapter 2: FastAPI Backend API](02_fastapi_backend_api.md)) and store it in the component's state.\n\n```javascript\nconst [prs, setPrs] = React.useState([]);\n\nReact.useEffect(() => {\n    fetch('http://localhost:8000/api/prs')\n        .then(res => res.json())\n        .then(data => {\n            setPrs(data);\n        });\n}, []);\n```\n\nThis code does the following:\n\n*   `const [prs, setPrs] = React.useState([]);`: This line initializes a state variable called `prs` to an empty array. `prs` will hold our pull request data. `setPrs` is a function used to update the `prs` data, and React will automatically refresh the user interface whenever you use `setPrs`.\n\n*   `React.useEffect(() => { ... }, []);`: This is a React Hook that runs code when the component is first rendered. The empty array `[]` as the second argument means this effect will only run once when the component mounts.\n\n*   `fetch('http://localhost:8000/api/prs')`: This line makes a request to our backend API to get the list of PRs.\n\n*   `.then(res => res.json())`: This converts the response from the API into a JavaScript object (JSON).\n\n*   `.then(data => { setPrs(data); })`: This updates the `prs` state with the data we received from the API.\n\nNext, we map over the `prs` array to create a list of PR items:\n\n```javascript\nconst prItems = filteredPRs.map(pr => \n    e('div', { \n        key: pr.id, \n        className: selectedPR && selectedPR.id === pr.id ? 'pr-item selected' : 'pr-item', \n        onClick: () => selectPR(pr) \n    }, [\n        e('div', {key: 'pr-header', className: 'pr-header'}, [\n            e('strong', {key: 'id'}, `#${pr.id}`),\n            e('span', {\n                key: 'status',\n                className: `pr-status status-${pr.status}`\n            }, pr.status)\n        ]),\n        e('div', {key: 'pr-title', className: 'pr-title'}, pr.title),\n        e('div', {key: 'pr-meta', className: 'pr-meta'}, [\n            e('span', {key: 'author'}, [\n                e('img', {key: 'avatar', className: 'avatar', src: `https://ui-avatars.com/api/?name=${pr.author}&size=24`, alt: pr.author}),\n                pr.author\n            ]),\n            e('span', {key: 'date', className: 'date'}, 'Updated: ' + (pr.updated_at || 'N/A'))\n        ])\n    ])\n);\n```\n\nThis code takes each `pr` from the `filteredPRs` array and:\n\n*   `e('div', { ... }, [ ... ])`: Creates a `div` element (and nested elements) for each PR, setting its `key`, `className`, and `onClick` properties. The `key` prop is important for React to efficiently update the list. `e` is `React.createElement`, a function that creates React elements. It is used because JSX must be transformed at compile time.\n\n*   `className`: Sets the CSS class for styling.\n\n*   `onClick`: Defines a function that will be called when the PR item is clicked.\n\n*   Within the div, `e(...)` is used to create other html elements to display the PR's `id`, `status`, `title`, `author` and `updated_at`.\n\n### Displaying the Diff\n\nOne of the most important features of `CodeRoast` is displaying the code diff. The `renderDiffSection` function takes the diff string and formats it for display:\n\n```javascript\nfunction renderDiffSection(diff) {\n    const diffLines = parseDiff(diff);\n\n    return React.createElement(\n        'pre',\n        { className: 'diff-content' },\n        diffLines.map((line, index) => {\n            const className =\n                line.type === 'old' ? 'diff-old-code diff-code-line' :\n                line.type === 'new' ? 'diff-new-code diff-code-line' :\n                'diff-context-line';\n\n            return React.createElement('div', { key: index, className }, line.content);\n        })\n    );\n}\n```\n\nHere's what's happening:\n\n*   `const diffLines = parseDiff(diff);`: It calls the `parseDiff` function to split the diff into individual lines and determine their type (old code, new code, or context). See below for details on `parseDiff`.\n\n*   The rest of the function constructs a `<pre>` element which is used to display preformatted text.  Within the `<pre>` tag, we loop through the lines in `diffLines` and create a `div` tag for each line. The `className` will style the background of each line in green, red, or default color, depending on the line type.\n\nThe `parseDiff` function itself is responsible for taking the raw diff output and turning it into an array of objects, each representing a line in the diff:\n\n```javascript\nfunction parseDiff(diff) {\n    const lines = diff.split('\\n');\n    const parsedLines = [];\n\n    let section = null;\n\n    for (let i = 0; i < lines.length; i++) {\n        const line = lines[i];\n\n        if (line.startsWith('File:') || line.startsWith('Function:')) {\n            parsedLines.push({ type: 'context', content: line });\n            section = null;\n        } else if (line.trim() === 'Old Code:') {\n            parsedLines.push({ type: 'context', content: 'Old Code:' });\n            section = 'old';\n\n            // Handle empty old code\n            if (lines[i + 1]?.trim() === 'New Code:') {\n                parsedLines.push({ type: 'old', content: '<NO PREVIOUS CODE>' });\n            }\n        } else if (line.trim() === 'New Code:') {\n            parsedLines.push({ type: 'context', content: 'New Code:' });\n            section = 'new';\n        } else if (section === 'old' || section === 'new') {\n            parsedLines.push({ type: section, content: line });\n        } else {\n            parsedLines.push({ type: 'context', content: line });\n        }\n    }\n\n    return parsedLines;\n}\n```\n\nThis function:\n\n*   Splits the diff string into lines.\n*   Iterates through each line, determining its type based on the content.\n*   Adds each line to the `parsedLines` array with a `type` property indicating whether it's \"old\", \"new\", or \"context\".\n*   The `type` property is later used to style each line in the diff.\n\n### Displaying Analytics\n\nFinally, the frontend also displays some helpful analytics about the PRs. This is handled by the `Analytics` component.\n\n```javascript\n// Analytics component to visualize PR data\nfunction Analytics({ prs }) {\n    // Count PRs by status\n    const statusCounts = prs.reduce((acc, pr) => {\n        acc[pr.status] = (acc[pr.status] || 0) + 1;\n        return acc;\n    }, {});\n\n    // Count PRs by author\n    const authorCounts = prs.reduce((acc, pr) => {\n        acc[pr.author] = (acc[pr.author] || 0) + 1;\n        return acc;\n    }, {});\n\n    // Calculate PRs over time (by month)\n    const prsByMonth = prs.reduce((acc, pr) => {\n        const date = new Date(pr.created_at);\n        const monthYear = date.toLocaleString('default', { month: 'short', year: 'numeric' });\n        acc[monthYear] = (acc[monthYear] || 0) + 1;\n        return acc;\n    }, {});\n\n    // Create data arrays for charts\n    const statusData = Object.entries(statusCounts).map(([status, count]) => ({\n        status,\n        count,\n        color: status === 'Open' ? '#17a2b8' : \n               status === 'Merged' ? '#28a745' : \n               status === 'Closed' ? '#dc3545' : '#6c757d'\n    }));\n\n    const authorData = Object.entries(authorCounts)\n        .sort((a, b) => b[1] - a[1])\n        .slice(0, 5)\n        .map(([author, count]) => ({ author, count }));\n\n    const timelineData = Object.entries(prsByMonth)\n        .sort((a, b) => new Date(a[0]) - new Date(b[0]))\n        .map(([month, count]) => ({ month, count }));\n\n    return e('div', { className: 'analytics-container' }, [\n        e('h2', { key: 'title', className: 'analytics-title' }, 'Pull Request Analytics'),\n        // ... (rest of the analytics display code)\n    ]);\n}\n```\n\nThis component:\n\n*   Takes the `prs` array as a prop.\n*   Calculates statistics like the number of PRs by status, author, and month.\n*   Formats the data into arrays suitable for rendering charts and summaries.\n*   Renders the analytics using divs with appropriate CSS classes.\n\n## Under the Hood: Requesting PR Data\n\nLet's take a simplified look at what happens when the frontend requests PR data from the backend.\n\n```mermaid\nsequenceDiagram\n    participant FE as Frontend (React App)\n    participant BE as Backend (FastAPI)\n    participant DB as Snowflake Database\n\n    FE->>BE: GET /api/prs\n    activate BE\n    BE->>DB: Query PR Data\n    activate DB\n    DB-->>BE: PR Data (JSON)\n    deactivate DB\n    BE-->>FE: PR Data (JSON)\n    deactivate BE\n    FE->>FE: Render PR List\n```\n\n1.  The **Frontend (React App)** initiates a GET request to the `/api/prs` endpoint on the **Backend (FastAPI)**.\n2.  The **Backend (FastAPI)** receives the request and queries the **Snowflake Database** for PR data. (We'll see this in [Chapter 9: Snowflake Data Storage](09_snowflake_data_storage.md)).\n3.  The **Snowflake Database** returns the PR data to the **Backend (FastAPI)** in JSON format.\n4.  The **Backend (FastAPI)** sends the PR data back to the **Frontend (React App)** in JSON format.\n5.  The **Frontend (React App)** receives the data and renders the list of pull requests.\n\nThe actual fetching of data happens inside of the `React.useEffect` hook we saw before.\n\n```javascript\nReact.useEffect(() => {\n    fetch('http://localhost:8000/api/prs')\n        .then(res => res.json())\n        .then(data => {\n            setPrs(data);\n        });\n}, []);\n```\n\nThis hook ensures that the PR data is fetched when the component is first rendered and updates the component's state with the fetched data.\n\n## Conclusion\n\nIn this chapter, we explored the `CodeRoast` frontend, built with React. We learned about components, JSX, state, props, and how they're used to display PR lists, diffs, and analytics. We also took a peek under the hood to see how the frontend interacts with the backend to fetch data.\n\nNow that you have a basic understanding of the frontend, let's move on to the next chapter, where we'll dive into the backend API built with FastAPI: [Chapter 2: FastAPI Backend API](02_fastapi_backend_api.md).\n\n---\n# Chapter 2: FastAPI Backend API\n\nWelcome back to CodeRoast! In [Chapter 1: Frontend Web Application (React)](01_frontend_web_application__react_.md), we built a snazzy frontend that displays pull requests, diffs, and AI insights. But that frontend needs someone to talk to \u2013 someone who can fetch data, run the AI magic, and keep everything organized. That's where our FastAPI backend comes in!\n\n## What is a Backend API, and Why FastAPI?\n\nThink of the backend like the engine in our car analogy. The frontend is the dashboard, showing you information and letting you control things. The backend is the engine, doing all the heavy lifting.\n\nSpecifically, the backend acts as an *API (Application Programming Interface)*. This means it provides a structured way for the frontend (and other applications) to request information and functionality.\n\n**FastAPI** is a modern, high-performance Python web framework for building APIs. We chose FastAPI because it's:\n\n*   **Fast:** It's built for speed, so our app feels responsive.\n*   **Easy to use:** It has a simple and intuitive syntax, making development quicker.\n*   **Automatic Data Validation:** It automatically checks that the data sent to the API is in the correct format, preventing errors.\n*   **Automatic API Documentation:** It generates interactive API documentation automatically, making it easy to understand how to use the API.\n\n## Key Concepts\n\nLet's break down the key concepts behind our FastAPI backend:\n\n1.  **Routes (Endpoints):** These are specific URLs that our backend responds to. For example, `/api/prs` might be a route that returns a list of all pull requests. Think of them as specific \"doors\" to access different functionalities of our backend.\n\n2.  **Requests:** These are messages sent from the frontend (or any other client) to the backend, asking for something. They often include data, like the ID of a pull request the frontend wants to see.\n\n3.  **Responses:** These are messages sent back from the backend to the frontend, containing the requested information or the result of an action. Responses are often in JSON format, which is easy for the frontend to process.\n\n4.  **Data Models:** These are Python classes that define the structure of our data. For example, we might have a `PullRequest` data model that defines the fields for each pull request (like `id`, `title`, `author`, etc.).  Data models help ensure the data is consistent and well-defined.\n\n## The CodeRoast Backend: A Tour\n\nLet's look at how these concepts come together in our `CodeRoast` backend, using code snippets from `main.py`.\n\n### Defining a Route\n\nHere's an example of a route that fetches all pull requests:\n\n```python\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nimport snowflake.connector\nimport os\nfrom dotenv import load_dotenv\nimport pandas as pd\n\nload_dotenv()\n\napp = FastAPI()\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n@app.get(\"/api/prs\")\ndef fetch_all_prs():\n    # Fetch data from Snowflake and return it\n    # For simplicity, let's return a test data\n    test_data = [\n        {\n            \"id\": 1,\n            \"title\": \"Add new feature\",\n            \"author\": \"johndoe\",\n            \"status\": \"Open\",\n            \"created_at\": \"2023-01-01\",\n            \"updated_at\": \"2023-01-02\"\n        },\n        {\n            \"id\": 2,\n            \"title\": \"Fix bug in login\",\n            \"author\": \"janedoe\",\n            \"status\": \"Merged\",\n            \"created_at\": \"2023-01-03\",\n            \"updated_at\": \"2023-01-04\"\n        }\n    ]\n    return test_data\n```\n\nLet's break this down:\n\n*   `@app.get(\"/api/prs\")`: This is a *decorator* that tells FastAPI to create a route at the URL `/api/prs`. The `@app.get` part specifies that this route should handle GET requests (requests for data).\n*   `def fetch_all_prs():`: This defines a function called `fetch_all_prs` that will be executed when someone makes a GET request to `/api/prs`.\n*   `return test_data`: This returns the `test_data` list as a JSON response. FastAPI automatically converts Python dictionaries and lists into JSON.\n\nSo, if you open your web browser and go to `http://localhost:8000/api/prs`, you'll see a JSON response containing the `test_data`. (Note: make sure your backend is running!)\n\n### Handling Parameters\n\nWhat if we want to fetch information about a specific pull request, identified by its ID? We can use a *path parameter* in our route:\n\n```python\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nimport snowflake.connector\nimport os\nfrom dotenv import load_dotenv\nimport pandas as pd\n\nload_dotenv()\n\napp = FastAPI()\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n@app.get(\"/api/prs/{pr_id}\")\ndef fetch_pr_analysis(pr_id: int):\n    # Fetch the analysis data\n    # For simplicity, let's return a test data\n    test_data = {\n        \"pr_id\": pr_id,\n        \"ai_summary\": \"This PR adds a new feature that improves user experience\",\n        \"merge_confidence\": \"High\",\n        \"code_quality\": \"Good code quality. No major issues found.\",\n        \"diff\": \"```\\n+ def new_function():\\n+     return 'new feature'\\n```\",\n        \"doc_string\": \"This PR introduces a new function that returns 'new feature'.\"\n    }\n    return test_data\n```\n\nHere's what's new:\n\n*   `@app.get(\"/api/prs/{pr_id}\")`: This defines a route that includes a path parameter called `pr_id`. The curly braces `{}` indicate a parameter.\n*   `def fetch_pr_analysis(pr_id: int):`:  The `pr_id: int` part of the function definition tells FastAPI that `pr_id` is an integer parameter. FastAPI will automatically validate that the value passed in the URL is actually an integer.\n*   `\"pr_id\": pr_id`: We are using the `pr_id` parameter inside the function to construct the response.\n\nIf you go to `http://localhost:8000/api/prs/123`, you'll see a JSON response containing the `test_data`, but with the `\"pr_id\"` field set to `123`.\n\n### Connecting to Snowflake Database\n\nIn a real application, we wouldn't just return test data. We would fetch the data from a database.  In our case, we use [Snowflake Data Storage](09_snowflake_data_storage.md). Let's look at how we can connect to Snowflake and query the database (simplified):\n\n```python\nimport os\nimport snowflake.connector\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nimport pandas as pd\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\napp = FastAPI()\n\n# Snowflake credentials\nSNOWFLAKE_USER = os.getenv(\"SNOWFLAKE_USER\", \"raghavg332\")\nSNOWFLAKE_PASSWORD = os.getenv(\"SNOWFLAKE_PASSWORD\", \"Qa29Kh4MptfGHEW\")\nSNOWFLAKE_ACCOUNT = os.getenv(\"SNOWFLAKE_ACCOUNT\", \"KXCIVVH-LL27432\")\nSNOWFLAKE_WAREHOUSE = os.getenv(\"SNOWFLAKE_WAREHOUSE\", \"COMPUTE_WH\")\nSNOWFLAKE_DATABASE = os.getenv(\"SNOWFLAKE_DATABASE\", \"PR_DASHBOARD\")\nSNOWFLAKE_SCHEMA = os.getenv(\"SNOWFLAKE_SCHEMA\", \"PUBLIC\")\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\ndef get_conn():\n    \"\"\"Establish connection to Snowflake database\"\"\"\n    try:\n        conn = snowflake.connector.connect(\n            user=SNOWFLAKE_USER,\n            password=SNOWFLAKE_PASSWORD,\n            account=SNOWFLAKE_ACCOUNT,\n            warehouse=SNOWFLAKE_WAREHOUSE,\n            database=SNOWFLAKE_DATABASE,\n            schema=SNOWFLAKE_SCHEMA\n        )\n        return conn\n    except Exception as e:\n        print(f\"Error connecting to Snowflake: {e}\")\n        raise e\n\n@app.get(\"/api/prs/{pr_id}\")\ndef fetch_pr_analysis(pr_id: int):\n    try:\n        conn = get_conn()\n        cursor = conn.cursor(snowflake.connector.DictCursor)\n\n        # Fetch the analysis data\n        cursor.execute(f\"SELECT * FROM pr_analysis WHERE pr_id = {pr_id}\")\n        result = cursor.fetchone()\n\n        if not result:\n            cursor.close()\n            conn.close()\n            raise HTTPException(status_code=404, detail=\"PR analysis not found\")\n\n        cursor.close()\n        conn.close()\n        return result\n    except Exception as e:\n        print(f\"Error fetching PR analysis: {e}\")\n        raise HTTPException(status_code=500, detail=f\"Error: {str(e)}\")\n```\n\nHere's what's happening:\n\n*   `import snowflake.connector`: This imports the Snowflake connector library, which allows us to connect to a Snowflake database.\n*   `SNOWFLAKE_USER = os.getenv(\"SNOWFLAKE_USER\", \"raghavg332\")`: This imports environment variable, which allows us to securely config the snowflake credentials.\n*   `get_conn()`: This function establishes a connection to Snowflake using your credentials. **Important:** Make sure you have Snowflake configured correctly.\n*   `conn = get_conn()`: This calls the `get_conn` function to establish the connection.\n*   `cursor = conn.cursor(snowflake.connector.DictCursor)`: This creates a cursor object, which allows us to execute SQL queries.  The `snowflake.connector.DictCursor` is important here: it tells the connector to return results as Python dictionaries, which are easier to work with.\n*   `cursor.execute(f\"SELECT * FROM pr_analysis WHERE pr_id = {pr_id}\")`: This executes a SQL query to fetch the analysis data for the specified `pr_id`.\n*   `result = cursor.fetchone()`: This fetches the first row of the result set.\n*   `return result`: This returns the result (which is a dictionary) as a JSON response.\n\n## Under the Hood: Fetching PR Data\n\nLet's take a simplified look at what happens when the frontend requests PR data from the backend.\n\n```mermaid\nsequenceDiagram\n    participant FE as Frontend (React App)\n    participant BE as Backend (FastAPI)\n    participant DB as Snowflake Database\n\n    FE->>BE: GET /api/prs/123\n    activate BE\n    BE->>DB: SELECT * FROM pr_analysis WHERE pr_id = 123\n    activate DB\n    DB-->>BE: PR Analysis Data (JSON)\n    deactivate DB\n    BE-->>FE: PR Analysis Data (JSON)\n    deactivate BE\n    FE->>FE: Render PR Details\n```\n\n1.  The **Frontend (React App)** initiates a GET request to the `/api/prs/123` endpoint on the **Backend (FastAPI)**.\n2.  The **Backend (FastAPI)** receives the request and executes a SQL query on the **Snowflake Database** to fetch the analysis data for the PR with ID 123.\n3.  The **Snowflake Database** returns the PR analysis data to the **Backend (FastAPI)** in JSON format.\n4.  The **Backend (FastAPI)** sends the PR analysis data back to the **Frontend (React App)** in JSON format.\n5.  The **Frontend (React App)** receives the data and renders the details of the pull request.\n\nThe actual fetching of data happens inside of the `fetch_pr_analysis` function we saw before, where it:\n\n1. Establishes a connection to the snowflake database.\n2. Constructs and executes a select query.\n3. Formats the data for transmission back to the frontend as a JSON response.\n\n## Conclusion\n\nIn this chapter, we explored the `CodeRoast` backend, built with FastAPI. We learned about routes, requests, responses, data models, and how they're used to create a powerful API. We also saw how the backend interacts with a Snowflake database to fetch data.\n\nNow that you have a basic understanding of the backend, let's move on to the next chapter, where we'll dive into how we receive GitHub Webhooks: [Chapter 3: GitHub Webhook Integration](03_github_webhook_integration.md).\n\n---\n# Chapter 3: GitHub Webhook Integration\n\nIn [Chapter 2: FastAPI Backend API](02_fastapi_backend_api.md), we set up our FastAPI backend to respond to requests. But how does our backend *know* when something important happens on GitHub, like a new pull request being opened? That's where GitHub Webhooks come in!\n\nImagine you're waiting for a package to arrive. Instead of constantly checking the tracking number, you can sign up for notifications that tell you exactly when it's delivered. GitHub Webhooks are like those notifications, but for code!\n\n## What Problem Do Webhooks Solve?\n\nWithout webhooks, our backend would have to constantly ask GitHub, \"Hey, are there any new pull requests? Hey, are there any updates to existing pull requests?\" This is inefficient and wastes resources.\n\nWebhooks solve this problem by letting GitHub *tell* our backend when something interesting happens. This is much more efficient!\n\n**Central Use Case:** When a developer opens a new pull request (PR) on GitHub, we want our `CodeRoast` system to automatically start analyzing that PR. Webhooks make this possible.\n\n## Key Concepts\n\nLet's break down the key concepts of GitHub Webhook Integration:\n\n1.  **Event:** An event is something that happens in a GitHub repository, like opening a pull request, pushing code, or commenting on an issue.\n\n2.  **Webhook:** A webhook is a way for GitHub to send a notification to our backend whenever a specific event occurs. It's like a \"callback\" \u2013 GitHub calls back to our server when something happens.\n\n3.  **Payload:** The payload is the data that GitHub sends to our backend in the webhook notification. It's a JSON object containing information about the event that occurred, like the pull request's details, the commit messages, and the author's information. This is the \"package\" containing all the information.\n\n4.  **Signature:** A signature is a security measure that ensures the webhook notification is actually coming from GitHub and hasn't been tampered with. It's like a digital \"seal\" of authenticity.\n\n## How to Use Webhooks: A Simple Example\n\nLet's imagine a very simple scenario: when a pull request is opened, we want to print a message to our backend's console.\n\nHere's how the process works:\n\n1.  **Configure a Webhook on GitHub:** In your GitHub repository settings, you create a webhook. You tell GitHub:\n    *   The URL of your backend (where to send the notifications).\n    *   Which events to listen for (e.g., \"pull request\").\n    *   A secret (to verify the signature).\n\n2.  **Open a Pull Request:** A developer opens a new pull request on the repository.\n\n3.  **GitHub Sends a Webhook:** GitHub detects the \"pull request opened\" event and sends a webhook notification (a POST request) to our backend's URL.\n\n4.  **Backend Receives the Webhook:** Our backend receives the POST request, verifies the signature to ensure it's from GitHub, and then extracts the pull request information from the payload.\n\n5.  **Backend Processes the Information:** In our simple example, our backend just prints a message. In reality, it would trigger the [Pull Request Analysis Pipeline](04_pull_request_analysis_pipeline.md).\n\n## Code Example: Receiving a Webhook\n\nHere's a simplified code example showing how our FastAPI backend can receive and process a GitHub webhook:\n\n```python\nfrom fastapi import FastAPI, Request, Header\nfrom fastapi.responses import JSONResponse\nimport hmac\nimport hashlib\nimport json\n\napp = FastAPI()\n\nGITHUB_SECRET = \"asdfg\" # Replace with your actual secret\n\ndef verify_signature(payload, signature, secret):\n    mac = hmac.new(secret.encode(), msg=payload, digestmod=hashlib.sha256)\n    expected = f\"sha256={mac.hexdigest()}\"\n    return hmac.compare_digest(expected, signature)\n\n@app.post(\"/webhook\")\nasync def github_webhook(\n    request: Request,\n    x_hub_signature_256: str = Header(None),\n    x_github_event: str = Header(None)\n):\n    body = await request.body()\n\n    # Verify webhook signature\n    if not verify_signature(body, x_hub_signature_256, GITHUB_SECRET):\n        return JSONResponse(content={\"error\": \"Invalid signature\"}, status_code=403)\n\n    payload = json.loads(body)\n\n    if x_github_event == \"pull_request\":\n        action = payload[\"action\"]\n        pr = payload[\"pull_request\"]\n\n        if action in [\"opened\", \"synchronize\"]:\n            print(f\"New PR opened: {pr['title']}\")\n\n    return {\"message\": \"OK\"}\n```\n\nLet's break this down:\n\n*   `@app.post(\"/webhook\")`: This creates a route that listens for POST requests at the `/webhook` endpoint. GitHub will send its webhook notifications to this URL.\n*   `request: Request`: This allows us to access the body of the POST request (the payload).\n*   `x_hub_signature_256: str = Header(None)`: This extracts the `X-Hub-Signature-256` header from the request. This header contains the signature that we need to verify.\n*   `x_github_event: str = Header(None)`: This extracts the `X-GitHub-Event` header, which tells us what type of event triggered the webhook (e.g., \"pull_request\").\n*   `body = await request.body()`: This reads the raw bytes from the request body.\n*   `verify_signature(body, x_hub_signature_256, GITHUB_SECRET)`: This calls a function to verify that the signature is valid (we'll explain this in more detail later).\n*   `payload = json.loads(body)`: This converts the JSON payload into a Python dictionary.\n*   `if x_github_event == \"pull_request\"`: This checks if the event type is \"pull_request\".\n*   `if action in [\"opened\", \"synchronize\"]`: This checks if the pull request action is \"opened\" (a new PR) or \"synchronize\" (a PR was updated).\n*   `print(f\"New PR opened: {pr['title']}\")`: This prints the title of the new pull request to the console.\n\n**Example Input (Payload):**\n\nWhen a pull request is opened, GitHub will send a JSON payload to our `/webhook` endpoint. This payload contains lots of information about the pull request. Here's a simplified example:\n\n```json\n{\n  \"action\": \"opened\",\n  \"pull_request\": {\n    \"url\": \"https://api.github.com/repos/your-org/your-repo/pulls/123\",\n    \"title\": \"Add a new feature\"\n  },\n  \"repository\": {\n    \"full_name\": \"your-org/your-repo\"\n  }\n}\n```\n\n**Example Output:**\n\nWhen our backend receives this payload, it will print the following message to the console:\n\n```\nNew PR opened: Add a new feature\n```\n\nThis example demonstrates the basic flow of receiving and processing a GitHub webhook. In a real application, you would use the data in the payload to trigger more complex actions, like analyzing the code changes in the pull request.\n\n## Under the Hood: How Webhooks Work\n\nLet's take a closer look at how webhooks work internally.\n\nHere's a sequence diagram illustrating the flow:\n\n```mermaid\nsequenceDiagram\n    participant Dev as Developer\n    participant GH as GitHub\n    participant BE as Backend (FastAPI)\n\n    Dev->>GH: Opens/Updates Pull Request\n    activate GH\n    GH->>BE: POST /webhook (with payload)\n    activate BE\n    BE->>BE: Verify Signature\n    alt Signature Valid\n        BE->>BE: Process Payload\n        BE-->>GH: 200 OK\n    else Signature Invalid\n        BE-->>GH: 403 Forbidden\n    end\n    deactivate BE\n    deactivate GH\n```\n\n1.  A **Developer** opens or updates a pull request on **GitHub**.\n\n2.  **GitHub** detects the event and sends a POST request to our **Backend (FastAPI)** at the `/webhook` endpoint. The request includes a JSON payload containing information about the event.\n\n3.  The **Backend (FastAPI)** receives the POST request and immediately verifies the signature to ensure the request is actually coming from GitHub.\n\n4.  If the signature is valid, the **Backend (FastAPI)** processes the payload and performs the appropriate actions (e.g., triggers the [Pull Request Analysis Pipeline](04_pull_request_analysis_pipeline.md)). The Backend sends an HTTP 200 OK to GitHub.\n\n5.  If the signature is invalid, the **Backend (FastAPI)** rejects the request and sends an HTTP 403 Forbidden to GitHub.\n\n### Verifying the Signature\n\nThe `verify_signature` function is crucial for security. Here's how it works:\n\n```python\nimport hmac\nimport hashlib\n\ndef verify_signature(payload, signature, secret):\n    mac = hmac.new(secret.encode(), msg=payload, digestmod=hashlib.sha256)\n    expected = f\"sha256={mac.hexdigest()}\"\n    return hmac.compare_digest(expected, signature)\n```\n\n*   `hmac.new(secret.encode(), msg=payload, digestmod=hashlib.sha256)`: This creates a new HMAC (Hash-based Message Authentication Code) object using the secret key and the payload. It uses the SHA256 algorithm to generate the hash.\n*   `expected = f\"sha256={mac.hexdigest()}\"`: This calculates the expected signature by prefixing \"sha256=\" to the hexadecimal representation of the HMAC hash.\n*   `hmac.compare_digest(expected, signature)`: This securely compares the expected signature with the signature provided in the `X-Hub-Signature-256` header. This function is important to prevent timing attacks.\n\nThe secret key is configured in your GitHub webhook settings and should be stored securely on your backend. Make sure to *never* expose your secret key in your code or commit it to your repository! As you can see in the provided `backend/main.py`, the correct approach is to store the secret in an environment variable.\n\n## Conclusion\n\nIn this chapter, we learned about GitHub Webhook Integration. We understand what webhooks are, why they're useful, and how to receive and process them in our FastAPI backend. We also saw how to verify the signature to ensure the webhook notifications are authentic.\n\nNow that we can receive notifications about pull requests, we can move on to the next chapter, where we'll dive into the [Pull Request Analysis Pipeline](04_pull_request_analysis_pipeline.md), which is responsible for analyzing the code changes in the pull request and generating AI-powered insights.\n\n---\n# Chapter 4: Pull Request Analysis Pipeline\n\nIn [Chapter 3: GitHub Webhook Integration](03_github_webhook_integration.md), we learned how to set up our backend to receive notifications from GitHub whenever a pull request (PR) is opened or updated. Now, what do we *do* with that information?\n\nThat's where the **Pull Request Analysis Pipeline** comes in!\n\nImagine a factory assembly line. Raw materials (code changes) enter, various machines perform operations (analysis, documentation suggestions), and then the finished product (insights) is stored in a database. Our pipeline is very similar: it takes a pull request, analyzes the code changes, generates AI insights, and stores the results.\n\n**Central Use Case:** When a developer opens a new pull request, we want `CodeRoast` to automatically analyze the code, provide feedback, and suggest documentation updates. The Pull Request Analysis Pipeline makes this happen!\n\n## Key Concepts\n\nLet's break down the key concepts behind the Pull Request Analysis Pipeline:\n\n1.  **Input: Pull Request Data:** This is the raw information about the PR, received from GitHub via the webhook. It includes the code diff, the PR description, the author, and more. Think of this as the \"raw materials\" entering our factory.\n\n2.  **Code Parsing:** Before we can analyze the code, we need to understand its structure. This involves parsing the code to identify functions, classes, and other elements. We will talk about it in detail in [Chapter 5: Tree-sitter Based Code Parsing](05_tree_sitter_based_code_parsing.md).\n\n3.  **AI Insights Generation:** This is where the magic happens! We use AI, powered by [Groq API Integration](06_groq_api_integration.md), to analyze the code and generate insights. This might include summarizing the PR, identifying potential issues, suggesting improvements, and estimating merge confidence. It's like adding value to a product in the factory.\n\n4.  **Documentation Updates (RAG):** We use RAG (Retrieval-Augmented Generation) to suggest updates to the project documentation based on the code changes. This ensures that our documentation stays up-to-date. See more details in [Chapter 7: RAG (Retrieval-Augmented Generation) Documentation Updates](07_rag__retrieval_augmented_generation__documentation_updates.md).\n\n5.  **Output: Stored Insights:** The final product of our pipeline is a set of AI-generated insights that are stored in a database (Snowflake in our case [Chapter 9: Snowflake Data Storage](09_snowflake_data_storage.md)). These insights can then be displayed in the frontend and used to improve the code review process.\n\n## How to Use the Pipeline\n\nLet's walk through how to use the pipeline with a simplified example.\n\nImagine we have a function called `process_pull_request` that represents our pipeline. It takes the pull request data as input and returns the AI insights.\n\n```python\ndef process_pull_request(pr_data):\n    # 1. Parse the code changes.  We skip for this simple case.\n    code_diff = pr_data[\"code_diff\"] # Assume code_diff is extracted\n\n    # 2. Generate AI insights. We skip complex logic here\n    ai_summary = \"This PR adds a new feature.\"\n    merge_confidence = \"High\"\n\n    # 3. Store the results. We skip database insert here.\n    insights = {\n        \"ai_summary\": ai_summary,\n        \"merge_confidence\": merge_confidence,\n    }\n\n    return insights\n```\n\nThis simplified code does the following:\n\n1.  Takes `pr_data` which contains information about the PR.\n2.  Simulates extracting the code diff.\n3.  Simulates generating AI insights (in a real application, we'd use the Groq API).\n4.  Returns a dictionary containing the AI insights.\n\n**Example Input:**\n\n```python\npr_data = {\n    \"pr_title\": \"Add new feature\",\n    \"pr_description\": \"This PR adds a new feature to calculate statistics.\",\n    \"code_diff\": \"```diff\\n+def calculate_mean(data):\\n+  return sum(data) / len(data)\\n```\",\n    \"author\": \"johndoe\",\n    \"status\": \"open\"\n}\n```\n\n**Example Output:**\n\n```python\n{\n  \"ai_summary\": \"This PR adds a new feature.\",\n  \"merge_confidence\": \"High\"\n}\n```\n\nIn reality, this function is more complex and involves several steps, including connecting to external services and parsing the code. We will cover it later in this chapter!\n\n## Under the Hood: How the Pipeline Works\n\nLet's take a closer look at how the Pull Request Analysis Pipeline works internally.\n\nHere's a sequence diagram illustrating the flow:\n\n```mermaid\nsequenceDiagram\n    participant GH as GitHub\n    participant BE as Backend (FastAPI)\n    participant CP as Code Parser\n    participant AI as Groq API\n    participant DB as Snowflake Database\n\n    GH->>BE: POST /webhook (PR data)\n    activate BE\n    BE->>CP: Extract code diffs\n    activate CP\n    CP-->>BE: Code diffs (parsed)\n    deactivate CP\n    BE->>AI: Analyze code\n    activate AI\n    AI-->>BE: AI Insights (JSON)\n    deactivate AI\n    BE->>DB: Store Insights\n    activate DB\n    DB-->>BE: OK\n    deactivate DB\n    BE-->>GH: 200 OK\n    deactivate BE\n```\n\n1.  **GitHub** sends a POST request to our **Backend (FastAPI)** at the `/webhook` endpoint, containing the pull request data.\n2.  The **Backend (FastAPI)** receives the data and passes the code to the **Code Parser** to extract the changed functions (detailed in [Chapter 5: Tree-sitter Based Code Parsing](05_tree_sitter_based_code_parsing.md)).\n3.  The **Code Parser** returns the extracted code diffs to the **Backend (FastAPI)**.\n4.  The **Backend (FastAPI)** sends the code diffs and PR details to the **Groq API** for analysis and insight generation.\n5.  The **Groq API** analyzes the code and returns AI insights (summary, merge confidence, etc.) in JSON format to the **Backend (FastAPI)**.\n6.  The **Backend (FastAPI)** stores these insights in the **Snowflake Database**.\n7.  The **Backend (FastAPI)** sends an HTTP 200 OK response to GitHub.\n\n### Diving into the Code\n\nLet's look at a code example of how the `process_pull_request` function is implemented. This code is located in `backend/prReview.py`.\n\n```python\ndef process_pull_request(g, repo_name, pr_number, sender=None):\n    repo = g.get_repo(repo_name)\n    pr = repo.get_pull(pr_number)\n    pr_description = pr.title # title is used as description\n    pr_title = pr.title\n    pr_author = pr.user.login\n    pr_status = pr.state\n    code_diff = get_lines_changed(pr) # Step 1\n    # ... (rest of the function)\n```\n\nThis code does the following:\n\n1.  `repo = g.get_repo(repo_name)`: Gets the repository object from GitHub using the `repo_name`.  `g` is the authenticated Github instance.\n2.  `pr = repo.get_pull(pr_number)`: Gets the pull request object from the repository using the `pr_number`.\n3.  `pr_description = pr.title`: Sets the pull request description.\n4.  `code_diff = get_lines_changed(pr)`: Calls the `get_lines_changed` function to get the code diff. This function uses github api to get the diff and returns a dictionary containing the added, modified, and deleted lines.\n\nLet's look at how we get the diff code:\n\n```python\ndef get_lines_changed(pr):\n    files = pr.get_files()\n    file_changes = {}\n    for file in files:\n        if file.status == \"removed\":\n            continue\n        file_changes[file.filename] = parse_changed_lines(file.patch)\n\n    return file_changes\n\ndef parse_changed_lines(patch: str):\n    added_or_modified_lines = set()\n    deleted_lines = set()\n    for line in patch.splitlines():\n        if line.startswith(\"@@\"):\n            match = re.findall(r\"@@ -(\\d+)(?:,(\\d+))? \\+(\\d+)(?:,(\\d+))? @@\", line)\n            if match:\n                old_start, old_count, new_start, new_count = match[0]\n\n                old_start = int(old_start)\n                old_count = int(old_count or 1)\n                for l in range(old_start, old_start + old_count):\n                    deleted_lines.add(l)\n\n                new_start = int(new_start)\n                new_count = int(new_count or 1)\n                for l in range(new_start, new_start + new_count):\n                    added_or_modified_lines.add(l)\n\n    return {\n        \"added_or_modified_lines\": added_or_modified_lines,\n        \"deleted_lines\": deleted_lines\n    }\n```\n\nThis code does the following:\n\n1.  `files = pr.get_files()`: Get all files that changed in this PR.\n2.  The code loops through each of the `files`:\n    *   `if file.status == \"removed\": continue`: Skip the files that were removed.\n    *   `file_changes[file.filename] = parse_changed_lines(file.patch)`: Call `parse_changed_lines` to parse the changes of each file and then store. The `parse_changed_lines` uses regex matching to find out which lines were added or deleted in this file.\n\nThe following code then uses the data extracted above to call the Groq API:\n\n```python\n    # Get the review data and post a comment\n    review_data = review_and_store_pr(pr_description, file_diff_func, pr_title, pr_author, pr_status)\n    \n    # Post the review as a comment on the PR\n    post_review_comment(g, repo_name, pr_number, review_data)\n```\n\nThe `review_and_store_pr` performs the RAG (Retrieval-Augmented Generation) to update project documentation and calls Groq to generate the PR review and stores the review into the snowflake database. `post_review_comment` then posts the review as a comment on the PR. Details can be found in [Chapter 8: PR Review Comment Posting (GitHub)](08_pr_review_comment_posting__github_.md)\n\n## Conclusion\n\nIn this chapter, we explored the Pull Request Analysis Pipeline. We learned about the key concepts, how to use the pipeline, and how it works internally. We saw how the pipeline takes a pull request as input, analyzes the code changes, generates AI insights, suggests documentation updates, and stores the results in a database.\n\nNow that we understand the overall pipeline, let's dive deeper into one of the key components: parsing the code changes using Tree-sitter in [Chapter 5: Tree-sitter Based Code Parsing](05_tree_sitter_based_code_parsing.md).\n\n---\n# Chapter 5: Tree-sitter Based Code Parsing\n\nIn [Chapter 4: Pull Request Analysis Pipeline](04_pull_request_analysis_pipeline.md), we learned how the `CodeRoast` pipeline analyzes pull requests. One of the key steps in that pipeline is understanding the *structure* of the code changes.  Instead of just treating the code as plain text, we want to know *what* code has changed - what functions have been added, deleted or modified?\n\nThat's where **Tree-sitter** comes in!\n\nImagine trying to understand a sentence without knowing grammar. You might get some of the words, but you'd miss the relationships between them. Tree-sitter is like a grammar checker for code. It helps us understand the structure of the code so we can analyze it more effectively.\n\n**Central Use Case:** When a developer modifies a function in a pull request, we want `CodeRoast` to *only* analyze that function, rather than the entire file. This focused analysis saves time and improves the accuracy of our AI insights.\n\n## Key Concepts\n\nLet's break down the key concepts behind Tree-sitter based code parsing:\n\n1.  **Parsing:** Parsing is the process of taking code (which is just a string of characters) and turning it into a structured representation that a computer can understand. Think of it like taking a sentence and breaking it down into nouns, verbs, and adjectives.\n\n2.  **Abstract Syntax Tree (AST):** The AST is the structured representation of the code that results from parsing. It's like a tree diagram that shows the relationships between the different parts of the code. Each \"node\" in the tree represents a different element of the code, like a function, a variable, or an expression.\n\n3.  **Tree-sitter:** Tree-sitter is a library that makes it easy to parse code and create ASTs. It's fast, reliable, and supports many different programming languages. It's the engine that drives our code parsing.\n\n4.  **Grammar:** A grammar defines the rules for a programming language. Tree-sitter uses grammars to understand the structure of the code. Each language has its own grammar (e.g., Python grammar, JavaScript grammar). These grammars tells Tree-sitter what to expect.\n\n## How to Use Tree-sitter: A Simple Example\n\nLet's imagine we have a simple Python function and we want to use Tree-sitter to extract its name and code.\n\nHere's the function:\n\n```python\ndef greet(name):\n    \"\"\"Greets the person passed in as a parameter.\"\"\"\n    print(f\"Hello, {name}!\")\n```\n\nHere's how we can use Tree-sitter to parse this code and extract the function name and code (simplified):\n\n```python\nfrom tree_sitter import Language, Parser\nimport tree_sitter_python as tspython\n\n# Set up Tree-sitter\nPY_LANGUAGE = Language(tspython.language())\nparser = Parser()\nparser.set_language(PY_LANGUAGE)\n\n# The code to parse\ncode = \"\"\"\ndef greet(name):\n    \\\"\\\"\\\"Greets the person passed in as a parameter.\\\"\\\"\\\"\n    print(f\"Hello, {name}!\")\n\"\"\"\n\n# Parse the code\ntree = parser.parse(bytes(code, \"utf8\"))\n\n# Get the root node of the AST\nroot_node = tree.root_node\n\n# Find the function definition node\nfunction_node = root_node.children[0] # Assume function is the first child\n\n# Extract the function name\nfunction_name_node = function_node.child_by_field_name('name')\nfunction_name = function_name_node.text.decode()\n\n# Extract the function code\nfunction_code = function_node.text.decode()\n\nprint(f\"Function Name: {function_name}\")\nprint(f\"Function Code: {function_code}\")\n```\n\nThis simplified code does the following:\n\n1.  **Sets up Tree-sitter:** It imports the necessary libraries and creates a Tree-sitter parser for Python.  We are using `tree_sitter_python` to obtain the python grammar.\n2.  **Parses the code:** It takes the Python code and uses the parser to create an AST.\n3.  **Navigates the AST:**\n    *   `function_node = root_node.children[0]`: Since the example only consists of the definition of the `greet` function, the function node will be the first child. This code accesses the function node.\n    *   `function_name_node = function_node.child_by_field_name('name')`: This line gets the node that represents the name of the function.\n4.  **Extracts information:** It extracts the function name and code from the AST.\n5.  **Prints the results:** It prints the function name and code to the console.\n\n**Example Output:**\n\n```\nFunction Name: greet\nFunction Code: def greet(name):\n    \"\"\"Greets the person passed in as a parameter.\"\"\"\n    print(f\"Hello, {name}!\")\n```\n\nThis example demonstrates how we can use Tree-sitter to parse code and extract specific elements. In `CodeRoast`, we use this technique to identify the functions that have been changed in a pull request.\n\n## Under the Hood: How Tree-sitter Works\n\nLet's take a closer look at how Tree-sitter works internally.\n\nHere's a sequence diagram illustrating the flow:\n\n```mermaid\nsequenceDiagram\n    participant Code as Code (String)\n    participant Parser as Tree-sitter Parser\n    participant AST as Abstract Syntax Tree\n    participant Extractor as Information Extractor\n\n    Code->>Parser: Parse code\n    activate Parser\n    Parser->>AST: Build AST\n    activate AST\n    AST-->>Parser: AST\n    deactivate AST\n    Parser-->>Extractor: AST\n    deactivate Parser\n    Extractor->>AST: Navigate and Extract\n    activate AST\n    AST-->>Extractor: Function Name and Code\n    deactivate AST\n    Extractor-->>Code: Function Name and Code\n```\n\n1.  The **Code (String)** is passed to the **Tree-sitter Parser**.\n2.  The **Tree-sitter Parser** uses the grammar to build an **Abstract Syntax Tree (AST)**.\n3.  The **AST** is returned to the **Tree-sitter Parser**.\n4.  The **Tree-sitter Parser** passes the **AST** to the **Information Extractor**.\n5.  The **Information Extractor** navigates the **AST** to extract the function name and code.\n6.  The extracted function name and code are returned to the **Code**.\n\n### Diving into the Code\n\nLet's look at the code that extracts functions from the code. This code can be found in `backend/prReview.py`.\n\n```python\ndef extract_functions(code, language_name, changed_lines):\n    parser = get_paraser(language_name)\n    tree = parser.parse(bytes(code, \"utf8\"))\n    root_node = tree.root_node\n\n    functions = []\n\n    def node_within_lines(node):\n        start_line = node.start_point[0] + 1\n        end_line = node.end_point[0] + 1\n        return any([start_line <= line <= end_line for line in changed_lines])\n    \n    def traverse(node):\n        if language_name == \"python\" and node.type == \"function_definition\":\n            if node_within_lines(node):\n                func_name_node = node.child_by_field_name('name')\n                func_name = func_name_node.text.decode()\n                functions.append({'name': func_name, 'code': node.text.decode()})\n        elif language_name == 'javascript' and node.type in ['function_declaration', 'method_definition', 'arrow_function']:\n            if node_within_lines(node):\n                func_name_node = node.child_by_field_name('name')\n                func_name = func_name_node.text.decode()\n                functions.append({'name': func_name, 'code': node.text.decode()})\n        elif language_name == 'cpp' and node.type in ['function_definition']:\n            if node_within_lines(node):\n                func_name_node = node.child_by_field_name('name')\n                func_name = func_name_node.text.decode()\n                functions.append({'name': func_name, 'code': node.text.decode()})\n        for child in node.children:\n            traverse(child)\n\n    traverse(root_node)\n    return functions\n```\n\nThis code does the following:\n\n1.  `parser = get_paraser(language_name)`: Gets the Tree-sitter parser for the given language (Python, JavaScript, C++ are supported).\n2.  `tree = parser.parse(bytes(code, \"utf8\"))`: Parses the code and creates an AST.\n3.  `root_node = tree.root_node`: Gets the root node of the AST.\n4.  The code defines a helper function `node_within_lines` to checks whether a node is within changed lines to skip parsing nodes that are irrelevant.\n5.  `traverse(root_node)`: Calls the `traverse` function to recursively traverse the AST and extract the function names and code.\n    *   It first checks if the language is Python and the node type is `function_definition`.\n    *   If it is, it extracts the function name and code and adds them to the `functions` list.\n    *   Then it traverses the child nodes.\n6.  It does similar logic for Javascript and C++\n7.  It returns the `functions` list.\n\nThe `get_paraser` function loads the grammars for supported languages:\n\n```python\ndef get_paraser(language_name):\n\n    PY_LANGUAGE = Language(tspython.language())\n    JS_LANGUAGE = Language(tsjavascript.language())\n    CPP_LANGUAGE = Language(tscpp.language())\n\n    LANGUAGES = {\n        'python': PY_LANGUAGE,\n        'javascript': JS_LANGUAGE,\n        'cpp': CPP_LANGUAGE,\n    }\n    parser = Parser(LANGUAGES[language_name])\n    return parser\n```\n\nThis code does the following:\n\n1.  `PY_LANGUAGE = Language(tspython.language())`: Loads the Python grammar.\n2.  `JS_LANGUAGE = Language(tsjavascript.language())`: Loads the JavaScript grammar.\n3.  `CPP_LANGUAGE = Language(tscpp.language())`: Loads the C++ grammar.\n4.  `LANGUAGES = { ... }`: Creates a dictionary that maps language names to their corresponding grammars.\n5.  `parser = Parser(LANGUAGES[language_name])`: Creates a Tree-sitter parser for the given language.\n6.  It returns the `parser`.\n\n## Conclusion\n\nIn this chapter, we explored Tree-sitter based code parsing. We learned about the key concepts, how to use Tree-sitter to extract information from code, and how it works internally. We saw how Tree-sitter helps us understand the structure of code so we can analyze it more effectively.\n\nNow that we can parse code changes, let's move on to the next chapter and see how we use AI to generate insights about those changes using [Groq API Integration](06_groq_api_integration.md).\n\n---\n# Chapter 6: Groq API Integration\n\nIn [Chapter 5: Tree-sitter Based Code Parsing](05_tree_sitter_based_code_parsing.md), we learned how to parse the code changes in a pull request. Now, how do we *understand* what those changes mean and generate useful insights?\n\nThat's where the **Groq API Integration** comes in!\n\nThink of the Groq API as a super-smart AI assistant. We send it the code and a description of the pull request, and it sends back a summary of the changes, a code quality assessment, and even suggestions for improving the documentation.\n\n**Central Use Case:** When a developer submits a pull request, we want `CodeRoast` to automatically generate a helpful summary of the changes and identify potential problems in the code. The Groq API integration makes this possible.\n\n## Key Concepts\n\nLet's break down the key concepts behind the Groq API Integration:\n\n1.  **API (Application Programming Interface):**  An API is a way for different software systems to talk to each other.  The Groq API lets us send requests to Groq's AI models and get responses back. It is like ordering from a restaurant; we send the order (request), and the restaurant delivers the food (response).\n\n2.  **Groq API Key:** This is a secret code that identifies us to the Groq API. Think of it like a password that allows us to access the Groq API. You'll need to obtain your own API key from Groq to use their service.\n\n3.  **Prompt:** A prompt is the message we send to the Groq API, telling it what we want it to do.  For example, our prompt might include the code changes, the pull request description, and a request to summarize the changes. It's like writing instructions to your super-smart AI assistant.\n\n4.  **AI Model:** An AI model is a pre-trained algorithm that can perform a specific task, like summarizing text or assessing code quality. Groq offers a variety of AI models that we can use.\n\n5.  **JSON (JavaScript Object Notation):** JSON is a way to format data that's easy for computers to read and write. We send data to the Groq API in JSON format, and it sends data back to us in JSON format.\n\n## How to Use the Groq API: A Simple Example\n\nLet's imagine we want to use the Groq API to summarize a simple code change.\n\nHere's the code change:\n\n```python\ndef add(x, y):\n  \"\"\"Adds two numbers together.\"\"\"\n  return x + y\n```\n\nHere's how we can use the Groq API to summarize this code (simplified):\n\n```python\nimport requests\nimport json\n\n# Your Groq API key\nGROQ_API_KEY = \"YOUR_GROQ_API_KEY\" # Replace with your actual key\n\n# The API endpoint\nGROQ_ENDPOINT = \"https://api.groq.com/openai/v1/chat/completions\"\n\n# The prompt\nprompt = \"\"\"\nSummarize the following Python code:\n\ndef add(x, y):\n  \\\"\\\"\\\"Adds two numbers together.\\\"\\\"\\\"\n  return x + y\n\"\"\"\n\n# Construct the payload\npayload = {\n    \"model\": \"llama3-70b-8192\", # You should check with Groq for the latest model\n    \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n}\n\n# Set the headers\nheaders = {\n    \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\n# Make the request\nresponse = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n\n# Parse the response\nresult = response.json()\n\n# Extract the summary\nsummary = result[\"choices\"][0][\"message\"][\"content\"].strip()\n\n# Print the summary\nprint(summary)\n```\n\nThis simplified code does the following:\n\n1.  **Sets up the API key and endpoint:** It defines the `GROQ_API_KEY` and `GROQ_ENDPOINT` variables.  **Important:** Replace `\"YOUR_GROQ_API_KEY\"` with your actual Groq API key.\n2.  **Constructs the prompt:** It defines the `prompt` variable, which tells the Groq API what we want it to do.\n3.  **Constructs the payload:** It creates a dictionary containing the prompt and the AI model to use.\n    *   `\"model\": \"llama3-70b-8192\"`:  This tells the Groq API to use the \"llama3-70b-8192\" AI model. Please check Groq documentation for the latest models and their API names.\n4.  **Sets the headers:** It defines the `headers` variable, which includes our Groq API key and tells the Groq API that we're sending data in JSON format.\n5.  **Makes the request:** It uses the `requests.post` function to send the prompt to the Groq API.\n6.  **Parses the response:** It uses the `response.json()` function to convert the JSON response from the Groq API into a Python dictionary.\n7.  **Extracts the summary:** It extracts the summary from the response and prints it to the console.\n\n**Example Output:**\n\n```\nThis code defines a function called \"add\" that takes two arguments, x and y, and returns their sum. The docstring indicates that the function adds two numbers together.\n```\n\nThis example demonstrates how we can use the Groq API to summarize code. In `CodeRoast`, we use this technique to generate summaries of pull requests, assess code quality, and suggest documentation updates.\n\n## Under the Hood: How the Groq API Integration Works\n\nLet's take a closer look at how the Groq API Integration works internally.\n\nHere's a sequence diagram illustrating the flow:\n\n```mermaid\nsequenceDiagram\n    participant BE as Backend (FastAPI)\n    participant Groq as Groq API\n\n    BE->>Groq: POST /openai/v1/chat/completions (Prompt + Code)\n    activate Groq\n    Groq->>Groq: AI Model Analysis\n    Groq-->>BE: JSON (Summary, Quality, etc.)\n    deactivate Groq\n    BE->>BE: Process Results\n```\n\n1.  The **Backend (FastAPI)** constructs a prompt containing the code changes and sends a POST request to the **Groq API** at the `/openai/v1/chat/completions` endpoint.\n\n2.  The **Groq API** receives the prompt and uses its AI models to analyze the code.\n\n3.  The **Groq API** returns a JSON response containing the summary, code quality assessment, and documentation suggestions to the **Backend (FastAPI)**.\n\n4.  The **Backend (FastAPI)** processes the results and stores them in the database or displays them in the frontend.\n\n### Diving into the Code\n\nLet's look at the code that calls the Groq API and processes the response. This code can be found in `backend/ai_analyse.py` and `backend/prReview.py`. First, let's look at the `build_full_prompt` function:\n\n```python\ndef build_full_prompt(pr_description: str, code_diff_str: str) -> str:\n    return f\"\"\"\nYou are an AI code reviewer and documentation assistant.\n\nA developer submitted the following pull request. Please do the following:\n1. \ud83d\udd0d Summarize the overall purpose of the PR. Be a little detailed\n2. \u2705 Review the code for:\n   - Syntax issues in Python (Make up some Issue)\n   - Style (PEP8)\n   - Functionality correctness (does it match the description?)\n3. Based on the review, provide a merge confidence score from 1 to 10.\n4. \ud83d\udcdd Extract or suggest updated docstrings for any changed or added functions/classes.\n5. The Code Diff is made up of the old and new version of the code, if any of the field is empty it means, the function is either added or deleted respectively.\n\n## PR Description:\n{pr_description}\n\n## Code Diff:\n{code_diff_str}\n\nRespond ONLY in this JSON format without any additional text:\n\n{{\n  \"ai_summary\": \"...\",\n  \"merge_confidence\": \"1 - 10\",\n  \"code_quality\": {{\n    \"syntax_check\": \"...\",\n    \"style_check\": \"...\",\n    \"functionality_check\": \"...\",\n    \"final_rating\": \"Excellent / Good / Needs Work / Critical Issues\"\n  }}\n}}\n\"\"\"\n```\n\nThis code constructs the prompt that we send to the Groq API. It includes:\n\n*   A description of the role the AI should play (\"AI code reviewer and documentation assistant\").\n*   Instructions on what the AI should do (summarize the PR, review the code, suggest documentation updates).\n*   The pull request description and code diff.\n*   **Important:** It tells the AI to respond in JSON format. This makes it easy for us to parse the response.\n\nThe `review_and_store_pr` function calls the Groq API and process the response.\n\n```python\nimport requests\nimport json\n\ndef review_and_store_pr(pr_description: str, code_diff: dict, pr_title: str, pr_author: str, pr_status: str):\n    code_diff_str = \"\"\n    for file in list(code_diff.keys()):\n        code_diff_str += f\"File: {file}\\n\\n\"\n        diff = code_diff[file]\n\n        for func_name in list(diff.keys()):\n            old_code = diff[func_name]['old_code']\n            new_code = diff[func_name]['new_code']\n            if old_code is None:\n                old_code = \"\"\n            if new_code is None:\n                new_code = \"\"\n            code_diff_str += f\"Function: {func_name}\\n\\nOld Code:\\n{old_code}\\n\\nNew Code:\\n{new_code}\"\n    docstring = update_documentation_with_rag(pr_description, code_diff)\n    pr_id = random.randint(1000, 9999)\n    prompt = build_full_prompt(pr_description, code_diff_str)\n\n    payload = {\n        \"model\": \"llama3-70b-8192\",\n        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n    }\n\n    try:\n        response = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n        response.raise_for_status()\n        result = response.json()\n\n        try:\n            content = result[\"choices\"][0][\"message\"][\"content\"].strip()\n            review_data = json.loads(content)\n\n            # Flatten the code quality object for easier insert\n            code_quality_str = (\n                f\"Syntax: {review_data['code_quality']['syntax_check']}\\n\"\n                f\"Style: {review_data['code_quality']['style_check']}\\n\"\n                f\"Functionality: {review_data['code_quality']['functionality_check']}\\n\"\n                f\"Rating: {review_data['code_quality']['final_rating']}\"\n            )\n            cur.execute(f\"\"\"\n                INSERT INTO PULL_REQUESTS (\n                    ID, TITLE, AUTHOR, STATUS, CREATED_AT, UPDATED_AT\n                )\n                VALUES (%s, %s, %s, %s, %s, %s);\n            \"\"\", (\n                pr_id,\n                pr_title,\n                pr_author,\n                pr_status,\n                datetime.datetime.now(),\n                datetime.datetime.now()\n            ))\n            conn.commit()\n\n            # Insert into Snowflake\n            cur.execute(f\"\"\"\n                INSERT INTO PR_ANALYSIS (\n                    PR_ID, AI_SUMMARY, MERGE_CONFIDENCE, CODE_QUALITY,\n                    PR_DESCRIPTION, DOCSTRINGS, DIFF\n                )\n                VALUES (%s, %s, %s, %s, %s, %s, %s);\n            \"\"\", (\n                pr_id,\n                review_data[\"ai_summary\"],\n                review_data[\"merge_confidence\"],\n                code_quality_str,\n                pr_description,\n                docstring['doc_update'],\n                code_diff_str\n            ))\n\n            conn.commit()\n\n            logging.info(f\"[\u2705] PR #{pr_id} analysis saved to Snowflake.\")\n            print(f\"[\u2705] PR #{pr_id} analysis saved to Snowflake.\")\n            return review_data\n\n        except json.JSONDecodeError:\n            logging.error(\"[\u274c] Groq returned non-JSON output.\")\n            print(\"[\u274c] Groq returned non-JSON output.\")\n            return {\"error\": \"Invalid JSON from Groq\"}\n\n    except Exception as e:\n        logging.exception(\"Groq API or Snowflake insert failed\")\n        return {\"error\": str(e)}\n```\n\nThis code does the following:\n\n1.  First loops through the `code_diff` to construct the `code_diff_str`\n2.  Calls the `build_full_prompt` function to construct the prompt.\n3.  Constructs the `payload`, setting the ai model to be used.\n4.  Calls the `requests.post` function to send the request to the Groq API.\n5.  Parses the JSON response using `response.json()`.\n6.  Extracts the AI insights from the response and stores them in the database ([Chapter 9: Snowflake Data Storage](09_snowflake_data_storage.md)).\n7.  Handles potential errors, like invalid JSON responses.\n\n## Conclusion\n\nIn this chapter, we explored the Groq API Integration. We learned about the key concepts, how to use the Groq API to generate AI insights, and how it works internally. We saw how the Groq API helps us understand code changes and generate useful summaries, code quality assessments, and documentation suggestions.\n\nNow that we can use AI to generate insights, let's move on to the next chapter and see how we use RAG (Retrieval-Augmented Generation) to suggest updates to the project documentation in [Chapter 7: RAG (Retrieval-Augmented Generation) Documentation Updates](07_rag__retrieval_augmented_generation__documentation_updates.md).\n\n---\n# Chapter 7: RAG (Retrieval-Augmented Generation) Documentation Updates\n\nIn [Chapter 6: Groq API Integration](06_groq_api_integration.md), we learned how to use the Groq API to generate insights about code changes. But what about the *documentation*? How do we keep it up-to-date when code changes are made?\n\nThat's where **RAG (Retrieval-Augmented Generation) Documentation Updates** comes in!\n\nImagine you're reading a project's documentation, and it's completely out of sync with the current code. Frustrating, right? RAG helps solve this. It's like having a helpful assistant who automatically finds the relevant sections in your existing documentation and suggests updates based on new code changes.\n\n**Central Use Case:** When a developer adds a new function, we want `CodeRoast` to automatically find the relevant part of the project documentation and suggest adding a description of the new function. This keeps our documentation fresh and useful!\n\n## Key Concepts\n\nLet's break down the key concepts behind RAG Documentation Updates:\n\n1.  **Retrieval:** This is the process of finding the relevant parts of the existing documentation that are related to the code changes. Think of it like searching a library for books that are related to a specific topic. In CodeRoast, we load a `project_docs.txt` and then retrieve the most relevant parts.\n\n2.  **Augmentation:** This is the process of combining the retrieved documentation with the code changes to create a prompt for the AI model. It's like giving the AI model the context it needs to understand the code changes and suggest appropriate documentation updates.\n\n3.  **Generation:** This is the process of using the AI model to generate the suggested documentation updates. It's like asking the AI model to write a new section for the documentation based on the code changes and the existing documentation. We call the Groq API to do the actual writing.\n\n4.  **Markdown:** Markdown is a simple way to format text. It's used to write the project documentation. It's like using a simple set of rules to make the documentation look nice.\n\n## How to Use RAG: A Simple Example\n\nLet's imagine we have a project with a simple `project_docs.txt` file:\n\n```\n# My Project Documentation\n\nThis project is all about doing cool stuff.\n\n## Functions\n\n### add(x, y)\nAdds two numbers together.\n```\n\nAnd we add a new function:\n\n```python\ndef subtract(x, y):\n  \"\"\"Subtracts two numbers.\"\"\"\n  return x - y\n```\n\nHere's how we can use RAG to suggest adding documentation for this new function (simplified):\n\n```python\nimport os\nimport requests\nimport json\n\n# Your Groq API key\nGROQ_API_KEY = \"YOUR_GROQ_API_KEY\" # Replace with your actual key\nGROQ_ENDPOINT = \"https://api.groq.com/openai/v1/chat/completions\"\n\n# The existing documentation\nexisting_docs = \"\"\"\n# My Project Documentation\n\nThis project is all about doing cool stuff.\n\n## Functions\n\n### add(x, y)\nAdds two numbers together.\n\"\"\"\n\n# The new function\nnew_function = \"\"\"\ndef subtract(x, y):\n  \\\"\\\"\\\"Subtracts two numbers.\\\"\\\"\\\"\n  return x - y\n\"\"\"\n\n# The prompt\nprompt = f\"\"\"\nYou are an expert technical writer. Update the following documentation to include this new function:\n\n{new_function}\n\nHere is the existing documentation:\n\n{existing_docs}\n\nRespond in Markdown format.\n\"\"\"\n\n# Construct the payload\npayload = {\n    \"model\": \"llama3-70b-8192\", # You should check with Groq for the latest model\n    \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n}\n\n# Set the headers\nheaders = {\n    \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\n# Make the request\nresponse = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n\n# Parse the response\nresult = response.json()\n\n# Extract the summary\ndocumentation_update = result[\"choices\"][0][\"message\"][\"content\"].strip()\n\n# Print the summary\nprint(documentation_update)\n```\n\nThis simplified code does the following:\n\n1.  **Sets up the API key and endpoint:** It defines the `GROQ_API_KEY` and `GROQ_ENDPOINT` variables.  **Important:** Replace `\"YOUR_GROQ_API_KEY\"` with your actual Groq API key.\n2.  **Defines the existing documentation and the new function:** It defines the `existing_docs` and `new_function` variables.\n3.  **Constructs the prompt:** It defines the `prompt` variable, which tells the Groq API what we want it to do.\n4.  **Calls the Groq API:** It makes a request to the Groq API to generate the documentation update.\n5.  **Prints the documentation update:** It prints the documentation update to the console.\n\n**Example Output:**\n\n```markdown\n### subtract(x, y)\nSubtracts two numbers.\n```\n\nThis output suggests adding a new section to the `project_docs.txt` file for the `subtract` function.\n\n## Under the Hood: How RAG Works\n\nLet's take a closer look at how RAG works internally.\n\nHere's a sequence diagram illustrating the flow:\n\n```mermaid\nsequenceDiagram\n    participant BE as Backend (FastAPI)\n    participant Docs as Existing Documentation\n    participant Code as Code Changes\n    participant Groq as Groq API\n\n    BE->>Docs: Retrieve Relevant Sections\n    BE->>Code: Augment with Code Changes\n    BE->>Groq: POST /openai/v1/chat/completions (Prompt + Context)\n    activate Groq\n    Groq->>Groq: AI Model Generation\n    Groq-->>BE: Markdown (Documentation Update)\n    deactivate Groq\n    BE->>BE: Process Results\n```\n\n1.  The **Backend (FastAPI)** retrieves the relevant sections from the **Existing Documentation** (`project_docs.txt`). In our basic example, we just load the whole document.\n2.  The **Backend (FastAPI)** augments the retrieved sections with the **Code Changes** to create a prompt.\n3.  The **Backend (FastAPI)** sends a POST request to the **Groq API** with the prompt.\n4.  The **Groq API** uses its AI models to generate the documentation update.\n5.  The **Groq API** returns the documentation update in Markdown format to the **Backend (FastAPI)**.\n6.  The **Backend (FastAPI)** processes the results and stores them in the database or displays them in the frontend.\n\n### Diving into the Code\n\nLet's look at the code that implements the RAG documentation update. This code can be found in `backend/ai_analyse.py` and `backend/prReview.py` inside the `update_documentation_with_rag` function.\n\n```python\ndef update_documentation_with_rag(pr_description: str, code_diff: str, docs_path=\"backend/project_docs.txt\"):\n    \"\"\"\n    Performs a simple RAG-based call to Groq to update documentation.\n    \"\"\"\n    # Step 1: Load the existing documentation\n    try:\n        with open(docs_path, \"r\") as f:\n            existing_docs = f.read()\n    except FileNotFoundError:\n        logging.error(f\"[\u274c] Documentation file not found at {docs_path}\")\n        return {\"error\": \"Documentation file not found.\"}\n\n    # Step 2: Very basic RAG - extract top paragraphs that contain overlap\n    paragraphs = existing_docs.split(\"\\n\\n\")\n    keywords = pr_description.lower().split()\n    ranked_paragraphs = sorted(paragraphs, key=lambda para: sum(word in para.lower() for word in keywords), reverse=True)\n    context = \"\\n\\n\".join(ranked_paragraphs[:3])  # Top 3 paragraphs as \"retrieved context\"\n\n    # Step 3: Build the prompt\n    doc_prompt = f\"\"\"\nYou are an expert technical writer and code documentation assistant.\n\nA developer has made the following changes to the codebase:\n\n## PR Description:\n{pr_description}\n\n## Code Diff:\n{code_diff}\n\n## Relevant Existing Documentation:\n{context}\n\nPlease provide additions or updates to the documentation based on the above code change. \nFormat your response in **Markdown**, and only include sections that should be added or updated. \nIf the change doesn't require doc updates, say so.\n\"\"\"\n\n    # Step 4: Call Groq API\n    payload = {\n        \"model\": \"llama3-70b-8192\",\n        \"messages\": [{\"role\": \"user\", \"content\": doc_prompt}]\n    }\n\n    try:\n        # The Groq API code from before\n        response = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n        response.raise_for_status()\n        result = response.json()\n        return {\n            \"doc_update\": result[\"choices\"][0][\"message\"][\"content\"].strip()\n        }\n\n    except Exception as e:\n        logging.exception(\"Groq API call for documentation update failed\")\n        return {\"error\": str(e)}\n```\n\nThis code does the following:\n\n1.  **Loads the existing documentation:**\n    *   `with open(docs_path, \"r\") as f: existing_docs = f.read()`: This code reads the contents of the `project_docs.txt` file into the `existing_docs` variable.  It also has error handling in case the documentation is not found.\n2.  **Retrieves relevant sections:**\n    *   It splits the documentation into paragraphs and ranks them based on how many keywords from the pull request description they contain.\n    *   This is a simplified approach to RAG. More sophisticated techniques could involve vector embeddings and similarity search.\n    *   `context = \"\\n\\n\".join(ranked_paragraphs[:3])`: This joins the top 3 paragraphs to get the context.\n3.  **Builds the prompt:**\n    *   It constructs a prompt that includes the pull request description, the code diff, and the relevant sections from the existing documentation.\n    *   It tells the AI model to provide additions or updates to the documentation in Markdown format.\n4.  **Calls the Groq API:**\n    *   This part is the same as in the [Groq API Integration](06_groq_api_integration.md) chapter.  It sends the prompt to the Groq API and gets the documentation update.\n\n## Conclusion\n\nIn this chapter, we explored RAG (Retrieval-Augmented Generation) Documentation Updates. We learned about the key concepts, how to use RAG to suggest documentation updates, and how it works internally. We saw how RAG helps keep our project documentation up-to-date with the latest code changes.\n\nNow that we can generate documentation updates, let's move on to the next chapter and see how we post those updates (and other AI insights) as comments on GitHub pull requests in [Chapter 8: PR Review Comment Posting (GitHub)](08_pr_review_comment_posting__github_.md).\n\n\nRelevant Code Snippets:\n--- File: README.md ---\n# \ud83e\udd16 Code Roast : An AI-Powered Pull Request Review System\n\n> An end-to-end intelligent platform for automated code review, documentation updates, and pull request analysis \u2014 powered by LLMs, Tree-sitter, and Snowflake.\n\n---\n\n## \ud83d\ude80 Overview\n\nThis system integrates with GitHub to automatically:\n- Analyze pull requests using AI (via [Groq](https://groq.com))\n- Parse and understand diffs using [Tree-sitter](https://tree-sitter.github.io/)\n- Perform RAG (Retrieval-Augmented Generation) to update project documentation\n- Store all insights in **Snowflake** for analytics\n- Display all PRs, diffs, AI feedback, and doc suggestions via a beautiful **Web App**\n\nWhether you're an engineering team lead, reviewer, or contributor \u2014 you get fast, reliable, and insightful AI-powered review feedback at every step.\n\n---\n\n## \ud83e\udde0 Features\n\n- \u2705 **AI PR Summary**  \n  Instant analysis of pull request purpose, syntax/style/functionality, and merge confidence.\n\n- \ud83d\udcdd **Automatic Docstring Suggestions**  \n  Extract or generate missing/updated docstrings for new or changed functions.\n\n- \ud83d\udcd8 **RAG-based Documentation Updates**  \n  Contextual retrieval from `project_docs.txt` with AI-generated Markdown additions.\n\n- \ud83d\udcc4 **Code Diff Parsing**  \n  Tree-sitter-based code structure diffing to isolate changed functions only.\n\n- \ud83d\udcac **GitHub Comment Posting**  \n  Summaries are posted as comments directly on PRs, no need to open another tool.\n\n- \ud83d\udcca **Snowflake Integration**  \n  All AI-generated insights and PR metadata stored for reporting, audit, and search.\n\n- \ud83c\udf10 **Web App Dashboard**  \n  Browse all PRs, view diffs, AI reviews, and doc updates in a single UI.\n\n---\n\n## \ud83e\uddf1 Tech Stack\n\n| Layer              | Tech Used                                      |\n|--------------------|-----------------------------------------------|\n| **AI Models**       | [Groq API (LLaMA 3)](https://groq.com)        |\n| **Diff Parsing**    | [Tree-sitter](https://tree-sitter.github.io/) |\n| **Backend API**     | [FastAPI](https://fastapi.tiangolo.com)       |\n| **Git Integration** | GitHub App + `PyGithub`                       |\n| **RAG Engine**      | Basic keyword-matching from local docs        |\n| **Database**        | [Snowflake](https://www.snowflake.com)        |\n| **Frontend**        | HTML, JS, CSS (Markdown + Diff Viewers)       |\n\n---\n\n## \u2699\ufe0f How It Works\n\n1. \ud83d\udd14 **GitHub Webhook Trigger**\n   - On PR open/update \u2192 sends payload to FastAPI backend\n\n2. \ud83e\udde0 **AI Review Processing**\n   - Extracts code diffs, changed functions\n   - Groq generates AI summary, confidence rating, and docstrings\n\n3. \ud83d\udcd8 **Documentation Updates**\n   - Local project docs indexed via simple RAG\n   - Groq suggests Markdown additions\n\n4. \ud83d\udcbe **Data Storage**\n   - All results are saved in Snowflake for dashboard & analytics\n\n5. \ud83c\udf10 **Web App UI**\n   - Explore PRs, view diffs, AI reviews, and doc updates\n\n\n--- File: backend/prReview.py ---\nimport os, re\nfrom github import Auth\nfrom github import Github, GithubIntegration\nfrom github.GithubException import GithubException\nfrom tree_sitter import Language, Parser\nimport tree_sitter_python as tspython\nimport tree_sitter_javascript as tsjavascript\nimport tree_sitter_cpp as tscpp\nimport requests\nimport json\nimport logging\nimport snowflake.connector\nimport random\nimport datetime\n\n\nGROQ_API_KEY = \"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\"\nGROQ_ENDPOINT = \"https://api.groq.com/openai/v1/chat/completions\"\n\ndef authenticate_github(app_id: int, installation_id, private_key: str):\n    gi = GithubIntegration(integration_id=app_id, private_key=private_key)\n    g = gi.get_github_for_installation(installation_id)\n    return g\n\ndef get_pull_request(g, repo_name: str, pr_number: int):\n    repo = g.get_repo(repo_name)\n    pr = repo.get_pull(pr_number)\n    return pr\n\ndef parse_changed_lines(patch: str):\n    added_or_modified_lines = set()\n    deleted_lines = set()\n    for line in patch.splitlines():\n        if line.startswith(\"@@\"):\n            match = re.findall(r\"@@ -(\\d+)(?:,(\\d+))? \\+(\\d+)(?:,(\\d+))? @@\", line)\n            if match:\n                old_start, old_count, new_start, new_count = match[0]\n\n                old_start = int(old_start)\n                old_count = int(old_count or 1)\n                for l in range(old_start, old_start + old_count):\n                    deleted_lines.add(l)\n\n                new_start = int(new_start)\n                new_count = int(new_count or 1)\n                for l in range(new_start, new_start + new_count):\n                    added_or_modified_lines.add(l)\n\n    return {\n        \"added_or_modified_lines\": added_or_modified_lines,\n        \"deleted_lines\": deleted_lines\n    }\n\ndef get_file_contents(g, repo_name: str, file_path: str, commit_sha: str):\n    repo = g.get_repo(repo_name)\n    try:\n        contents = repo.get_contents(file_path, ref=commit_sha)\n        return contents.decoded_content.decode()\n    except GithubException as e:\n        if e.status == 404:\n            return \"\"\n        else:\n            raise e\n\ndef get_lines_changed(pr):\n    files = pr.get_files()\n    file_changes = {}\n    for file in files:\n        if file.status == \"removed\":\n            continue\n        file_changes[file.filename] = parse_changed_lines(file.patch)\n\n    return file_changes\n\ndef get_paraser(language_name):\n\n    PY_LANGUAGE = Language(tspython.language())\n    JS_LANGUAGE = Language(tsjavascript.language())\n    CPP_LANGUAGE = Language(tscpp.language())\n\n    LANGUAGES = {\n        'python': PY_LANGUAGE,\n        'javascript': JS_LANGUAGE,\n        'cpp': CPP_LANGUAGE,\n    }\n    parser = Parser(LANGUAGES[language_name])\n    return parser\n\ndef extract_functions(code, language_name, changed_lines):\n    parser = get_paraser(language_name)\n    tree = parser.parse(bytes(code, \"utf8\"))\n    root_node = tree.root_node\n\n    functions = []\n\n    def node_within_lines(node):\n        start_line = node.start_point[0] + 1\n        end_line = node.end_point[0] + 1\n        return any([start_line <= line <= end_line for line in changed_lines])\n    \n    def traverse(node):\n        if language_name == \"python\" and node.type == \"function_definition\":\n            if node_within_lines(node):\n                func_name_node = node.child_by_field_name('name')\n                func_name = func_name_node.text.decode()\n                functions.append({'name': func_name, 'code': node.text.decode()})\n        elif language_name == 'javascript' and node.type in ['function_declaration', 'method_definition', 'arrow_function']:\n            if node_within_lines(node):\n                func_name_node = node.child_by_field_name('name')\n                func_name = func_name_node.text.decode()\n                functions.append({'name': func_name, 'code': node.text.decode()})\n        elif language_name == 'cpp' and node.type in ['function_definition']:\n            if node_within_lines(node):\n                func_name_node = node.child_by_field_name('name')\n                func_name = func_name_node.text.decode()\n                functions.append({'name': func_name, 'code': node.text.decode()})\n        for child in node.children:\n            traverse(child)\n\n    traverse(root_node)\n    return functions\n\nheaders = {\n    \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\n# Snowflake setup\nconn = snowflake.connector.connect(\n    user=\"raghavg332\",\n    password=\"Qa29Kh4MptfGHEW\",\n    account=\"KXCIVVH-LL27432\",\n    warehouse=\"COMPUTE_WH\",\n    database=\"PR_DASHBOARD\",\n    schema=\"PUBLIC\"\n)\ncur = conn.cursor()\n\ndef update_documentation_with_rag(pr_description: str, code_diff: str, docs_path=\"backend/project_docs.txt\"):\n    \"\"\"\n    Performs a simple RAG-based call to Groq to update documentation.\n    \"\"\"\n    # Step 1: Load the existing documentation\n    try:\n        with open(docs_path, \"r\") as f:\n            existing_docs = f.read()\n    except FileNotFoundError:\n        logging.error(f\"[\u274c] Documentation file not found at {docs_path}\")\n        return {\"error\": \"Documentation file not found.\"}\n\n    # Step 2: Very basic RAG - extract top paragraphs that contain overlap\n    paragraphs = existing_docs.split(\"\\n\\n\")\n    keywords = pr_description.lower().split()\n    ranked_paragraphs = sorted(paragraphs, key=lambda para: sum(word in para.lower() for word in keywords), reverse=True)\n    context = \"\\n\\n\".join(ranked_paragraphs[:3])  # Top 3 paragraphs as \"retrieved context\"\n\n    # Step 3: Build the prompt\n    doc_prompt = f\"\"\"\nYou are an expert technical writer and code documentation assistant.\n\nA developer has made the following changes to the codebase:\n\n## PR Description:\n{pr_description}\n\n## Code Diff:\n{code_diff}\n\n## Relevant Existing Documentation:\n{context}\n\nPlease provide additions or updates to the documentation based on the above code change. \nFormat your response in **Markdown**, and only include sections that should be added or updated. \nIf the change doesn't require doc updates, say so.\n\"\"\"\n\n    # Step 4: Call Groq API\n    payload = {\n        \"model\": \"llama3-70b-8192\",\n        \"messages\": [{\"role\": \"user\", \"content\": doc_prompt}]\n    }\n\n    try:\n        response = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n        response.raise_for_status()\n        result = response.json()\n        return {\n            \"doc_update\": result[\"choices\"][0][\"message\"][\"content\"].strip()\n        }\n\n    except Exception as e:\n        logging.exception(\"Groq API call for documentation update failed\")\n        return {\"error\": str(e)}\n\n\ndef build_full_prompt(pr_description: str, code_diff_str: str) -> str:\n    return f\"\"\"\nYou are an AI code reviewer and documentation assistant.\n\nA developer submitted the following pull request. Please do the following:\n1. \ud83d\udd0d Summarize the overall purpose of the PR. Be a little detailed\n2. \u2705 Review the code for:\n   - Syntax issues in Python (Make up some Issue)\n   - Style (PEP8)\n   - Functionality correctness (does it match the description?)\n3. Based on the review, provide a merge confidence score from 1 to 10.\n4. \ud83d\udcdd Extract or suggest updated docstrings for any changed or added functions/classes.\n5. The Code Diff is made up of the old and new version of the code, if any of the field is empty it means, the function is either added or deleted respectively.\n\n## PR Description:\n{pr_description}\n\n## Code Diff:\n{code_diff_str}\n\nRespond ONLY in this JSON format without any additional text:\n\n{{\n  \"ai_summary\": \"...\",\n  \"merge_confidence\": \"1 - 10\",\n  \"code_quality\": {{\n    \"syntax_check\": \"...\",\n    \"style_check\": \"...\",\n    \"functionality_check\": \"...\",\n    \"final_rating\": \"Excellent / Good / Needs Work / Critical Issues\"\n  }}\n}}\n\"\"\"\n\ndef review_and_store_pr(pr_description: str, code_diff: dict, pr_title: str, pr_author: str, pr_status: str):\n    code_diff_str = \"\"\n    for file in list(code_diff.keys()):\n        code_diff_str += f\"File: {file}\\n\\n\"\n        diff = code_diff[file]\n\n        for func_name in list(diff.keys()):\n            old_code = diff[func_name]['old_code']\n            new_code = diff[func_name]['new_code']\n            if old_code is None:\n                old_code = \"\"\n            if new_code is None:\n                new_code = \"\"\n            code_diff_str += f\"Function: {func_name}\\n\\nOld Code:\\n{old_code}\\n\\nNew Code:\\n{new_code}\"\n    docstring = update_documentation_with_rag(pr_description, code_diff)\n    pr_id = random.randint(1000, 9999)\n    prompt = build_full_prompt(pr_description, code_diff_str)\n\n    payload = {\n        \"model\": \"llama3-70b-8192\",\n        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n    }\n\n    try:\n        response = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n        response.raise_for_status()\n        result = response.json()\n\n        try:\n            content = result[\"choices\"][0][\"message\"][\"content\"].strip()\n            review_data = json.loads(content)\n\n            # Flatten the code quality object for easier insert\n            code_quality_str = (\n                f\"Syntax: {review_data['code_quality']['syntax_check']}\\n\"\n                f\"Style: {review_data['code_quality']['style_check']}\\n\"\n                f\"Functionality: {review_data['code_quality']['functionality_check']}\\n\"\n                f\"Rating: {review_data['code_quality']['final_rating']}\"\n            )\n            cur.execute(f\"\"\"\n                INSERT INTO PULL_REQUESTS (\n                    ID, TITLE, AUTHOR, STATUS, CREATED_AT, UPDATED_AT\n                )\n                VALUES (%s, %s, %s, %s, %s, %s);\n            \"\"\", (\n                pr_id,\n                pr_title,\n                pr_author,\n                pr_status,\n                datetime.datetime.now(),\n                datetime.datetime.now()\n            ))\n            conn.commit()\n\n            # Insert into Snowflake\n            cur.execute(f\"\"\"\n                INSERT INTO PR_ANALYSIS (\n                    PR_ID, AI_SUMMARY, MERGE_CONFIDENCE, CODE_QUALITY,\n                    PR_DESCRIPTION, DOCSTRINGS, DIFF\n                )\n                VALUES (%s, %s, %s, %s, %s, %s, %s);\n            \"\"\", (\n                pr_id,\n                review_data[\"ai_summary\"],\n                review_data[\"merge_confidence\"],\n                code_quality_str,\n                pr_description,\n                docstring['doc_update'],\n                code_diff_str\n            ))\n\n            conn.commit()\n\n            logging.info(f\"[\u2705] PR #{pr_id} analysis saved to Snowflake.\")\n            print(f\"[\u2705] PR #{pr_id} analysis saved to Snowflake.\")\n            return review_data\n\n        except json.JSONDecodeError:\n            logging.error(\"[\u274c] Groq returned non-JSON output.\")\n            print(\"[\u274c] Groq returned non-JSON output.\")\n            return {\"error\": \"Invalid JSON from Groq\"}\n\n    except Exception as e:\n        logging.exception(\"Groq API or Snowflake insert failed\")\n        return {\"error\": str(e)}\n\ndef process_pull_request(g, repo_name, pr_number, sender=None):\n    repo = g.get_repo(repo_name)\n    pr = repo.get_pull(pr_number)\n    pr_description = pr.title\n    pr_title = pr.title\n    pr_author = pr.user.login\n    pr_status = pr.state\n    code_diff = get_lines_changed(pr)\n    file_diff_func = {}\n    for file in list(code_diff.keys()):\n        diff_func = {}\n        new_code = get_file_contents(g, repo_name, file, pr.head.sha)\n        old_code = get_file_contents(g, repo_name, file, pr.base.sha)\n        added_or_modified_functions_after = extract_functions(new_code, \"python\", code_diff[file]['added_or_modified_lines'])\n        deleted_functions_before = extract_functions(old_code, \"python\", code_diff[file]['deleted_lines'])\n        old_functions_all = extract_functions(old_code, \"python\", set(range(1, len(old_code.split(\"\\n\")))))\n        new_functions_all = extract_functions(new_code, \"python\", set(range(1, len(new_code.split(\"\\n\")))))\n        for func in added_or_modified_functions_after:\n            matched_old_function = next((f for f in old_functions_all if f['name'] ==  func['name']), None)\n            diff_func[func['name']] = {\n                \"new_code\": func['code'],\n                \"old_code\": matched_old_function['code'] if matched_old_function else None\n            }\n        \n        for func in deleted_functions_before:\n            matched_new_function = next((f for f in new_functions_all if f['name'] ==  func['name']), None)\n            diff_func[func['name']] = {\n                \"new_code\": matched_new_function['code'] if matched_new_function else None,\n                \"old_code\": func['code']\n            }\n        \n        file_diff_func[file] = diff_func\n    \n    # Get the review data and post a comment\n    review_data = review_and_store_pr(pr_description, file_diff_func, pr_title, pr_author, pr_status)\n    \n    # Post the review as a comment on the PR\n    post_review_comment(g, repo_name, pr_number, review_data)\n\n\ndef post_review_comment(g, repo_name, pr_number, review_data):\n    \"\"\"\n    Posts the AI review as a comment on the pull request.\n    \n    Args:\n        g: Authenticated GitHub instance\n        repo_name: Repository name (owner/repo)\n        pr_number: Pull request number\n        review_data: The AI review data dictionary\n    \"\"\"\n    try:\n        # Get the repo and PR objects\n        repo = g.get_repo(repo_name)\n        pr = repo.get_pull(pr_number)\n        \n        # Format the comment body\n        comment_body = f\"\"\"\n## \ud83e\udd16 AI Code Review\n\n### Summary\n{review_data.get('ai_summary', 'No summary available')}\n\n### Merge Confidence: **{review_data.get('merge_confidence', 'Unknown')}**\n\n### Code Quality Review\n- **Syntax:** {review_data.get('code_quality', {}).get('syntax_check', 'Not analyzed')}\n- **Style:** {review_data.get('code_quality', {}).get('style_check', 'Not analyzed')}\n- **Functionality:** {review_data.get('code_quality', {}).get('functionality_check', 'Not analyzed')}\n- **Overall Rating:** {review_data.get('code_quality', {}).get('final_rating', 'Not rated')}\n\n---\n*This review was automatically generated by AI.*\n\"\"\"\n        \n        # Create the comment on the PR\n        pr.create_issue_comment(comment_body)\n        logging.info(f\"[\u2705] Posted AI review comment on PR #{pr_number} in {repo_name}\")\n        print(f\"[\u2705] Posted AI review comment on PR #{pr_number} in {repo_name}\")\n        return True\n    except Exception as e:\n        logging.exception(f\"[\u274c] Failed to post comment on PR #{pr_number}: {str(e)}\")\n        print(f\"[\u274c] Failed to post comment on PR #{pr_number}: {str(e)}\")\n        return False\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 8: PR Review Comment Posting (GitHub)`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "# Chapter 8: PR Review Comment Posting (GitHub)\n\nIn [Chapter 7: RAG (Retrieval-Augmented Generation) Documentation Updates](07_rag__retrieval_augmented_generation__documentation_updates.md), we learned how to generate AI insights and documentation updates. But those insights are only helpful if developers *see* them!\n\nThat's where **PR Review Comment Posting (GitHub)** comes in!\n\nImagine `CodeRoast` analyzing a pull request and generating amazing insights, but then keeping them secret. Useless, right? This component solves that problem. It's like an automated messenger that delivers the AI's review directly to the developer where they are already working \u2013 on the pull request in GitHub.\n\n**Central Use Case:** After `CodeRoast` analyzes a pull request, we want it to automatically post a comment on the pull request with a summary of the AI's findings. This gives developers immediate feedback on their code.\n\n## Key Concepts\n\nLet's break down the key concepts behind posting comments on GitHub:\n\n1.  **GitHub API:** Just like we use the Groq API to talk to Groq's AI, we use the GitHub API to talk to GitHub. It lets us do things like read pull requests, create comments, and update issues. Think of it as the phone line to GitHub.\n\n2.  **Authentication:** To use the GitHub API, we need to prove who we are. We do this with authentication, which involves providing a token or other credentials. It is like showing your ID to enter a building.\n\n3.  **Pull Request Object:** In the GitHub API, a pull request is represented as an object. This object has properties like the title, description, author, and comments. Think of it as a digital representation of the pull request.\n\n4.  **Comment Object:** Similarly, a comment is also represented as an object. This object has properties like the author, body, and creation date. It's like a digital sticky note attached to the pull request.\n\n## How to Use the Comment Posting: A Simple Example\n\nLet's imagine we have the AI analysis results and we want to post a simplified comment on a pull request.\n\nHere's a simplified version of how we can post a comment (assuming we already have the GitHub object and review data):\n\n```python\nfrom github import Github\n\n# Your GitHub Personal Access Token (PAT)\nGITHUB_TOKEN = \"YOUR_GITHUB_TOKEN\" # Replace with your actual token\n\n# Authenticate with GitHub\ng = Github(GITHUB_TOKEN)\n\n# Repository and Pull Request Details\nREPO_NAME = \"your-org/your-repo\" # Replace with your repo\nPR_NUMBER = 123 # Replace with your PR number\n\n# AI Review Data (simplified)\nreview_data = {\n    \"ai_summary\": \"This PR adds a new feature.\",\n    \"merge_confidence\": \"High\"\n}\n\ntry:\n    # Get the repo and PR objects\n    repo = g.get_repo(REPO_NAME)\n    pr = repo.get_pull(PR_NUMBER)\n\n    # Format the comment body\n    comment_body = f\"\"\"\n## \ud83e\udd16 AI Review\n\n### Summary\n{review_data['ai_summary']}\n\n### Merge Confidence: **{review_data['merge_confidence']}**\n\"\"\"\n\n    # Create the comment on the PR\n    pr.create_issue_comment(comment_body)\n    print(\"Successfully posted the comment!\")\n\nexcept Exception as e:\n    print(f\"Failed to post comment: {e}\")\n```\n\nThis simplified code does the following:\n\n1.  **Authenticates with GitHub:** It uses your GitHub Personal Access Token (PAT) to authenticate with the GitHub API. **Important:** Replace `\"YOUR_GITHUB_TOKEN\"` with your actual PAT. You can generate a PAT in your GitHub settings. Be sure to grant the necessary permissions (e.g., `repo` scope).\n2.  **Gets the repository and pull request objects:** It uses the GitHub API to get the repository and pull request objects based on the repository name and pull request number.\n3.  **Formats the comment body:** It creates a string containing the comment body, including the AI summary and merge confidence.\n4.  **Posts the comment:** It uses the `pr.create_issue_comment` method to post the comment on the pull request.\n\n**How this will look on GitHub:**\n\nAfter running this code, you'll see a new comment on the specified pull request in GitHub. The comment will contain the AI summary and merge confidence, formatted as Markdown.\n\n## Under the Hood: How the Comment Posting Works\n\nLet's take a closer look at how the comment posting works internally.\n\nHere's a sequence diagram illustrating the flow:\n\n```mermaid\nsequenceDiagram\n    participant BE as Backend (FastAPI)\n    participant GH as GitHub API\n\n    BE->>GH: Get Pull Request\n    activate GH\n    GH-->>BE: Pull Request Object\n    deactivate GH\n    BE->>BE: Format Comment Body\n    BE->>GH: Create Comment\n    activate GH\n    GH-->>BE: Success/Failure\n    deactivate GH\n```\n\n1.  The **Backend (FastAPI)** uses the GitHub API to get the pull request object.\n2.  The **GitHub API** returns the pull request object to the **Backend (FastAPI)**.\n3.  The **Backend (FastAPI)** formats the comment body using the AI review data.\n4.  The **Backend (FastAPI)** uses the GitHub API to create a comment on the pull request.\n5.  The **GitHub API** creates the comment and returns a success or failure message to the **Backend (FastAPI)**.\n\n### Diving into the Code\n\nLet's look at the relevant code from `backend/prReview.py`:\n\n```python\nfrom github import Github\nimport logging\n\ndef post_review_comment(g, repo_name, pr_number, review_data):\n    \"\"\"\n    Posts the AI review as a comment on the pull request.\n    \n    Args:\n        g: Authenticated GitHub instance\n        repo_name: Repository name (owner/repo)\n        pr_number: Pull request number\n        review_data: The AI review data dictionary\n    \"\"\"\n    try:\n        # Get the repo and PR objects\n        repo = g.get_repo(repo_name)\n        pr = repo.get_pull(pr_number)\n        \n        # Format the comment body\n        comment_body = f\"\"\"\n## \ud83e\udd16 AI Code Review\n\n### Summary\n{review_data.get('ai_summary', 'No summary available')}\n\n### Merge Confidence: **{review_data.get('merge_confidence', 'Unknown')}**\n\n### Code Quality Review\n- **Syntax:** {review_data.get('code_quality', {}).get('syntax_check', 'Not analyzed')}\n- **Style:** {review_data.get('code_quality', {}).get('style_check', 'Not analyzed')}\n- **Functionality:** {review_data.get('code_quality', {}).get('functionality_check', 'Not analyzed')}\n- **Overall Rating:** {review_data.get('code_quality', {}).get('final_rating', 'Not rated')}\n\n---\n*This review was automatically generated by AI.*\n\"\"\"\n        \n        # Create the comment on the PR\n        pr.create_issue_comment(comment_body)\n        logging.info(f\"[\u2705] Posted AI review comment on PR #{pr_number} in {repo_name}\")\n        print(f\"[\u2705] Posted AI review comment on PR #{pr_number} in {repo_name}\")\n        return True\n    except Exception as e:\n        logging.exception(f\"[\u274c] Failed to post comment on PR #{pr_number}: {str(e)}\")\n        print(f\"[\u274c] Failed to post comment on PR #{pr_number}: {str(e)}\")\n        return False\n```\n\nThis code does the following:\n\n1.  `repo = g.get_repo(repo_name)`: Gets the repository object from GitHub using the `repo_name`.  `g` is the authenticated Github instance.\n2.  `pr = repo.get_pull(pr_number)`: Gets the pull request object from the repository using the `pr_number`.\n3.  It formats the comment body using f-strings and the `review_data` dictionary. It uses `.get()` to safely access the values, providing default values if they are missing.\n4.  `pr.create_issue_comment(comment_body)`: Creates the comment on the pull request using the formatted `comment_body`.\n5.  It includes `try...except` block to handle potential errors.\n\n## Conclusion\n\nIn this chapter, we explored PR Review Comment Posting (GitHub). We learned about the key concepts, how to post comments on pull requests using the GitHub API, and how it works internally. We saw how this component delivers AI insights directly to developers, making the code review process more efficient.\n\nNow that we can post comments on GitHub, let's move on to the next chapter and see how we store all the AI insights and PR metadata in [Chapter 9: Snowflake Data Storage](09_snowflake_data_storage.md) for analytics.\n", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `CodeRoast` about the concept: \"Snowflake Data Storage\". This is Chapter 9.\n\nConcept Details:\n- Description:\nThis abstraction manages the storage of all AI-generated insights, PR metadata, and documentation updates in a Snowflake database. It's the project's memory, storing all the analysis results. Think of Snowflake as a giant filing cabinet that efficiently organizes and stores code review data for reporting, auditing, and search.\n\n\nComplete Tutorial Structure:\n1. [Frontend Web Application (React)](01_frontend_web_application__react_.md)\n2. [FastAPI Backend API](02_fastapi_backend_api.md)\n3. [GitHub Webhook Integration](03_github_webhook_integration.md)\n4. [Pull Request Analysis Pipeline](04_pull_request_analysis_pipeline.md)\n5. [Tree-sitter Based Code Parsing](05_tree_sitter_based_code_parsing.md)\n6. [Groq API Integration](06_groq_api_integration.md)\n7. [RAG (Retrieval-Augmented Generation) Documentation Updates](07_rag__retrieval_augmented_generation__documentation_updates.md)\n8. [PR Review Comment Posting (GitHub)](08_pr_review_comment_posting__github_.md)\n9. [Snowflake Data Storage](09_snowflake_data_storage.md)\n\nContext from previous chapters (summary):\n# Chapter 1: Frontend Web Application (React)\n\nWelcome to CodeRoast! In this chapter, we'll be diving into the heart of the user experience: the frontend web application built with React. Imagine you're a code reviewer, and you want to quickly see all the Pull Requests (PRs) needing your attention, along with AI-powered feedback. Our React frontend makes that happen!\n\nThis chapter will guide you through understanding how this frontend works and how it displays all the amazing insights generated by the rest of the CodeRoast system.\n\n## What is a Frontend, and Why React?\n\nThink of a frontend like the dashboard of a car. You see the speedometer, fuel gauge, and warning lights, but you don't need to know *how* the engine works to drive. Similarly, our frontend lets you interact with CodeRoast without worrying about the complex AI and analysis happening behind the scenes.\n\nReact is a popular JavaScript library for building user interfaces. It helps us create dynamic and interactive web pages. We chose React because it's efficient, organized, and makes it easier to manage complex user interfaces like our PR dashboard.\n\n## Key Concepts\n\nLet's break down the key concepts you'll encounter in our React frontend:\n\n1.  **Components:** React is all about components. Think of them as building blocks for your UI. Each component is responsible for rendering a specific part of the page, like a PR list, a diff view, or an analytics chart.\n\n2.  **JSX:** React uses JSX, which looks like HTML but is actually JavaScript. It allows us to write UI elements directly in our JavaScript code, making it more readable and maintainable.\n\n3.  **State:** State is data that can change over time, and when it changes, React automatically updates the UI. For example, the selected PR, the filter applied to the PR list, or the current theme (light/dark) are all parts of the component's state.\n\n4.  **Props:** Props (short for properties) are how we pass data from a parent component to a child component. Imagine passing a message from one React component to another.\n\n## The CodeRoast Frontend: A Tour\n\nLet's look at how these concepts come together in our `CodeRoast` frontend, using code snippets from `frontend/app.js`.\n\n### Displaying a List of Pull Requests\n\nFirst, let's see how we display the list of PRs. We fetch the PR data from our backend API (more on that in [Chapter 2: FastAPI Backend API](02_fastapi_backend_api.md)) and store it in the component's state.\n\n```javascript\nconst [prs, setPrs] = React.useState([]);\n\nReact.useEffect(() => {\n    fetch('http://localhost:8000/api/prs')\n        .then(res => res.json())\n        .then(data => {\n            setPrs(data);\n        });\n}, []);\n```\n\nThis code does the following:\n\n*   `const [prs, setPrs] = React.useState([]);`: This line initializes a state variable called `prs` to an empty array. `prs` will hold our pull request data. `setPrs` is a function used to update the `prs` data, and React will automatically refresh the user interface whenever you use `setPrs`.\n\n*   `React.useEffect(() => { ... }, []);`: This is a React Hook that runs code when the component is first rendered. The empty array `[]` as the second argument means this effect will only run once when the component mounts.\n\n*   `fetch('http://localhost:8000/api/prs')`: This line makes a request to our backend API to get the list of PRs.\n\n*   `.then(res => res.json())`: This converts the response from the API into a JavaScript object (JSON).\n\n*   `.then(data => { setPrs(data); })`: This updates the `prs` state with the data we received from the API.\n\nNext, we map over the `prs` array to create a list of PR items:\n\n```javascript\nconst prItems = filteredPRs.map(pr => \n    e('div', { \n        key: pr.id, \n        className: selectedPR && selectedPR.id === pr.id ? 'pr-item selected' : 'pr-item', \n        onClick: () => selectPR(pr) \n    }, [\n        e('div', {key: 'pr-header', className: 'pr-header'}, [\n            e('strong', {key: 'id'}, `#${pr.id}`),\n            e('span', {\n                key: 'status',\n                className: `pr-status status-${pr.status}`\n            }, pr.status)\n        ]),\n        e('div', {key: 'pr-title', className: 'pr-title'}, pr.title),\n        e('div', {key: 'pr-meta', className: 'pr-meta'}, [\n            e('span', {key: 'author'}, [\n                e('img', {key: 'avatar', className: 'avatar', src: `https://ui-avatars.com/api/?name=${pr.author}&size=24`, alt: pr.author}),\n                pr.author\n            ]),\n            e('span', {key: 'date', className: 'date'}, 'Updated: ' + (pr.updated_at || 'N/A'))\n        ])\n    ])\n);\n```\n\nThis code takes each `pr` from the `filteredPRs` array and:\n\n*   `e('div', { ... }, [ ... ])`: Creates a `div` element (and nested elements) for each PR, setting its `key`, `className`, and `onClick` properties. The `key` prop is important for React to efficiently update the list. `e` is `React.createElement`, a function that creates React elements. It is used because JSX must be transformed at compile time.\n\n*   `className`: Sets the CSS class for styling.\n\n*   `onClick`: Defines a function that will be called when the PR item is clicked.\n\n*   Within the div, `e(...)` is used to create other html elements to display the PR's `id`, `status`, `title`, `author` and `updated_at`.\n\n### Displaying the Diff\n\nOne of the most important features of `CodeRoast` is displaying the code diff. The `renderDiffSection` function takes the diff string and formats it for display:\n\n```javascript\nfunction renderDiffSection(diff) {\n    const diffLines = parseDiff(diff);\n\n    return React.createElement(\n        'pre',\n        { className: 'diff-content' },\n        diffLines.map((line, index) => {\n            const className =\n                line.type === 'old' ? 'diff-old-code diff-code-line' :\n                line.type === 'new' ? 'diff-new-code diff-code-line' :\n                'diff-context-line';\n\n            return React.createElement('div', { key: index, className }, line.content);\n        })\n    );\n}\n```\n\nHere's what's happening:\n\n*   `const diffLines = parseDiff(diff);`: It calls the `parseDiff` function to split the diff into individual lines and determine their type (old code, new code, or context). See below for details on `parseDiff`.\n\n*   The rest of the function constructs a `<pre>` element which is used to display preformatted text.  Within the `<pre>` tag, we loop through the lines in `diffLines` and create a `div` tag for each line. The `className` will style the background of each line in green, red, or default color, depending on the line type.\n\nThe `parseDiff` function itself is responsible for taking the raw diff output and turning it into an array of objects, each representing a line in the diff:\n\n```javascript\nfunction parseDiff(diff) {\n    const lines = diff.split('\\n');\n    const parsedLines = [];\n\n    let section = null;\n\n    for (let i = 0; i < lines.length; i++) {\n        const line = lines[i];\n\n        if (line.startsWith('File:') || line.startsWith('Function:')) {\n            parsedLines.push({ type: 'context', content: line });\n            section = null;\n        } else if (line.trim() === 'Old Code:') {\n            parsedLines.push({ type: 'context', content: 'Old Code:' });\n            section = 'old';\n\n            // Handle empty old code\n            if (lines[i + 1]?.trim() === 'New Code:') {\n                parsedLines.push({ type: 'old', content: '<NO PREVIOUS CODE>' });\n            }\n        } else if (line.trim() === 'New Code:') {\n            parsedLines.push({ type: 'context', content: 'New Code:' });\n            section = 'new';\n        } else if (section === 'old' || section === 'new') {\n            parsedLines.push({ type: section, content: line });\n        } else {\n            parsedLines.push({ type: 'context', content: line });\n        }\n    }\n\n    return parsedLines;\n}\n```\n\nThis function:\n\n*   Splits the diff string into lines.\n*   Iterates through each line, determining its type based on the content.\n*   Adds each line to the `parsedLines` array with a `type` property indicating whether it's \"old\", \"new\", or \"context\".\n*   The `type` property is later used to style each line in the diff.\n\n### Displaying Analytics\n\nFinally, the frontend also displays some helpful analytics about the PRs. This is handled by the `Analytics` component.\n\n```javascript\n// Analytics component to visualize PR data\nfunction Analytics({ prs }) {\n    // Count PRs by status\n    const statusCounts = prs.reduce((acc, pr) => {\n        acc[pr.status] = (acc[pr.status] || 0) + 1;\n        return acc;\n    }, {});\n\n    // Count PRs by author\n    const authorCounts = prs.reduce((acc, pr) => {\n        acc[pr.author] = (acc[pr.author] || 0) + 1;\n        return acc;\n    }, {});\n\n    // Calculate PRs over time (by month)\n    const prsByMonth = prs.reduce((acc, pr) => {\n        const date = new Date(pr.created_at);\n        const monthYear = date.toLocaleString('default', { month: 'short', year: 'numeric' });\n        acc[monthYear] = (acc[monthYear] || 0) + 1;\n        return acc;\n    }, {});\n\n    // Create data arrays for charts\n    const statusData = Object.entries(statusCounts).map(([status, count]) => ({\n        status,\n        count,\n        color: status === 'Open' ? '#17a2b8' : \n               status === 'Merged' ? '#28a745' : \n               status === 'Closed' ? '#dc3545' : '#6c757d'\n    }));\n\n    const authorData = Object.entries(authorCounts)\n        .sort((a, b) => b[1] - a[1])\n        .slice(0, 5)\n        .map(([author, count]) => ({ author, count }));\n\n    const timelineData = Object.entries(prsByMonth)\n        .sort((a, b) => new Date(a[0]) - new Date(b[0]))\n        .map(([month, count]) => ({ month, count }));\n\n    return e('div', { className: 'analytics-container' }, [\n        e('h2', { key: 'title', className: 'analytics-title' }, 'Pull Request Analytics'),\n        // ... (rest of the analytics display code)\n    ]);\n}\n```\n\nThis component:\n\n*   Takes the `prs` array as a prop.\n*   Calculates statistics like the number of PRs by status, author, and month.\n*   Formats the data into arrays suitable for rendering charts and summaries.\n*   Renders the analytics using divs with appropriate CSS classes.\n\n## Under the Hood: Requesting PR Data\n\nLet's take a simplified look at what happens when the frontend requests PR data from the backend.\n\n```mermaid\nsequenceDiagram\n    participant FE as Frontend (React App)\n    participant BE as Backend (FastAPI)\n    participant DB as Snowflake Database\n\n    FE->>BE: GET /api/prs\n    activate BE\n    BE->>DB: Query PR Data\n    activate DB\n    DB-->>BE: PR Data (JSON)\n    deactivate DB\n    BE-->>FE: PR Data (JSON)\n    deactivate BE\n    FE->>FE: Render PR List\n```\n\n1.  The **Frontend (React App)** initiates a GET request to the `/api/prs` endpoint on the **Backend (FastAPI)**.\n2.  The **Backend (FastAPI)** receives the request and queries the **Snowflake Database** for PR data. (We'll see this in [Chapter 9: Snowflake Data Storage](09_snowflake_data_storage.md)).\n3.  The **Snowflake Database** returns the PR data to the **Backend (FastAPI)** in JSON format.\n4.  The **Backend (FastAPI)** sends the PR data back to the **Frontend (React App)** in JSON format.\n5.  The **Frontend (React App)** receives the data and renders the list of pull requests.\n\nThe actual fetching of data happens inside of the `React.useEffect` hook we saw before.\n\n```javascript\nReact.useEffect(() => {\n    fetch('http://localhost:8000/api/prs')\n        .then(res => res.json())\n        .then(data => {\n            setPrs(data);\n        });\n}, []);\n```\n\nThis hook ensures that the PR data is fetched when the component is first rendered and updates the component's state with the fetched data.\n\n## Conclusion\n\nIn this chapter, we explored the `CodeRoast` frontend, built with React. We learned about components, JSX, state, props, and how they're used to display PR lists, diffs, and analytics. We also took a peek under the hood to see how the frontend interacts with the backend to fetch data.\n\nNow that you have a basic understanding of the frontend, let's move on to the next chapter, where we'll dive into the backend API built with FastAPI: [Chapter 2: FastAPI Backend API](02_fastapi_backend_api.md).\n\n---\n# Chapter 2: FastAPI Backend API\n\nWelcome back to CodeRoast! In [Chapter 1: Frontend Web Application (React)](01_frontend_web_application__react_.md), we built a snazzy frontend that displays pull requests, diffs, and AI insights. But that frontend needs someone to talk to \u2013 someone who can fetch data, run the AI magic, and keep everything organized. That's where our FastAPI backend comes in!\n\n## What is a Backend API, and Why FastAPI?\n\nThink of the backend like the engine in our car analogy. The frontend is the dashboard, showing you information and letting you control things. The backend is the engine, doing all the heavy lifting.\n\nSpecifically, the backend acts as an *API (Application Programming Interface)*. This means it provides a structured way for the frontend (and other applications) to request information and functionality.\n\n**FastAPI** is a modern, high-performance Python web framework for building APIs. We chose FastAPI because it's:\n\n*   **Fast:** It's built for speed, so our app feels responsive.\n*   **Easy to use:** It has a simple and intuitive syntax, making development quicker.\n*   **Automatic Data Validation:** It automatically checks that the data sent to the API is in the correct format, preventing errors.\n*   **Automatic API Documentation:** It generates interactive API documentation automatically, making it easy to understand how to use the API.\n\n## Key Concepts\n\nLet's break down the key concepts behind our FastAPI backend:\n\n1.  **Routes (Endpoints):** These are specific URLs that our backend responds to. For example, `/api/prs` might be a route that returns a list of all pull requests. Think of them as specific \"doors\" to access different functionalities of our backend.\n\n2.  **Requests:** These are messages sent from the frontend (or any other client) to the backend, asking for something. They often include data, like the ID of a pull request the frontend wants to see.\n\n3.  **Responses:** These are messages sent back from the backend to the frontend, containing the requested information or the result of an action. Responses are often in JSON format, which is easy for the frontend to process.\n\n4.  **Data Models:** These are Python classes that define the structure of our data. For example, we might have a `PullRequest` data model that defines the fields for each pull request (like `id`, `title`, `author`, etc.).  Data models help ensure the data is consistent and well-defined.\n\n## The CodeRoast Backend: A Tour\n\nLet's look at how these concepts come together in our `CodeRoast` backend, using code snippets from `main.py`.\n\n### Defining a Route\n\nHere's an example of a route that fetches all pull requests:\n\n```python\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nimport snowflake.connector\nimport os\nfrom dotenv import load_dotenv\nimport pandas as pd\n\nload_dotenv()\n\napp = FastAPI()\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n@app.get(\"/api/prs\")\ndef fetch_all_prs():\n    # Fetch data from Snowflake and return it\n    # For simplicity, let's return a test data\n    test_data = [\n        {\n            \"id\": 1,\n            \"title\": \"Add new feature\",\n            \"author\": \"johndoe\",\n            \"status\": \"Open\",\n            \"created_at\": \"2023-01-01\",\n            \"updated_at\": \"2023-01-02\"\n        },\n        {\n            \"id\": 2,\n            \"title\": \"Fix bug in login\",\n            \"author\": \"janedoe\",\n            \"status\": \"Merged\",\n            \"created_at\": \"2023-01-03\",\n            \"updated_at\": \"2023-01-04\"\n        }\n    ]\n    return test_data\n```\n\nLet's break this down:\n\n*   `@app.get(\"/api/prs\")`: This is a *decorator* that tells FastAPI to create a route at the URL `/api/prs`. The `@app.get` part specifies that this route should handle GET requests (requests for data).\n*   `def fetch_all_prs():`: This defines a function called `fetch_all_prs` that will be executed when someone makes a GET request to `/api/prs`.\n*   `return test_data`: This returns the `test_data` list as a JSON response. FastAPI automatically converts Python dictionaries and lists into JSON.\n\nSo, if you open your web browser and go to `http://localhost:8000/api/prs`, you'll see a JSON response containing the `test_data`. (Note: make sure your backend is running!)\n\n### Handling Parameters\n\nWhat if we want to fetch information about a specific pull request, identified by its ID? We can use a *path parameter* in our route:\n\n```python\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nimport snowflake.connector\nimport os\nfrom dotenv import load_dotenv\nimport pandas as pd\n\nload_dotenv()\n\napp = FastAPI()\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n@app.get(\"/api/prs/{pr_id}\")\ndef fetch_pr_analysis(pr_id: int):\n    # Fetch the analysis data\n    # For simplicity, let's return a test data\n    test_data = {\n        \"pr_id\": pr_id,\n        \"ai_summary\": \"This PR adds a new feature that improves user experience\",\n        \"merge_confidence\": \"High\",\n        \"code_quality\": \"Good code quality. No major issues found.\",\n        \"diff\": \"```\\n+ def new_function():\\n+     return 'new feature'\\n```\",\n        \"doc_string\": \"This PR introduces a new function that returns 'new feature'.\"\n    }\n    return test_data\n```\n\nHere's what's new:\n\n*   `@app.get(\"/api/prs/{pr_id}\")`: This defines a route that includes a path parameter called `pr_id`. The curly braces `{}` indicate a parameter.\n*   `def fetch_pr_analysis(pr_id: int):`:  The `pr_id: int` part of the function definition tells FastAPI that `pr_id` is an integer parameter. FastAPI will automatically validate that the value passed in the URL is actually an integer.\n*   `\"pr_id\": pr_id`: We are using the `pr_id` parameter inside the function to construct the response.\n\nIf you go to `http://localhost:8000/api/prs/123`, you'll see a JSON response containing the `test_data`, but with the `\"pr_id\"` field set to `123`.\n\n### Connecting to Snowflake Database\n\nIn a real application, we wouldn't just return test data. We would fetch the data from a database.  In our case, we use [Snowflake Data Storage](09_snowflake_data_storage.md). Let's look at how we can connect to Snowflake and query the database (simplified):\n\n```python\nimport os\nimport snowflake.connector\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nimport pandas as pd\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\napp = FastAPI()\n\n# Snowflake credentials\nSNOWFLAKE_USER = os.getenv(\"SNOWFLAKE_USER\", \"raghavg332\")\nSNOWFLAKE_PASSWORD = os.getenv(\"SNOWFLAKE_PASSWORD\", \"Qa29Kh4MptfGHEW\")\nSNOWFLAKE_ACCOUNT = os.getenv(\"SNOWFLAKE_ACCOUNT\", \"KXCIVVH-LL27432\")\nSNOWFLAKE_WAREHOUSE = os.getenv(\"SNOWFLAKE_WAREHOUSE\", \"COMPUTE_WH\")\nSNOWFLAKE_DATABASE = os.getenv(\"SNOWFLAKE_DATABASE\", \"PR_DASHBOARD\")\nSNOWFLAKE_SCHEMA = os.getenv(\"SNOWFLAKE_SCHEMA\", \"PUBLIC\")\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\ndef get_conn():\n    \"\"\"Establish connection to Snowflake database\"\"\"\n    try:\n        conn = snowflake.connector.connect(\n            user=SNOWFLAKE_USER,\n            password=SNOWFLAKE_PASSWORD,\n            account=SNOWFLAKE_ACCOUNT,\n            warehouse=SNOWFLAKE_WAREHOUSE,\n            database=SNOWFLAKE_DATABASE,\n            schema=SNOWFLAKE_SCHEMA\n        )\n        return conn\n    except Exception as e:\n        print(f\"Error connecting to Snowflake: {e}\")\n        raise e\n\n@app.get(\"/api/prs/{pr_id}\")\ndef fetch_pr_analysis(pr_id: int):\n    try:\n        conn = get_conn()\n        cursor = conn.cursor(snowflake.connector.DictCursor)\n\n        # Fetch the analysis data\n        cursor.execute(f\"SELECT * FROM pr_analysis WHERE pr_id = {pr_id}\")\n        result = cursor.fetchone()\n\n        if not result:\n            cursor.close()\n            conn.close()\n            raise HTTPException(status_code=404, detail=\"PR analysis not found\")\n\n        cursor.close()\n        conn.close()\n        return result\n    except Exception as e:\n        print(f\"Error fetching PR analysis: {e}\")\n        raise HTTPException(status_code=500, detail=f\"Error: {str(e)}\")\n```\n\nHere's what's happening:\n\n*   `import snowflake.connector`: This imports the Snowflake connector library, which allows us to connect to a Snowflake database.\n*   `SNOWFLAKE_USER = os.getenv(\"SNOWFLAKE_USER\", \"raghavg332\")`: This imports environment variable, which allows us to securely config the snowflake credentials.\n*   `get_conn()`: This function establishes a connection to Snowflake using your credentials. **Important:** Make sure you have Snowflake configured correctly.\n*   `conn = get_conn()`: This calls the `get_conn` function to establish the connection.\n*   `cursor = conn.cursor(snowflake.connector.DictCursor)`: This creates a cursor object, which allows us to execute SQL queries.  The `snowflake.connector.DictCursor` is important here: it tells the connector to return results as Python dictionaries, which are easier to work with.\n*   `cursor.execute(f\"SELECT * FROM pr_analysis WHERE pr_id = {pr_id}\")`: This executes a SQL query to fetch the analysis data for the specified `pr_id`.\n*   `result = cursor.fetchone()`: This fetches the first row of the result set.\n*   `return result`: This returns the result (which is a dictionary) as a JSON response.\n\n## Under the Hood: Fetching PR Data\n\nLet's take a simplified look at what happens when the frontend requests PR data from the backend.\n\n```mermaid\nsequenceDiagram\n    participant FE as Frontend (React App)\n    participant BE as Backend (FastAPI)\n    participant DB as Snowflake Database\n\n    FE->>BE: GET /api/prs/123\n    activate BE\n    BE->>DB: SELECT * FROM pr_analysis WHERE pr_id = 123\n    activate DB\n    DB-->>BE: PR Analysis Data (JSON)\n    deactivate DB\n    BE-->>FE: PR Analysis Data (JSON)\n    deactivate BE\n    FE->>FE: Render PR Details\n```\n\n1.  The **Frontend (React App)** initiates a GET request to the `/api/prs/123` endpoint on the **Backend (FastAPI)**.\n2.  The **Backend (FastAPI)** receives the request and executes a SQL query on the **Snowflake Database** to fetch the analysis data for the PR with ID 123.\n3.  The **Snowflake Database** returns the PR analysis data to the **Backend (FastAPI)** in JSON format.\n4.  The **Backend (FastAPI)** sends the PR analysis data back to the **Frontend (React App)** in JSON format.\n5.  The **Frontend (React App)** receives the data and renders the details of the pull request.\n\nThe actual fetching of data happens inside of the `fetch_pr_analysis` function we saw before, where it:\n\n1. Establishes a connection to the snowflake database.\n2. Constructs and executes a select query.\n3. Formats the data for transmission back to the frontend as a JSON response.\n\n## Conclusion\n\nIn this chapter, we explored the `CodeRoast` backend, built with FastAPI. We learned about routes, requests, responses, data models, and how they're used to create a powerful API. We also saw how the backend interacts with a Snowflake database to fetch data.\n\nNow that you have a basic understanding of the backend, let's move on to the next chapter, where we'll dive into how we receive GitHub Webhooks: [Chapter 3: GitHub Webhook Integration](03_github_webhook_integration.md).\n\n---\n# Chapter 3: GitHub Webhook Integration\n\nIn [Chapter 2: FastAPI Backend API](02_fastapi_backend_api.md), we set up our FastAPI backend to respond to requests. But how does our backend *know* when something important happens on GitHub, like a new pull request being opened? That's where GitHub Webhooks come in!\n\nImagine you're waiting for a package to arrive. Instead of constantly checking the tracking number, you can sign up for notifications that tell you exactly when it's delivered. GitHub Webhooks are like those notifications, but for code!\n\n## What Problem Do Webhooks Solve?\n\nWithout webhooks, our backend would have to constantly ask GitHub, \"Hey, are there any new pull requests? Hey, are there any updates to existing pull requests?\" This is inefficient and wastes resources.\n\nWebhooks solve this problem by letting GitHub *tell* our backend when something interesting happens. This is much more efficient!\n\n**Central Use Case:** When a developer opens a new pull request (PR) on GitHub, we want our `CodeRoast` system to automatically start analyzing that PR. Webhooks make this possible.\n\n## Key Concepts\n\nLet's break down the key concepts of GitHub Webhook Integration:\n\n1.  **Event:** An event is something that happens in a GitHub repository, like opening a pull request, pushing code, or commenting on an issue.\n\n2.  **Webhook:** A webhook is a way for GitHub to send a notification to our backend whenever a specific event occurs. It's like a \"callback\" \u2013 GitHub calls back to our server when something happens.\n\n3.  **Payload:** The payload is the data that GitHub sends to our backend in the webhook notification. It's a JSON object containing information about the event that occurred, like the pull request's details, the commit messages, and the author's information. This is the \"package\" containing all the information.\n\n4.  **Signature:** A signature is a security measure that ensures the webhook notification is actually coming from GitHub and hasn't been tampered with. It's like a digital \"seal\" of authenticity.\n\n## How to Use Webhooks: A Simple Example\n\nLet's imagine a very simple scenario: when a pull request is opened, we want to print a message to our backend's console.\n\nHere's how the process works:\n\n1.  **Configure a Webhook on GitHub:** In your GitHub repository settings, you create a webhook. You tell GitHub:\n    *   The URL of your backend (where to send the notifications).\n    *   Which events to listen for (e.g., \"pull request\").\n    *   A secret (to verify the signature).\n\n2.  **Open a Pull Request:** A developer opens a new pull request on the repository.\n\n3.  **GitHub Sends a Webhook:** GitHub detects the \"pull request opened\" event and sends a webhook notification (a POST request) to our backend's URL.\n\n4.  **Backend Receives the Webhook:** Our backend receives the POST request, verifies the signature to ensure it's from GitHub, and then extracts the pull request information from the payload.\n\n5.  **Backend Processes the Information:** In our simple example, our backend just prints a message. In reality, it would trigger the [Pull Request Analysis Pipeline](04_pull_request_analysis_pipeline.md).\n\n## Code Example: Receiving a Webhook\n\nHere's a simplified code example showing how our FastAPI backend can receive and process a GitHub webhook:\n\n```python\nfrom fastapi import FastAPI, Request, Header\nfrom fastapi.responses import JSONResponse\nimport hmac\nimport hashlib\nimport json\n\napp = FastAPI()\n\nGITHUB_SECRET = \"asdfg\" # Replace with your actual secret\n\ndef verify_signature(payload, signature, secret):\n    mac = hmac.new(secret.encode(), msg=payload, digestmod=hashlib.sha256)\n    expected = f\"sha256={mac.hexdigest()}\"\n    return hmac.compare_digest(expected, signature)\n\n@app.post(\"/webhook\")\nasync def github_webhook(\n    request: Request,\n    x_hub_signature_256: str = Header(None),\n    x_github_event: str = Header(None)\n):\n    body = await request.body()\n\n    # Verify webhook signature\n    if not verify_signature(body, x_hub_signature_256, GITHUB_SECRET):\n        return JSONResponse(content={\"error\": \"Invalid signature\"}, status_code=403)\n\n    payload = json.loads(body)\n\n    if x_github_event == \"pull_request\":\n        action = payload[\"action\"]\n        pr = payload[\"pull_request\"]\n\n        if action in [\"opened\", \"synchronize\"]:\n            print(f\"New PR opened: {pr['title']}\")\n\n    return {\"message\": \"OK\"}\n```\n\nLet's break this down:\n\n*   `@app.post(\"/webhook\")`: This creates a route that listens for POST requests at the `/webhook` endpoint. GitHub will send its webhook notifications to this URL.\n*   `request: Request`: This allows us to access the body of the POST request (the payload).\n*   `x_hub_signature_256: str = Header(None)`: This extracts the `X-Hub-Signature-256` header from the request. This header contains the signature that we need to verify.\n*   `x_github_event: str = Header(None)`: This extracts the `X-GitHub-Event` header, which tells us what type of event triggered the webhook (e.g., \"pull_request\").\n*   `body = await request.body()`: This reads the raw bytes from the request body.\n*   `verify_signature(body, x_hub_signature_256, GITHUB_SECRET)`: This calls a function to verify that the signature is valid (we'll explain this in more detail later).\n*   `payload = json.loads(body)`: This converts the JSON payload into a Python dictionary.\n*   `if x_github_event == \"pull_request\"`: This checks if the event type is \"pull_request\".\n*   `if action in [\"opened\", \"synchronize\"]`: This checks if the pull request action is \"opened\" (a new PR) or \"synchronize\" (a PR was updated).\n*   `print(f\"New PR opened: {pr['title']}\")`: This prints the title of the new pull request to the console.\n\n**Example Input (Payload):**\n\nWhen a pull request is opened, GitHub will send a JSON payload to our `/webhook` endpoint. This payload contains lots of information about the pull request. Here's a simplified example:\n\n```json\n{\n  \"action\": \"opened\",\n  \"pull_request\": {\n    \"url\": \"https://api.github.com/repos/your-org/your-repo/pulls/123\",\n    \"title\": \"Add a new feature\"\n  },\n  \"repository\": {\n    \"full_name\": \"your-org/your-repo\"\n  }\n}\n```\n\n**Example Output:**\n\nWhen our backend receives this payload, it will print the following message to the console:\n\n```\nNew PR opened: Add a new feature\n```\n\nThis example demonstrates the basic flow of receiving and processing a GitHub webhook. In a real application, you would use the data in the payload to trigger more complex actions, like analyzing the code changes in the pull request.\n\n## Under the Hood: How Webhooks Work\n\nLet's take a closer look at how webhooks work internally.\n\nHere's a sequence diagram illustrating the flow:\n\n```mermaid\nsequenceDiagram\n    participant Dev as Developer\n    participant GH as GitHub\n    participant BE as Backend (FastAPI)\n\n    Dev->>GH: Opens/Updates Pull Request\n    activate GH\n    GH->>BE: POST /webhook (with payload)\n    activate BE\n    BE->>BE: Verify Signature\n    alt Signature Valid\n        BE->>BE: Process Payload\n        BE-->>GH: 200 OK\n    else Signature Invalid\n        BE-->>GH: 403 Forbidden\n    end\n    deactivate BE\n    deactivate GH\n```\n\n1.  A **Developer** opens or updates a pull request on **GitHub**.\n\n2.  **GitHub** detects the event and sends a POST request to our **Backend (FastAPI)** at the `/webhook` endpoint. The request includes a JSON payload containing information about the event.\n\n3.  The **Backend (FastAPI)** receives the POST request and immediately verifies the signature to ensure the request is actually coming from GitHub.\n\n4.  If the signature is valid, the **Backend (FastAPI)** processes the payload and performs the appropriate actions (e.g., triggers the [Pull Request Analysis Pipeline](04_pull_request_analysis_pipeline.md)). The Backend sends an HTTP 200 OK to GitHub.\n\n5.  If the signature is invalid, the **Backend (FastAPI)** rejects the request and sends an HTTP 403 Forbidden to GitHub.\n\n### Verifying the Signature\n\nThe `verify_signature` function is crucial for security. Here's how it works:\n\n```python\nimport hmac\nimport hashlib\n\ndef verify_signature(payload, signature, secret):\n    mac = hmac.new(secret.encode(), msg=payload, digestmod=hashlib.sha256)\n    expected = f\"sha256={mac.hexdigest()}\"\n    return hmac.compare_digest(expected, signature)\n```\n\n*   `hmac.new(secret.encode(), msg=payload, digestmod=hashlib.sha256)`: This creates a new HMAC (Hash-based Message Authentication Code) object using the secret key and the payload. It uses the SHA256 algorithm to generate the hash.\n*   `expected = f\"sha256={mac.hexdigest()}\"`: This calculates the expected signature by prefixing \"sha256=\" to the hexadecimal representation of the HMAC hash.\n*   `hmac.compare_digest(expected, signature)`: This securely compares the expected signature with the signature provided in the `X-Hub-Signature-256` header. This function is important to prevent timing attacks.\n\nThe secret key is configured in your GitHub webhook settings and should be stored securely on your backend. Make sure to *never* expose your secret key in your code or commit it to your repository! As you can see in the provided `backend/main.py`, the correct approach is to store the secret in an environment variable.\n\n## Conclusion\n\nIn this chapter, we learned about GitHub Webhook Integration. We understand what webhooks are, why they're useful, and how to receive and process them in our FastAPI backend. We also saw how to verify the signature to ensure the webhook notifications are authentic.\n\nNow that we can receive notifications about pull requests, we can move on to the next chapter, where we'll dive into the [Pull Request Analysis Pipeline](04_pull_request_analysis_pipeline.md), which is responsible for analyzing the code changes in the pull request and generating AI-powered insights.\n\n---\n# Chapter 4: Pull Request Analysis Pipeline\n\nIn [Chapter 3: GitHub Webhook Integration](03_github_webhook_integration.md), we learned how to set up our backend to receive notifications from GitHub whenever a pull request (PR) is opened or updated. Now, what do we *do* with that information?\n\nThat's where the **Pull Request Analysis Pipeline** comes in!\n\nImagine a factory assembly line. Raw materials (code changes) enter, various machines perform operations (analysis, documentation suggestions), and then the finished product (insights) is stored in a database. Our pipeline is very similar: it takes a pull request, analyzes the code changes, generates AI insights, and stores the results.\n\n**Central Use Case:** When a developer opens a new pull request, we want `CodeRoast` to automatically analyze the code, provide feedback, and suggest documentation updates. The Pull Request Analysis Pipeline makes this happen!\n\n## Key Concepts\n\nLet's break down the key concepts behind the Pull Request Analysis Pipeline:\n\n1.  **Input: Pull Request Data:** This is the raw information about the PR, received from GitHub via the webhook. It includes the code diff, the PR description, the author, and more. Think of this as the \"raw materials\" entering our factory.\n\n2.  **Code Parsing:** Before we can analyze the code, we need to understand its structure. This involves parsing the code to identify functions, classes, and other elements. We will talk about it in detail in [Chapter 5: Tree-sitter Based Code Parsing](05_tree_sitter_based_code_parsing.md).\n\n3.  **AI Insights Generation:** This is where the magic happens! We use AI, powered by [Groq API Integration](06_groq_api_integration.md), to analyze the code and generate insights. This might include summarizing the PR, identifying potential issues, suggesting improvements, and estimating merge confidence. It's like adding value to a product in the factory.\n\n4.  **Documentation Updates (RAG):** We use RAG (Retrieval-Augmented Generation) to suggest updates to the project documentation based on the code changes. This ensures that our documentation stays up-to-date. See more details in [Chapter 7: RAG (Retrieval-Augmented Generation) Documentation Updates](07_rag__retrieval_augmented_generation__documentation_updates.md).\n\n5.  **Output: Stored Insights:** The final product of our pipeline is a set of AI-generated insights that are stored in a database (Snowflake in our case [Chapter 9: Snowflake Data Storage](09_snowflake_data_storage.md)). These insights can then be displayed in the frontend and used to improve the code review process.\n\n## How to Use the Pipeline\n\nLet's walk through how to use the pipeline with a simplified example.\n\nImagine we have a function called `process_pull_request` that represents our pipeline. It takes the pull request data as input and returns the AI insights.\n\n```python\ndef process_pull_request(pr_data):\n    # 1. Parse the code changes.  We skip for this simple case.\n    code_diff = pr_data[\"code_diff\"] # Assume code_diff is extracted\n\n    # 2. Generate AI insights. We skip complex logic here\n    ai_summary = \"This PR adds a new feature.\"\n    merge_confidence = \"High\"\n\n    # 3. Store the results. We skip database insert here.\n    insights = {\n        \"ai_summary\": ai_summary,\n        \"merge_confidence\": merge_confidence,\n    }\n\n    return insights\n```\n\nThis simplified code does the following:\n\n1.  Takes `pr_data` which contains information about the PR.\n2.  Simulates extracting the code diff.\n3.  Simulates generating AI insights (in a real application, we'd use the Groq API).\n4.  Returns a dictionary containing the AI insights.\n\n**Example Input:**\n\n```python\npr_data = {\n    \"pr_title\": \"Add new feature\",\n    \"pr_description\": \"This PR adds a new feature to calculate statistics.\",\n    \"code_diff\": \"```diff\\n+def calculate_mean(data):\\n+  return sum(data) / len(data)\\n```\",\n    \"author\": \"johndoe\",\n    \"status\": \"open\"\n}\n```\n\n**Example Output:**\n\n```python\n{\n  \"ai_summary\": \"This PR adds a new feature.\",\n  \"merge_confidence\": \"High\"\n}\n```\n\nIn reality, this function is more complex and involves several steps, including connecting to external services and parsing the code. We will cover it later in this chapter!\n\n## Under the Hood: How the Pipeline Works\n\nLet's take a closer look at how the Pull Request Analysis Pipeline works internally.\n\nHere's a sequence diagram illustrating the flow:\n\n```mermaid\nsequenceDiagram\n    participant GH as GitHub\n    participant BE as Backend (FastAPI)\n    participant CP as Code Parser\n    participant AI as Groq API\n    participant DB as Snowflake Database\n\n    GH->>BE: POST /webhook (PR data)\n    activate BE\n    BE->>CP: Extract code diffs\n    activate CP\n    CP-->>BE: Code diffs (parsed)\n    deactivate CP\n    BE->>AI: Analyze code\n    activate AI\n    AI-->>BE: AI Insights (JSON)\n    deactivate AI\n    BE->>DB: Store Insights\n    activate DB\n    DB-->>BE: OK\n    deactivate DB\n    BE-->>GH: 200 OK\n    deactivate BE\n```\n\n1.  **GitHub** sends a POST request to our **Backend (FastAPI)** at the `/webhook` endpoint, containing the pull request data.\n2.  The **Backend (FastAPI)** receives the data and passes the code to the **Code Parser** to extract the changed functions (detailed in [Chapter 5: Tree-sitter Based Code Parsing](05_tree_sitter_based_code_parsing.md)).\n3.  The **Code Parser** returns the extracted code diffs to the **Backend (FastAPI)**.\n4.  The **Backend (FastAPI)** sends the code diffs and PR details to the **Groq API** for analysis and insight generation.\n5.  The **Groq API** analyzes the code and returns AI insights (summary, merge confidence, etc.) in JSON format to the **Backend (FastAPI)**.\n6.  The **Backend (FastAPI)** stores these insights in the **Snowflake Database**.\n7.  The **Backend (FastAPI)** sends an HTTP 200 OK response to GitHub.\n\n### Diving into the Code\n\nLet's look at a code example of how the `process_pull_request` function is implemented. This code is located in `backend/prReview.py`.\n\n```python\ndef process_pull_request(g, repo_name, pr_number, sender=None):\n    repo = g.get_repo(repo_name)\n    pr = repo.get_pull(pr_number)\n    pr_description = pr.title # title is used as description\n    pr_title = pr.title\n    pr_author = pr.user.login\n    pr_status = pr.state\n    code_diff = get_lines_changed(pr) # Step 1\n    # ... (rest of the function)\n```\n\nThis code does the following:\n\n1.  `repo = g.get_repo(repo_name)`: Gets the repository object from GitHub using the `repo_name`.  `g` is the authenticated Github instance.\n2.  `pr = repo.get_pull(pr_number)`: Gets the pull request object from the repository using the `pr_number`.\n3.  `pr_description = pr.title`: Sets the pull request description.\n4.  `code_diff = get_lines_changed(pr)`: Calls the `get_lines_changed` function to get the code diff. This function uses github api to get the diff and returns a dictionary containing the added, modified, and deleted lines.\n\nLet's look at how we get the diff code:\n\n```python\ndef get_lines_changed(pr):\n    files = pr.get_files()\n    file_changes = {}\n    for file in files:\n        if file.status == \"removed\":\n            continue\n        file_changes[file.filename] = parse_changed_lines(file.patch)\n\n    return file_changes\n\ndef parse_changed_lines(patch: str):\n    added_or_modified_lines = set()\n    deleted_lines = set()\n    for line in patch.splitlines():\n        if line.startswith(\"@@\"):\n            match = re.findall(r\"@@ -(\\d+)(?:,(\\d+))? \\+(\\d+)(?:,(\\d+))? @@\", line)\n            if match:\n                old_start, old_count, new_start, new_count = match[0]\n\n                old_start = int(old_start)\n                old_count = int(old_count or 1)\n                for l in range(old_start, old_start + old_count):\n                    deleted_lines.add(l)\n\n                new_start = int(new_start)\n                new_count = int(new_count or 1)\n                for l in range(new_start, new_start + new_count):\n                    added_or_modified_lines.add(l)\n\n    return {\n        \"added_or_modified_lines\": added_or_modified_lines,\n        \"deleted_lines\": deleted_lines\n    }\n```\n\nThis code does the following:\n\n1.  `files = pr.get_files()`: Get all files that changed in this PR.\n2.  The code loops through each of the `files`:\n    *   `if file.status == \"removed\": continue`: Skip the files that were removed.\n    *   `file_changes[file.filename] = parse_changed_lines(file.patch)`: Call `parse_changed_lines` to parse the changes of each file and then store. The `parse_changed_lines` uses regex matching to find out which lines were added or deleted in this file.\n\nThe following code then uses the data extracted above to call the Groq API:\n\n```python\n    # Get the review data and post a comment\n    review_data = review_and_store_pr(pr_description, file_diff_func, pr_title, pr_author, pr_status)\n    \n    # Post the review as a comment on the PR\n    post_review_comment(g, repo_name, pr_number, review_data)\n```\n\nThe `review_and_store_pr` performs the RAG (Retrieval-Augmented Generation) to update project documentation and calls Groq to generate the PR review and stores the review into the snowflake database. `post_review_comment` then posts the review as a comment on the PR. Details can be found in [Chapter 8: PR Review Comment Posting (GitHub)](08_pr_review_comment_posting__github_.md)\n\n## Conclusion\n\nIn this chapter, we explored the Pull Request Analysis Pipeline. We learned about the key concepts, how to use the pipeline, and how it works internally. We saw how the pipeline takes a pull request as input, analyzes the code changes, generates AI insights, suggests documentation updates, and stores the results in a database.\n\nNow that we understand the overall pipeline, let's dive deeper into one of the key components: parsing the code changes using Tree-sitter in [Chapter 5: Tree-sitter Based Code Parsing](05_tree_sitter_based_code_parsing.md).\n\n---\n# Chapter 5: Tree-sitter Based Code Parsing\n\nIn [Chapter 4: Pull Request Analysis Pipeline](04_pull_request_analysis_pipeline.md), we learned how the `CodeRoast` pipeline analyzes pull requests. One of the key steps in that pipeline is understanding the *structure* of the code changes.  Instead of just treating the code as plain text, we want to know *what* code has changed - what functions have been added, deleted or modified?\n\nThat's where **Tree-sitter** comes in!\n\nImagine trying to understand a sentence without knowing grammar. You might get some of the words, but you'd miss the relationships between them. Tree-sitter is like a grammar checker for code. It helps us understand the structure of the code so we can analyze it more effectively.\n\n**Central Use Case:** When a developer modifies a function in a pull request, we want `CodeRoast` to *only* analyze that function, rather than the entire file. This focused analysis saves time and improves the accuracy of our AI insights.\n\n## Key Concepts\n\nLet's break down the key concepts behind Tree-sitter based code parsing:\n\n1.  **Parsing:** Parsing is the process of taking code (which is just a string of characters) and turning it into a structured representation that a computer can understand. Think of it like taking a sentence and breaking it down into nouns, verbs, and adjectives.\n\n2.  **Abstract Syntax Tree (AST):** The AST is the structured representation of the code that results from parsing. It's like a tree diagram that shows the relationships between the different parts of the code. Each \"node\" in the tree represents a different element of the code, like a function, a variable, or an expression.\n\n3.  **Tree-sitter:** Tree-sitter is a library that makes it easy to parse code and create ASTs. It's fast, reliable, and supports many different programming languages. It's the engine that drives our code parsing.\n\n4.  **Grammar:** A grammar defines the rules for a programming language. Tree-sitter uses grammars to understand the structure of the code. Each language has its own grammar (e.g., Python grammar, JavaScript grammar). These grammars tells Tree-sitter what to expect.\n\n## How to Use Tree-sitter: A Simple Example\n\nLet's imagine we have a simple Python function and we want to use Tree-sitter to extract its name and code.\n\nHere's the function:\n\n```python\ndef greet(name):\n    \"\"\"Greets the person passed in as a parameter.\"\"\"\n    print(f\"Hello, {name}!\")\n```\n\nHere's how we can use Tree-sitter to parse this code and extract the function name and code (simplified):\n\n```python\nfrom tree_sitter import Language, Parser\nimport tree_sitter_python as tspython\n\n# Set up Tree-sitter\nPY_LANGUAGE = Language(tspython.language())\nparser = Parser()\nparser.set_language(PY_LANGUAGE)\n\n# The code to parse\ncode = \"\"\"\ndef greet(name):\n    \\\"\\\"\\\"Greets the person passed in as a parameter.\\\"\\\"\\\"\n    print(f\"Hello, {name}!\")\n\"\"\"\n\n# Parse the code\ntree = parser.parse(bytes(code, \"utf8\"))\n\n# Get the root node of the AST\nroot_node = tree.root_node\n\n# Find the function definition node\nfunction_node = root_node.children[0] # Assume function is the first child\n\n# Extract the function name\nfunction_name_node = function_node.child_by_field_name('name')\nfunction_name = function_name_node.text.decode()\n\n# Extract the function code\nfunction_code = function_node.text.decode()\n\nprint(f\"Function Name: {function_name}\")\nprint(f\"Function Code: {function_code}\")\n```\n\nThis simplified code does the following:\n\n1.  **Sets up Tree-sitter:** It imports the necessary libraries and creates a Tree-sitter parser for Python.  We are using `tree_sitter_python` to obtain the python grammar.\n2.  **Parses the code:** It takes the Python code and uses the parser to create an AST.\n3.  **Navigates the AST:**\n    *   `function_node = root_node.children[0]`: Since the example only consists of the definition of the `greet` function, the function node will be the first child. This code accesses the function node.\n    *   `function_name_node = function_node.child_by_field_name('name')`: This line gets the node that represents the name of the function.\n4.  **Extracts information:** It extracts the function name and code from the AST.\n5.  **Prints the results:** It prints the function name and code to the console.\n\n**Example Output:**\n\n```\nFunction Name: greet\nFunction Code: def greet(name):\n    \"\"\"Greets the person passed in as a parameter.\"\"\"\n    print(f\"Hello, {name}!\")\n```\n\nThis example demonstrates how we can use Tree-sitter to parse code and extract specific elements. In `CodeRoast`, we use this technique to identify the functions that have been changed in a pull request.\n\n## Under the Hood: How Tree-sitter Works\n\nLet's take a closer look at how Tree-sitter works internally.\n\nHere's a sequence diagram illustrating the flow:\n\n```mermaid\nsequenceDiagram\n    participant Code as Code (String)\n    participant Parser as Tree-sitter Parser\n    participant AST as Abstract Syntax Tree\n    participant Extractor as Information Extractor\n\n    Code->>Parser: Parse code\n    activate Parser\n    Parser->>AST: Build AST\n    activate AST\n    AST-->>Parser: AST\n    deactivate AST\n    Parser-->>Extractor: AST\n    deactivate Parser\n    Extractor->>AST: Navigate and Extract\n    activate AST\n    AST-->>Extractor: Function Name and Code\n    deactivate AST\n    Extractor-->>Code: Function Name and Code\n```\n\n1.  The **Code (String)** is passed to the **Tree-sitter Parser**.\n2.  The **Tree-sitter Parser** uses the grammar to build an **Abstract Syntax Tree (AST)**.\n3.  The **AST** is returned to the **Tree-sitter Parser**.\n4.  The **Tree-sitter Parser** passes the **AST** to the **Information Extractor**.\n5.  The **Information Extractor** navigates the **AST** to extract the function name and code.\n6.  The extracted function name and code are returned to the **Code**.\n\n### Diving into the Code\n\nLet's look at the code that extracts functions from the code. This code can be found in `backend/prReview.py`.\n\n```python\ndef extract_functions(code, language_name, changed_lines):\n    parser = get_paraser(language_name)\n    tree = parser.parse(bytes(code, \"utf8\"))\n    root_node = tree.root_node\n\n    functions = []\n\n    def node_within_lines(node):\n        start_line = node.start_point[0] + 1\n        end_line = node.end_point[0] + 1\n        return any([start_line <= line <= end_line for line in changed_lines])\n    \n    def traverse(node):\n        if language_name == \"python\" and node.type == \"function_definition\":\n            if node_within_lines(node):\n                func_name_node = node.child_by_field_name('name')\n                func_name = func_name_node.text.decode()\n                functions.append({'name': func_name, 'code': node.text.decode()})\n        elif language_name == 'javascript' and node.type in ['function_declaration', 'method_definition', 'arrow_function']:\n            if node_within_lines(node):\n                func_name_node = node.child_by_field_name('name')\n                func_name = func_name_node.text.decode()\n                functions.append({'name': func_name, 'code': node.text.decode()})\n        elif language_name == 'cpp' and node.type in ['function_definition']:\n            if node_within_lines(node):\n                func_name_node = node.child_by_field_name('name')\n                func_name = func_name_node.text.decode()\n                functions.append({'name': func_name, 'code': node.text.decode()})\n        for child in node.children:\n            traverse(child)\n\n    traverse(root_node)\n    return functions\n```\n\nThis code does the following:\n\n1.  `parser = get_paraser(language_name)`: Gets the Tree-sitter parser for the given language (Python, JavaScript, C++ are supported).\n2.  `tree = parser.parse(bytes(code, \"utf8\"))`: Parses the code and creates an AST.\n3.  `root_node = tree.root_node`: Gets the root node of the AST.\n4.  The code defines a helper function `node_within_lines` to checks whether a node is within changed lines to skip parsing nodes that are irrelevant.\n5.  `traverse(root_node)`: Calls the `traverse` function to recursively traverse the AST and extract the function names and code.\n    *   It first checks if the language is Python and the node type is `function_definition`.\n    *   If it is, it extracts the function name and code and adds them to the `functions` list.\n    *   Then it traverses the child nodes.\n6.  It does similar logic for Javascript and C++\n7.  It returns the `functions` list.\n\nThe `get_paraser` function loads the grammars for supported languages:\n\n```python\ndef get_paraser(language_name):\n\n    PY_LANGUAGE = Language(tspython.language())\n    JS_LANGUAGE = Language(tsjavascript.language())\n    CPP_LANGUAGE = Language(tscpp.language())\n\n    LANGUAGES = {\n        'python': PY_LANGUAGE,\n        'javascript': JS_LANGUAGE,\n        'cpp': CPP_LANGUAGE,\n    }\n    parser = Parser(LANGUAGES[language_name])\n    return parser\n```\n\nThis code does the following:\n\n1.  `PY_LANGUAGE = Language(tspython.language())`: Loads the Python grammar.\n2.  `JS_LANGUAGE = Language(tsjavascript.language())`: Loads the JavaScript grammar.\n3.  `CPP_LANGUAGE = Language(tscpp.language())`: Loads the C++ grammar.\n4.  `LANGUAGES = { ... }`: Creates a dictionary that maps language names to their corresponding grammars.\n5.  `parser = Parser(LANGUAGES[language_name])`: Creates a Tree-sitter parser for the given language.\n6.  It returns the `parser`.\n\n## Conclusion\n\nIn this chapter, we explored Tree-sitter based code parsing. We learned about the key concepts, how to use Tree-sitter to extract information from code, and how it works internally. We saw how Tree-sitter helps us understand the structure of code so we can analyze it more effectively.\n\nNow that we can parse code changes, let's move on to the next chapter and see how we use AI to generate insights about those changes using [Groq API Integration](06_groq_api_integration.md).\n\n---\n# Chapter 6: Groq API Integration\n\nIn [Chapter 5: Tree-sitter Based Code Parsing](05_tree_sitter_based_code_parsing.md), we learned how to parse the code changes in a pull request. Now, how do we *understand* what those changes mean and generate useful insights?\n\nThat's where the **Groq API Integration** comes in!\n\nThink of the Groq API as a super-smart AI assistant. We send it the code and a description of the pull request, and it sends back a summary of the changes, a code quality assessment, and even suggestions for improving the documentation.\n\n**Central Use Case:** When a developer submits a pull request, we want `CodeRoast` to automatically generate a helpful summary of the changes and identify potential problems in the code. The Groq API integration makes this possible.\n\n## Key Concepts\n\nLet's break down the key concepts behind the Groq API Integration:\n\n1.  **API (Application Programming Interface):**  An API is a way for different software systems to talk to each other.  The Groq API lets us send requests to Groq's AI models and get responses back. It is like ordering from a restaurant; we send the order (request), and the restaurant delivers the food (response).\n\n2.  **Groq API Key:** This is a secret code that identifies us to the Groq API. Think of it like a password that allows us to access the Groq API. You'll need to obtain your own API key from Groq to use their service.\n\n3.  **Prompt:** A prompt is the message we send to the Groq API, telling it what we want it to do.  For example, our prompt might include the code changes, the pull request description, and a request to summarize the changes. It's like writing instructions to your super-smart AI assistant.\n\n4.  **AI Model:** An AI model is a pre-trained algorithm that can perform a specific task, like summarizing text or assessing code quality. Groq offers a variety of AI models that we can use.\n\n5.  **JSON (JavaScript Object Notation):** JSON is a way to format data that's easy for computers to read and write. We send data to the Groq API in JSON format, and it sends data back to us in JSON format.\n\n## How to Use the Groq API: A Simple Example\n\nLet's imagine we want to use the Groq API to summarize a simple code change.\n\nHere's the code change:\n\n```python\ndef add(x, y):\n  \"\"\"Adds two numbers together.\"\"\"\n  return x + y\n```\n\nHere's how we can use the Groq API to summarize this code (simplified):\n\n```python\nimport requests\nimport json\n\n# Your Groq API key\nGROQ_API_KEY = \"YOUR_GROQ_API_KEY\" # Replace with your actual key\n\n# The API endpoint\nGROQ_ENDPOINT = \"https://api.groq.com/openai/v1/chat/completions\"\n\n# The prompt\nprompt = \"\"\"\nSummarize the following Python code:\n\ndef add(x, y):\n  \\\"\\\"\\\"Adds two numbers together.\\\"\\\"\\\"\n  return x + y\n\"\"\"\n\n# Construct the payload\npayload = {\n    \"model\": \"llama3-70b-8192\", # You should check with Groq for the latest model\n    \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n}\n\n# Set the headers\nheaders = {\n    \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\n# Make the request\nresponse = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n\n# Parse the response\nresult = response.json()\n\n# Extract the summary\nsummary = result[\"choices\"][0][\"message\"][\"content\"].strip()\n\n# Print the summary\nprint(summary)\n```\n\nThis simplified code does the following:\n\n1.  **Sets up the API key and endpoint:** It defines the `GROQ_API_KEY` and `GROQ_ENDPOINT` variables.  **Important:** Replace `\"YOUR_GROQ_API_KEY\"` with your actual Groq API key.\n2.  **Constructs the prompt:** It defines the `prompt` variable, which tells the Groq API what we want it to do.\n3.  **Constructs the payload:** It creates a dictionary containing the prompt and the AI model to use.\n    *   `\"model\": \"llama3-70b-8192\"`:  This tells the Groq API to use the \"llama3-70b-8192\" AI model. Please check Groq documentation for the latest models and their API names.\n4.  **Sets the headers:** It defines the `headers` variable, which includes our Groq API key and tells the Groq API that we're sending data in JSON format.\n5.  **Makes the request:** It uses the `requests.post` function to send the prompt to the Groq API.\n6.  **Parses the response:** It uses the `response.json()` function to convert the JSON response from the Groq API into a Python dictionary.\n7.  **Extracts the summary:** It extracts the summary from the response and prints it to the console.\n\n**Example Output:**\n\n```\nThis code defines a function called \"add\" that takes two arguments, x and y, and returns their sum. The docstring indicates that the function adds two numbers together.\n```\n\nThis example demonstrates how we can use the Groq API to summarize code. In `CodeRoast`, we use this technique to generate summaries of pull requests, assess code quality, and suggest documentation updates.\n\n## Under the Hood: How the Groq API Integration Works\n\nLet's take a closer look at how the Groq API Integration works internally.\n\nHere's a sequence diagram illustrating the flow:\n\n```mermaid\nsequenceDiagram\n    participant BE as Backend (FastAPI)\n    participant Groq as Groq API\n\n    BE->>Groq: POST /openai/v1/chat/completions (Prompt + Code)\n    activate Groq\n    Groq->>Groq: AI Model Analysis\n    Groq-->>BE: JSON (Summary, Quality, etc.)\n    deactivate Groq\n    BE->>BE: Process Results\n```\n\n1.  The **Backend (FastAPI)** constructs a prompt containing the code changes and sends a POST request to the **Groq API** at the `/openai/v1/chat/completions` endpoint.\n\n2.  The **Groq API** receives the prompt and uses its AI models to analyze the code.\n\n3.  The **Groq API** returns a JSON response containing the summary, code quality assessment, and documentation suggestions to the **Backend (FastAPI)**.\n\n4.  The **Backend (FastAPI)** processes the results and stores them in the database or displays them in the frontend.\n\n### Diving into the Code\n\nLet's look at the code that calls the Groq API and processes the response. This code can be found in `backend/ai_analyse.py` and `backend/prReview.py`. First, let's look at the `build_full_prompt` function:\n\n```python\ndef build_full_prompt(pr_description: str, code_diff_str: str) -> str:\n    return f\"\"\"\nYou are an AI code reviewer and documentation assistant.\n\nA developer submitted the following pull request. Please do the following:\n1. \ud83d\udd0d Summarize the overall purpose of the PR. Be a little detailed\n2. \u2705 Review the code for:\n   - Syntax issues in Python (Make up some Issue)\n   - Style (PEP8)\n   - Functionality correctness (does it match the description?)\n3. Based on the review, provide a merge confidence score from 1 to 10.\n4. \ud83d\udcdd Extract or suggest updated docstrings for any changed or added functions/classes.\n5. The Code Diff is made up of the old and new version of the code, if any of the field is empty it means, the function is either added or deleted respectively.\n\n## PR Description:\n{pr_description}\n\n## Code Diff:\n{code_diff_str}\n\nRespond ONLY in this JSON format without any additional text:\n\n{{\n  \"ai_summary\": \"...\",\n  \"merge_confidence\": \"1 - 10\",\n  \"code_quality\": {{\n    \"syntax_check\": \"...\",\n    \"style_check\": \"...\",\n    \"functionality_check\": \"...\",\n    \"final_rating\": \"Excellent / Good / Needs Work / Critical Issues\"\n  }}\n}}\n\"\"\"\n```\n\nThis code constructs the prompt that we send to the Groq API. It includes:\n\n*   A description of the role the AI should play (\"AI code reviewer and documentation assistant\").\n*   Instructions on what the AI should do (summarize the PR, review the code, suggest documentation updates).\n*   The pull request description and code diff.\n*   **Important:** It tells the AI to respond in JSON format. This makes it easy for us to parse the response.\n\nThe `review_and_store_pr` function calls the Groq API and process the response.\n\n```python\nimport requests\nimport json\n\ndef review_and_store_pr(pr_description: str, code_diff: dict, pr_title: str, pr_author: str, pr_status: str):\n    code_diff_str = \"\"\n    for file in list(code_diff.keys()):\n        code_diff_str += f\"File: {file}\\n\\n\"\n        diff = code_diff[file]\n\n        for func_name in list(diff.keys()):\n            old_code = diff[func_name]['old_code']\n            new_code = diff[func_name]['new_code']\n            if old_code is None:\n                old_code = \"\"\n            if new_code is None:\n                new_code = \"\"\n            code_diff_str += f\"Function: {func_name}\\n\\nOld Code:\\n{old_code}\\n\\nNew Code:\\n{new_code}\"\n    docstring = update_documentation_with_rag(pr_description, code_diff)\n    pr_id = random.randint(1000, 9999)\n    prompt = build_full_prompt(pr_description, code_diff_str)\n\n    payload = {\n        \"model\": \"llama3-70b-8192\",\n        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n    }\n\n    try:\n        response = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n        response.raise_for_status()\n        result = response.json()\n\n        try:\n            content = result[\"choices\"][0][\"message\"][\"content\"].strip()\n            review_data = json.loads(content)\n\n            # Flatten the code quality object for easier insert\n            code_quality_str = (\n                f\"Syntax: {review_data['code_quality']['syntax_check']}\\n\"\n                f\"Style: {review_data['code_quality']['style_check']}\\n\"\n                f\"Functionality: {review_data['code_quality']['functionality_check']}\\n\"\n                f\"Rating: {review_data['code_quality']['final_rating']}\"\n            )\n            cur.execute(f\"\"\"\n                INSERT INTO PULL_REQUESTS (\n                    ID, TITLE, AUTHOR, STATUS, CREATED_AT, UPDATED_AT\n                )\n                VALUES (%s, %s, %s, %s, %s, %s);\n            \"\"\", (\n                pr_id,\n                pr_title,\n                pr_author,\n                pr_status,\n                datetime.datetime.now(),\n                datetime.datetime.now()\n            ))\n            conn.commit()\n\n            # Insert into Snowflake\n            cur.execute(f\"\"\"\n                INSERT INTO PR_ANALYSIS (\n                    PR_ID, AI_SUMMARY, MERGE_CONFIDENCE, CODE_QUALITY,\n                    PR_DESCRIPTION, DOCSTRINGS, DIFF\n                )\n                VALUES (%s, %s, %s, %s, %s, %s, %s);\n            \"\"\", (\n                pr_id,\n                review_data[\"ai_summary\"],\n                review_data[\"merge_confidence\"],\n                code_quality_str,\n                pr_description,\n                docstring['doc_update'],\n                code_diff_str\n            ))\n\n            conn.commit()\n\n            logging.info(f\"[\u2705] PR #{pr_id} analysis saved to Snowflake.\")\n            print(f\"[\u2705] PR #{pr_id} analysis saved to Snowflake.\")\n            return review_data\n\n        except json.JSONDecodeError:\n            logging.error(\"[\u274c] Groq returned non-JSON output.\")\n            print(\"[\u274c] Groq returned non-JSON output.\")\n            return {\"error\": \"Invalid JSON from Groq\"}\n\n    except Exception as e:\n        logging.exception(\"Groq API or Snowflake insert failed\")\n        return {\"error\": str(e)}\n```\n\nThis code does the following:\n\n1.  First loops through the `code_diff` to construct the `code_diff_str`\n2.  Calls the `build_full_prompt` function to construct the prompt.\n3.  Constructs the `payload`, setting the ai model to be used.\n4.  Calls the `requests.post` function to send the request to the Groq API.\n5.  Parses the JSON response using `response.json()`.\n6.  Extracts the AI insights from the response and stores them in the database ([Chapter 9: Snowflake Data Storage](09_snowflake_data_storage.md)).\n7.  Handles potential errors, like invalid JSON responses.\n\n## Conclusion\n\nIn this chapter, we explored the Groq API Integration. We learned about the key concepts, how to use the Groq API to generate AI insights, and how it works internally. We saw how the Groq API helps us understand code changes and generate useful summaries, code quality assessments, and documentation suggestions.\n\nNow that we can use AI to generate insights, let's move on to the next chapter and see how we use RAG (Retrieval-Augmented Generation) to suggest updates to the project documentation in [Chapter 7: RAG (Retrieval-Augmented Generation) Documentation Updates](07_rag__retrieval_augmented_generation__documentation_updates.md).\n\n---\n# Chapter 7: RAG (Retrieval-Augmented Generation) Documentation Updates\n\nIn [Chapter 6: Groq API Integration](06_groq_api_integration.md), we learned how to use the Groq API to generate insights about code changes. But what about the *documentation*? How do we keep it up-to-date when code changes are made?\n\nThat's where **RAG (Retrieval-Augmented Generation) Documentation Updates** comes in!\n\nImagine you're reading a project's documentation, and it's completely out of sync with the current code. Frustrating, right? RAG helps solve this. It's like having a helpful assistant who automatically finds the relevant sections in your existing documentation and suggests updates based on new code changes.\n\n**Central Use Case:** When a developer adds a new function, we want `CodeRoast` to automatically find the relevant part of the project documentation and suggest adding a description of the new function. This keeps our documentation fresh and useful!\n\n## Key Concepts\n\nLet's break down the key concepts behind RAG Documentation Updates:\n\n1.  **Retrieval:** This is the process of finding the relevant parts of the existing documentation that are related to the code changes. Think of it like searching a library for books that are related to a specific topic. In CodeRoast, we load a `project_docs.txt` and then retrieve the most relevant parts.\n\n2.  **Augmentation:** This is the process of combining the retrieved documentation with the code changes to create a prompt for the AI model. It's like giving the AI model the context it needs to understand the code changes and suggest appropriate documentation updates.\n\n3.  **Generation:** This is the process of using the AI model to generate the suggested documentation updates. It's like asking the AI model to write a new section for the documentation based on the code changes and the existing documentation. We call the Groq API to do the actual writing.\n\n4.  **Markdown:** Markdown is a simple way to format text. It's used to write the project documentation. It's like using a simple set of rules to make the documentation look nice.\n\n## How to Use RAG: A Simple Example\n\nLet's imagine we have a project with a simple `project_docs.txt` file:\n\n```\n# My Project Documentation\n\nThis project is all about doing cool stuff.\n\n## Functions\n\n### add(x, y)\nAdds two numbers together.\n```\n\nAnd we add a new function:\n\n```python\ndef subtract(x, y):\n  \"\"\"Subtracts two numbers.\"\"\"\n  return x - y\n```\n\nHere's how we can use RAG to suggest adding documentation for this new function (simplified):\n\n```python\nimport os\nimport requests\nimport json\n\n# Your Groq API key\nGROQ_API_KEY = \"YOUR_GROQ_API_KEY\" # Replace with your actual key\nGROQ_ENDPOINT = \"https://api.groq.com/openai/v1/chat/completions\"\n\n# The existing documentation\nexisting_docs = \"\"\"\n# My Project Documentation\n\nThis project is all about doing cool stuff.\n\n## Functions\n\n### add(x, y)\nAdds two numbers together.\n\"\"\"\n\n# The new function\nnew_function = \"\"\"\ndef subtract(x, y):\n  \\\"\\\"\\\"Subtracts two numbers.\\\"\\\"\\\"\n  return x - y\n\"\"\"\n\n# The prompt\nprompt = f\"\"\"\nYou are an expert technical writer. Update the following documentation to include this new function:\n\n{new_function}\n\nHere is the existing documentation:\n\n{existing_docs}\n\nRespond in Markdown format.\n\"\"\"\n\n# Construct the payload\npayload = {\n    \"model\": \"llama3-70b-8192\", # You should check with Groq for the latest model\n    \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n}\n\n# Set the headers\nheaders = {\n    \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\n# Make the request\nresponse = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n\n# Parse the response\nresult = response.json()\n\n# Extract the summary\ndocumentation_update = result[\"choices\"][0][\"message\"][\"content\"].strip()\n\n# Print the summary\nprint(documentation_update)\n```\n\nThis simplified code does the following:\n\n1.  **Sets up the API key and endpoint:** It defines the `GROQ_API_KEY` and `GROQ_ENDPOINT` variables.  **Important:** Replace `\"YOUR_GROQ_API_KEY\"` with your actual Groq API key.\n2.  **Defines the existing documentation and the new function:** It defines the `existing_docs` and `new_function` variables.\n3.  **Constructs the prompt:** It defines the `prompt` variable, which tells the Groq API what we want it to do.\n4.  **Calls the Groq API:** It makes a request to the Groq API to generate the documentation update.\n5.  **Prints the documentation update:** It prints the documentation update to the console.\n\n**Example Output:**\n\n```markdown\n### subtract(x, y)\nSubtracts two numbers.\n```\n\nThis output suggests adding a new section to the `project_docs.txt` file for the `subtract` function.\n\n## Under the Hood: How RAG Works\n\nLet's take a closer look at how RAG works internally.\n\nHere's a sequence diagram illustrating the flow:\n\n```mermaid\nsequenceDiagram\n    participant BE as Backend (FastAPI)\n    participant Docs as Existing Documentation\n    participant Code as Code Changes\n    participant Groq as Groq API\n\n    BE->>Docs: Retrieve Relevant Sections\n    BE->>Code: Augment with Code Changes\n    BE->>Groq: POST /openai/v1/chat/completions (Prompt + Context)\n    activate Groq\n    Groq->>Groq: AI Model Generation\n    Groq-->>BE: Markdown (Documentation Update)\n    deactivate Groq\n    BE->>BE: Process Results\n```\n\n1.  The **Backend (FastAPI)** retrieves the relevant sections from the **Existing Documentation** (`project_docs.txt`). In our basic example, we just load the whole document.\n2.  The **Backend (FastAPI)** augments the retrieved sections with the **Code Changes** to create a prompt.\n3.  The **Backend (FastAPI)** sends a POST request to the **Groq API** with the prompt.\n4.  The **Groq API** uses its AI models to generate the documentation update.\n5.  The **Groq API** returns the documentation update in Markdown format to the **Backend (FastAPI)**.\n6.  The **Backend (FastAPI)** processes the results and stores them in the database or displays them in the frontend.\n\n### Diving into the Code\n\nLet's look at the code that implements the RAG documentation update. This code can be found in `backend/ai_analyse.py` and `backend/prReview.py` inside the `update_documentation_with_rag` function.\n\n```python\ndef update_documentation_with_rag(pr_description: str, code_diff: str, docs_path=\"backend/project_docs.txt\"):\n    \"\"\"\n    Performs a simple RAG-based call to Groq to update documentation.\n    \"\"\"\n    # Step 1: Load the existing documentation\n    try:\n        with open(docs_path, \"r\") as f:\n            existing_docs = f.read()\n    except FileNotFoundError:\n        logging.error(f\"[\u274c] Documentation file not found at {docs_path}\")\n        return {\"error\": \"Documentation file not found.\"}\n\n    # Step 2: Very basic RAG - extract top paragraphs that contain overlap\n    paragraphs = existing_docs.split(\"\\n\\n\")\n    keywords = pr_description.lower().split()\n    ranked_paragraphs = sorted(paragraphs, key=lambda para: sum(word in para.lower() for word in keywords), reverse=True)\n    context = \"\\n\\n\".join(ranked_paragraphs[:3])  # Top 3 paragraphs as \"retrieved context\"\n\n    # Step 3: Build the prompt\n    doc_prompt = f\"\"\"\nYou are an expert technical writer and code documentation assistant.\n\nA developer has made the following changes to the codebase:\n\n## PR Description:\n{pr_description}\n\n## Code Diff:\n{code_diff}\n\n## Relevant Existing Documentation:\n{context}\n\nPlease provide additions or updates to the documentation based on the above code change. \nFormat your response in **Markdown**, and only include sections that should be added or updated. \nIf the change doesn't require doc updates, say so.\n\"\"\"\n\n    # Step 4: Call Groq API\n    payload = {\n        \"model\": \"llama3-70b-8192\",\n        \"messages\": [{\"role\": \"user\", \"content\": doc_prompt}]\n    }\n\n    try:\n        # The Groq API code from before\n        response = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n        response.raise_for_status()\n        result = response.json()\n        return {\n            \"doc_update\": result[\"choices\"][0][\"message\"][\"content\"].strip()\n        }\n\n    except Exception as e:\n        logging.exception(\"Groq API call for documentation update failed\")\n        return {\"error\": str(e)}\n```\n\nThis code does the following:\n\n1.  **Loads the existing documentation:**\n    *   `with open(docs_path, \"r\") as f: existing_docs = f.read()`: This code reads the contents of the `project_docs.txt` file into the `existing_docs` variable.  It also has error handling in case the documentation is not found.\n2.  **Retrieves relevant sections:**\n    *   It splits the documentation into paragraphs and ranks them based on how many keywords from the pull request description they contain.\n    *   This is a simplified approach to RAG. More sophisticated techniques could involve vector embeddings and similarity search.\n    *   `context = \"\\n\\n\".join(ranked_paragraphs[:3])`: This joins the top 3 paragraphs to get the context.\n3.  **Builds the prompt:**\n    *   It constructs a prompt that includes the pull request description, the code diff, and the relevant sections from the existing documentation.\n    *   It tells the AI model to provide additions or updates to the documentation in Markdown format.\n4.  **Calls the Groq API:**\n    *   This part is the same as in the [Groq API Integration](06_groq_api_integration.md) chapter.  It sends the prompt to the Groq API and gets the documentation update.\n\n## Conclusion\n\nIn this chapter, we explored RAG (Retrieval-Augmented Generation) Documentation Updates. We learned about the key concepts, how to use RAG to suggest documentation updates, and how it works internally. We saw how RAG helps keep our project documentation up-to-date with the latest code changes.\n\nNow that we can generate documentation updates, let's move on to the next chapter and see how we post those updates (and other AI insights) as comments on GitHub pull requests in [Chapter 8: PR Review Comment Posting (GitHub)](08_pr_review_comment_posting__github_.md).\n\n---\n# Chapter 8: PR Review Comment Posting (GitHub)\n\nIn [Chapter 7: RAG (Retrieval-Augmented Generation) Documentation Updates](07_rag__retrieval_augmented_generation__documentation_updates.md), we learned how to generate AI insights and documentation updates. But those insights are only helpful if developers *see* them!\n\nThat's where **PR Review Comment Posting (GitHub)** comes in!\n\nImagine `CodeRoast` analyzing a pull request and generating amazing insights, but then keeping them secret. Useless, right? This component solves that problem. It's like an automated messenger that delivers the AI's review directly to the developer where they are already working \u2013 on the pull request in GitHub.\n\n**Central Use Case:** After `CodeRoast` analyzes a pull request, we want it to automatically post a comment on the pull request with a summary of the AI's findings. This gives developers immediate feedback on their code.\n\n## Key Concepts\n\nLet's break down the key concepts behind posting comments on GitHub:\n\n1.  **GitHub API:** Just like we use the Groq API to talk to Groq's AI, we use the GitHub API to talk to GitHub. It lets us do things like read pull requests, create comments, and update issues. Think of it as the phone line to GitHub.\n\n2.  **Authentication:** To use the GitHub API, we need to prove who we are. We do this with authentication, which involves providing a token or other credentials. It is like showing your ID to enter a building.\n\n3.  **Pull Request Object:** In the GitHub API, a pull request is represented as an object. This object has properties like the title, description, author, and comments. Think of it as a digital representation of the pull request.\n\n4.  **Comment Object:** Similarly, a comment is also represented as an object. This object has properties like the author, body, and creation date. It's like a digital sticky note attached to the pull request.\n\n## How to Use the Comment Posting: A Simple Example\n\nLet's imagine we have the AI analysis results and we want to post a simplified comment on a pull request.\n\nHere's a simplified version of how we can post a comment (assuming we already have the GitHub object and review data):\n\n```python\nfrom github import Github\n\n# Your GitHub Personal Access Token (PAT)\nGITHUB_TOKEN = \"YOUR_GITHUB_TOKEN\" # Replace with your actual token\n\n# Authenticate with GitHub\ng = Github(GITHUB_TOKEN)\n\n# Repository and Pull Request Details\nREPO_NAME = \"your-org/your-repo\" # Replace with your repo\nPR_NUMBER = 123 # Replace with your PR number\n\n# AI Review Data (simplified)\nreview_data = {\n    \"ai_summary\": \"This PR adds a new feature.\",\n    \"merge_confidence\": \"High\"\n}\n\ntry:\n    # Get the repo and PR objects\n    repo = g.get_repo(REPO_NAME)\n    pr = repo.get_pull(PR_NUMBER)\n\n    # Format the comment body\n    comment_body = f\"\"\"\n## \ud83e\udd16 AI Review\n\n### Summary\n{review_data['ai_summary']}\n\n### Merge Confidence: **{review_data['merge_confidence']}**\n\"\"\"\n\n    # Create the comment on the PR\n    pr.create_issue_comment(comment_body)\n    print(\"Successfully posted the comment!\")\n\nexcept Exception as e:\n    print(f\"Failed to post comment: {e}\")\n```\n\nThis simplified code does the following:\n\n1.  **Authenticates with GitHub:** It uses your GitHub Personal Access Token (PAT) to authenticate with the GitHub API. **Important:** Replace `\"YOUR_GITHUB_TOKEN\"` with your actual PAT. You can generate a PAT in your GitHub settings. Be sure to grant the necessary permissions (e.g., `repo` scope).\n2.  **Gets the repository and pull request objects:** It uses the GitHub API to get the repository and pull request objects based on the repository name and pull request number.\n3.  **Formats the comment body:** It creates a string containing the comment body, including the AI summary and merge confidence.\n4.  **Posts the comment:** It uses the `pr.create_issue_comment` method to post the comment on the pull request.\n\n**How this will look on GitHub:**\n\nAfter running this code, you'll see a new comment on the specified pull request in GitHub. The comment will contain the AI summary and merge confidence, formatted as Markdown.\n\n## Under the Hood: How the Comment Posting Works\n\nLet's take a closer look at how the comment posting works internally.\n\nHere's a sequence diagram illustrating the flow:\n\n```mermaid\nsequenceDiagram\n    participant BE as Backend (FastAPI)\n    participant GH as GitHub API\n\n    BE->>GH: Get Pull Request\n    activate GH\n    GH-->>BE: Pull Request Object\n    deactivate GH\n    BE->>BE: Format Comment Body\n    BE->>GH: Create Comment\n    activate GH\n    GH-->>BE: Success/Failure\n    deactivate GH\n```\n\n1.  The **Backend (FastAPI)** uses the GitHub API to get the pull request object.\n2.  The **GitHub API** returns the pull request object to the **Backend (FastAPI)**.\n3.  The **Backend (FastAPI)** formats the comment body using the AI review data.\n4.  The **Backend (FastAPI)** uses the GitHub API to create a comment on the pull request.\n5.  The **GitHub API** creates the comment and returns a success or failure message to the **Backend (FastAPI)**.\n\n### Diving into the Code\n\nLet's look at the relevant code from `backend/prReview.py`:\n\n```python\nfrom github import Github\nimport logging\n\ndef post_review_comment(g, repo_name, pr_number, review_data):\n    \"\"\"\n    Posts the AI review as a comment on the pull request.\n    \n    Args:\n        g: Authenticated GitHub instance\n        repo_name: Repository name (owner/repo)\n        pr_number: Pull request number\n        review_data: The AI review data dictionary\n    \"\"\"\n    try:\n        # Get the repo and PR objects\n        repo = g.get_repo(repo_name)\n        pr = repo.get_pull(pr_number)\n        \n        # Format the comment body\n        comment_body = f\"\"\"\n## \ud83e\udd16 AI Code Review\n\n### Summary\n{review_data.get('ai_summary', 'No summary available')}\n\n### Merge Confidence: **{review_data.get('merge_confidence', 'Unknown')}**\n\n### Code Quality Review\n- **Syntax:** {review_data.get('code_quality', {}).get('syntax_check', 'Not analyzed')}\n- **Style:** {review_data.get('code_quality', {}).get('style_check', 'Not analyzed')}\n- **Functionality:** {review_data.get('code_quality', {}).get('functionality_check', 'Not analyzed')}\n- **Overall Rating:** {review_data.get('code_quality', {}).get('final_rating', 'Not rated')}\n\n---\n*This review was automatically generated by AI.*\n\"\"\"\n        \n        # Create the comment on the PR\n        pr.create_issue_comment(comment_body)\n        logging.info(f\"[\u2705] Posted AI review comment on PR #{pr_number} in {repo_name}\")\n        print(f\"[\u2705] Posted AI review comment on PR #{pr_number} in {repo_name}\")\n        return True\n    except Exception as e:\n        logging.exception(f\"[\u274c] Failed to post comment on PR #{pr_number}: {str(e)}\")\n        print(f\"[\u274c] Failed to post comment on PR #{pr_number}: {str(e)}\")\n        return False\n```\n\nThis code does the following:\n\n1.  `repo = g.get_repo(repo_name)`: Gets the repository object from GitHub using the `repo_name`.  `g` is the authenticated Github instance.\n2.  `pr = repo.get_pull(pr_number)`: Gets the pull request object from the repository using the `pr_number`.\n3.  It formats the comment body using f-strings and the `review_data` dictionary. It uses `.get()` to safely access the values, providing default values if they are missing.\n4.  `pr.create_issue_comment(comment_body)`: Creates the comment on the pull request using the formatted `comment_body`.\n5.  It includes `try...except` block to handle potential errors.\n\n## Conclusion\n\nIn this chapter, we explored PR Review Comment Posting (GitHub). We learned about the key concepts, how to post comments on pull requests using the GitHub API, and how it works internally. We saw how this component delivers AI insights directly to developers, making the code review process more efficient.\n\nNow that we can post comments on GitHub, let's move on to the next chapter and see how we store all the AI insights and PR metadata in [Chapter 9: Snowflake Data Storage](09_snowflake_data_storage.md) for analytics.\n\n\nRelevant Code Snippets:\n--- File: README.md ---\n# \ud83e\udd16 Code Roast : An AI-Powered Pull Request Review System\n\n> An end-to-end intelligent platform for automated code review, documentation updates, and pull request analysis \u2014 powered by LLMs, Tree-sitter, and Snowflake.\n\n---\n\n## \ud83d\ude80 Overview\n\nThis system integrates with GitHub to automatically:\n- Analyze pull requests using AI (via [Groq](https://groq.com))\n- Parse and understand diffs using [Tree-sitter](https://tree-sitter.github.io/)\n- Perform RAG (Retrieval-Augmented Generation) to update project documentation\n- Store all insights in **Snowflake** for analytics\n- Display all PRs, diffs, AI feedback, and doc suggestions via a beautiful **Web App**\n\nWhether you're an engineering team lead, reviewer, or contributor \u2014 you get fast, reliable, and insightful AI-powered review feedback at every step.\n\n---\n\n## \ud83e\udde0 Features\n\n- \u2705 **AI PR Summary**  \n  Instant analysis of pull request purpose, syntax/style/functionality, and merge confidence.\n\n- \ud83d\udcdd **Automatic Docstring Suggestions**  \n  Extract or generate missing/updated docstrings for new or changed functions.\n\n- \ud83d\udcd8 **RAG-based Documentation Updates**  \n  Contextual retrieval from `project_docs.txt` with AI-generated Markdown additions.\n\n- \ud83d\udcc4 **Code Diff Parsing**  \n  Tree-sitter-based code structure diffing to isolate changed functions only.\n\n- \ud83d\udcac **GitHub Comment Posting**  \n  Summaries are posted as comments directly on PRs, no need to open another tool.\n\n- \ud83d\udcca **Snowflake Integration**  \n  All AI-generated insights and PR metadata stored for reporting, audit, and search.\n\n- \ud83c\udf10 **Web App Dashboard**  \n  Browse all PRs, view diffs, AI reviews, and doc updates in a single UI.\n\n---\n\n## \ud83e\uddf1 Tech Stack\n\n| Layer              | Tech Used                                      |\n|--------------------|-----------------------------------------------|\n| **AI Models**       | [Groq API (LLaMA 3)](https://groq.com)        |\n| **Diff Parsing**    | [Tree-sitter](https://tree-sitter.github.io/) |\n| **Backend API**     | [FastAPI](https://fastapi.tiangolo.com)       |\n| **Git Integration** | GitHub App + `PyGithub`                       |\n| **RAG Engine**      | Basic keyword-matching from local docs        |\n| **Database**        | [Snowflake](https://www.snowflake.com)        |\n| **Frontend**        | HTML, JS, CSS (Markdown + Diff Viewers)       |\n\n---\n\n## \u2699\ufe0f How It Works\n\n1. \ud83d\udd14 **GitHub Webhook Trigger**\n   - On PR open/update \u2192 sends payload to FastAPI backend\n\n2. \ud83e\udde0 **AI Review Processing**\n   - Extracts code diffs, changed functions\n   - Groq generates AI summary, confidence rating, and docstrings\n\n3. \ud83d\udcd8 **Documentation Updates**\n   - Local project docs indexed via simple RAG\n   - Groq suggests Markdown additions\n\n4. \ud83d\udcbe **Data Storage**\n   - All results are saved in Snowflake for dashboard & analytics\n\n5. \ud83c\udf10 **Web App UI**\n   - Explore PRs, view diffs, AI reviews, and doc updates\n\n\n--- File: backend/ai_analyse.py ---\nimport requests\nimport json\nimport logging\nimport snowflake.connector\nimport random\nimport datetime\n\nGROQ_API_KEY = \"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\"\nGROQ_ENDPOINT = \"https://api.groq.com/openai/v1/chat/completions\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\n# Snowflake setup\nconn = snowflake.connector.connect(\n    user=\"raghavg332\",\n    password=\"Qa29Kh4MptfGHEW\",\n    account=\"KXCIVVH-LL27432\",\n    warehouse=\"COMPUTE_WH\",\n    database=\"PR_DASHBOARD\",\n    schema=\"PUBLIC\"\n)\ncur = conn.cursor()\n\ndef update_documentation_with_rag(pr_description: str, code_diff: str, docs_path=\"backend/project_docs.txt\"):\n    \"\"\"\n    Performs a simple RAG-based call to Groq to update documentation.\n    \"\"\"\n    # Step 1: Load the existing documentation\n    try:\n        with open(docs_path, \"r\") as f:\n            existing_docs = f.read()\n    except FileNotFoundError:\n        logging.error(f\"[\u274c] Documentation file not found at {docs_path}\")\n        return {\"error\": \"Documentation file not found.\"}\n\n    # Step 2: Very basic RAG - extract top paragraphs that contain overlap\n    paragraphs = existing_docs.split(\"\\n\\n\")\n    keywords = pr_description.lower().split()\n    ranked_paragraphs = sorted(paragraphs, key=lambda para: sum(word in para.lower() for word in keywords), reverse=True)\n    context = \"\\n\\n\".join(ranked_paragraphs[:3])  # Top 3 paragraphs as \"retrieved context\"\n\n    # Step 3: Build the prompt\n    doc_prompt = f\"\"\"\nYou are an expert technical writer and code documentation assistant.\n\nA developer has made the following changes to the codebase:\n\n## PR Description:\n{pr_description}\n\n## Code Diff:\n{code_diff}\n\n## Relevant Existing Documentation:\n{context}\n\nPlease provide additions or updates to the documentation based on the above code change. \nFormat your response in **Markdown**, and only include sections that should be added or updated. \nIf the change doesn't require doc updates, say so.\n\"\"\"\n\n    # Step 4: Call Groq API\n    payload = {\n        \"model\": \"llama3-70b-8192\",\n        \"messages\": [{\"role\": \"user\", \"content\": doc_prompt}]\n    }\n\n    try:\n        response = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n        response.raise_for_status()\n        result = response.json()\n        return {\n            \"doc_update\": result[\"choices\"][0][\"message\"][\"content\"].strip()\n        }\n\n    except Exception as e:\n        logging.exception(\"Groq API call for documentation update failed\")\n        return {\"error\": str(e)}\n\n\ndef build_full_prompt(pr_description: str, code_diff: str) -> str:\n    return f\"\"\"\nYou are an AI code reviewer and documentation assistant.\n\nA developer submitted the following pull request. Please do the following:\n1. \ud83d\udd0d Summarize the overall purpose of the PR.\n2. \ud83e\udde0 Rate the merge confidence as: High / Medium / Low.\n3. \u2705 Review the code for:\n   - Syntax issues\n   - Style (PEP8)\n   - Functionality correctness (does it match the description?)\n4. \ud83d\udcdd Extract or suggest updated docstrings for any changed or added functions/classes.\n\n## PR Description:\n{pr_description}\n\n## Code Diff:\n{code_diff}\n\nRespond ONLY in this JSON format:\n\n{{\n  \"ai_summary\": \"...\",\n  \"merge_confidence\": \"1-10\",\n  \"code_quality\": {{\n    \"syntax_check\": \"...\",\n    \"style_check\": \"...\",\n    \"functionality_check\": \"...\",\n    \"final_rating\": \"Excellent / Good / Needs Work / Critical Issues\"\n  }}\n}}\n\"\"\"\n\ndef review_and_store_pr(pr_description: str, code_diff: str, pr_title: str, pr_author: str, pr_status: str):\n    docstring = update_documentation_with_rag(pr_description, code_diff)\n    print(docstring)\n    print('\\n\\n')\n    pr_id = random.randint(1000, 9999)\n    prompt = build_full_prompt(pr_description, code_diff)\n\n    payload = {\n        \"model\": \"llama3-70b-8192\",\n        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n    }\n\n    try:\n        response = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n        response.raise_for_status()\n        result = response.json()\n\n        try:\n            content = result[\"choices\"][0][\"message\"][\"content\"].strip()\n            review_data = json.loads(content)\n\n            # Flatten the code quality object for easier insert\n            code_quality_str = (\n                f\"Syntax: {review_data['code_quality']['syntax_check']}\\n\"\n                f\"Style: {review_data['code_quality']['style_check']}\\n\"\n                f\"Functionality: {review_data['code_quality']['functionality_check']}\\n\"\n                f\"Rating: {review_data['code_quality']['final_rating']}\"\n            )\n            cur.execute(f\"\"\"\n                INSERT INTO PULL_REQUESTS (\n                    ID, TITLE, AUTHOR, STATUS, CREATED_AT, UPDATED_AT\n                )\n                VALUES (%s, %s, %s, %s, %s, %s);\n            \"\"\", (\n                pr_id,\n                pr_title,\n                pr_author,\n                pr_status,\n                datetime.datetime.now(),\n                datetime.datetime.now()\n            ))\n            conn.commit()\n\n            # Insert into Snowflake\n            cur.execute(f\"\"\"\n                INSERT INTO PR_ANALYSIS (\n                    PR_ID, AI_SUMMARY, MERGE_CONFIDENCE, CODE_QUALITY,\n                    PR_DESCRIPTION, DOCSTRINGS, DIFF\n                )\n                VALUES (%s, %s, %s, %s, %s, %s, %s);\n            \"\"\", (\n                pr_id,\n                review_data[\"ai_summary\"],\n                review_data[\"merge_confidence\"],\n                code_quality_str,\n                pr_description,\n                docstring['doc_update'],\n                code_diff\n            ))\n\n            conn.commit()\n\n            logging.info(f\"[\u2705] PR #{pr_id} analysis saved to Snowflake.\")\n            return review_data\n\n        except json.JSONDecodeError:\n            logging.error(\"[\u274c] Groq returned non-JSON output.\")\n            return {\"error\": \"Invalid JSON from Groq\"}\n\n    except Exception as e:\n        logging.exception(\"Groq API or Snowflake insert failed\")\n        return {\"error\": str(e)}\n\n# # \ud83e\uddea Example test\n# if __name__ == \"__main__\":\n#     code_diff = \"\"\"\n# diff --git a/app/main.py b/app/main.py\n# index a1b2c3d..d4e5f6g 100644\n# --- a/app/main.py\n# +++ b/app/main.py\n# @@ def main():\n# -    print(\"Hello\")\n# +    log_greeting(\"Hello, world!\")\n# +    print(\"Execution completed.\")\n\n# +def log_greeting(message):\n# +    \\\"\\\"\\\"Logs the greeting message to a file\\\"\\\"\\\"\n# +    with open(\"logs.txt\", \"a\") as log_file:\n# +        log_file.write(f\"Greeting logged: {message}\\\\n\")\n# \"\"\"\n#     pr_description = \"Update main function to greet the world.\"\n#     pr_title = \"Update main function\"\n#     pr_author = \"johndoe\"\n#     pr_status = \"OPEN\"\n\n#     result = review_and_store_pr(pr_description, code_diff, pr_title, pr_author, pr_status)\n#     print(json.dumps(result, indent=2))\n\n--- File: backend/prReview.py ---\nimport os, re\nfrom github import Auth\nfrom github import Github, GithubIntegration\nfrom github.GithubException import GithubException\nfrom tree_sitter import Language, Parser\nimport tree_sitter_python as tspython\nimport tree_sitter_javascript as tsjavascript\nimport tree_sitter_cpp as tscpp\nimport requests\nimport json\nimport logging\nimport snowflake.connector\nimport random\nimport datetime\n\n\nGROQ_API_KEY = \"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\"\nGROQ_ENDPOINT = \"https://api.groq.com/openai/v1/chat/completions\"\n\ndef authenticate_github(app_id: int, installation_id, private_key: str):\n    gi = GithubIntegration(integration_id=app_id, private_key=private_key)\n    g = gi.get_github_for_installation(installation_id)\n    return g\n\ndef get_pull_request(g, repo_name: str, pr_number: int):\n    repo = g.get_repo(repo_name)\n    pr = repo.get_pull(pr_number)\n    return pr\n\ndef parse_changed_lines(patch: str):\n    added_or_modified_lines = set()\n    deleted_lines = set()\n    for line in patch.splitlines():\n        if line.startswith(\"@@\"):\n            match = re.findall(r\"@@ -(\\d+)(?:,(\\d+))? \\+(\\d+)(?:,(\\d+))? @@\", line)\n            if match:\n                old_start, old_count, new_start, new_count = match[0]\n\n                old_start = int(old_start)\n                old_count = int(old_count or 1)\n                for l in range(old_start, old_start + old_count):\n                    deleted_lines.add(l)\n\n                new_start = int(new_start)\n                new_count = int(new_count or 1)\n                for l in range(new_start, new_start + new_count):\n                    added_or_modified_lines.add(l)\n\n    return {\n        \"added_or_modified_lines\": added_or_modified_lines,\n        \"deleted_lines\": deleted_lines\n    }\n\ndef get_file_contents(g, repo_name: str, file_path: str, commit_sha: str):\n    repo = g.get_repo(repo_name)\n    try:\n        contents = repo.get_contents(file_path, ref=commit_sha)\n        return contents.decoded_content.decode()\n    except GithubException as e:\n        if e.status == 404:\n            return \"\"\n        else:\n            raise e\n\ndef get_lines_changed(pr):\n    files = pr.get_files()\n    file_changes = {}\n    for file in files:\n        if file.status == \"removed\":\n            continue\n        file_changes[file.filename] = parse_changed_lines(file.patch)\n\n    return file_changes\n\ndef get_paraser(language_name):\n\n    PY_LANGUAGE = Language(tspython.language())\n    JS_LANGUAGE = Language(tsjavascript.language())\n    CPP_LANGUAGE = Language(tscpp.language())\n\n    LANGUAGES = {\n        'python': PY_LANGUAGE,\n        'javascript': JS_LANGUAGE,\n        'cpp': CPP_LANGUAGE,\n    }\n    parser = Parser(LANGUAGES[language_name])\n    return parser\n\ndef extract_functions(code, language_name, changed_lines):\n    parser = get_paraser(language_name)\n    tree = parser.parse(bytes(code, \"utf8\"))\n    root_node = tree.root_node\n\n    functions = []\n\n    def node_within_lines(node):\n        start_line = node.start_point[0] + 1\n        end_line = node.end_point[0] + 1\n        return any([start_line <= line <= end_line for line in changed_lines])\n    \n    def traverse(node):\n        if language_name == \"python\" and node.type == \"function_definition\":\n            if node_within_lines(node):\n                func_name_node = node.child_by_field_name('name')\n                func_name = func_name_node.text.decode()\n                functions.append({'name': func_name, 'code': node.text.decode()})\n        elif language_name == 'javascript' and node.type in ['function_declaration', 'method_definition', 'arrow_function']:\n            if node_within_lines(node):\n                func_name_node = node.child_by_field_name('name')\n                func_name = func_name_node.text.decode()\n                functions.append({'name': func_name, 'code': node.text.decode()})\n        elif language_name == 'cpp' and node.type in ['function_definition']:\n            if node_within_lines(node):\n                func_name_node = node.child_by_field_name('name')\n                func_name = func_name_node.text.decode()\n                functions.append({'name': func_name, 'code': node.text.decode()})\n        for child in node.children:\n            traverse(child)\n\n    traverse(root_node)\n    return functions\n\nheaders = {\n    \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\n# Snowflake setup\nconn = snowflake.connector.connect(\n    user=\"raghavg332\",\n    password=\"Qa29Kh4MptfGHEW\",\n    account=\"KXCIVVH-LL27432\",\n    warehouse=\"COMPUTE_WH\",\n    database=\"PR_DASHBOARD\",\n    schema=\"PUBLIC\"\n)\ncur = conn.cursor()\n\ndef update_documentation_with_rag(pr_description: str, code_diff: str, docs_path=\"backend/project_docs.txt\"):\n    \"\"\"\n    Performs a simple RAG-based call to Groq to update documentation.\n    \"\"\"\n    # Step 1: Load the existing documentation\n    try:\n        with open(docs_path, \"r\") as f:\n            existing_docs = f.read()\n    except FileNotFoundError:\n        logging.error(f\"[\u274c] Documentation file not found at {docs_path}\")\n        return {\"error\": \"Documentation file not found.\"}\n\n    # Step 2: Very basic RAG - extract top paragraphs that contain overlap\n    paragraphs = existing_docs.split(\"\\n\\n\")\n    keywords = pr_description.lower().split()\n    ranked_paragraphs = sorted(paragraphs, key=lambda para: sum(word in para.lower() for word in keywords), reverse=True)\n    context = \"\\n\\n\".join(ranked_paragraphs[:3])  # Top 3 paragraphs as \"retrieved context\"\n\n    # Step 3: Build the prompt\n    doc_prompt = f\"\"\"\nYou are an expert technical writer and code documentation assistant.\n\nA developer has made the following changes to the codebase:\n\n## PR Description:\n{pr_description}\n\n## Code Diff:\n{code_diff}\n\n## Relevant Existing Documentation:\n{context}\n\nPlease provide additions or updates to the documentation based on the above code change. \nFormat your response in **Markdown**, and only include sections that should be added or updated. \nIf the change doesn't require doc updates, say so.\n\"\"\"\n\n    # Step 4: Call Groq API\n    payload = {\n        \"model\": \"llama3-70b-8192\",\n        \"messages\": [{\"role\": \"user\", \"content\": doc_prompt}]\n    }\n\n    try:\n        response = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n        response.raise_for_status()\n        result = response.json()\n        return {\n            \"doc_update\": result[\"choices\"][0][\"message\"][\"content\"].strip()\n        }\n\n    except Exception as e:\n        logging.exception(\"Groq API call for documentation update failed\")\n        return {\"error\": str(e)}\n\n\ndef build_full_prompt(pr_description: str, code_diff_str: str) -> str:\n    return f\"\"\"\nYou are an AI code reviewer and documentation assistant.\n\nA developer submitted the following pull request. Please do the following:\n1. \ud83d\udd0d Summarize the overall purpose of the PR. Be a little detailed\n2. \u2705 Review the code for:\n   - Syntax issues in Python (Make up some Issue)\n   - Style (PEP8)\n   - Functionality correctness (does it match the description?)\n3. Based on the review, provide a merge confidence score from 1 to 10.\n4. \ud83d\udcdd Extract or suggest updated docstrings for any changed or added functions/classes.\n5. The Code Diff is made up of the old and new version of the code, if any of the field is empty it means, the function is either added or deleted respectively.\n\n## PR Description:\n{pr_description}\n\n## Code Diff:\n{code_diff_str}\n\nRespond ONLY in this JSON format without any additional text:\n\n{{\n  \"ai_summary\": \"...\",\n  \"merge_confidence\": \"1 - 10\",\n  \"code_quality\": {{\n    \"syntax_check\": \"...\",\n    \"style_check\": \"...\",\n    \"functionality_check\": \"...\",\n    \"final_rating\": \"Excellent / Good / Needs Work / Critical Issues\"\n  }}\n}}\n\"\"\"\n\ndef review_and_store_pr(pr_description: str, code_diff: dict, pr_title: str, pr_author: str, pr_status: str):\n    code_diff_str = \"\"\n    for file in list(code_diff.keys()):\n        code_diff_str += f\"File: {file}\\n\\n\"\n        diff = code_diff[file]\n\n        for func_name in list(diff.keys()):\n            old_code = diff[func_name]['old_code']\n            new_code = diff[func_name]['new_code']\n            if old_code is None:\n                old_code = \"\"\n            if new_code is None:\n                new_code = \"\"\n            code_diff_str += f\"Function: {func_name}\\n\\nOld Code:\\n{old_code}\\n\\nNew Code:\\n{new_code}\"\n    docstring = update_documentation_with_rag(pr_description, code_diff)\n    pr_id = random.randint(1000, 9999)\n    prompt = build_full_prompt(pr_description, code_diff_str)\n\n    payload = {\n        \"model\": \"llama3-70b-8192\",\n        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n    }\n\n    try:\n        response = requests.post(GROQ_ENDPOINT, headers=headers, json=payload)\n        response.raise_for_status()\n        result = response.json()\n\n        try:\n            content = result[\"choices\"][0][\"message\"][\"content\"].strip()\n            review_data = json.loads(content)\n\n            # Flatten the code quality object for easier insert\n            code_quality_str = (\n                f\"Syntax: {review_data['code_quality']['syntax_check']}\\n\"\n                f\"Style: {review_data['code_quality']['style_check']}\\n\"\n                f\"Functionality: {review_data['code_quality']['functionality_check']}\\n\"\n                f\"Rating: {review_data['code_quality']['final_rating']}\"\n            )\n            cur.execute(f\"\"\"\n                INSERT INTO PULL_REQUESTS (\n                    ID, TITLE, AUTHOR, STATUS, CREATED_AT, UPDATED_AT\n                )\n                VALUES (%s, %s, %s, %s, %s, %s);\n            \"\"\", (\n                pr_id,\n                pr_title,\n                pr_author,\n                pr_status,\n                datetime.datetime.now(),\n                datetime.datetime.now()\n            ))\n            conn.commit()\n\n            # Insert into Snowflake\n            cur.execute(f\"\"\"\n                INSERT INTO PR_ANALYSIS (\n                    PR_ID, AI_SUMMARY, MERGE_CONFIDENCE, CODE_QUALITY,\n                    PR_DESCRIPTION, DOCSTRINGS, DIFF\n                )\n                VALUES (%s, %s, %s, %s, %s, %s, %s);\n            \"\"\", (\n                pr_id,\n                review_data[\"ai_summary\"],\n                review_data[\"merge_confidence\"],\n                code_quality_str,\n                pr_description,\n                docstring['doc_update'],\n                code_diff_str\n            ))\n\n            conn.commit()\n\n            logging.info(f\"[\u2705] PR #{pr_id} analysis saved to Snowflake.\")\n            print(f\"[\u2705] PR #{pr_id} analysis saved to Snowflake.\")\n            return review_data\n\n        except json.JSONDecodeError:\n            logging.error(\"[\u274c] Groq returned non-JSON output.\")\n            print(\"[\u274c] Groq returned non-JSON output.\")\n            return {\"error\": \"Invalid JSON from Groq\"}\n\n    except Exception as e:\n        logging.exception(\"Groq API or Snowflake insert failed\")\n        return {\"error\": str(e)}\n\ndef process_pull_request(g, repo_name, pr_number, sender=None):\n    repo = g.get_repo(repo_name)\n    pr = repo.get_pull(pr_number)\n    pr_description = pr.title\n    pr_title = pr.title\n    pr_author = pr.user.login\n    pr_status = pr.state\n    code_diff = get_lines_changed(pr)\n    file_diff_func = {}\n    for file in list(code_diff.keys()):\n        diff_func = {}\n        new_code = get_file_contents(g, repo_name, file, pr.head.sha)\n        old_code = get_file_contents(g, repo_name, file, pr.base.sha)\n        added_or_modified_functions_after = extract_functions(new_code, \"python\", code_diff[file]['added_or_modified_lines'])\n        deleted_functions_before = extract_functions(old_code, \"python\", code_diff[file]['deleted_lines'])\n        old_functions_all = extract_functions(old_code, \"python\", set(range(1, len(old_code.split(\"\\n\")))))\n        new_functions_all = extract_functions(new_code, \"python\", set(range(1, len(new_code.split(\"\\n\")))))\n        for func in added_or_modified_functions_after:\n            matched_old_function = next((f for f in old_functions_all if f['name'] ==  func['name']), None)\n            diff_func[func['name']] = {\n                \"new_code\": func['code'],\n                \"old_code\": matched_old_function['code'] if matched_old_function else None\n            }\n        \n        for func in deleted_functions_before:\n            matched_new_function = next((f for f in new_functions_all if f['name'] ==  func['name']), None)\n            diff_func[func['name']] = {\n                \"new_code\": matched_new_function['code'] if matched_new_function else None,\n                \"old_code\": func['code']\n            }\n        \n        file_diff_func[file] = diff_func\n    \n    # Get the review data and post a comment\n    review_data = review_and_store_pr(pr_description, file_diff_func, pr_title, pr_author, pr_status)\n    \n    # Post the review as a comment on the PR\n    post_review_comment(g, repo_name, pr_number, review_data)\n\n\ndef post_review_comment(g, repo_name, pr_number, review_data):\n    \"\"\"\n    Posts the AI review as a comment on the pull request.\n    \n    Args:\n        g: Authenticated GitHub instance\n        repo_name: Repository name (owner/repo)\n        pr_number: Pull request number\n        review_data: The AI review data dictionary\n    \"\"\"\n    try:\n        # Get the repo and PR objects\n        repo = g.get_repo(repo_name)\n        pr = repo.get_pull(pr_number)\n        \n        # Format the comment body\n        comment_body = f\"\"\"\n## \ud83e\udd16 AI Code Review\n\n### Summary\n{review_data.get('ai_summary', 'No summary available')}\n\n### Merge Confidence: **{review_data.get('merge_confidence', 'Unknown')}**\n\n### Code Quality Review\n- **Syntax:** {review_data.get('code_quality', {}).get('syntax_check', 'Not analyzed')}\n- **Style:** {review_data.get('code_quality', {}).get('style_check', 'Not analyzed')}\n- **Functionality:** {review_data.get('code_quality', {}).get('functionality_check', 'Not analyzed')}\n- **Overall Rating:** {review_data.get('code_quality', {}).get('final_rating', 'Not rated')}\n\n---\n*This review was automatically generated by AI.*\n\"\"\"\n        \n        # Create the comment on the PR\n        pr.create_issue_comment(comment_body)\n        logging.info(f\"[\u2705] Posted AI review comment on PR #{pr_number} in {repo_name}\")\n        print(f\"[\u2705] Posted AI review comment on PR #{pr_number} in {repo_name}\")\n        return True\n    except Exception as e:\n        logging.exception(f\"[\u274c] Failed to post comment on PR #{pr_number}: {str(e)}\")\n        print(f\"[\u274c] Failed to post comment on PR #{pr_number}: {str(e)}\")\n        return False\n\n--- File: main.py ---\nfrom fastapi import FastAPI, HTTPException\r\nfrom fastapi.middleware.cors import CORSMiddleware\r\nimport snowflake.connector\r\nimport os\r\nfrom dotenv import load_dotenv\r\nimport pandas as pd\r\n\r\nload_dotenv()\r\n\r\napp = FastAPI()\r\n\r\n# Snowflake credentials\r\nSNOWFLAKE_USER = os.getenv(\"SNOWFLAKE_USER\", \"raghavg332\")\r\nSNOWFLAKE_PASSWORD = os.getenv(\"SNOWFLAKE_PASSWORD\", \"Qa29Kh4MptfGHEW\")\r\nSNOWFLAKE_ACCOUNT = os.getenv(\"SNOWFLAKE_ACCOUNT\", \"KXCIVVH-LL27432\")\r\nSNOWFLAKE_WAREHOUSE = os.getenv(\"SNOWFLAKE_WAREHOUSE\", \"COMPUTE_WH\")\r\nSNOWFLAKE_DATABASE = os.getenv(\"SNOWFLAKE_DATABASE\", \"PR_DASHBOARD\")\r\nSNOWFLAKE_SCHEMA = os.getenv(\"SNOWFLAKE_SCHEMA\", \"PUBLIC\")\r\n\r\napp.add_middleware(\r\n    CORSMiddleware,\r\n    allow_origins=[\"*\"],\r\n    allow_methods=[\"*\"],\r\n    allow_headers=[\"*\"],\r\n)\r\n\r\ndef get_conn():\r\n    \"\"\"Establish connection to Snowflake database\"\"\"\r\n    try:\r\n        conn = snowflake.connector.connect(\r\n            user=SNOWFLAKE_USER,\r\n            password=SNOWFLAKE_PASSWORD,\r\n            account=SNOWFLAKE_ACCOUNT,\r\n            warehouse=SNOWFLAKE_WAREHOUSE,\r\n            database=SNOWFLAKE_DATABASE,\r\n            schema=SNOWFLAKE_SCHEMA\r\n        )\r\n        return conn\r\n    except Exception as e:\r\n        print(f\"Error connecting to Snowflake: {e}\")\r\n        raise e\r\n\r\n@app.get(\"/api/prs\")\r\ndef fetch_all_prs():\r\n    try:\r\n        conn = get_conn()\r\n        cursor = conn.cursor(snowflake.connector.DictCursor)\r\n        \r\n        # Using cursor to get results as dictionaries\r\n        cursor.execute(\"SELECT id, title, author, status, created_at, updated_at FROM pull_requests ORDER BY updated_at DESC\")\r\n        results = cursor.fetchall()\r\n        \r\n        # Format the data to match frontend expectations\r\n        formatted_results = []\r\n        for row in results:\r\n            # Capitalize the status to match frontend expectations\r\n            status = row['STATUS'].capitalize() if row['STATUS'] else 'Unknown'\r\n            \r\n            formatted_results.append({\r\n                \"id\": row['ID'],\r\n                \"title\": row['TITLE'],\r\n                \"author\": row['AUTHOR'],\r\n                \"status\": status,\r\n                \"created_at\": row['CREATED_AT'].strftime(\"%Y-%m-%d\") if row['CREATED_AT'] else None,\r\n                \"updated_at\": row['UPDATED_AT'].strftime(\"%Y-%m-%d\") if row['UPDATED_AT'] else None\r\n            })\r\n        \r\n        cursor.close()\r\n        conn.close()\r\n        return formatted_results\r\n    except Exception as e:\r\n        print(f\"Error fetching PRs: {e}\")\r\n        raise HTTPException(status_code=500, detail=f\"Database error: {str(e)}\")\r\n\r\n@app.get(\"/api/prs/{pr_id}\")\r\ndef fetch_pr_analysis(pr_id: int):\r\n    try:\r\n        conn = get_conn()\r\n        cursor = conn.cursor(snowflake.connector.DictCursor)\r\n        \r\n        # Fetch the analysis data\r\n        cursor.execute(f\"SELECT * FROM pr_analysis WHERE pr_id = {pr_id}\")\r\n        result = cursor.fetchone()\r\n        \r\n        if not result:\r\n            cursor.close()\r\n            conn.close()\r\n            raise HTTPException(status_code=404, detail=\"PR analysis not found\")\r\n        \r\n        # Format the data to match frontend expectations\r\n        formatted_result = {\r\n            \"pr_id\": result['PR_ID'],\r\n            \"ai_summary\": result['AI_SUMMARY'],\r\n            \"merge_confidence\": result['MERGE_CONFIDENCE'].capitalize(),  # Capitalize for frontend\r\n            \"merge_confidence_score\": float(result['MERGE_CONFIDENCE']) if result.get('MERGE_CONFIDENCE') else 0,\r\n            \"code_quality\": result['CODE_QUALITY'],\r\n            \"diff\": result['DIFF'],\r\n            \"doc_string\": result['DOCSTRINGS'] if 'DOCSTRINGS' in result else \"No documentation available for this PR.\"\r\n        }\r\n        \r\n        cursor.close()\r\n        conn.close()\r\n        return formatted_result\r\n    except Exception as e:\r\n        print(f\"Error fetching PR analysis: {e}\")\r\n        raise HTTPException(status_code=500, detail=f\"Error: {str(e)}\")\r\n\r\n'''\r\n@app.get(\"/api/prs\")\r\ndef fetch_all_prs():\r\n    # Test data\r\n    test_data = [\r\n        {\r\n            \"id\": 1,\r\n            \"title\": \"Add new feature\",\r\n            \"author\": \"johndoe\",\r\n            \"status\": \"Open\",\r\n            \"created_at\": \"2023-01-01\",\r\n            \"updated_at\": \"2023-01-02\"\r\n        },\r\n        {\r\n            \"id\": 2,\r\n            \"title\": \"Fix bug in login\",\r\n            \"author\": \"janedoe\",\r\n            \"status\": \"Merged\",\r\n            \"created_at\": \"2023-01-03\",\r\n            \"updated_at\": \"2023-01-04\"\r\n        }\r\n    ]\r\n    return test_data\r\n\r\n@app.get(\"/api/prs/{pr_id}\")\r\ndef fetch_pr_analysis(pr_id: int):\r\n    # Test data\r\n    test_data = {\r\n        \"pr_id\": pr_id,\r\n        \"ai_summary\": \"This PR adds a new feature that improves user experience\",\r\n        \"merge_confidence\": \"High\",\r\n        \"code_quality\": \"Good code quality. No major issues found.\",\r\n        \"diff\": \"```\\n+ def new_function():\\n+     return 'new feature'\\n```\",\r\n        \"doc_string\": \"This PR introduces a new function that returns 'new feature'.\"\r\n    }\r\n    return test_data\r\n'''\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 9: Snowflake Data Storage`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "# Chapter 9: Snowflake Data Storage\n\nIn [Chapter 8: PR Review Comment Posting (GitHub)](08_pr_review_comment_posting__github_.md), we learned how to post AI insights as comments on GitHub pull requests. But what happens to all that valuable information *after* it's been posted? Does it just disappear?\n\nThat's where **Snowflake Data Storage** comes in!\n\nImagine `CodeRoast` generating all these insightful AI reviews, suggesting documentation updates, and identifying potential code issues. Now, imagine being able to analyze *all* of that data over time. Which code areas are most often flagged? Which developers consistently produce high-quality code? This component makes that possible. It's like a giant filing cabinet that efficiently organizes and stores code review data for reporting, auditing, and search.\n\n**Central Use Case:** We want to store all the AI-generated insights, PR metadata (like author, title, and status), and documentation updates in a central location so we can later generate reports on code quality trends, identify areas for improvement, and track the impact of `CodeRoast` over time.\n\n## Key Concepts\n\nLet's break down the key concepts behind Snowflake Data Storage:\n\n1.  **Data Warehouse:** Think of a data warehouse like a giant spreadsheet that's designed for storing and analyzing large amounts of data. Snowflake is a cloud-based data warehouse. It's optimized for analytical queries, not for running applications (like a traditional database).\n\n2.  **Tables:** Tables are like individual sheets in our giant spreadsheet. Each table stores a specific type of data, such as pull request details or AI analysis results.\n\n3.  **Rows:** Rows are the individual entries in our tables. Each row represents a single piece of data, such as a specific pull request or a single AI analysis result.\n\n4.  **Columns:** Columns are the fields in our tables. Each column stores a specific attribute of the data, such as the pull request title, author, or merge confidence.\n\n5.  **SQL (Structured Query Language):** SQL is the language we use to interact with the Snowflake data warehouse. We use SQL to create tables, insert data, and query the data for analysis.\n\n## How to Use Snowflake: A Simple Example\n\nLet's imagine we want to store information about pull requests in Snowflake.\n\nHere's how we can create a table to store pull request data (simplified):\n\n```sql\nCREATE TABLE pull_requests (\n    id INT,\n    title VARCHAR(255),\n    author VARCHAR(255),\n    status VARCHAR(50),\n    created_at DATE,\n    updated_at DATE\n);\n```\n\nThis SQL code does the following:\n\n*   `CREATE TABLE pull_requests`: This tells Snowflake to create a new table named `pull_requests`.\n*   `id INT`: This defines a column named `id` that stores integers (whole numbers). This will likely store the ID of a PR\n*   `title VARCHAR(255)`: This defines a column named `title` that stores text (strings) with a maximum length of 255 characters. This will likely store the name of a PR\n*   `author VARCHAR(255)`: This defines a column named `author` that stores the name of the PR's author.\n*   `status VARCHAR(50)`: This defines a column named `status` that stores the status of the pull request (e.g., \"Open\", \"Merged\", \"Closed\").\n*   `created_at DATE`: This defines a column named `created_at` that stores the date when the pull request was created.\n*   `updated_at DATE`: This defines a column named `updated_at` that stores the date when the pull request was last updated.\n\nNow, let's imagine we have a pull request and we want to insert its information into the table:\n\n```sql\nINSERT INTO pull_requests (id, title, author, status, created_at, updated_at)\nVALUES (1, 'Add new feature', 'johndoe', 'Open', '2023-01-01', '2023-01-02');\n```\n\nThis SQL code does the following:\n\n*   `INSERT INTO pull_requests`: This tells Snowflake to insert a new row into the `pull_requests` table.\n*   `(id, title, author, status, created_at, updated_at)`: This specifies the columns that we're inserting data into.\n*   `VALUES (1, 'Add new feature', 'johndoe', 'Open', '2023-01-01', '2023-01-02')`: This specifies the values that we're inserting into the columns.\n\nFinally, let's imagine we want to query the table to get all the open pull requests:\n\n```sql\nSELECT * FROM pull_requests WHERE status = 'Open';\n```\n\nThis SQL code does the following:\n\n*   `SELECT * FROM pull_requests`: This tells Snowflake to select all columns (`*`) from the `pull_requests` table.\n*   `WHERE status = 'Open'`: This specifies a condition that filters the results to only include rows where the `status` column is equal to `'Open'`.\n\n## Under the Hood: How Data Storage Works\n\nLet's take a closer look at how the data storage works internally.\n\nHere's a sequence diagram illustrating the flow:\n\n```mermaid\nsequenceDiagram\n    participant BE as Backend (FastAPI)\n    participant SF as Snowflake\n\n    BE->>SF: Execute SQL (INSERT statement)\n    activate SF\n    SF->>SF: Store Data\n    SF-->>BE: Success/Failure\n    deactivate SF\n```\n\n1.  The **Backend (FastAPI)** constructs a SQL `INSERT` statement to insert the AI insights and pull request metadata into Snowflake.\n2.  The **Backend (FastAPI)** sends the SQL `INSERT` statement to **Snowflake**.\n3.  **Snowflake** receives the SQL statement, stores the data in the appropriate table, and returns a success or failure message to the **Backend (FastAPI)**.\n\n### Diving into the Code\n\nLet's look at the code that inserts the data into Snowflake. This code can be found in `backend/ai_analyse.py` inside the `review_and_store_pr` function.\n\n```python\nimport snowflake.connector\nimport random\nimport datetime\n\n# Snowflake setup\nconn = snowflake.connector.connect(\n    user=\"raghavg332\",\n    password=\"Qa29Kh4MptfGHEW\",\n    account=\"KXCIVVH-LL27432\",\n    warehouse=\"COMPUTE_WH\",\n    database=\"PR_DASHBOARD\",\n    schema=\"PUBLIC\"\n)\ncur = conn.cursor()\n# ... (rest of the function)\n\n    cur.execute(f\"\"\"\n        INSERT INTO PULL_REQUESTS (\n            ID, TITLE, AUTHOR, STATUS, CREATED_AT, UPDATED_AT\n        )\n        VALUES (%s, %s, %s, %s, %s, %s);\n    \"\"\", (\n        pr_id,\n        pr_title,\n        pr_author,\n        pr_status,\n        datetime.datetime.now(),\n        datetime.datetime.now()\n    ))\n    conn.commit()\n\n    # Insert into Snowflake\n    cur.execute(f\"\"\"\n        INSERT INTO PR_ANALYSIS (\n            PR_ID, AI_SUMMARY, MERGE_CONFIDENCE, CODE_QUALITY,\n            PR_DESCRIPTION, DOCSTRINGS, DIFF\n        )\n        VALUES (%s, %s, %s, %s, %s, %s, %s);\n    \"\"\", (\n        pr_id,\n        review_data[\"ai_summary\"],\n        review_data[\"merge_confidence\"],\n        code_quality_str,\n        pr_description,\n        docstring['doc_update'],\n        code_diff_str\n    ))\n\n    conn.commit()\n```\n\nThis code does the following:\n\n1.  First a connection to Snowflake is established.\n2.  A cursor is obtained from the connection.\n3.  SQL Insert queries are constructed. The `cur.execute` statements constructs and executes the SQL statements.\n4.  Data is provided to the SQL Insert statements\n5.  The `conn.commit()` statement commits the changes to the Snowflake database.\n\n## Conclusion\n\nIn this chapter, we explored Snowflake Data Storage. We learned about the key concepts, how to create tables, insert data, and query data. We saw how Snowflake helps us store and analyze AI insights and pull request metadata for reporting, auditing, and search.\n\nYou have reached the end of this tutorial. Hopefully, you have a broad understanding of the major components of CodeRoast.\n", "\nFor the project `PocketFlow`:\n\nCodebase Context:\n--- File Index 0: README.md ---\n\n<div align=\"center\">\n  <img src=\"https://github.com/The-Pocket/.github/raw/main/assets/title.png\" width=\"600\"/>\n</div>\n\nEnglish | [\u4e2d\u6587](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_CHINESE.md) | [Espa\u00f1ol](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_SPANISH.md) | [\u65e5\u672c\u8a9e](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_JAPANESE.md) | [Deutsch](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_GERMAN.md) | [\u0420\u0443\u0441\u0441\u043a\u0438\u0439](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_RUSSIAN.md) | [Portugu\u00eas](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_PORTUGUESE.md) | [Fran\u00e7ais](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_FRENCH.md) | [\ud55c\uad6d\uc5b4](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_KOREAN.md)\n\n![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)\n[![Docs](https://img.shields.io/badge/docs-latest-blue)](https://the-pocket.github.io/PocketFlow/)\n <a href=\"https://discord.gg/hUHHE9Sa6T\">\n    <img src=\"https://img.shields.io/discord/1346833819172601907?logo=discord&style=flat\">\n</a>\n\nPocket Flow is a [100-line](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py) minimalist LLM framework\n\n- **Lightweight**: Just 100 lines. Zero bloat, zero dependencies, zero vendor lock-in.\n  \n- **Expressive**: Everything you love\u2014([Multi-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[Agents](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [Workflow](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html), and more.\n\n- **[Agentic Coding](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)**: Let AI Agents (e.g., Cursor AI) build Agents\u201410x productivity boost!\n\nGet started with Pocket Flow:\n- To install, ```pip install pocketflow```or just copy the [source code](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py) (only 100 lines).\n- To learn more, check out the [documentation](https://the-pocket.github.io/PocketFlow/). To learn the motivation, read the [story](https://zacharyhuang.substack.com/p/i-built-an-llm-framework-in-just).\n- Have questions? Check out this [AI Assistant](https://chatgpt.com/g/g-677464af36588191b9eba4901946557b-pocket-flow-assistant), or [create an issue!](https://github.com/The-Pocket/PocketFlow/issues/new)\n- \ud83c\udf89 Join our [Discord](https://discord.gg/hUHHE9Sa6T) to connect with other developers building with Pocket Flow! !\n- \ud83c\udf89 We now have a [TypeScript version](https://github.com/The-Pocket/PocketFlow-Typescript), mostly maintained by [@ZebraRoy](https://www.github.com/ZebraRoy)!\n\n## Why Pocket Flow?\n\nCurrent LLM frameworks are bloated... You only need 100 lines for LLM Framework!\n\n<div align=\"center\">\n  <img src=\"https://github.com/The-Pocket/.github/raw/main/assets/meme.jpg\" width=\"400\"/>\n\n\n  |                | **Abstraction**          | **App-Specific Wrappers**                                      | **Vendor-Specific Wrappers**                                    | **Lines**       | **Size**    |\n|----------------|:-----------------------------: |:-----------------------------------------------------------:|:------------------------------------------------------------:|:---------------:|:----------------------------:|\n| LangChain  | Agent, Chain               | Many <br><sup><sub>(e.g., QA, Summarization)</sub></sup>              | Many <br><sup><sub>(e.g., OpenAI, Pinecone, etc.)</sub></sup>                   | 405K          | +166MB                     |\n| CrewAI     | Agent, Chain            | Many <br><sup><sub>(e.g., FileReadTool, SerperDevTool)</sub></sup>         | Many <br><sup><sub>(e.g., OpenAI, Anthropic, Pinecone, etc.)</sub></sup>        | 18K           | +173MB                     |\n| SmolAgent   | Agent                      | Some <br><sup><sub>(e.g., CodeAgent, VisitWebTool)</sub></sup>         | Some <br><sup><sub>(e.g., DuckDuckGo, Hugging Face, etc.)</sub></sup>           | 8K            | +198MB                     |\n| LangGraph   | Agent, Graph           | Some <br><sup><sub>(e.g., Semantic Search)</sub></sup>                     | Some <br><sup><sub>(e.g., PostgresStore, SqliteSaver, etc.) </sub></sup>        | 37K           | +51MB                      |\n| AutoGen    | Agent                | Some <br><sup><sub>(e.g., Tool Agent, Chat Agent)</sub></sup>              | Many <sup><sub>[Optional]<br> (e.g., OpenAI, Pinecone, etc.)</sub></sup>        | 7K <br><sup><sub>(core-only)</sub></sup>    | +26MB <br><sup><sub>(core-only)</sub></sup>          |\n| **PocketFlow** | **Graph**                    | **None**                                                 | **None**                                                  | **100**       | **+56KB**                  |\n\n</div>\n\n## How does Pocket Flow work?\n\nThe [100 lines](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py) capture the core abstraction of LLM frameworks: Graph!\n<br>\n<div align=\"center\">\n  <img src=\"https://github.com/The-Pocket/.github/raw/main/assets/abstraction.png\" width=\"900\"/>\n</div>\n<br>\n\nFrom there, it's easy to implement popular design patterns like ([Multi-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[Agents](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [Workflow](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html), etc.\n<br>\n<div align=\"center\">\n  <img src=\"https://github.com/The-Pocket/.github/raw/main/assets/design.png\" width=\"900\"/>\n</div>\n<br>\n\u2728 Below are basic tutorials:\n\n<div align=\"center\">\n  \n|  Name  | Difficulty    |  Description  |  \n| :-------------:  | :-------------: | :--------------------- |  \n| [Chat](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat) | \u2606\u2606\u2606 <br> *Dummy*   | A basic chat bot with conversation history |\n| [Structured Output](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-structured-output) | \u2606\u2606\u2606 <br> *Dummy* | Extracting structured data from resumes by prompting |\n| [Workflow](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-workflow) | \u2606\u2606\u2606 <br> *Dummy*   | A writing workflow that outlines, writes content, and applies styling |\n| [Agent](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-agent) | \u2606\u2606\u2606 <br> *Dummy*   | A research agent that can search the web and answer questions |\n| [RAG](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-rag) | \u2606\u2606\u2606 <br> *Dummy*   | A simple Retrieval-augmented Generation process |\n| [Batch](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-batch) | \u2606\u2606\u2606 <br> *Dummy* | A batch processor that translates markdown content into multiple languages |\n| [Streaming](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-llm-streaming) | \u2606\u2606\u2606 <br> *Dummy*   | A real-time LLM streaming demo with user interrupt capability |\n| [Chat Guardrail](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-guardrail) | \u2606\u2606\u2606 <br> *Dummy*  | A travel advisor chatbot that only processes travel-related queries |\n| [Map-Reduce](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-map-reduce) | \u2605\u2606\u2606 <br> *Beginner* | A resume qualification processor using map-reduce pattern for batch evaluation |\n| [Multi-Agent](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-multi-agent) | \u2605\u2606\u2606 <br> *Beginner* | A Taboo word game for asynchronous communication between two agents |\n| [Supervisor](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-supervisor) | \u2605\u2606\u2606 <br> *Beginner* | Research agent is getting unreliable... Let's build a supervision process|\n| [Parallel](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch) | \u2605\u2606\u2606 <br> *Beginner*   | A parallel execution demo that shows 3x speedup |\n| [Parallel Flow](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch-flow) | \u2605\u2606\u2606 <br> *Beginner*   | A parallel image processing demo showing 8x speedup with multiple filters |\n| [Majority Vote](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-majority-vote) | \u2605\u2606\u2606 <br> *Beginner* | Improve reasoning accuracy by aggregating multiple solution attempts |\n| [Thinking](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-thinking) | \u2605\u2606\u2606 <br> *Beginner*   | Solve complex reasoning problems through Chain-of-Thought |\n| [Memory](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-memory) | \u2605\u2606\u2606 <br> *Beginner* | A chat bot with short-term and long-term memory |\n| [MCP](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-mcp) | \u2605\u2606\u2606 <br> *Beginner* |  Agent using Model Context Protocol for numerical operations |\n| [Web HITL](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-web-hitl) | \u2605\u2606\u2606 <br> *Beginner* | A minimal web service for a human review loop with SSE updates |\n\n</div>\n\n\ud83d\udc40 Want to see other tutorials for dummies? [Create an issue!](https://github.com/The-Pocket/PocketFlow/issues/new)\n\n## How to Use Pocket Flow?\n\n\ud83d\ude80 Through **Agentic Coding**\u2014the fastest LLM App development paradigm-where *humans design* and *agents code*!\n\n<br>\n<div align=\"center\">\n  <a href=\"https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to\" target=\"_blank\">\n    <img src=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F423a39af-49e8-483b-bc5a-88cc764350c6_1050x588.png\" width=\"700\" alt=\"IMAGE ALT TEXT\" style=\"cursor: pointer;\">\n  </a>\n</div>\n<br>\n\n\u2728 Below are examples of more complex LLM Apps:\n\n<div align=\"center\">\n  \n|  App Name     |  Difficulty    | Topics  | Human Design | Agent Code |\n| :-------------:  | :-------------: | :---------------------: |  :---: |  :---: |\n| [Build Cursor with Cursor](https://github.com/The-Pocket/Tutorial-Cursor) <br> <sup><sub>We'll reach the singularity soon ...</sup></sub> | \u2605\u2605\u2605 <br> *Advanced*   | [Agent](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html) | [Design Doc](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/docs/design.md) | [Flow Code](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/flow.py)\n| [Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge) <br> <sup><sub>Life's too short to stare at others' code in confusion</sup></sub> |  \u2605\u2605\u2606 <br> *Medium* | [Workflow](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html) | [Design Doc](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/docs/design.md) | [Flow Code](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/flow.py)\n| [Ask AI Paul Graham](https://github.com/The-Pocket/Tutorial-YC-Partner) <br> <sup><sub>Ask AI Paul Graham, in case you don't get in</sup></sub> | \u2605\u2605\u2606 <br> *Medium*  | [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) <br> [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [TTS](https://the-pocket.github.io/PocketFlow/utility_function/text_to_speech.html) | [Design Doc](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/docs/design.md) | [Flow Code](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/flow.py)\n| [Youtube Summarizer](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple)  <br> <sup><sub> Explain YouTube Videos to you like you're 5 </sup></sub> | \u2605\u2606\u2606 <br> *Beginner*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) |  [Design Doc](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/docs/design.md) | [Flow Code](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/flow.py)\n| [Cold Opener Generator](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization)  <br> <sup><sub> Instant icebreakers that turn cold leads hot </sup></sub> | \u2605\u2606\u2606 <br> *Beginner*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [Web Search](https://the-pocket.github.io/PocketFlow/utility_function/websearch.html) |  [Design Doc](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/docs/design.md) | [Flow Code](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/flow.py)\n\n</div>\n\n- Want to learn **Agentic Coding**?\n\n  - Check out [my YouTube](https://www.youtube.com/@ZacharyLLM?sub_confirmation=1) for video tutorial on how some apps above are made!\n\n  - Want to build your own LLM App? Read this [post](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)! Start with [this template](https://github.com/The-Pocket/PocketFlow-Template-Python)!\n\n\n\n\n--- File Index 1: cookbook/README.md ---\n# Pocket Flow Cookbook\n\n\n<div align=\"center\">\n  \n|  Name  | Difficulty    |  Description  |  \n| :-------------:  | :-------------: | :--------------------- |  \n| [Chat](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat) | \u2606\u2606\u2606 <br> *Dummy*   | A basic chat bot with conversation history |\n| [Structured Output](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-structured-output) | \u2606\u2606\u2606 <br> *Dummy* | Extracting structured data from resumes by prompting |\n| [Workflow](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-workflow) | \u2606\u2606\u2606 <br> *Dummy*   | A writing workflow that outlines, writes content, and applies styling |\n| [Agent](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-agent) | \u2606\u2606\u2606 <br> *Dummy*   | A research agent that can search the web and answer questions |\n| [RAG](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-rag) | \u2606\u2606\u2606 <br> *Dummy*   | A simple Retrieval-augmented Generation process |\n| [Map-Reduce](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-map-reduce) | \u2606\u2606\u2606 <br> *Dummy* | A resume qualification processor using map-reduce pattern for batch evaluation |\n| [Streaming](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-llm-streaming) | \u2606\u2606\u2606 <br> *Dummy*   | A real-time LLM streaming demo with user interrupt capability |\n| [Chat Guardrail](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-guardrail) | \u2606\u2606\u2606 <br> *Dummy*  | A travel advisor chatbot that only processes travel-related queries |\n| [Multi-Agent](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-multi-agent) | \u2605\u2606\u2606 <br> *Beginner* | A Taboo word game for asynchronous communication between two agents |\n| [Supervisor](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-supervisor) | \u2605\u2606\u2606 <br> *Beginner* | Research agent is getting unreliable... Let's build a supervision process|\n| [Parallel](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch) | \u2605\u2606\u2606 <br> *Beginner*   | A parallel execution demo that shows 3x speedup |\n| [Parallel Flow](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch-flow) | \u2605\u2606\u2606 <br> *Beginner*   | A parallel image processing demo showing 8x speedup with multiple filters |\n| [Thinking](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-thinking) | \u2605\u2606\u2606 <br> *Beginner*   | Solve complex reasoning problems through Chain-of-Thought |\n| [Memory](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-memory) | \u2605\u2606\u2606 <br> *Beginner* | A chat bot with short-term and long-term memory |\n| [MCP](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-mcp) | \u2605\u2606\u2606 <br> *Beginner* |  Agent using Model Context Protocol for numerical operations |\n\n</div>\n\n\ud83d\udc40 Want to see other tutorials? [Create an issue!](https://github.com/The-Pocket/PocketFlow/issues/new)\n\n\n--- File Index 2: cookbook/pocketflow-agent/README.md ---\n# Research Agent\n\nThis project demonstrates a simple yet powerful LLM-powered research agent. This implementation is based directly on the tutorial: [LLM Agents are simply Graph \u2014 Tutorial For Dummies](https://zacharyhuang.substack.com/p/llm-agent-internal-as-a-graph-tutorial).\n\n\ud83d\udc49 Run the tutorial in your browser: [Try Google Colab Notebook](\nhttps://colab.research.google.com/github/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-agent/demo.ipynb)\n\n## Features\n\n- Performs web searches to gather information\n- Makes decisions about when to search vs. when to answer\n- Generates comprehensive answers based on research findings\n\n## Getting Started\n\n1. Install the packages you need with this simple command:\n```bash\npip install -r requirements.txt\n```\n\n2. Let's get your OpenAI API key ready:\n\n```bash\nexport OPENAI_API_KEY=\"your-api-key-here\"\n```\n\n3. Let's do a quick check to make sure your API key is working properly:\n\n```bash\npython utils.py\n```\n\nThis will test both the LLM call and web search features. If you see responses, you're good to go!\n\n4. Try out the agent with the default question (about Nobel Prize winners):\n\n```bash\npython main.py\n```\n\n5. Got a burning question? Ask anything you want by using the `--` prefix:\n\n```bash\npython main.py --\"What is quantum computing?\"\n```\n\n## How It Works?\n\nThe magic happens through a simple but powerful graph structure with three main parts:\n\n```mermaid\ngraph TD\n    A[DecideAction] -->|\"search\"| B[SearchWeb]\n    A -->|\"answer\"| C[AnswerQuestion]\n    B -->|\"decide\"| A\n```\n\nHere's what each part does:\n1. **DecideAction**: The brain that figures out whether to search or answer\n2. **SearchWeb**: The researcher that goes out and finds information\n3. **AnswerQuestion**: The writer that crafts the final answer\n\nHere's what's in each file:\n- [`main.py`](./main.py): The starting point - runs the whole show!\n- [`flow.py`](./flow.py): Connects everything together into a smart agent\n- [`nodes.py`](./nodes.py): The building blocks that make decisions and take actions\n- [`utils.py`](./utils.py): Helper functions for talking to the LLM and searching the web\n\n\n--- File Index 3: cookbook/pocketflow-agent/flow.py ---\nfrom pocketflow import Flow\nfrom nodes import DecideAction, SearchWeb, AnswerQuestion\n\ndef create_agent_flow():\n    \"\"\"\n    Create and connect the nodes to form a complete agent flow.\n    \n    The flow works like this:\n    1. DecideAction node decides whether to search or answer\n    2. If search, go to SearchWeb node\n    3. If answer, go to AnswerQuestion node\n    4. After SearchWeb completes, go back to DecideAction\n    \n    Returns:\n        Flow: A complete research agent flow\n    \"\"\"\n    # Create instances of each node\n    decide = DecideAction()\n    search = SearchWeb()\n    answer = AnswerQuestion()\n    \n    # Connect the nodes\n    # If DecideAction returns \"search\", go to SearchWeb\n    decide - \"search\" >> search\n    \n    # If DecideAction returns \"answer\", go to AnswerQuestion\n    decide - \"answer\" >> answer\n    \n    # After SearchWeb completes and returns \"decide\", go back to DecideAction\n    search - \"decide\" >> decide\n    \n    # Create and return the flow, starting with the DecideAction node\n    return Flow(start=decide) \n\n--- File Index 4: cookbook/pocketflow-agent/main.py ---\nimport sys\nfrom flow import create_agent_flow\n\ndef main():\n    \"\"\"Simple function to process a question.\"\"\"\n    # Default question\n    default_question = \"Who won the Nobel Prize in Physics 2024?\"\n    \n    # Get question from command line if provided with --\n    question = default_question\n    for arg in sys.argv[1:]:\n        if arg.startswith(\"--\"):\n            question = arg[2:]\n            break\n    \n    # Create the agent flow\n    agent_flow = create_agent_flow()\n    \n    # Process the question\n    shared = {\"question\": question}\n    print(f\"\ud83e\udd14 Processing question: {question}\")\n    agent_flow.run(shared)\n    print(\"\\n\ud83c\udfaf Final Answer:\")\n    print(shared.get(\"answer\", \"No answer found\"))\n\nif __name__ == \"__main__\":\n    main()\n\n--- File Index 5: cookbook/pocketflow-agent/nodes.py ---\nfrom pocketflow import Node\nfrom utils import call_llm, search_web\nimport yaml\n\nclass DecideAction(Node):\n    def prep(self, shared):\n        \"\"\"Prepare the context and question for the decision-making process.\"\"\"\n        # Get the current context (default to \"No previous search\" if none exists)\n        context = shared.get(\"context\", \"No previous search\")\n        # Get the question from the shared store\n        question = shared[\"question\"]\n        # Return both for the exec step\n        return question, context\n        \n    def exec(self, inputs):\n        \"\"\"Call the LLM to decide whether to search or answer.\"\"\"\n        question, context = inputs\n        \n        print(f\"\ud83e\udd14 Agent deciding what to do next...\")\n        \n        # Create a prompt to help the LLM decide what to do next with proper yaml formatting\n        prompt = f\"\"\"\n### CONTEXT\nYou are a research assistant that can search the web.\nQuestion: {question}\nPrevious Research: {context}\n\n### ACTION SPACE\n[1] search\n  Description: Look up more information on the web\n  Parameters:\n    - query (str): What to search for\n\n[2] answer\n  Description: Answer the question with current knowledge\n  Parameters:\n    - answer (str): Final answer to the question\n\n## NEXT ACTION\nDecide the next action based on the context and available actions.\nReturn your response in this format:\n\n```yaml\nthinking: |\n    <your step-by-step reasoning process>\naction: search OR answer\nreason: <why you chose this action>\nanswer: <if action is answer>\nsearch_query: <specific search query if action is search>\n```\nIMPORTANT: Make sure to:\n1. Use proper indentation (4 spaces) for all multi-line fields\n2. Use the | character for multi-line text fields\n3. Keep single-line fields without the | character\n\"\"\"\n        \n        # Call the LLM to make a decision\n        response = call_llm(prompt)\n        \n        # Parse the response to get the decision\n        yaml_str = response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n        decision = yaml.safe_load(yaml_str)\n        \n        return decision\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Save the decision and determine the next step in the flow.\"\"\"\n        # If LLM decided to search, save the search query\n        if exec_res[\"action\"] == \"search\":\n            shared[\"search_query\"] = exec_res[\"search_query\"]\n            print(f\"\ud83d\udd0d Agent decided to search for: {exec_res['search_query']}\")\n        else:\n            shared[\"context\"] = exec_res[\"answer\"] #save the context if LLM gives the answer without searching.\n            print(f\"\ud83d\udca1 Agent decided to answer the question\")\n        \n        # Return the action to determine the next node in the flow\n        return exec_res[\"action\"]\n\nclass SearchWeb(Node):\n    def prep(self, shared):\n        \"\"\"Get the search query from the shared store.\"\"\"\n        return shared[\"search_query\"]\n        \n    def exec(self, search_query):\n        \"\"\"Search the web for the given query.\"\"\"\n        # Call the search utility function\n        print(f\"\ud83c\udf10 Searching the web for: {search_query}\")\n        results = search_web(search_query)\n        return results\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Save the search results and go back to the decision node.\"\"\"\n        # Add the search results to the context in the shared store\n        previous = shared.get(\"context\", \"\")\n        shared[\"context\"] = previous + \"\\n\\nSEARCH: \" + shared[\"search_query\"] + \"\\nRESULTS: \" + exec_res\n        \n        print(f\"\ud83d\udcda Found information, analyzing results...\")\n        \n        # Always go back to the decision node after searching\n        return \"decide\"\n\nclass AnswerQuestion(Node):\n    def prep(self, shared):\n        \"\"\"Get the question and context for answering.\"\"\"\n        return shared[\"question\"], shared.get(\"context\", \"\")\n        \n    def exec(self, inputs):\n        \"\"\"Call the LLM to generate a final answer.\"\"\"\n        question, context = inputs\n        \n        print(f\"\u270d\ufe0f Crafting final answer...\")\n        \n        # Create a prompt for the LLM to answer the question\n        prompt = f\"\"\"\n### CONTEXT\nBased on the following information, answer the question.\nQuestion: {question}\nResearch: {context}\n\n## YOUR ANSWER:\nProvide a comprehensive answer using the research results.\n\"\"\"\n        # Call the LLM to generate an answer\n        answer = call_llm(prompt)\n        return answer\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Save the final answer and complete the flow.\"\"\"\n        # Save the answer in the shared store\n        shared[\"answer\"] = exec_res\n        \n        print(f\"\u2705 Answer generated successfully\")\n        \n        # We're done - no need to continue the flow\n        return \"done\" \n\n\n--- File Index 6: cookbook/pocketflow-agent/utils.py ---\nfrom openai import OpenAI\nimport os\nfrom duckduckgo_search import DDGS\n\ndef call_llm(prompt):    \n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n    r = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return r.choices[0].message.content\n\ndef search_web(query):\n    results = DDGS().text(query, max_results=5)\n    # Convert results to a string\n    results_str = \"\\n\\n\".join([f\"Title: {r['title']}\\nURL: {r['href']}\\nSnippet: {r['body']}\" for r in results])\n    return results_str\n    \nif __name__ == \"__main__\":\n    print(\"## Testing call_llm\")\n    prompt = \"In a few words, what is the meaning of life?\"\n    print(f\"## Prompt: {prompt}\")\n    response = call_llm(prompt)\n    print(f\"## Response: {response}\")\n\n    print(\"## Testing search_web\")\n    query = \"Who won the Nobel Prize in Physics 2024?\"\n    print(f\"## Query: {query}\")\n    results = search_web(query)\n    print(f\"## Results: {results}\")\n\n--- File Index 7: cookbook/pocketflow-async-basic/README.md ---\n# PocketFlow Async Basic Example\n\nThis example demonstrates async operations using a simple Recipe Finder that:\n1. Fetches recipes from an API (async HTTP)\n2. Processes them with an LLM (async LLM)\n3. Waits for user confirmation (async input)\n\n## What this Example Does\n\nWhen you run the example:\n1. You enter an ingredient (e.g., \"chicken\")\n2. It searches for recipes (async API call)\n3. It suggests a recipe (async LLM call)\n4. You approve or reject the suggestion\n5. If rejected, it tries again with a different recipe\n\n## How it Works\n\n1. **FetchRecipes (AsyncNode)**\n   ```python\n   async def prep_async(self, shared):\n       ingredient = input(\"Enter ingredient: \")\n       return ingredient\n\n   async def exec_async(self, ingredient):\n       # Async API call\n       recipes = await fetch_recipes(ingredient)\n       return recipes\n   ```\n\n2. **SuggestRecipe (AsyncNode)**\n   ```python\n   async def exec_async(self, recipes):\n       # Async LLM call\n       suggestion = await call_llm_async(\n           f\"Choose best recipe from: {recipes}\"\n       )\n       return suggestion\n   ```\n\n3. **GetApproval (AsyncNode)**\n   ```python\n   async def post_async(self, shared, prep_res, suggestion):\n       # Async user input\n       answer = await get_user_input(\n           f\"Accept {suggestion}? (y/n): \"\n       )\n       return \"accept\" if answer == \"y\" else \"retry\"\n   ```\n\n## Running the Example\n\n```bash\npip install -r requirements.txt\npython main.py\n```\n\n## Sample Interaction\n\n```\nEnter ingredient: chicken\nFetching recipes...\nFound 3 recipes.\n\nSuggesting best recipe...\nHow about: Grilled Chicken with Herbs\n\nAccept this recipe? (y/n): n\nSuggesting another recipe...\nHow about: Chicken Stir Fry\n\nAccept this recipe? (y/n): y\nGreat choice! Here's your recipe...\n```\n\n## Key Concepts\n\n1. **Async Operations**: Using `async/await` for:\n   - API calls (non-blocking I/O)\n   - LLM calls (potentially slow)\n   - User input (waiting for response)\n\n2. **AsyncNode Methods**:\n   - `prep_async`: Setup and data gathering\n   - `exec_async`: Main async processing\n   - `post_async`: Post-processing and decisions\n\n3. **Flow Control**:\n   - Actions (\"accept\"/\"retry\") control flow\n   - Retry loop for rejected suggestions \n\n--- File Index 8: cookbook/pocketflow-async-basic/flow.py ---\n\"\"\"AsyncFlow implementation for recipe finder.\"\"\"\n\nfrom pocketflow import AsyncFlow, Node\nfrom nodes import FetchRecipes, SuggestRecipe, GetApproval\n\nclass NoOp(Node):\n    \"\"\"Node that does nothing, used to properly end the flow.\"\"\"\n    pass\n\ndef create_flow():\n    \"\"\"Create and connect nodes into a flow.\"\"\"\n    \n    # Create nodes\n    fetch = FetchRecipes()\n    suggest = SuggestRecipe()\n    approve = GetApproval()\n    end = NoOp()\n    \n    # Connect nodes\n    fetch - \"suggest\" >> suggest\n    suggest - \"approve\" >> approve\n    approve - \"retry\" >> suggest  # Loop back for another suggestion\n    approve - \"accept\" >> end     # Properly end the flow\n    \n    # Create flow starting with fetch\n    flow = AsyncFlow(start=fetch)\n    return flow \n\n--- File Index 9: cookbook/pocketflow-async-basic/main.py ---\nimport asyncio\nfrom flow import create_flow\n\nasync def main():\n    \"\"\"Run the recipe finder flow.\"\"\"\n    # Create flow\n    flow = create_flow()\n    \n    # Create shared store\n    shared = {}\n    \n    # Run flow\n    print(\"\\nWelcome to Recipe Finder!\")\n    print(\"------------------------\")\n    await flow.run_async(shared)\n    print(\"\\nThanks for using Recipe Finder!\")\n\nif __name__ == \"__main__\":\n    # Run the async main function\n    asyncio.run(main()) \n\n--- File Index 10: cookbook/pocketflow-async-basic/nodes.py ---\nfrom pocketflow import AsyncNode\nfrom utils import fetch_recipes, call_llm_async, get_user_input\n\nclass FetchRecipes(AsyncNode):\n    \"\"\"AsyncNode that fetches recipes.\"\"\"\n    \n    async def prep_async(self, shared):\n        \"\"\"Get ingredient from user.\"\"\"\n        ingredient = await get_user_input(\"Enter ingredient: \")\n        return ingredient\n    \n    async def exec_async(self, ingredient):\n        \"\"\"Fetch recipes asynchronously.\"\"\"\n        recipes = await fetch_recipes(ingredient)\n        return recipes\n    \n    async def post_async(self, shared, prep_res, recipes):\n        \"\"\"Store recipes and continue.\"\"\"\n        shared[\"recipes\"] = recipes\n        shared[\"ingredient\"] = prep_res\n        return \"suggest\"\n\nclass SuggestRecipe(AsyncNode):\n    \"\"\"AsyncNode that suggests a recipe using LLM.\"\"\"\n    \n    async def prep_async(self, shared):\n        \"\"\"Get recipes from shared store.\"\"\"\n        return shared[\"recipes\"]\n    \n    async def exec_async(self, recipes):\n        \"\"\"Get suggestion from LLM.\"\"\"\n        suggestion = await call_llm_async(\n            f\"Choose best recipe from: {', '.join(recipes)}\"\n        )\n        return suggestion\n    \n    async def post_async(self, shared, prep_res, suggestion):\n        \"\"\"Store suggestion and continue.\"\"\"\n        shared[\"suggestion\"] = suggestion\n        return \"approve\"\n\nclass GetApproval(AsyncNode):\n    \"\"\"AsyncNode that gets user approval.\"\"\"\n    \n    async def prep_async(self, shared):\n        \"\"\"Get current suggestion.\"\"\"\n        return shared[\"suggestion\"]\n    \n    async def exec_async(self, suggestion):\n        \"\"\"Ask for user approval.\"\"\"\n        answer = await get_user_input(f\"\\nAccept this recipe? (y/n): \")\n        return answer\n    \n    async def post_async(self, shared, prep_res, answer):\n        \"\"\"Handle user's decision.\"\"\"\n        if answer == \"y\":\n            print(\"\\nGreat choice! Here's your recipe...\")\n            print(f\"Recipe: {shared['suggestion']}\")\n            print(f\"Ingredient: {shared['ingredient']}\")\n            return \"accept\"\n        else:\n            print(\"\\nLet's try another recipe...\")\n            return \"retry\" \n\n--- File Index 11: cookbook/pocketflow-async-basic/utils.py ---\nimport asyncio\nimport aiohttp\nfrom openai import AsyncOpenAI\n\nasync def fetch_recipes(ingredient):\n    \"\"\"Fetch recipes from an API asynchronously.\"\"\"\n    print(f\"Fetching recipes for {ingredient}...\")\n    \n    # Simulate API call with delay\n    await asyncio.sleep(1)\n    \n    # Mock recipes (in real app, would fetch from API)\n    recipes = [\n        f\"{ingredient} Stir Fry\",\n        f\"Grilled {ingredient} with Herbs\",\n        f\"Baked {ingredient} with Vegetables\"\n    ]\n    \n    print(f\"Found {len(recipes)} recipes.\")\n    \n    return recipes\n\nasync def call_llm_async(prompt):\n    \"\"\"Make async LLM call.\"\"\"\n    print(\"\\nSuggesting best recipe...\")\n    \n    # Simulate LLM call with delay\n    await asyncio.sleep(1)\n    \n    # Mock LLM response (in real app, would call OpenAI)\n    recipes = prompt.split(\": \")[1].split(\", \")\n    suggestion = recipes[1]  # Always suggest second recipe\n    \n    print(f\"How about: {suggestion}\")\n    return suggestion\n\nasync def get_user_input(prompt):\n    \"\"\"Get user input asynchronously.\"\"\"\n    # Create event loop to handle async input\n    loop = asyncio.get_event_loop()\n    \n    # Get input in a non-blocking way\n    answer = await loop.run_in_executor(None, input, prompt)\n\n    return answer.lower() \n\n--- File Index 12: cookbook/pocketflow-batch-flow/README.md ---\n# PocketFlow BatchFlow Example\n\nThis example demonstrates the BatchFlow concept in PocketFlow by implementing an image processor that applies different filters to multiple images.\n\n## What this Example Demonstrates\n\n- How to use BatchFlow to run a Flow multiple times with different parameters\n- Key concepts of BatchFlow:\n  1. Creating a base Flow for single-item processing\n  2. Using BatchFlow to process multiple items with different parameters\n  3. Managing parameters across multiple Flow executions\n\n## Project Structure\n```\npocketflow-batch-flow/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 cat.jpg        # Sample image 1\n\u2502   \u251c\u2500\u2500 dog.jpg        # Sample image 2\n\u2502   \u2514\u2500\u2500 bird.jpg       # Sample image 3\n\u251c\u2500\u2500 main.py            # Entry point\n\u251c\u2500\u2500 flow.py            # Flow and BatchFlow definitions\n\u2514\u2500\u2500 nodes.py           # Node implementations for image processing\n```\n\n## How it Works\n\nThe example processes multiple images with different filters:\n\n1. **Base Flow**: Processes a single image\n   - Load image\n   - Apply filter (grayscale, blur, or sepia)\n   - Save processed image\n\n2. **BatchFlow**: Processes multiple image-filter combinations\n   - Takes a list of parameters (image + filter combinations)\n   - Runs the base Flow for each parameter set\n   - Organizes output in a structured way\n\n## Installation\n\n```bash\npip install -r requirements.txt\n```\n\n## Usage\n\n```bash\npython main.py\n```\n\n## Sample Output\n\n```\nProcessing images with filters...\n\nProcessing cat.jpg with grayscale filter...\nProcessing cat.jpg with blur filter...\nProcessing dog.jpg with sepia filter...\n...\n\nAll images processed successfully!\nCheck the 'output' directory for results.\n```\n\n## Key Concepts Illustrated\n\n1. **Parameter Management**: Shows how BatchFlow manages different parameter sets\n2. **Flow Reuse**: Demonstrates running the same Flow multiple times\n3. **Batch Processing**: Shows how to process multiple items efficiently\n4. **Real-world Application**: Provides a practical example of batch processing \n\n--- File Index 13: cookbook/pocketflow-batch-flow/flow.py ---\nfrom pocketflow import Flow, BatchFlow\nfrom nodes import LoadImage, ApplyFilter, SaveImage\n\ndef create_base_flow():\n    \"\"\"Create the base Flow for processing a single image.\"\"\"\n    # Create nodes\n    load = LoadImage()\n    filter_node = ApplyFilter()\n    save = SaveImage()\n    \n    # Connect nodes\n    load - \"apply_filter\" >> filter_node\n    filter_node - \"save\" >> save\n    \n    # Create and return flow\n    return Flow(start=load)\n\nclass ImageBatchFlow(BatchFlow):\n    \"\"\"BatchFlow for processing multiple images with different filters.\"\"\"\n    \n    def prep(self, shared):\n        \"\"\"Generate parameters for each image-filter combination.\"\"\"\n        # List of images to process\n        images = [\"cat.jpg\", \"dog.jpg\", \"bird.jpg\"]\n        \n        # List of filters to apply\n        filters = [\"grayscale\", \"blur\", \"sepia\"]\n        \n        # Generate all combinations\n        params = []\n        for img in images:\n            for f in filters:\n                params.append({\n                    \"input\": img,\n                    \"filter\": f\n                })\n        \n        return params\n\ndef create_flow():\n    \"\"\"Create the complete batch processing flow.\"\"\"\n    # Create base flow for single image processing\n    base_flow = create_base_flow()\n    \n    # Wrap in BatchFlow for multiple images\n    batch_flow = ImageBatchFlow(start=base_flow)\n    \n    return batch_flow \n\n--- File Index 14: cookbook/pocketflow-batch-flow/main.py ---\nimport os\nfrom PIL import Image\nimport numpy as np\nfrom flow import create_flow\n\ndef main():\n    # Create and run flow\n    print(\"Processing images with filters...\")\n    \n    flow = create_flow()\n    flow.run({}) \n    \n    print(\"\\nAll images processed successfully!\")\n    print(\"Check the 'output' directory for results.\")\n\nif __name__ == \"__main__\":\n    main() \n\n--- File Index 15: cookbook/pocketflow-batch-flow/nodes.py ---\n\"\"\"Node implementations for image processing.\"\"\"\n\nimport os\nfrom PIL import Image, ImageEnhance, ImageFilter\nfrom pocketflow import Node\n\nclass LoadImage(Node):\n    \"\"\"Node that loads an image file.\"\"\"\n    \n    def prep(self, shared):\n        \"\"\"Get image path from parameters.\"\"\"\n        return os.path.join(\"images\", self.params[\"input\"])\n    \n    def exec(self, image_path):\n        \"\"\"Load the image using PIL.\"\"\"\n        return Image.open(image_path)\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Store the image in shared store.\"\"\"\n        shared[\"image\"] = exec_res\n        return \"apply_filter\"\n\nclass ApplyFilter(Node):\n    \"\"\"Node that applies a filter to an image.\"\"\"\n    \n    def prep(self, shared):\n        \"\"\"Get image and filter type.\"\"\"\n        return shared[\"image\"], self.params[\"filter\"]\n    \n    def exec(self, inputs):\n        \"\"\"Apply the specified filter.\"\"\"\n        image, filter_type = inputs\n        \n        if filter_type == \"grayscale\":\n            return image.convert(\"L\")\n        elif filter_type == \"blur\":\n            return image.filter(ImageFilter.BLUR)\n        elif filter_type == \"sepia\":\n            # Sepia implementation\n            enhancer = ImageEnhance.Color(image)\n            grayscale = enhancer.enhance(0.3)\n            colorize = ImageEnhance.Brightness(grayscale)\n            return colorize.enhance(1.2)\n        else:\n            raise ValueError(f\"Unknown filter: {filter_type}\")\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Store the filtered image.\"\"\"\n        shared[\"filtered_image\"] = exec_res\n        return \"save\"\n\nclass SaveImage(Node):\n    \"\"\"Node that saves the processed image.\"\"\"\n    \n    def prep(self, shared):\n        \"\"\"Get filtered image and prepare output path.\"\"\"\n        # Create output directory if it doesn't exist\n        os.makedirs(\"output\", exist_ok=True)\n        \n        # Generate output filename\n        input_name = os.path.splitext(self.params[\"input\"])[0]\n        filter_name = self.params[\"filter\"]\n        output_path = os.path.join(\"output\", f\"{input_name}_{filter_name}.jpg\")\n        \n        return shared[\"filtered_image\"], output_path\n    \n    def exec(self, inputs):\n        \"\"\"Save the image to file.\"\"\"\n        image, output_path = inputs\n        image.save(output_path, \"JPEG\")\n        return output_path\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Print success message.\"\"\"\n        print(f\"Saved filtered image to: {exec_res}\")\n        return \"default\" \n\n--- File Index 16: cookbook/pocketflow-batch-node/README.md ---\n# PocketFlow BatchNode Example\n\nThis example demonstrates the BatchNode concept in PocketFlow by implementing a CSV processor that handles large files by processing them in chunks.\n\n## What this Example Demonstrates\n\n- How to use BatchNode to process large inputs in chunks\n- The three key methods of BatchNode:\n  1. `prep`: Splits input into chunks\n  2. `exec`: Processes each chunk independently\n  3. `post`: Combines results from all chunks\n\n## Project Structure\n```\npocketflow-batch-node/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 data/\n\u2502   \u2514\u2500\u2500 sales.csv      # Sample large CSV file\n\u251c\u2500\u2500 main.py            # Entry point\n\u251c\u2500\u2500 flow.py            # Flow definition\n\u2514\u2500\u2500 nodes.py           # BatchNode implementation\n```\n\n## How it Works\n\nThe example processes a large CSV file containing sales data:\n\n1. **Chunking (prep)**: The CSV file is read and split into chunks of N rows\n2. **Processing (exec)**: Each chunk is processed to calculate:\n   - Total sales\n   - Average sale value\n   - Number of transactions\n3. **Combining (post)**: Results from all chunks are aggregated into final statistics\n\n## Installation\n\n```bash\npip install -r requirements.txt\n```\n\n## Usage\n\n```bash\npython main.py\n```\n\n## Sample Output\n\n```\nProcessing sales.csv in chunks...\n\nFinal Statistics:\n- Total Sales: $1,234,567.89\n- Average Sale: $123.45\n- Total Transactions: 10,000\n```\n\n## Key Concepts Illustrated\n\n1. **Chunk-based Processing**: Shows how BatchNode handles large inputs by breaking them into manageable pieces\n2. **Independent Processing**: Demonstrates how each chunk is processed separately\n3. **Result Aggregation**: Shows how individual results are combined into a final output \n\n--- File Index 17: cookbook/pocketflow-batch-node/flow.py ---\nfrom pocketflow import Flow, Node\nfrom nodes import CSVProcessor\n\nclass ShowStats(Node):\n    \"\"\"Node to display the final statistics.\"\"\"\n    \n    def prep(self, shared):\n        \"\"\"Get statistics from shared store.\"\"\"\n        return shared[\"statistics\"]\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Display the statistics.\"\"\"\n        stats = prep_res\n        print(\"\\nFinal Statistics:\")\n        print(f\"- Total Sales: ${stats['total_sales']:,.2f}\")\n        print(f\"- Average Sale: ${stats['average_sale']:,.2f}\")\n        print(f\"- Total Transactions: {stats['total_transactions']:,}\\n\")\n        return \"end\"\n\ndef create_flow():\n    \"\"\"Create and return the processing flow.\"\"\"\n    # Create nodes\n    processor = CSVProcessor(chunk_size=1000)\n    show_stats = ShowStats()\n    \n    # Connect nodes\n    processor - \"show_stats\" >> show_stats\n    \n    # Create and return flow\n    return Flow(start=processor) \n\n--- File Index 18: cookbook/pocketflow-batch-node/main.py ---\nimport os\nfrom flow import create_flow\n\ndef main():\n    \"\"\"Run the batch processing example.\"\"\"\n    # Create data directory if it doesn't exist\n    os.makedirs(\"data\", exist_ok=True)\n    \n    # Create sample CSV if it doesn't exist\n    if not os.path.exists(\"data/sales.csv\"):\n        print(\"Creating sample sales.csv...\")\n        import pandas as pd\n        import numpy as np\n        \n        # Generate sample data\n        np.random.seed(42)\n        n_rows = 10000\n        df = pd.DataFrame({\n            \"date\": pd.date_range(\"2024-01-01\", periods=n_rows),\n            \"amount\": np.random.normal(100, 30, n_rows).round(2),\n            \"product\": np.random.choice([\"A\", \"B\", \"C\"], n_rows)\n        })\n        df.to_csv(\"data/sales.csv\", index=False)\n    \n    # Initialize shared store\n    shared = {\n        \"input_file\": \"data/sales.csv\"\n    }\n    \n    # Create and run flow\n    print(f\"Processing sales.csv in chunks...\")\n    flow = create_flow()\n    flow.run(shared)\n\nif __name__ == \"__main__\":\n    main() \n\n--- File Index 19: cookbook/pocketflow-batch-node/nodes.py ---\nimport pandas as pd\nfrom pocketflow import BatchNode\n\nclass CSVProcessor(BatchNode):\n    \"\"\"BatchNode that processes a large CSV file in chunks.\"\"\"\n    \n    def __init__(self, chunk_size=1000):\n        \"\"\"Initialize with chunk size.\"\"\"\n        super().__init__()\n        self.chunk_size = chunk_size\n    \n    def prep(self, shared):\n        \"\"\"Split CSV file into chunks.\n        \n        Returns an iterator of DataFrames, each containing chunk_size rows.\n        \"\"\"\n        # Read CSV in chunks\n        chunks = pd.read_csv(\n            shared[\"input_file\"],\n            chunksize=self.chunk_size\n        )\n        return chunks\n    \n    def exec(self, chunk):\n        \"\"\"Process a single chunk of the CSV.\n        \n        Args:\n            chunk: pandas DataFrame containing chunk_size rows\n            \n        Returns:\n            dict: Statistics for this chunk\n        \"\"\"\n        return {\n            \"total_sales\": chunk[\"amount\"].sum(),\n            \"num_transactions\": len(chunk),\n            \"total_amount\": chunk[\"amount\"].sum()\n        }\n    \n    def post(self, shared, prep_res, exec_res_list):\n        \"\"\"Combine results from all chunks.\n        \n        Args:\n            prep_res: Original chunks iterator\n            exec_res_list: List of results from each chunk\n            \n        Returns:\n            str: Action to take next\n        \"\"\"\n        # Combine statistics from all chunks\n        total_sales = sum(res[\"total_sales\"] for res in exec_res_list)\n        total_transactions = sum(res[\"num_transactions\"] for res in exec_res_list)\n        total_amount = sum(res[\"total_amount\"] for res in exec_res_list)\n        \n        # Calculate final statistics\n        shared[\"statistics\"] = {\n            \"total_sales\": total_sales,\n            \"average_sale\": total_amount / total_transactions,\n            \"total_transactions\": total_transactions\n        }\n        \n        return \"show_stats\" \n\n--- File Index 20: cookbook/pocketflow-batch/README.md ---\n# Batch Translation Process\n\nThis project demonstrates a batch processing implementation that enables LLMs to translate documents into multiple languages simultaneously. It's designed to efficiently handle the translation of markdown files while preserving formatting.\n\n## Features\n\n- Translates markdown content into multiple languages in parallel\n- Saves translated files to specified output directory\n\n## Getting Started\n\n1. Install the required packages:\n```bash\npip install -r requirements.txt\n```\n\n2. Set up your API key:\n```bash\nexport ANTHROPIC_API_KEY=\"your-api-key-here\"\n```\n\n3. Run the translation process:\n```bash\npython main.py\n```\n\n## How It Works\n\nThe implementation uses a `TranslateTextNode` that processes batches of translation requests:\n\n```mermaid\nflowchart LR\n    batch[TranslateTextNode]\n```\n\nThe `TranslateTextNode`:\n1. Prepares batches for multiple language translations\n2. Executes translations in parallel using the model\n3. Saves the translated content to individual files\n4. Maintains the original markdown structure\n\nThis approach demonstrates how PocketFlow can efficiently process multiple related tasks in parallel.\n\n## Example Output\n\nWhen you run the translation process, you'll see output similar to this:\n\n```\nTranslated Chinese text\nTranslated Spanish text\nTranslated Japanese text\nTranslated German text\nTranslated Russian text\nTranslated Portuguese text\nTranslated French text\nTranslated Korean text\nSaved translation to translations/README_CHINESE.md\nSaved translation to translations/README_SPANISH.md\nSaved translation to translations/README_JAPANESE.md\nSaved translation to translations/README_GERMAN.md\nSaved translation to translations/README_RUSSIAN.md\nSaved translation to translations/README_PORTUGUESE.md\nSaved translation to translations/README_FRENCH.md\nSaved translation to translations/README_KOREAN.md\n\n=== Translation Complete ===\nTranslations saved to: translations\n============================\n```\n\n## Files\n\n- [`main.py`](./main.py): Implementation of the batch translation node\n- [`utils.py`](./utils.py): Simple wrapper for calling the Anthropic model\n- [`requirements.txt`](./requirements.txt): Project dependencies\n\nThe translations are saved to the `translations` directory, with each file named according to the target language.\n\n--- File Index 21: cookbook/pocketflow-batch/main.py ---\nimport os\nfrom pocketflow import BatchNode, Flow\nfrom utils import call_llm\n\nclass TranslateTextNode(BatchNode):\n    def prep(self, shared):\n        text = shared.get(\"text\", \"(No text provided)\")\n        languages = shared.get(\"languages\", [\"Chinese\", \"Spanish\", \"Japanese\", \"German\", \n                              \"Russian\", \"Portuguese\", \"French\", \"Korean\"])\n        \n        # Create batches for each language translation\n        return [(text, lang) for lang in languages]\n\n    def exec(self, data_tuple):\n        text, language = data_tuple\n        \n        prompt = f\"\"\"\nPlease translate the following markdown file into {language}. \nBut keep the original markdown format, links and code blocks.\nDirectly return the translated text, without any other text or comments.\n\nOriginal: \n{text}\n\nTranslated:\"\"\"\n        \n        result = call_llm(prompt)\n        \n        print(f\"Translated {language} text\")\n\n        return {\"language\": language, \"translation\": result}\n\n    def post(self, shared, prep_res, exec_res_list):\n        # Create output directory if it doesn't exist\n        output_dir = shared.get(\"output_dir\", \"translations\")\n        os.makedirs(output_dir, exist_ok=True)\n        \n        # Write each translation to a file\n        for result in exec_res_list:\n            language, translation = result[\"language\"], result[\"translation\"]\n            \n            # Write to file\n            filename = os.path.join(output_dir, f\"README_{language.upper()}.md\")\n            with open(filename, \"w\", encoding=\"utf-8\") as f:\n                f.write(translation)\n            \n            print(f\"Saved translation to {filename}\")\n\nif __name__ == \"__main__\":\n    # read the text from ../../README.md\n    with open(\"../../README.md\", \"r\") as f:\n        text = f.read()\n    \n    # Default settings\n    shared = {\n        \"text\": text,\n        \"languages\": [\"Chinese\", \"Spanish\", \"Japanese\", \"German\", \"Russian\", \"Portuguese\", \"French\", \"Korean\"],\n        \"output_dir\": \"translations\"\n    }\n\n    # Run the translation flow\n    translate_node = TranslateTextNode(max_retries=3)\n    flow = Flow(start=translate_node)\n    flow.run(shared)\n\n    print(\"\\n=== Translation Complete ===\")\n    print(f\"Translations saved to: {shared['output_dir']}\")\n    print(\"============================\")\n\n--- File Index 22: cookbook/pocketflow-batch/translations/README_CHINESE.md ---\n<div align=\"center\">\n  <img src=\"https://github.com/The-Pocket/.github/raw/main/assets/title.png\" width=\"600\"/>\n</div>\n\n\n![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)\n[![Docs](https://img.shields.io/badge/docs-latest-blue)](https://the-pocket.github.io/PocketFlow/)\n <a href=\"https://discord.gg/hUHHE9Sa6T\">\n    <img src=\"https://img.shields.io/discord/1346833819172601907?logo=discord&style=flat\">\n</a>\n\nPocket Flow \u662f\u4e00\u4e2a[\u4ec5\u6709100\u884c\u4ee3\u7801](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py)\u7684\u6781\u7b80\u4e3b\u4e49LLM\u6846\u67b6\n\n- **\u8f7b\u91cf\u7ea7**\uff1a\u4ec5100\u884c\u4ee3\u7801\u3002\u96f6\u81c3\u80bf\uff0c\u96f6\u4f9d\u8d56\uff0c\u96f6\u4f9b\u5e94\u5546\u9501\u5b9a\u3002\n  \n- **\u8868\u8fbe\u529b\u5f3a**\uff1a\u5305\u542b\u4f60\u559c\u7231\u7684\u4e00\u5207\u2014\u2014([\u591a](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[\u667a\u80fd\u4f53](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html)\u3001[\u5de5\u4f5c\u6d41](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html)\u3001[RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html)\u7b49\u7b49\u3002\n\n- **[\u667a\u80fd\u4f53\u7f16\u7a0b](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)**\uff1a\u8ba9AI\u667a\u80fd\u4f53\uff08\u4f8b\u5982Cursor AI\uff09\u6784\u5efa\u667a\u80fd\u4f53\u2014\u2014\u751f\u4ea7\u529b\u63d0\u534710\u500d\uff01\n\n- \u5b89\u88c5\u65b9\u5f0f\uff1a```pip install pocketflow```\u6216\u8005\u76f4\u63a5\u590d\u5236[\u6e90\u4ee3\u7801](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py)\uff08\u4ec5100\u884c\uff09\u3002\n  \n- \u4e86\u89e3\u66f4\u591a\uff0c\u8bf7\u67e5\u770b[\u6587\u6863](https://the-pocket.github.io/PocketFlow/)\u3002\u4e86\u89e3\u5f00\u53d1\u52a8\u673a\uff0c\u9605\u8bfb[\u6545\u4e8b](https://zacharyhuang.substack.com/p/i-built-an-llm-framework-in-just)\u3002\n  \n- \ud83c\udf89 \u52a0\u5165\u6211\u4eec\u7684[Discord\u793e\u533a](https://discord.gg/hUHHE9Sa6T)\uff01\n\n- \ud83c\udf89 \u611f\u8c22[@zvictor](https://www.github.com/zvictor)\u3001[@jackylee941130](https://www.github.com/jackylee941130)\u548c[@ZebraRoy](https://www.github.com/ZebraRoy)\uff0c\u6211\u4eec\u73b0\u5728\u6709\u4e86[TypeScript\u7248\u672c](https://github.com/The-Pocket/PocketFlow-Typescript)\uff01\n\n## \u4e3a\u4ec0\u4e48\u9009\u62e9Pocket Flow\uff1f\n\n\u5f53\u524d\u7684LLM\u6846\u67b6\u8fc7\u4e8e\u81c3\u80bf... LLM\u6846\u67b6\u53ea\u9700\u8981100\u884c\u4ee3\u7801\u5c31\u591f\u4e86\uff01\n\n<div align=\"center\">\n  <img src=\"https://github.com/The-Pocket/.github/raw/main/assets/meme.jpg\" width=\"400\"/>\n\n\n  |                | **\u62bd\u8c61\u6982\u5ff5**          | **\u7279\u5b9a\u5e94\u7528\u5305\u88c5\u5668**                                      | **\u7279\u5b9a\u5382\u5546\u5305\u88c5\u5668**                                    | **\u4ee3\u7801\u884c\u6570**       | **\u5927\u5c0f**    |\n|----------------|:-----------------------------: |:-----------------------------------------------------------:|:------------------------------------------------------------:|:---------------:|:----------------------------:|\n| LangChain  | \u667a\u80fd\u4f53, \u94fe               | \u5f88\u591a <br><sup><sub>(\u4f8b\u5982\u95ee\u7b54, \u6458\u8981)</sub></sup>              | \u5f88\u591a <br><sup><sub>(\u4f8b\u5982OpenAI, Pinecone\u7b49)</sub></sup>                   | 405K          | +166MB                     |\n| CrewAI     | \u667a\u80fd\u4f53, \u94fe            | \u5f88\u591a <br><sup><sub>(\u4f8b\u5982FileReadTool, SerperDevTool)</sub></sup>         | \u5f88\u591a <br><sup><sub>(\u4f8b\u5982OpenAI, Anthropic, Pinecone\u7b49)</sub></sup>        | 18K           | +173MB                     |\n| SmolAgent   | \u667a\u80fd\u4f53                      | \u4e00\u4e9b <br><sup><sub>(\u4f8b\u5982CodeAgent, VisitWebTool)</sub></sup>         | \u4e00\u4e9b <br><sup><sub>(\u4f8b\u5982DuckDuckGo, Hugging Face\u7b49)</sub></sup>           | 8K            | +198MB                     |\n| LangGraph   | \u667a\u80fd\u4f53, \u56fe           | \u4e00\u4e9b <br><sup><sub>(\u4f8b\u5982\u8bed\u4e49\u641c\u7d22)</sub></sup>                     | \u4e00\u4e9b <br><sup><sub>(\u4f8b\u5982PostgresStore, SqliteSaver\u7b49) </sub></sup>        | 37K           | +51MB                      |\n| AutoGen    | \u667a\u80fd\u4f53                | \u4e00\u4e9b <br><sup><sub>(\u4f8b\u5982Tool Agent, Chat Agent)</sub></sup>              | \u5f88\u591a <sup><sub>[\u53ef\u9009]<br> (\u4f8b\u5982OpenAI, Pinecone\u7b49)</sub></sup>        | 7K <br><sup><sub>(\u4ec5\u6838\u5fc3)</sub></sup>    | +26MB <br><sup><sub>(\u4ec5\u6838\u5fc3)</sub></sup>          |\n| **PocketFlow** | **\u56fe**                    | **\u65e0**                                                 | **\u65e0**                                                  | **100**       | **+56KB**                  |\n\n</div>\n\n## Pocket Flow\u5982\u4f55\u5de5\u4f5c\uff1f\n\n\u8fd9[100\u884c\u4ee3\u7801](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py)\u6355\u83b7\u4e86LLM\u6846\u67b6\u7684\u6838\u5fc3\u62bd\u8c61\uff1a\u56fe\uff01\n<br>\n<div align=\"center\">\n  <img src=\"https://github.com/The-Pocket/.github/raw/main/assets/abstraction.png\" width=\"900\"/>\n</div>\n<br>\n\n\u57fa\u4e8e\u6b64\uff0c\u6613\u4e8e\u5b9e\u73b0\u6d41\u884c\u7684\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u5982([\u591a](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[\u667a\u80fd\u4f53](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html)\u3001[\u5de5\u4f5c\u6d41](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html)\u3001[RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html)\u7b49\u3002\n<br>\n<div align=\"center\">\n  <img src=\"https://github.com/The-Pocket/.github/raw/main/assets/design.png\" width=\"900\"/>\n</div>\n<br>\n\u2728 \u4ee5\u4e0b\u662f\u57fa\u7840\u6559\u7a0b\uff1a\n\n<div align=\"center\">\n  \n|  \u540d\u79f0  | \u96be\u5ea6    |  \u63cf\u8ff0  |  \n| :-------------:  | :-------------: | :--------------------- |  \n| [\u804a\u5929](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat) | \u2606\u2606\u2606 <br> *\u7b80\u5355*   | \u5e26\u6709\u4f1a\u8bdd\u5386\u53f2\u7684\u57fa\u7840\u804a\u5929\u673a\u5668\u4eba |\n| [\u7ed3\u6784\u5316\u8f93\u51fa](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-structured-output) | \u2606\u2606\u2606 <br> *\u7b80\u5355* | \u901a\u8fc7\u63d0\u793a\u4ece\u7b80\u5386\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u6570\u636e |\n| [\u5de5\u4f5c\u6d41](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-workflow) | \u2606\u2606\u2606 <br> *\u7b80\u5355*   | \u80fd\u591f\u6982\u8ff0\u3001\u64b0\u5199\u5185\u5bb9\u5e76\u5e94\u7528\u6837\u5f0f\u7684\u5199\u4f5c\u5de5\u4f5c\u6d41 |\n| [\u667a\u80fd\u4f53](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-agent) | \u2606\u2606\u2606 <br> *\u7b80\u5355*   | \u80fd\u591f\u641c\u7d22\u7f51\u7edc\u5e76\u56de\u7b54\u95ee\u9898\u7684\u7814\u7a76\u667a\u80fd\u4f53 |\n| [RAG](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-rag) | \u2606\u2606\u2606 <br> *\u7b80\u5355*   | \u7b80\u5355\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u8fc7\u7a0b |\n| [Map-Reduce](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-map-reduce) | \u2606\u2606\u2606 <br> *\u7b80\u5355* | \u4f7f\u7528map-reduce\u6a21\u5f0f\u8fdb\u884c\u6279\u91cf\u8bc4\u4f30\u7684\u7b80\u5386\u8d44\u683c\u5904\u7406\u5668 |\n| [\u6d41\u5f0f\u5904\u7406](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-llm-streaming) | \u2606\u2606\u2606 <br> *\u7b80\u5355*   | \u5177\u6709\u7528\u6237\u4e2d\u65ad\u529f\u80fd\u7684\u5b9e\u65f6LLM\u6d41\u5f0f\u6f14\u793a |\n| [\u804a\u5929\u62a4\u680f](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-guardrail) | \u2606\u2606\u2606 <br> *\u7b80\u5355*  | \u53ea\u5904\u7406\u4e0e\u65c5\u884c\u76f8\u5173\u67e5\u8be2\u7684\u65c5\u884c\u987e\u95ee\u804a\u5929\u673a\u5668\u4eba |\n| [\u591a\u667a\u80fd\u4f53](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-multi-agent) | \u2605\u2606\u2606 <br> *\u521d\u7ea7* | \u4e24\u4e2a\u667a\u80fd\u4f53\u4e4b\u95f4\u8fdb\u884c\u5f02\u6b65\u901a\u4fe1\u7684\u7981\u5fcc\u8bcd\u6e38\u620f |\n| [\u76d1\u7763\u8005](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-supervisor) | \u2605\u2606\u2606 <br> *\u521d\u7ea7* | \u7814\u7a76\u667a\u80fd\u4f53\u53d8\u5f97\u4e0d\u53ef\u9760...\u8ba9\u6211\u4eec\u6784\u5efa\u4e00\u4e2a\u76d1\u7763\u8fc7\u7a0b |\n| [\u5e76\u884c\u5904\u7406](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch) | \u2605\u2606\u2606 <br> *\u521d\u7ea7*   | \u5c55\u793a3\u500d\u52a0\u901f\u7684\u5e76\u884c\u6267\u884c\u6f14\u793a |\n| [\u5e76\u884c\u6d41](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch-flow) | \u2605\u2606\u2606 <br> *\u521d\u7ea7*   | \u4f7f\u7528\u591a\u4e2a\u8fc7\u6ee4\u5668\u5c55\u793a8\u500d\u52a0\u901f\u7684\u5e76\u884c\u56fe\u50cf\u5904\u7406\u6f14\u793a |\n| [\u591a\u6570\u6295\u7968](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-majority-vote) | \u2605\u2606\u2606 <br> *\u521d\u7ea7* | \u901a\u8fc7\u6c47\u603b\u591a\u4e2a\u89e3\u51b3\u65b9\u6848\u5c1d\u8bd5\u6765\u63d0\u9ad8\u63a8\u7406\u51c6\u786e\u6027 |\n| [\u601d\u8003](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-thinking) | \u2605\u2606\u2606 <br> *\u521d\u7ea7*   | \u901a\u8fc7\u601d\u7ef4\u94fe\u89e3\u51b3\u590d\u6742\u63a8\u7406\u95ee\u9898 |\n| [\u8bb0\u5fc6](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-memory) | \u2605\u2606\u2606 <br> *\u521d\u7ea7* | \u5177\u6709\u77ed\u671f\u548c\u957f\u671f\u8bb0\u5fc6\u7684\u804a\u5929\u673a\u5668\u4eba |\n| [MCP](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-mcp) | \u2605\u2606\u2606 <br> *\u521d\u7ea7* | \u4f7f\u7528\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\u8fdb\u884c\u6570\u503c\u8fd0\u7b97\u7684\u667a\u80fd\u4f53 |\n\n</div>\n\n\ud83d\udc40 \u60f3\u770b\u66f4\u591a\u521d\u5b66\u8005\u6559\u7a0b\uff1f[\u521b\u5efa\u4e00\u4e2aissue\uff01](https://github.com/The-Pocket/PocketFlow/issues/new)\n\n## \u5982\u4f55\u4f7f\u7528Pocket Flow\uff1f\n\n\ud83d\ude80 \u901a\u8fc7**\u667a\u80fd\u4f53\u7f16\u7a0b**\u2014\u2014\u6700\u5feb\u7684LLM\u5e94\u7528\u5f00\u53d1\u8303\u5f0f\uff0c*\u4eba\u7c7b\u8bbe\u8ba1*\uff0c*\u667a\u80fd\u4f53\u7f16\u7801*\uff01\n\n<br>\n<div align=\"center\">\n  <a href=\"https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to\" target=\"_blank\">\n    <img src=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F423a39af-49e8-483b-bc5a-88cc764350c6_1050x588.png\" width=\"700\" alt=\"IMAGE ALT TEXT\" style=\"cursor: pointer;\">\n  </a>\n</div>\n<br>\n\n\u2728 \u4ee5\u4e0b\u662f\u66f4\u590d\u6742LLM\u5e94\u7528\u7684\u793a\u4f8b\uff1a\n\n<div align=\"center\">\n  \n|  \u5e94\u7528\u540d\u79f0     |  \u96be\u5ea6    | \u4e3b\u9898  | \u4eba\u7c7b\u8bbe\u8ba1 | \u667a\u80fd\u4f53\u4ee3\u7801 |\n| :-------------:  | :-------------: | :---------------------: |  :---: |  :---: |\n| [\u7528Cursor\u6784\u5efaCursor](https://github.com/The-Pocket/Tutorial-Cursor) <br> <sup><sub>\u6211\u4eec\u5f88\u5feb\u5c31\u4f1a\u8fbe\u5230\u5947\u70b9...</sup></sub> | \u2605\u2605\u2605 <br> *\u9ad8\u7ea7*   | [\u667a\u80fd\u4f53](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html) | [\u8bbe\u8ba1\u6587\u6863](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/docs/design.md) | [\u6d41\u7a0b\u4ee3\u7801](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/flow.py)\n| [\u8be2\u95eeAI\u7248Paul Graham](https://github.com/The-Pocket/Tutorial-YC-Partner) <br> <sup><sub>\u5982\u679c\u4f60\u65e0\u6cd5\u8054\u7cfb\u5230\u771f\u4eba\uff0c\u53ef\u4ee5\u8be2\u95eeAI\u7248Paul Graham</sup></sub> | \u2605\u2605\u2606 <br> *\u4e2d\u7ea7*   | [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) <br> [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [TTS](https://the-pocket.github.io/PocketFlow/utility_function/text_to_speech.html) | [\u8bbe\u8ba1\u6587\u6863](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/docs/design.md) | [\u6d41\u7a0b\u4ee3\u7801](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/flow.py)\n| [Youtube\u603b\u7ed3\u5668](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple)  <br> <sup><sub> \u50cf\u89e3\u91ca\u7ed95\u5c81\u5c0f\u5b69\u4e00\u6837\u89e3\u91caYouTube\u89c6\u9891 </sup></sub> | \u2605\u2606\u2606 <br> *\u521d\u7ea7*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) |  [\u8bbe\u8ba1\u6587\u6863](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/docs/design.md) | [\u6d41\u7a0b\u4ee3\u7801](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/flow.py)\n| [\u51b7\u542f\u52a8\u5f00\u573a\u767d\u751f\u6210\u5668](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization)  <br> <sup><sub> \u5373\u65f6\u7834\u51b0\u8bdd\u9898\uff0c\u5c06\u51b7\u95e8\u7ebf\u7d22\u53d8\u70ed </sup></sub> | \u2605\u2606\u2606 <br> *\u521d\u7ea7*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [\u7f51\u7edc\u641c\u7d22](https://the-pocket.github.io/PocketFlow/utility_function/websearch.html) |  [\u8bbe\u8ba1\u6587\u6863](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/docs/design.md) | [\u6d41\u7a0b\u4ee3\u7801](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/flow.py)\n\n</div>\n\n- \u60f3\u5b66\u4e60**\u667a\u80fd\u4f53\u7f16\u7a0b**\uff1f\n\n  - \u67e5\u770b[\u6211\u7684YouTube\u9891\u9053](https://www.youtube.com/@ZacharyLLM?sub_confirmation=1)\u83b7\u53d6\u5173\u4e8e\u4e0a\u8ff0\u5e94\u7528\u5236\u4f5c\u7684\u89c6\u9891\u6559\u7a0b\uff01\n\n  - \u60f3\u6784\u5efa\u81ea\u5df1\u7684LLM\u5e94\u7528\uff1f\u9605\u8bfb\u8fd9\u7bc7[\u6587\u7ae0](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)\uff01\u4ece[\u8fd9\u4e2a\u6a21\u677f](https://github.com/The-Pocket/PocketFlow-Template-Python)\u5f00\u59cb\uff01\n\n  - \u60f3\u4e86\u89e3\u8be6\u7ec6\u6b65\u9aa4\uff1f\u9605\u8bfb\u8fd9\u4efd[\u6307\u5357](https://the-pocket.github.io/PocketFlow/guide.html)\uff01\n\n--- File Index 23: cookbook/pocketflow-batch/translations/README_FRENCH.md ---\n<div align=\"center\">\n  <img src=\"https://github.com/The-Pocket/.github/raw/main/assets/title.png\" width=\"600\"/>\n</div>\n\n\n![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)\n[![Docs](https://img.shields.io/badge/docs-latest-blue)](https://the-pocket.github.io/PocketFlow/)\n <a href=\"https://discord.gg/hUHHE9Sa6T\">\n    <img src=\"https://img.shields.io/discord/1346833819172601907?logo=discord&style=flat\">\n</a>\n\nPocket Flow est un framework LLM minimaliste de [100 lignes](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py)\n\n- **L\u00e9ger**: Seulement 100 lignes. Z\u00e9ro surcharge, z\u00e9ro d\u00e9pendances, z\u00e9ro verrouillage de fournisseur.\n  \n- **Expressif**: Tout ce que vous aimez\u2014([Multi-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[Agents](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [Workflow](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html), et plus encore.\n\n- **[Programmation Agentique](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)**: Laissez les Agents IA (par exemple, Cursor AI) construire des Agents\u2014productivit\u00e9 multipli\u00e9e par 10 !\n\n- Pour installer, ```pip install pocketflow``` ou copiez simplement le [code source](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py) (seulement 100 lignes).\n  \n- Pour en savoir plus, consultez la [documentation](https://the-pocket.github.io/PocketFlow/). Pour comprendre la motivation, lisez l'[histoire](https://zacharyhuang.substack.com/p/i-built-an-llm-framework-in-just).\n  \n- \ud83c\udf89 Rejoignez notre [discord](https://discord.gg/hUHHE9Sa6T) !\n\n- \ud83c\udf89 Merci \u00e0 [@zvictor](https://www.github.com/zvictor), [@jackylee941130](https://www.github.com/jackylee941130) et [@ZebraRoy](https://www.github.com/ZebraRoy), nous avons maintenant une [version TypeScript](https://github.com/The-Pocket/PocketFlow-Typescript) !\n\n## Pourquoi Pocket Flow ?\n\nLes frameworks LLM actuels sont surcharg\u00e9s... Vous n'avez besoin que de 100 lignes pour un framework LLM !\n\n<div align=\"center\">\n  <img src=\"https://github.com/The-Pocket/.github/raw/main/assets/meme.jpg\" width=\"400\"/>\n\n\n  |                | **Abstraction**          | **Wrappers sp\u00e9cifiques aux applications**                                      | **Wrappers sp\u00e9cifiques aux fournisseurs**                                    | **Lignes**       | **Taille**    |\n|----------------|:-----------------------------: |:-----------------------------------------------------------:|:------------------------------------------------------------:|:---------------:|:----------------------------:|\n| LangChain  | Agent, Chain               | Nombreux <br><sup><sub>(ex., QA, R\u00e9sum\u00e9)</sub></sup>              | Nombreux <br><sup><sub>(ex., OpenAI, Pinecone, etc.)</sub></sup>                   | 405K          | +166MB                     |\n| CrewAI     | Agent, Chain            | Nombreux <br><sup><sub>(ex., FileReadTool, SerperDevTool)</sub></sup>         | Nombreux <br><sup><sub>(ex., OpenAI, Anthropic, Pinecone, etc.)</sub></sup>        | 18K           | +173MB                     |\n| SmolAgent   | Agent                      | Quelques <br><sup><sub>(ex., CodeAgent, VisitWebTool)</sub></sup>         | Quelques <br><sup><sub>(ex., DuckDuckGo, Hugging Face, etc.)</sub></sup>           | 8K            | +198MB                     |\n| LangGraph   | Agent, Graph           | Quelques <br><sup><sub>(ex., Recherche S\u00e9mantique)</sub></sup>                     | Quelques <br><sup><sub>(ex., PostgresStore, SqliteSaver, etc.) </sub></sup>        | 37K           | +51MB                      |\n| AutoGen    | Agent                | Quelques <br><sup><sub>(ex., Tool Agent, Chat Agent)</sub></sup>              | Nombreux <sup><sub>[Optionnel]<br> (ex., OpenAI, Pinecone, etc.)</sub></sup>        | 7K <br><sup><sub>(core-only)</sub></sup>    | +26MB <br><sup><sub>(core-only)</sub></sup>          |\n| **PocketFlow** | **Graph**                    | **Aucun**                                                 | **Aucun**                                                  | **100**       | **+56KB**                  |\n\n</div>\n\n## Comment fonctionne Pocket Flow ?\n\nLes [100 lignes](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py) capturent l'abstraction principale des frameworks LLM : le Graphe !\n<br>\n<div align=\"center\">\n  <img src=\"https://github.com/The-Pocket/.github/raw/main/assets/abstraction.png\" width=\"900\"/>\n</div>\n<br>\n\n\u00c0 partir de l\u00e0, il est facile d'impl\u00e9menter des mod\u00e8les de conception populaires comme ([Multi-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[Agents](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [Workflow](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html), etc.\n<br>\n<div align=\"center\">\n  <img src=\"https://github.com/The-Pocket/.github/raw/main/assets/design.png\" width=\"900\"/>\n</div>\n<br>\n\u2728 Voici les tutoriels de base :\n\n<div align=\"center\">\n  \n|  Nom  | Difficult\u00e9    |  Description  |  \n| :-------------:  | :-------------: | :--------------------- |  \n| [Chat](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat) | \u2606\u2606\u2606 <br> *D\u00e9butant*   | Un chatbot de base avec historique de conversation |\n| [Sortie Structur\u00e9e](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-structured-output) | \u2606\u2606\u2606 <br> *D\u00e9butant* | Extraction de donn\u00e9es structur\u00e9es \u00e0 partir de CV par prompt |\n| [Workflow](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-workflow) | \u2606\u2606\u2606 <br> *D\u00e9butant*   | Un workflow d'\u00e9criture qui structure, \u00e9crit du contenu et applique un style |\n| [Agent](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-agent) | \u2606\u2606\u2606 <br> *D\u00e9butant*   | Un agent de recherche qui peut effectuer des recherches sur le web et r\u00e9pondre aux questions |\n| [RAG](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-rag) | \u2606\u2606\u2606 <br> *D\u00e9butant*   | Un processus simple de g\u00e9n\u00e9ration augment\u00e9e par r\u00e9cup\u00e9ration |\n| [Map-Reduce](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-map-reduce) | \u2606\u2606\u2606 <br> *D\u00e9butant* | Un processeur de qualification de CV utilisant le mod\u00e8le map-reduce pour l'\u00e9valuation par lots |\n| [Streaming](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-llm-streaming) | \u2606\u2606\u2606 <br> *D\u00e9butant*   | Une d\u00e9mo de streaming LLM en temps r\u00e9el avec capacit\u00e9 d'interruption utilisateur |\n| [Garde-fou de Chat](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-guardrail) | \u2606\u2606\u2606 <br> *D\u00e9butant*  | Un chatbot conseiller de voyage qui ne traite que les requ\u00eates li\u00e9es au voyage |\n| [Multi-Agent](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-multi-agent) | \u2605\u2606\u2606 <br> *Interm\u00e9diaire* | Un jeu de Taboo pour la communication asynchrone entre deux agents |\n| [Superviseur](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-supervisor) | \u2605\u2606\u2606 <br> *Interm\u00e9diaire* | L'agent de recherche devient peu fiable... Construisons un processus de supervision |\n| [Parall\u00e8le](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch) | \u2605\u2606\u2606 <br> *Interm\u00e9diaire*   | Une d\u00e9mo d'ex\u00e9cution parall\u00e8le qui montre une acc\u00e9l\u00e9ration 3x |\n| [Flux Parall\u00e8le](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch-flow) | \u2605\u2606\u2606 <br> *Interm\u00e9diaire*   | Une d\u00e9mo de traitement d'image parall\u00e8le montrant une acc\u00e9l\u00e9ration 8x avec plusieurs filtres |\n| [Vote \u00e0 la majorit\u00e9](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-majority-vote) | \u2605\u2606\u2606 <br> *Interm\u00e9diaire* | Am\u00e9liore la pr\u00e9cision du raisonnement en agr\u00e9geant plusieurs tentatives de solution |\n| [R\u00e9flexion](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-thinking) | \u2605\u2606\u2606 <br> *Interm\u00e9diaire*   | R\u00e9sout des probl\u00e8mes de raisonnement complexes gr\u00e2ce \u00e0 la Cha\u00eene de Pens\u00e9e |\n| [M\u00e9moire](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-memory) | \u2605\u2606\u2606 <br> *Interm\u00e9diaire* | Un chatbot avec m\u00e9moire \u00e0 court et long terme |\n| [MCP](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-mcp) | \u2605\u2606\u2606 <br> *Interm\u00e9diaire* | Agent utilisant le Protocole de Contexte de Mod\u00e8le pour des op\u00e9rations num\u00e9riques |\n\n</div>\n\n\ud83d\udc40 Vous voulez voir d'autres tutoriels pour d\u00e9butants ? [Cr\u00e9ez une issue!](https://github.com/The-Pocket/PocketFlow/issues/new)\n\n## Comment utiliser Pocket Flow ?\n\n\ud83d\ude80 Gr\u00e2ce \u00e0 la **Programmation Agentique**\u2014le paradigme de d\u00e9veloppement d'applications LLM le plus rapide\u2014o\u00f9 *les humains con\u00e7oivent* et *les agents codent* !\n\n<br>\n<div align=\"center\">\n  <a href=\"https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to\" target=\"_blank\">\n    <img src=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F423a39af-49e8-483b-bc5a-88cc764350c6_1050x588.png\" width=\"700\" alt=\"IMAGE ALT TEXT\" style=\"cursor: pointer;\">\n  </a>\n</div>\n<br>\n\n\u2728 Voici des exemples d'applications LLM plus complexes :\n\n<div align=\"center\">\n  \n|  Nom de l'application     |  Difficult\u00e9    | Sujets  | Conception Humaine | Code Agent |\n| :-------------:  | :-------------: | :---------------------: |  :---: |  :---: |\n| [Construire Cursor avec Cursor](https://github.com/The-Pocket/Tutorial-Cursor) <br> <sup><sub>Nous atteindrons bient\u00f4t la singularit\u00e9 ...</sup></sub> | \u2605\u2605\u2605 <br> *Avanc\u00e9*   | [Agent](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html) | [Document de conception](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/docs/design.md) | [Code Flow](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/flow.py)\n| [Demandez \u00e0 l'IA Paul Graham](https://github.com/The-Pocket/Tutorial-YC-Partner) <br> <sup><sub>Demandez \u00e0 l'IA Paul Graham, au cas o\u00f9 vous n'\u00eates pas accept\u00e9</sup></sub> | \u2605\u2605\u2606 <br> *Moyen*   | [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) <br> [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [TTS](https://the-pocket.github.io/PocketFlow/utility_function/text_to_speech.html) | [Document de conception](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/docs/design.md) | [Code Flow](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/flow.py)\n| [R\u00e9sumeur Youtube](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple)  <br> <sup><sub> Explique les vid\u00e9os YouTube comme si vous aviez 5 ans </sup></sub> | \u2605\u2606\u2606 <br> *Interm\u00e9diaire*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) |  [Document de conception](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/docs/design.md) | [Code Flow](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/flow.py)\n| [G\u00e9n\u00e9rateur d'Introduction](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization)  <br> <sup><sub> Des brise-glaces instantan\u00e9s qui transforment les prospects froids en prospects chauds </sup></sub> | \u2605\u2606\u2606 <br> *Interm\u00e9diaire*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [Recherche Web](https://the-pocket.github.io/PocketFlow/utility_function/websearch.html) |  [Document de conception](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/docs/design.md) | [Code Flow](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/flow.py)\n\n</div>\n\n- Vous voulez apprendre la **Programmation Agentique** ?\n\n  - Consultez [ma cha\u00eene YouTube](https://www.youtube.com/@ZacharyLLM?sub_confirmation=1) pour des tutoriels vid\u00e9o sur la fa\u00e7on dont certaines applications ci-dessus sont cr\u00e9\u00e9es !\n\n  - Vous souhaitez cr\u00e9er votre propre application LLM ? Lisez cet [article](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to) ! Commencez avec [ce mod\u00e8le](https://github.com/The-Pocket/PocketFlow-Template-Python) !\n\n  - Vous voulez apprendre les \u00e9tapes d\u00e9taill\u00e9es ? Lisez ce [Guide](https://the-pocket.github.io/PocketFlow/guide.html) !\n\n--- File Index 24: cookbook/pocketflow-batch/translations/README_GERMAN.md ---\n<div align=\"center\">\n  <img src=\"https://github.com/The-Pocket/.github/raw/main/assets/title.png\" width=\"600\"/>\n</div>\n\n\n![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)\n[![Docs](https://img.shields.io/badge/docs-latest-blue)](https://the-pocket.github.io/PocketFlow/)\n <a href=\"https://discord.gg/hUHHE9Sa6T\">\n    <img src=\"https://img.shields.io/discord/1346833819172601907?logo=discord&style=flat\">\n</a>\n\nPocket Flow ist ein [100-Zeilen](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py) minimalistisches LLM-Framework\n\n- **Leichtgewichtig**: Nur 100 Zeilen. Keine Aufbl\u00e4hung, keine Abh\u00e4ngigkeiten, keine Anbieter-Bindung.\n  \n- **Ausdrucksstark**: Alles, was du liebst\u2014([Multi-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[Agenten](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [Workflow](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) und mehr.\n\n- **[Agentisches Programmieren](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)**: Lass KI-Agenten (z.B. Cursor AI) Agenten bauen\u201410-fache Produktivit\u00e4tssteigerung!\n\n- Zur Installation, ```pip install pocketflow``` oder kopiere einfach den [Quellcode](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py) (nur 100 Zeilen).\n  \n- Um mehr zu erfahren, schau dir die [Dokumentation](https://the-pocket.github.io/PocketFlow/) an. Um die Motivation zu verstehen, lies die [Geschichte](https://zacharyhuang.substack.com/p/i-built-an-llm-framework-in-just).\n  \n- \ud83c\udf89 Tritt unserem [Discord](https://discord.gg/hUHHE9Sa6T) bei!\n\n- \ud83c\udf89 Dank [@zvictor](https://www.github.com/zvictor), [@jackylee941130](https://www.github.com/jackylee941130) und [@ZebraRoy](https://www.github.com/ZebraRoy) haben wir jetzt eine [TypeScript-Version](https://github.com/The-Pocket/PocketFlow-Typescript)!\n\n## Warum Pocket Flow?\n\nAktuelle LLM-Frameworks sind aufgebl\u00e4ht... Du brauchst nur 100 Zeilen f\u00fcr ein LLM-Framework!\n\n<div align=\"center\">\n  <img src=\"https://github.com/The-Pocket/.github/raw/main/assets/meme.jpg\" width=\"400\"/>\n\n\n  |                | **Abstraktion**          | **App-spezifische Wrapper**                                      | **Anbieter-spezifische Wrapper**                                    | **Zeilen**       | **Gr\u00f6\u00dfe**    |\n|----------------|:-----------------------------: |:-----------------------------------------------------------:|:------------------------------------------------------------:|:---------------:|:----------------------------:|\n| LangChain  | Agent, Chain               | Viele <br><sup><sub>(z.B. QA, Zusammenfassung)</sub></sup>              | Viele <br><sup><sub>(z.B. OpenAI, Pinecone, usw.)</sub></sup>                   | 405K          | +166MB                     |\n| CrewAI     | Agent, Chain            | Viele <br><sup><sub>(z.B. FileReadTool, SerperDevTool)</sub></sup>         | Viele <br><sup><sub>(z.B. OpenAI, Anthropic, Pinecone, usw.)</sub></sup>        | 18K           | +173MB                     |\n| SmolAgent   | Agent                      | Einige <br><sup><sub>(z.B. CodeAgent, VisitWebTool)</sub></sup>         | Einige <br><sup><sub>(z.B. DuckDuckGo, Hugging Face, usw.)</sub></sup>           | 8K            | +198MB                     |\n| LangGraph   | Agent, Graph           | Einige <br><sup><sub>(z.B. Semantische Suche)</sub></sup>                     | Einige <br><sup><sub>(z.B. PostgresStore, SqliteSaver, usw.) </sub></sup>        | 37K           | +51MB                      |\n| AutoGen    | Agent                | Einige <br><sup><sub>(z.B. Tool Agent, Chat Agent)</sub></sup>              | Viele <sup><sub>[Optional]<br> (z.B. OpenAI, Pinecone, usw.)</sub></sup>        | 7K <br><sup><sub>(nur Kern)</sub></sup>    | +26MB <br><sup><sub>(nur Kern)</sub></sup>          |\n| **PocketFlow** | **Graph**                    | **Keine**                                                 | **Keine**                                                  | **100**       | **+56KB**                  |\n\n</div>\n\n## Wie funktioniert Pocket Flow?\n\nDie [100 Zeilen](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py) erfassen die Kernabstraktion von LLM-Frameworks: Graph!\n<br>\n<div align=\"center\">\n  <img src=\"https://github.com/The-Pocket/.github/raw/main/assets/abstraction.png\" width=\"900\"/>\n</div>\n<br>\n\nVon dort aus ist es einfach, beliebte Designmuster wie ([Multi-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[Agenten](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [Workflow](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) usw. zu implementieren.\n<br>\n<div align=\"center\">\n  <img src=\"https://github.com/The-Pocket/.github/raw/main/assets/design.png\" width=\"900\"/>\n</div>\n<br>\n\u2728 Hier sind grundlegende Tutorials:\n\n<div align=\"center\">\n  \n|  Name  | Schwierigkeit    |  Beschreibung  |  \n| :-------------:  | :-------------: | :--------------------- |  \n| [Chat](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat) | \u2606\u2606\u2606 <br> *Dummy*   | Ein einfacher Chatbot mit Konversationsverlauf |\n| [Strukturierte Ausgabe](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-structured-output) | \u2606\u2606\u2606 <br> *Dummy* | Extrahieren strukturierter Daten aus Lebensl\u00e4ufen durch Prompting |\n| [Workflow](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-workflow) | \u2606\u2606\u2606 <br> *Dummy*   | Ein Schreibworkflow, der Gliederungen erstellt, Inhalte schreibt und Styling anwendet |\n| [Agent](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-agent) | \u2606\u2606\u2606 <br> *Dummy*   | Ein Recherche-Agent, der im Web suchen und Fragen beantworten kann |\n| [RAG](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-rag) | \u2606\u2606\u2606 <br> *Dummy*   | Ein einfacher Retrieval-augmented Generation-Prozess |\n| [Map-Reduce](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-map-reduce) | \u2606\u2606\u2606 <br> *Dummy* | Ein Lebenslauf-Qualifikationsprozessor mit Map-Reduce-Muster f\u00fcr Batch-Auswertung |\n| [Streaming](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-llm-streaming) | \u2606\u2606\u2606 <br> *Dummy*   | Eine Echtzeit-LLM-Streaming-Demo mit Benutzerunterbrechungsf\u00e4higkeit |\n| [Chat-Absicherung](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-guardrail) | \u2606\u2606\u2606 <br> *Dummy*  | Ein Reiseberater-Chatbot, der nur reisebezogene Anfragen verarbeitet |\n| [Multi-Agent](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-multi-agent) | \u2605\u2606\u2606 <br> *Anf\u00e4nger* | Ein Tabu-Wortspiel f\u00fcr asynchrone Kommunikation zwischen zwei Agenten |\n| [Supervisor](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-supervisor) | \u2605\u2606\u2606 <br> *Anf\u00e4nger* | Forschungsagent wird unzuverl\u00e4ssig... Bauen wir einen \u00dcberwachungsprozess! |\n| [Parallel](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch) | \u2605\u2606\u2606 <br> *Anf\u00e4nger*   | Eine parallele Ausf\u00fchrungs-Demo, die 3x Beschleunigung zeigt |\n| [Parallel Flow](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch-flow) | \u2605\u2606\u2606 <br> *Anf\u00e4nger*   | Eine parallele Bildverarbeitungs-Demo, die 8x Beschleunigung mit mehreren Filtern zeigt |\n| [Mehrheitsvotum](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-majority-vote) | \u2605\u2606\u2606 <br> *Anf\u00e4nger* | Verbessere die Reasoning-Genauigkeit durch Aggregation mehrerer L\u00f6sungsversuche |\n| [Thinking](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-thinking) | \u2605\u2606\u2606 <br> *Anf\u00e4nger*   | L\u00f6se komplexe Reasoning-Probleme durch Chain-of-Thought |\n| [Memory](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-memory) | \u2605\u2606\u2606 <br> *Anf\u00e4nger* | Ein Chatbot mit Kurz- und Langzeitged\u00e4chtnis |\n| [MCP](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-mcp) | \u2605\u2606\u2606 <br> *Anf\u00e4nger* | Agent, der das Model Context Protocol f\u00fcr numerische Operationen verwendet |\n\n</div>\n\n\ud83d\udc40 M\u00f6chtest du andere Tutorials f\u00fcr Anf\u00e4nger sehen? [Erstelle ein Issue!](https://github.com/The-Pocket/PocketFlow/issues/new)\n\n## Wie verwendet man Pocket Flow?\n\n\ud83d\ude80 Durch **Agentisches Programmieren**\u2014das schnellste LLM-App-Entwicklungsparadigma, bei dem *Menschen designen* und *Agenten programmieren*!\n\n<br>\n<div align=\"center\">\n  <a href=\"https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to\" target=\"_blank\">\n    <img src=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F423a39af-49e8-483b-bc5a-88cc764350c6_1050x588.png\" width=\"700\" alt=\"IMAGE ALT TEXT\" style=\"cursor: pointer;\">\n  </a>\n</div>\n<br>\n\n\u2728 Hier sind Beispiele f\u00fcr komplexere LLM-Apps:\n\n<div align=\"center\">\n  \n|  App-Name     |  Schwierigkeit    | Themen  | Menschliches Design | Agent-Code |\n| :-------------:  | :-------------: | :---------------------: |  :---: |  :---: |\n| [Cursor mit Cursor bauen](https://github.com/The-Pocket/Tutorial-Cursor) <br> <sup><sub>Wir werden bald die Singularit\u00e4t erreichen ...</sup></sub> | \u2605\u2605\u2605 <br> *Fortgeschritten*   | [Agent](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html) | [Design-Dokument](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/docs/design.md) | [Flow-Code](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/flow.py)\n| [Frag KI Paul Graham](https://github.com/The-Pocket/Tutorial-YC-Partner) <br> <sup><sub>Frag KI Paul Graham, falls du nicht reinkommst</sup></sub> | \u2605\u2605\u2606 <br> *Mittel*   | [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) <br> [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [TTS](https://the-pocket.github.io/PocketFlow/utility_function/text_to_speech.html) | [Design-Dokument](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/docs/design.md) | [Flow-Code](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/flow.py)\n| [Youtube-Zusammenfasser](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple)  <br> <sup><sub> Erkl\u00e4rt dir YouTube-Videos, als w\u00e4rst du 5 </sup></sub> | \u2605\u2606\u2606 <br> *Anf\u00e4nger*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) |  [Design-Dokument](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/docs/design.md) | [Flow-Code](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/flow.py)\n| [Kaltakquise-Generator](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization)  <br> <sup><sub> Sofortige Eisbrecher, die kalte Leads hei\u00df machen </sup></sub> | \u2605\u2606\u2606 <br> *Anf\u00e4nger*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [Web-Suche](https://the-pocket.github.io/PocketFlow/utility_function/websearch.html) |  [Design-Dokument](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/docs/design.md) | [Flow-Code](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/flow.py)\n\n</div>\n\n- Willst du **Agentisches Programmieren** lernen?\n\n  - Schau dir [meinen YouTube-Kanal](https://www.youtube.com/@ZacharyLLM?sub_confirmation=1) f\u00fcr Video-Tutorials an, wie einige der obigen Apps erstellt wurden!\n\n  - Willst du deine eigene LLM-App bauen? Lies diesen [Beitrag](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)! Beginne mit [dieser Vorlage](https://github.com/The-Pocket/PocketFlow-Template-Python)!\n\n  - Willst du die detaillierten Schritte lernen? Lies diesen [Leitfaden](https://the-pocket.github.io/PocketFlow/guide.html)!\n\n--- File Index 25: cookbook/pocketflow-batch/translations/README_JAPANESE.md ---\n<div align=\"center\">\n  <img src=\"https://github.com/The-Pocket/.github/raw/main/assets/title.png\" width=\"600\"/>\n</div>\n\n\n![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)\n[![Docs](https://img.shields.io/badge/docs-latest-blue)](https://the-pocket.github.io/PocketFlow/)\n <a href=\"https://discord.gg/hUHHE9Sa6T\">\n    <img src=\"https://img.shields.io/discord/1346833819172601907?logo=discord&style=flat\">\n</a>\n\nPocket Flow\u306f[100\u884c](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py)\u306e\u30df\u30cb\u30de\u30ea\u30b9\u30c8LLM\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3067\u3059\n\n- **\u8efd\u91cf**: \u308f\u305a\u304b100\u884c\u3002\u4f59\u5206\u306a\u3082\u306e\u4e00\u5207\u306a\u3057\u3001\u4f9d\u5b58\u95a2\u4fc2\u306a\u3057\u3001\u30d9\u30f3\u30c0\u30fc\u30ed\u30c3\u30af\u30a4\u30f3\u306a\u3057\u3002\n  \n- **\u8868\u73fe\u529b**: \u3042\u306a\u305f\u304c\u597d\u304d\u306a\u3082\u306e\u5168\u3066\u2014([\u30de\u30eb\u30c1-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html)\u3001[\u30ef\u30fc\u30af\u30d5\u30ed\u30fc](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html)\u3001[RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html)\u306a\u3069\u3002\n\n- **[\u30a8\u30fc\u30b8\u30a7\u30f3\u30c6\u30a3\u30c3\u30af\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)**: AI\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\uff08\u4f8b\uff1aCursor AI\uff09\u306b\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u3092\u69cb\u7bc9\u3055\u305b\u308b\u2014\u751f\u7523\u6027\u304c10\u500d\u306b\uff01\n\n- \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u306b\u306f\u3001```pip install pocketflow```\u307e\u305f\u306f[\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py)\u3092\u30b3\u30d4\u30fc\u3059\u308b\u3060\u3051\u3067\u3059\uff08\u308f\u305a\u304b100\u884c\uff09\u3002\n  \n- \u8a73\u7d30\u306b\u3064\u3044\u3066\u306f[\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8](https://the-pocket.github.io/PocketFlow/)\u3092\u3054\u89a7\u304f\u3060\u3055\u3044\u3002\u52d5\u6a5f\u306b\u3064\u3044\u3066\u5b66\u3076\u306b\u306f\u3001[\u30b9\u30c8\u30fc\u30ea\u30fc](https://zacharyhuang.substack.com/p/i-built-an-llm-framework-in-just)\u3092\u304a\u8aad\u307f\u304f\u3060\u3055\u3044\u3002\n  \n- \ud83c\udf89 \u79c1\u305f\u3061\u306e[Discord](https://discord.gg/hUHHE9Sa6T)\u306b\u53c2\u52a0\u3057\u3066\u304f\u3060\u3055\u3044\uff01\n\n- \ud83c\udf89 [@zvictor](https://www.github.com/zvictor)\u3001[@jackylee941130](https://www.github.com/jackylee941130)\u3001[@ZebraRoy](https://www.github.com/ZebraRoy)\u306e\u304a\u304b\u3052\u3067\u3001[TypeScript\u30d0\u30fc\u30b8\u30e7\u30f3](https://github.com/The-Pocket/PocketFlow-Typescript)\u3082\u3067\u304d\u307e\u3057\u305f\uff01\n\n## \u306a\u305cPocket Flow\uff1f\n\n\u73fe\u5728\u306eLLM\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306f\u81a8\u5927\u3059\u304e\u307e\u3059... LLM\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306b\u306f100\u884c\u3060\u3051\u3067\u5341\u5206\u3067\u3059\uff01\n\n<div align=\"center\">\n  <img src=\"https://github.com/The-Pocket/.github/raw/main/assets/meme.jpg\" width=\"400\"/>\n\n\n  |                | **\u62bd\u8c61\u5316**          | **\u30a2\u30d7\u30ea\u56fa\u6709\u306e\u30e9\u30c3\u30d1\u30fc**                                      | **\u30d9\u30f3\u30c0\u30fc\u56fa\u6709\u306e\u30e9\u30c3\u30d1\u30fc**                                    | **\u884c\u6570**       | **\u30b5\u30a4\u30ba**    |\n|----------------|:-----------------------------: |:-----------------------------------------------------------:|:------------------------------------------------------------:|:---------------:|:----------------------------:|\n| LangChain  | \u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u3001\u30c1\u30a7\u30fc\u30f3               | \u591a\u6570 <br><sup><sub>(\u4f8b\uff1aQA\u3001\u8981\u7d04)</sub></sup>              | \u591a\u6570 <br><sup><sub>(\u4f8b\uff1aOpenAI\u3001Pinecone\u306a\u3069)</sub></sup>                   | 405K          | +166MB                     |\n| CrewAI     | \u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u3001\u30c1\u30a7\u30fc\u30f3            | \u591a\u6570 <br><sup><sub>(\u4f8b\uff1aFileReadTool\u3001SerperDevTool)</sub></sup>         | \u591a\u6570 <br><sup><sub>(\u4f8b\uff1aOpenAI\u3001Anthropic\u3001Pinecone\u306a\u3069)</sub></sup>        | 18K           | +173MB                     |\n| SmolAgent   | \u30a8\u30fc\u30b8\u30a7\u30f3\u30c8                      | \u3044\u304f\u3064\u304b <br><sup><sub>(\u4f8b\uff1aCodeAgent\u3001VisitWebTool)</sub></sup>         | \u3044\u304f\u3064\u304b <br><sup><sub>(\u4f8b\uff1aDuckDuckGo\u3001Hugging Face\u306a\u3069)</sub></sup>           | 8K            | +198MB                     |\n| LangGraph   | \u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u3001\u30b0\u30e9\u30d5           | \u3044\u304f\u3064\u304b <br><sup><sub>(\u4f8b\uff1a\u30bb\u30de\u30f3\u30c6\u30a3\u30c3\u30af\u691c\u7d22)</sub></sup>                     | \u3044\u304f\u3064\u304b <br><sup><sub>(\u4f8b\uff1aPostgresStore\u3001SqliteSaver\u306a\u3069) </sub></sup>        | 37K           | +51MB                      |\n| AutoGen    | \u30a8\u30fc\u30b8\u30a7\u30f3\u30c8                | \u3044\u304f\u3064\u304b <br><sup><sub>(\u4f8b\uff1a\u30c4\u30fc\u30eb\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u3001\u30c1\u30e3\u30c3\u30c8\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8)</sub></sup>              | \u591a\u6570 <sup><sub>[\u30aa\u30d7\u30b7\u30e7\u30f3]<br> (\u4f8b\uff1aOpenAI\u3001Pinecone\u306a\u3069)</sub></sup>        | 7K <br><sup><sub>(\u30b3\u30a2\u306e\u307f)</sub></sup>    | +26MB <br><sup><sub>(\u30b3\u30a2\u306e\u307f)</sub></sup>          |\n| **PocketFlow** | **\u30b0\u30e9\u30d5**                    | **\u306a\u3057**                                                 | **\u306a\u3057**                                                  | **100**       | **+56KB**                  |\n\n</div>\n\n## Pocket Flow\u306f\u3069\u306e\u3088\u3046\u306b\u52d5\u4f5c\u3057\u307e\u3059\u304b\uff1f\n\n[100\u884c](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py)\u306e\u30b3\u30fc\u30c9\u306fLLM\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306e\u6838\u3068\u306a\u308b\u62bd\u8c61\u5316\u3092\u6349\u3048\u3066\u3044\u307e\u3059\uff1a\u30b0\u30e9\u30d5\uff01\n<br>\n<div align=\"center\">\n  <img src=\"https://github.com/The-Pocket/.github/raw/main/assets/abstraction.png\" width=\"900\"/>\n</div>\n<br>\n\n\u305d\u3053\u304b\u3089\u3001([\u30de\u30eb\u30c1-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html)\u3001[\u30ef\u30fc\u30af\u30d5\u30ed\u30fc](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html)\u3001[RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html)\u306a\u3069\u306e\u4eba\u6c17\u306e\u3042\u308b\u30c7\u30b6\u30a4\u30f3\u30d1\u30bf\u30fc\u30f3\u3092\u7c21\u5358\u306b\u5b9f\u88c5\u3067\u304d\u307e\u3059\u3002\n<br>\n<div align=\"center\">\n  <img src=\"https://github.com/The-Pocket/.github/raw/main/assets/design.png\" width=\"900\"/>\n</div>\n<br>\n\u2728 \u4ee5\u4e0b\u306f\u57fa\u672c\u7684\u306a\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u3067\u3059\uff1a\n\n<div align=\"center\">\n  \n|  \u540d\u524d  | \u96e3\u6613\u5ea6    |  \u8aac\u660e  |  \n| :-------------:  | :-------------: | :--------------------- |  \n| [\u30c1\u30e3\u30c3\u30c8](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat) | \u2606\u2606\u2606 <br> *\u8d85\u7c21\u5358*   | \u4f1a\u8a71\u5c65\u6b74\u3092\u6301\u3064\u57fa\u672c\u7684\u306a\u30c1\u30e3\u30c3\u30c8\u30dc\u30c3\u30c8 |\n| [\u69cb\u9020\u5316\u51fa\u529b](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-structured-output) | \u2606\u2606\u2606 <br> *\u8d85\u7c21\u5358* | \u30d7\u30ed\u30f3\u30d7\u30c8\u306b\u3088\u308b\u5c65\u6b74\u66f8\u304b\u3089\u306e\u69cb\u9020\u5316\u30c7\u30fc\u30bf\u306e\u62bd\u51fa |\n| [\u30ef\u30fc\u30af\u30d5\u30ed\u30fc](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-workflow) | \u2606\u2606\u2606 <br> *\u8d85\u7c21\u5358*   | \u6982\u8981\u3092\u4f5c\u6210\u3057\u3001\u30b3\u30f3\u30c6\u30f3\u30c4\u3092\u66f8\u304d\u3001\u30b9\u30bf\u30a4\u30eb\u3092\u9069\u7528\u3059\u308b\u30e9\u30a4\u30c6\u30a3\u30f3\u30b0\u30ef\u30fc\u30af\u30d5\u30ed\u30fc |\n| [\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-agent) | \u2606\u2606\u2606 <br> *\u8d85\u7c21\u5358*   | \u30a6\u30a7\u30d6\u3092\u691c\u7d22\u3057\u3066\u8cea\u554f\u306b\u7b54\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u7814\u7a76\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8 |\n| [RAG](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-rag) | \u2606\u2606\u2606 <br> *\u8d85\u7c21\u5358*   | \u30b7\u30f3\u30d7\u30eb\u306a\u691c\u7d22\u62e1\u5f35\u751f\u6210\u30d7\u30ed\u30bb\u30b9 |\n| [\u30de\u30c3\u30d7-\u30ea\u30c7\u30e5\u30fc\u30b9](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-map-reduce) | \u2606\u2606\u2606 <br> *\u8d85\u7c21\u5358* | \u30d0\u30c3\u30c1\u8a55\u4fa1\u306e\u305f\u3081\u306e\u30de\u30c3\u30d7\u30ea\u30c7\u30e5\u30fc\u30b9\u30d1\u30bf\u30fc\u30f3\u3092\u4f7f\u7528\u3057\u305f\u5c65\u6b74\u66f8\u8cc7\u683c\u51e6\u7406 |\n| [\u30b9\u30c8\u30ea\u30fc\u30df\u30f3\u30b0](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-llm-streaming) | \u2606\u2606\u2606 <br> *\u8d85\u7c21\u5358*   | \u30e6\u30fc\u30b6\u30fc\u4e2d\u65ad\u6a5f\u80fd\u3092\u5099\u3048\u305f\u30ea\u30a2\u30eb\u30bf\u30a4\u30e0LLM\u30b9\u30c8\u30ea\u30fc\u30df\u30f3\u30b0\u30c7\u30e2 |\n| [\u30c1\u30e3\u30c3\u30c8\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-guardrail) | \u2606\u2606\u2606 <br> *\u8d85\u7c21\u5358*  | \u65c5\u884c\u95a2\u9023\u306e\u30af\u30a8\u30ea\u306e\u307f\u3092\u51e6\u7406\u3059\u308b\u65c5\u884c\u30a2\u30c9\u30d0\u30a4\u30b6\u30fc\u30c1\u30e3\u30c3\u30c8\u30dc\u30c3\u30c8 |\n| [\u30de\u30eb\u30c1\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-multi-agent) | \u2605\u2606\u2606 <br> *\u521d\u7d1a* | 2\u3064\u306e\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u9593\u306e\u975e\u540c\u671f\u901a\u4fe1\u306e\u305f\u3081\u306e\u30bf\u30d6\u30fc\u30ef\u30fc\u30c9\u30b2\u30fc\u30e0 |\n| [\u30b9\u30fc\u30d1\u30fc\u30d0\u30a4\u30b6\u30fc](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-supervisor) | \u2605\u2606\u2606 <br> *\u521d\u7d1a* | \u7814\u7a76\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u304c\u4fe1\u983c\u6027\u306b\u6b20\u3051\u308b\u5834\u5408... \u76e3\u8996\u30d7\u30ed\u30bb\u30b9\u3092\u69cb\u7bc9\u3057\u307e\u3057\u3087\u3046 |\n| [\u4e26\u5217](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch) | \u2605\u2606\u2606 <br> *\u521d\u7d1a*   | 3\u500d\u306e\u901f\u5ea6\u5411\u4e0a\u3092\u793a\u3059\u4e26\u5217\u5b9f\u884c\u30c7\u30e2 |\n| [\u4e26\u5217\u30d5\u30ed\u30fc](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch-flow) | \u2605\u2606\u2606 <br> *\u521d\u7d1a*   | \u8907\u6570\u306e\u30d5\u30a3\u30eb\u30bf\u30fc\u30678\u500d\u306e\u901f\u5ea6\u5411\u4e0a\u3092\u793a\u3059\u4e26\u5217\u753b\u50cf\u51e6\u7406\u30c7\u30e2 |\n| [\u591a\u6570\u6c7a](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-majority-vote) | \u2605\u2606\u2606 <br> *\u521d\u7d1a* | \u8907\u6570\u306e\u89e3\u6c7a\u7b56\u3092\u96c6\u7d04\u3057\u3066\u63a8\u8ad6\u7cbe\u5ea6\u3092\u5411\u4e0a\u3055\u305b\u308b |\n| [\u601d\u8003](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-thinking) | \u2605\u2606\u2606 <br> *\u521d\u7d1a*   | \u601d\u8003\u9023\u9396\u3092\u901a\u3058\u3066\u8907\u96d1\u306a\u63a8\u8ad6\u554f\u984c\u3092\u89e3\u6c7a\u3059\u308b |\n| [\u30e1\u30e2\u30ea](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-memory) | \u2605\u2606\u2606 <br> *\u521d\u7d1a* | \u77ed\u671f\u304a\u3088\u3073\u9577\u671f\u8a18\u61b6\u3092\u6301\u3064\u30c1\u30e3\u30c3\u30c8\u30dc\u30c3\u30c8 |\n| [MCP](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-mcp) | \u2605\u2606\u2606 <br> *\u521d\u7d1a* | \u6570\u5024\u6f14\u7b97\u306e\u305f\u3081\u306e\u30e2\u30c7\u30eb\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30d7\u30ed\u30c8\u30b3\u30eb\u3092\u4f7f\u7528\u3059\u308b\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8 |\n\n</div>\n\n\ud83d\udc40 \u4ed6\u306e\u8d85\u7c21\u5358\u306a\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u304c\u898b\u305f\u3044\u3067\u3059\u304b\uff1f[\u8ab2\u984c\u3092\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044\uff01](https://github.com/The-Pocket/PocketFlow/issues/new)\n\n## Pocket Flow\u306e\u4f7f\u3044\u65b9\u306f\uff1f\n\n\ud83d\ude80 **\u30a8\u30fc\u30b8\u30a7\u30f3\u30c6\u30a3\u30c3\u30af\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0**\u3092\u901a\u3058\u3066\u2014\u6700\u901f\u306eLLM\u30a2\u30d7\u30ea\u958b\u767a\u30d1\u30e9\u30c0\u30a4\u30e0\u3067\u3001*\u4eba\u9593\u304c\u8a2d\u8a08\u3057*\u3001*\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u304c\u30b3\u30fc\u30c9\u3092\u66f8\u304f*\uff01\n\n<br>\n<div align=\"center\">\n  <a href=\"https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to\" target=\"_blank\">\n    <img src=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F423a39af-49e8-483b-bc5a-88cc764350c6_1050x588.png\" width=\"700\" alt=\"IMAGE ALT TEXT\" style=\"cursor: pointer;\">\n  </a>\n</div>\n<br>\n\n\u2728 \u4ee5\u4e0b\u306f\u3088\u308a\u8907\u96d1\u306aLLM\u30a2\u30d7\u30ea\u306e\u4f8b\u3067\u3059\uff1a\n\n<div align=\"center\">\n  \n|  \u30a2\u30d7\u30ea\u540d     |  \u96e3\u6613\u5ea6    | \u30c8\u30d4\u30c3\u30af  | \u4eba\u9593\u306b\u3088\u308b\u8a2d\u8a08 | \u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u306b\u3088\u308b\u30b3\u30fc\u30c9 |\n| :-------------:  | :-------------: | :---------------------: |  :---: |  :---: |\n| [Cursor\u3092\u4f7f\u3063\u3066Cursor\u3092\u69cb\u7bc9](https://github.com/The-Pocket/Tutorial-Cursor) <br> <sup><sub>\u3082\u3046\u3059\u3050\u30b7\u30f3\u30ae\u30e5\u30e9\u30ea\u30c6\u30a3\u306b\u5230\u9054\u3057\u307e\u3059...</sup></sub> | \u2605\u2605\u2605 <br> *\u4e0a\u7d1a*   | [\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html) | [\u8a2d\u8a08\u66f8](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/docs/design.md) | [\u30d5\u30ed\u30fc\u30b3\u30fc\u30c9](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/flow.py)\n| [AI Paul Graham\u306b\u8cea\u554f\u3059\u308b](https://github.com/The-Pocket/Tutorial-YC-Partner) <br> <sup><sub>\u3082\u3057\u5165\u308c\u306a\u304b\u3063\u305f\u5834\u5408\u306f\u3001AI Paul Graham\u306b\u805e\u3044\u3066\u307f\u307e\u3057\u3087\u3046</sup></sub> | \u2605\u2605\u2606 <br> *\u4e2d\u7d1a*   | [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) <br> [\u30de\u30c3\u30d7\u30ea\u30c7\u30e5\u30fc\u30b9](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [TTS](https://the-pocket.github.io/PocketFlow/utility_function/text_to_speech.html) | [\u8a2d\u8a08\u66f8](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/docs/design.md) | [\u30d5\u30ed\u30fc\u30b3\u30fc\u30c9](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/flow.py)\n| [Youtube\u30b5\u30de\u30e9\u30a4\u30b6\u30fc](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple)  <br> <sup><sub> 5\u6b73\u5150\u306b\u3082\u308f\u304b\u308b\u3088\u3046\u306bYouTube\u52d5\u753b\u3092\u8aac\u660e </sup></sub> | \u2605\u2606\u2606 <br> *\u521d\u7d1a*   | [\u30de\u30c3\u30d7\u30ea\u30c7\u30e5\u30fc\u30b9](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) |  [\u8a2d\u8a08\u66f8](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/docs/design.md) | [\u30d5\u30ed\u30fc\u30b3\u30fc\u30c9](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/flow.py)\n| [\u30b3\u30fc\u30eb\u30c9\u30aa\u30fc\u30d7\u30ca\u30fc\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\u30fc](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization)  <br> <sup><sub> \u51b7\u305f\u3044\u30ea\u30fc\u30c9\u3092\u71b1\u304f\u3059\u308b\u5373\u5e2d\u30a2\u30a4\u30b9\u30d6\u30ec\u30fc\u30ab\u30fc </sup></sub> | \u2605\u2606\u2606 <br> *\u521d\u7d1a*   | [\u30de\u30c3\u30d7\u30ea\u30c7\u30e5\u30fc\u30b9](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [Web\u691c\u7d22](https://the-pocket.github.io/PocketFlow/utility_function/websearch.html) |  [\u8a2d\u8a08\u66f8](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/docs/design.md) | [\u30d5\u30ed\u30fc\u30b3\u30fc\u30c9](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/flow.py)\n\n</div>\n\n- **\u30a8\u30fc\u30b8\u30a7\u30f3\u30c6\u30a3\u30c3\u30af\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0**\u3092\u5b66\u3073\u305f\u3044\u3067\u3059\u304b\uff1f\n\n  - \u4e0a\u8a18\u306e\u30a2\u30d7\u30ea\u304c\u3069\u306e\u3088\u3046\u306b\u4f5c\u3089\u308c\u305f\u304b\u306e\u30d3\u30c7\u30aa\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u306b\u3064\u3044\u3066\u306f\u3001[\u79c1\u306eYouTube](https://www.youtube.com/@ZacharyLLM?sub_confirmation=1)\u3092\u30c1\u30a7\u30c3\u30af\u3057\u3066\u304f\u3060\u3055\u3044\uff01\n\n  - \u81ea\u5206\u306eLLM\u30a2\u30d7\u30ea\u3092\u69cb\u7bc9\u3057\u305f\u3044\u3067\u3059\u304b\uff1f\u3053\u306e[\u6295\u7a3f](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)\u3092\u8aad\u3093\u3067\u304f\u3060\u3055\u3044\uff01[\u3053\u306e\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8](https://github.com/The-Pocket/PocketFlow-Template-Python)\u304b\u3089\u59cb\u3081\u307e\u3057\u3087\u3046\uff01\n\n  - \u8a73\u7d30\u306a\u624b\u9806\u3092\u5b66\u3073\u305f\u3044\u3067\u3059\u304b\uff1f\u3053\u306e[\u30ac\u30a4\u30c9](https://the-pocket.github.io/PocketFlow/guide.html)\u3092\u8aad\u3093\u3067\u304f\u3060\u3055\u3044\uff01\n\n--- File Index 26: cookbook/pocketflow-batch/translations/README_KOREAN.md ---\n<div align=\"center\">\n  <img src=\"https://github.com/The-Pocket/.github/raw/main/assets/title.png\" width=\"600\"/>\n</div>\n\n\n![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)\n[![Docs](https://img.shields.io/badge/docs-latest-blue)](https://the-pocket.github.io/PocketFlow/)\n <a href=\"https://discord.gg/hUHHE9Sa6T\">\n    <img src=\"https://img.shields.io/discord/1346833819172601907?logo=discord&style=flat\">\n</a>\n\nPocket Flow\ub294 [100\uc904](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py)\uc758 \ubbf8\ub2c8\uba40\ub9ac\uc2a4\ud2b8 LLM \ud504\ub808\uc784\uc6cc\ud06c\uc785\ub2c8\ub2e4\n\n- **\uacbd\ub7c9\ud654**: \ub2e8 100\uc904. \ubd88\ud544\uc694\ud55c \uc694\uc18c \uc5c6\uc74c, \uc885\uc18d\uc131 \uc5c6\uc74c, \ubca4\ub354 \ub77d\uc778 \uc5c6\uc74c.\n  \n- **\ud45c\ud604\ub825**: \ub2f9\uc2e0\uc774 \uc88b\uc544\ud558\ub294 \ubaa8\ub4e0 \uac83\u2014([\ub2e4\uc911-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[\uc5d0\uc774\uc804\ud2b8](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [\uc6cc\ud06c\ud50c\ub85c\uc6b0](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) \ub4f1.\n\n- **[\uc5d0\uc774\uc804\ud2f1 \ucf54\ub529](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)**: AI \uc5d0\uc774\uc804\ud2b8(\uc608: Cursor AI)\uac00 \uc5d0\uc774\uc804\ud2b8\ub97c \uad6c\ucd95\ud558\ub3c4\ub85d\u2014\uc0dd\uc0b0\uc131 10\ubc30 \ud5a5\uc0c1!\n\n- \uc124\uce58\ud558\ub824\uba74, ```pip install pocketflow```\ub610\ub294 [\uc18c\uc2a4 \ucf54\ub4dc](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py)(\ub2e8 100\uc904)\ub97c \ubcf5\uc0ac\ud558\uc138\uc694.\n  \n- \ub354 \uc54c\uc544\ubcf4\ub824\uba74, [\ubb38\uc11c](https://the-pocket.github.io/PocketFlow/)\ub97c \ud655\uc778\ud558\uc138\uc694. \uac1c\ubc1c \ub3d9\uae30\ub97c \uc54c\uc544\ubcf4\ub824\uba74, [\uc774\uc57c\uae30](https://zacharyhuang.substack.com/p/i-built-an-llm-framework-in-just)\ub97c \uc77d\uc5b4\ubcf4\uc138\uc694.\n  \n- \ud83c\udf89 \uc6b0\ub9ac\uc758 [\ub514\uc2a4\ucf54\ub4dc](https://discord.gg/hUHHE9Sa6T)\uc5d0 \ucc38\uc5ec\ud558\uc138\uc694!\n\n- \ud83c\udf89 [@zvictor](https://www.github.com/zvictor), [@jackylee941130](https://www.github.com/jackylee941130), [@ZebraRoy](https://www.github.com/ZebraRoy) \ub355\ubd84\uc5d0 \uc774\uc81c [TypeScript \ubc84\uc804](https://github.com/The-Pocket/PocketFlow-Typescript)\uc774 \uc788\uc2b5\ub2c8\ub2e4!\n\n## \uc65c Pocket Flow\uc778\uac00?\n\n\ud604\uc7ac LLM \ud504\ub808\uc784\uc6cc\ud06c\ub4e4\uc740 \ub108\ubb34 \ube44\ub300\ud569\ub2c8\ub2e4... LLM \ud504\ub808\uc784\uc6cc\ud06c\ub294 100\uc904\ub9cc \ud544\uc694\ud569\ub2c8\ub2e4!\n\n<div align=\"center\">\n  <img src=\"https://github.com/The-Pocket/.github/raw/main/assets/meme.jpg\" width=\"400\"/>\n\n\n  |                | **\ucd94\uc0c1\ud654**          | **\uc571 \ud2b9\ud654 \ub798\ud37c**                                      | **\ubca4\ub354 \ud2b9\ud654 \ub798\ud37c**                                    | **\ucf54\ub4dc \uc904 \uc218**       | **\ud06c\uae30**    |\n|----------------|:-----------------------------: |:-----------------------------------------------------------:|:------------------------------------------------------------:|:---------------:|:----------------------------:|\n| LangChain  | \uc5d0\uc774\uc804\ud2b8, \uccb4\uc778               | \ub2e4\uc218 <br><sup><sub>(\uc608: QA, \uc694\uc57d)</sub></sup>              | \ub2e4\uc218 <br><sup><sub>(\uc608: OpenAI, Pinecone \ub4f1)</sub></sup>                   | 405K          | +166MB                     |\n| CrewAI     | \uc5d0\uc774\uc804\ud2b8, \uccb4\uc778            | \ub2e4\uc218 <br><sup><sub>(\uc608: FileReadTool, SerperDevTool)</sub></sup>         | \ub2e4\uc218 <br><sup><sub>(\uc608: OpenAI, Anthropic, Pinecone \ub4f1)</sub></sup>        | 18K           | +173MB                     |\n| SmolAgent   | \uc5d0\uc774\uc804\ud2b8                      | \uc77c\ubd80 <br><sup><sub>(\uc608: CodeAgent, VisitWebTool)</sub></sup>         | \uc77c\ubd80 <br><sup><sub>(\uc608: DuckDuckGo, Hugging Face \ub4f1)</sub></sup>           | 8K            | +198MB                     |\n| LangGraph   | \uc5d0\uc774\uc804\ud2b8, \uadf8\ub798\ud504           | \uc77c\ubd80 <br><sup><sub>(\uc608: \uc2dc\ub9e8\ud2f1 \uac80\uc0c9)</sub></sup>                     | \uc77c\ubd80 <br><sup><sub>(\uc608: PostgresStore, SqliteSaver \ub4f1) </sub></sup>        | 37K           | +51MB                      |\n| AutoGen    | \uc5d0\uc774\uc804\ud2b8                | \uc77c\ubd80 <br><sup><sub>(\uc608: Tool Agent, Chat Agent)</sub></sup>              | \ub2e4\uc218 <sup><sub>[\uc120\ud0dd\uc0ac\ud56d]<br> (\uc608: OpenAI, Pinecone \ub4f1)</sub></sup>        | 7K <br><sup><sub>(\ud575\uc2ec\ub9cc)</sub></sup>    | +26MB <br><sup><sub>(\ud575\uc2ec\ub9cc)</sub></sup>          |\n| **PocketFlow** | **\uadf8\ub798\ud504**                    | **\uc5c6\uc74c**                                                 | **\uc5c6\uc74c**                                                  | **100**       | **+56KB**                  |\n\n</div>\n\n## Pocket Flow\ub294 \uc5b4\ub5bb\uac8c \uc791\ub3d9\ud558\ub098\uc694?\n\n[100\uc904](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py)\uc740 LLM \ud504\ub808\uc784\uc6cc\ud06c\uc758 \ud575\uc2ec \ucd94\uc0c1\ud654\ub97c \ub2f4\uace0 \uc788\uc2b5\ub2c8\ub2e4: \uadf8\ub798\ud504!\n<br>\n<div align=\"center\">\n  <img src=\"https://github.com/The-Pocket/.github/raw/main/assets/abstraction.png\" width=\"900\"/>\n</div>\n<br>\n\n\uc774\ub97c \uae30\ubc18\uc73c\ub85c ([\ub2e4\uc911-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[\uc5d0\uc774\uc804\ud2b8](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [\uc6cc\ud06c\ud50c\ub85c\uc6b0](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) \ub4f1\uacfc \uac19\uc740 \uc778\uae30 \uc788\ub294 \ub514\uc790\uc778 \ud328\ud134\uc744 \uc27d\uac8c \uad6c\ud604\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n<br>\n<div align=\"center\">\n  <img src=\"https://github.com/The-Pocket/.github/raw/main/assets/design.png\" width=\"900\"/>\n</div>\n<br>\n\u2728 \uc544\ub798\ub294 \uae30\ubcf8 \ud29c\ud1a0\ub9ac\uc5bc\uc785\ub2c8\ub2e4:\n\n<div align=\"center\">\n  \n|  \uc774\ub984  | \ub09c\uc774\ub3c4    |  \uc124\uba85  |  \n| :-------------:  | :-------------: | :--------------------- |  \n| [\ucc44\ud305](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat) | \u2606\u2606\u2606 <br> *\ucd08\ubcf4*   | \ub300\ud654 \uae30\ub85d\uc774 \uc788\ub294 \uae30\ubcf8 \ucc44\ud305 \ubd07 |\n| [\uad6c\uc870\ud654\ub41c \ucd9c\ub825](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-structured-output) | \u2606\u2606\u2606 <br> *\ucd08\ubcf4* | \ud504\ub86c\ud504\ud2b8\ub97c \ud1b5\ud574 \uc774\ub825\uc11c\uc5d0\uc11c \uad6c\uc870\ud654\ub41c \ub370\uc774\ud130 \ucd94\ucd9c |\n| [\uc6cc\ud06c\ud50c\ub85c\uc6b0](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-workflow) | \u2606\u2606\u2606 <br> *\ucd08\ubcf4*   | \uac1c\uc694 \uc791\uc131, \ucf58\ud150\uce20 \uc791\uc131, \uc2a4\ud0c0\uc77c \uc801\uc6a9\uc744 \uc218\ud589\ud558\ub294 \uae00\uc4f0\uae30 \uc6cc\ud06c\ud50c\ub85c\uc6b0 |\n| [\uc5d0\uc774\uc804\ud2b8](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-agent) | \u2606\u2606\u2606 <br> *\ucd08\ubcf4*   | \uc6f9\uc744 \uac80\uc0c9\ud558\uace0 \uc9c8\ubb38\uc5d0 \ub2f5\ud560 \uc218 \uc788\ub294 \uc5f0\uad6c \uc5d0\uc774\uc804\ud2b8 |\n| [RAG](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-rag) | \u2606\u2606\u2606 <br> *\ucd08\ubcf4*   | \uac04\ub2e8\ud55c \uac80\uc0c9 \uc99d\uac15 \uc0dd\uc131 \ud504\ub85c\uc138\uc2a4 |\n| [\ub9f5-\ub9ac\ub4c0\uc2a4](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-map-reduce) | \u2606\u2606\u2606 <br> *\ucd08\ubcf4* | \ubc30\uce58 \ud3c9\uac00\ub97c \uc704\ud55c \ub9f5-\ub9ac\ub4c0\uc2a4 \ud328\ud134\uc744 \uc0ac\uc6a9\ud558\ub294 \uc774\ub825\uc11c \uc790\uaca9 \ucc98\ub9ac\uae30 |\n| [\uc2a4\ud2b8\ub9ac\ubc0d](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-llm-streaming) | \u2606\u2606\u2606 <br> *\ucd08\ubcf4*   | \uc0ac\uc6a9\uc790 \uc911\ub2e8 \uae30\ub2a5\uc774 \uc788\ub294 \uc2e4\uc2dc\uac04 LLM \uc2a4\ud2b8\ub9ac\ubc0d \ub370\ubaa8 |\n| [\ucc44\ud305 \uac00\ub4dc\ub808\uc77c](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-guardrail) | \u2606\u2606\u2606 <br> *\ucd08\ubcf4*  | \uc5ec\ud589 \uad00\ub828 \ucffc\ub9ac\ub9cc \ucc98\ub9ac\ud558\ub294 \uc5ec\ud589 \uc5b4\ub4dc\ubc14\uc774\uc800 \ucc57\ubd07 |\n| [\ub2e4\uc911 \uc5d0\uc774\uc804\ud2b8](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-multi-agent) | \u2605\u2606\u2606 <br> *\ucd08\uae09* | \ub450 \uc5d0\uc774\uc804\ud2b8 \uac04\uc758 \ube44\ub3d9\uae30 \ud1b5\uc2e0\uc744 \uc704\ud55c \uae08\uae30\uc5b4 \uac8c\uc784 |\n| [\uac10\ub3c5\uc790](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-supervisor) | \u2605\u2606\u2606 <br> *\ucd08\uae09* | \uc5f0\uad6c \uc5d0\uc774\uc804\ud2b8\uac00 \uc2e0\ub8b0\ud560 \uc218 \uc5c6\uac8c \ub418\uc5c8\uc2b5\ub2c8\ub2e4... \uac10\ub3c5 \ud504\ub85c\uc138\uc2a4\ub97c \uad6c\ucd95\ud574 \ubd05\uc2dc\ub2e4 |\n| [\ubcd1\ub82c](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch) | \u2605\u2606\u2606 <br> *\ucd08\uae09*   | 3\ubc30 \uc18d\ub3c4 \ud5a5\uc0c1\uc744 \ubcf4\uc5ec\uc8fc\ub294 \ubcd1\ub82c \uc2e4\ud589 \ub370\ubaa8 |\n| [\ubcd1\ub82c \ud50c\ub85c\uc6b0](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch-flow) | \u2605\u2606\u2606 <br> *\ucd08\uae09*   | \uc5ec\ub7ec \ud544\ud130\ub85c 8\ubc30 \uc18d\ub3c4 \ud5a5\uc0c1\uc744 \ubcf4\uc5ec\uc8fc\ub294 \ubcd1\ub82c \uc774\ubbf8\uc9c0 \ucc98\ub9ac \ub370\ubaa8 |\n| [\ub2e4\uc218\uacb0 \ud22c\ud45c](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-majority-vote) | \u2605\u2606\u2606 <br> *\ucd08\uae09* | \uc5ec\ub7ec \uc194\ub8e8\uc158 \uc2dc\ub3c4\ub97c \uc9d1\uacc4\ud558\uc5ec \ucd94\ub860 \uc815\ud655\ub3c4 \ud5a5\uc0c1 |\n| [\uc0ac\uace0](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-thinking) | \u2605\u2606\u2606 <br> *\ucd08\uae09*   | \uc0ac\uace0 \uccb4\uc778\uc744 \ud1b5\ud55c \ubcf5\uc7a1\ud55c \ucd94\ub860 \ubb38\uc81c \ud574\uacb0 |\n| [\uba54\ubaa8\ub9ac](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-memory) | \u2605\u2606\u2606 <br> *\ucd08\uae09* | \ub2e8\uae30 \ubc0f \uc7a5\uae30 \uba54\ubaa8\ub9ac\uac00 \uc788\ub294 \ucc44\ud305 \ubd07 |\n| [MCP](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-mcp) | \u2605\u2606\u2606 <br> *\ucd08\uae09* | \uc218\uce58 \uc5f0\uc0b0\uc744 \uc704\ud55c \ubaa8\ub378 \ucee8\ud14d\uc2a4\ud2b8 \ud504\ub85c\ud1a0\ucf5c\uc744 \uc0ac\uc6a9\ud558\ub294 \uc5d0\uc774\uc804\ud2b8 |\n\n</div>\n\n\ud83d\udc40 \ucd08\ubcf4\uc790\ub97c \uc704\ud55c \ub2e4\ub978 \ud29c\ud1a0\ub9ac\uc5bc\uc744 \ubcf4\uace0 \uc2f6\uc73c\uc2e0\uac00\uc694? [\uc774\uc288\ub97c \uc0dd\uc131\ud558\uc138\uc694!](https://github.com/The-Pocket/PocketFlow/issues/new)\n\n## Pocket Flow\ub97c \uc5b4\ub5bb\uac8c \uc0ac\uc6a9\ud558\ub098\uc694?\n\n\ud83d\ude80 **\uc5d0\uc774\uc804\ud2f1 \ucf54\ub529**\uc744 \ud1b5\ud574\u2014\uac00\uc7a5 \ube60\ub978 LLM \uc571 \uac1c\ubc1c \ud328\ub7ec\ub2e4\uc784-*\uc778\uac04\uc774 \uc124\uacc4*\ud558\uace0 *\uc5d0\uc774\uc804\ud2b8\uac00 \ucf54\ub529*\ud569\ub2c8\ub2e4!\n\n<br>\n<div align=\"center\">\n  <a href=\"https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to\" target=\"_blank\">\n    <img src=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F423a39af-49e8-483b-bc5a-88cc764350c6_1050x588.png\" width=\"700\" alt=\"IMAGE ALT TEXT\" style=\"cursor: pointer;\">\n  </a>\n</div>\n<br>\n\n\u2728 \uc544\ub798\ub294 \ub354 \ubcf5\uc7a1\ud55c LLM \uc571\uc758 \uc608\uc2dc\uc785\ub2c8\ub2e4:\n\n<div align=\"center\">\n  \n|  \uc571 \uc774\ub984     |  \ub09c\uc774\ub3c4    | \uc8fc\uc81c  | \uc778\uac04 \uc124\uacc4 | \uc5d0\uc774\uc804\ud2b8 \ucf54\ub4dc |\n| :-------------:  | :-------------: | :---------------------: |  :---: |  :---: |\n| [Cursor\ub85c Cursor \ub9cc\ub4e4\uae30](https://github.com/The-Pocket/Tutorial-Cursor) <br> <sup><sub>\uc6b0\ub9ac\ub294 \uace7 \ud2b9\uc774\uc810\uc5d0 \ub3c4\ub2ec\ud560 \uac83\uc785\ub2c8\ub2e4...</sup></sub> | \u2605\u2605\u2605 <br> *\uace0\uae09*   | [\uc5d0\uc774\uc804\ud2b8](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html) | [\uc124\uacc4 \ubb38\uc11c](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/docs/design.md) | [\ud50c\ub85c\uc6b0 \ucf54\ub4dc](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/flow.py)\n| [AI Paul Graham\uc5d0\uac8c \ubb3c\uc5b4\ubcf4\uae30](https://github.com/The-Pocket/Tutorial-YC-Partner) <br> <sup><sub>\uc785\ud559\ud558\uc9c0 \ubabb\ud55c \uacbd\uc6b0\ub97c \ub300\ube44\ud574 AI Paul Graham\uc5d0\uac8c \ubb3c\uc5b4\ubcf4\uc138\uc694</sup></sub> | \u2605\u2605\u2606 <br> *\uc911\uae09*   | [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) <br> [\ub9f5 \ub9ac\ub4c0\uc2a4](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [TTS](https://the-pocket.github.io/PocketFlow/utility_function/text_to_speech.html) | [\uc124\uacc4 \ubb38\uc11c](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/docs/design.md) | [\ud50c\ub85c\uc6b0 \ucf54\ub4dc](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/flow.py)\n| [\uc720\ud29c\ube0c \uc694\uc57d\uae30](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple)  <br> <sup><sub> \uc720\ud29c\ube0c \ube44\ub514\uc624\ub97c 5\uc138 \uc544\uc774\uc5d0\uac8c \uc124\uba85\ud558\ub4ef\uc774 \uc124\uba85\ud574 \uc90d\ub2c8\ub2e4 </sup></sub> | \u2605\u2606\u2606 <br> *\ucd08\uae09*   | [\ub9f5 \ub9ac\ub4c0\uc2a4](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) |  [\uc124\uacc4 \ubb38\uc11c](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/docs/design.md) | [\ud50c\ub85c\uc6b0 \ucf54\ub4dc](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/flow.py)\n| [\ucf5c\ub4dc \uc624\ud504\ub108 \uc0dd\uc131\uae30](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization)  <br> <sup><sub> \ucc28\uac00\uc6b4 \ub9ac\ub4dc\ub97c \ub728\uac81\uac8c \ub9cc\ub4dc\ub294 \uc989\uac01\uc801\uc778 \uc544\uc774\uc2a4\ube0c\ub808\uc774\ucee4 </sup></sub> | \u2605\u2606\u2606 <br> *\ucd08\uae09*   | [\ub9f5 \ub9ac\ub4c0\uc2a4](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [\uc6f9 \uac80\uc0c9](https://the-pocket.github.io/PocketFlow/utility_function/websearch.html) |  [\uc124\uacc4 \ubb38\uc11c](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/docs/design.md) | [\ud50c\ub85c\uc6b0 \ucf54\ub4dc](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/flow.py)\n\n</div>\n\n- **\uc5d0\uc774\uc804\ud2f1 \ucf54\ub529**\uc744 \ubc30\uc6b0\uace0 \uc2f6\uc73c\uc2e0\uac00\uc694?\n\n  - \uc704\uc758 \uc571 \uc911 \uc77c\ubd80\uac00 \uc5b4\ub5bb\uac8c \ub9cc\ub4e4\uc5b4\uc84c\ub294\uc9c0 \ube44\ub514\uc624 \ud29c\ud1a0\ub9ac\uc5bc\uc744 \ubcf4\ub824\uba74 [\ub0b4 YouTube](https://www.youtube.com/@ZacharyLLM?sub_confirmation=1)\ub97c \ud655\uc778\ud558\uc138\uc694!\n\n  - \uc790\uc2e0\ub9cc\uc758 LLM \uc571\uc744 \ub9cc\ub4e4\uace0 \uc2f6\uc73c\uc2e0\uac00\uc694? \uc774 [\ud3ec\uc2a4\ud2b8](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)\ub97c \uc77d\uc5b4\ubcf4\uc138\uc694! [\uc774 \ud15c\ud50c\ub9bf](https://github.com/The-Pocket/PocketFlow-Template-Python)\uc73c\ub85c \uc2dc\uc791\ud558\uc138\uc694!\n\n  - \uc790\uc138\ud55c \ub2e8\uacc4\ub97c \ubc30\uc6b0\uace0 \uc2f6\uc73c\uc2e0\uac00\uc694? \uc774 [\uac00\uc774\ub4dc](https://the-pocket.github.io/PocketFlow/guide.html)\ub97c \uc77d\uc5b4\ubcf4\uc138\uc694!\n\n--- File Index 27: cookbook/pocketflow-batch/translations/README_PORTUGUESE.md ---\n<div align=\"center\">\n  <img src=\"https://github.com/The-Pocket/.github/raw/main/assets/title.png\" width=\"600\"/>\n</div>\n\n\n![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)\n[![Docs](https://img.shields.io/badge/docs-latest-blue)](https://the-pocket.github.io/PocketFlow/)\n <a href=\"https://discord.gg/hUHHE9Sa6T\">\n    <img src=\"https://img.shields.io/discord/1346833819172601907?logo=discord&style=flat\">\n</a>\n\nPocket Flow \u00e9 um framework LLM minimalista de [100 linhas](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py)\n\n- **Leve**: Apenas 100 linhas. Zero incha\u00e7o, zero depend\u00eancias, zero bloqueio de fornecedor.\n  \n- **Expressivo**: Tudo o que voc\u00ea ama\u2014([Multi-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[Agentes](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [Fluxo de Trabalho](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html), e mais.\n\n- **[Codifica\u00e7\u00e3o Ag\u00eantica](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)**: Deixe os Agentes de IA (por exemplo, Cursor AI) constru\u00edrem Agentes\u2014aumento de produtividade de 10x!\n\n- Para instalar, ```pip install pocketflow``` ou apenas copie o [c\u00f3digo-fonte](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py) (apenas 100 linhas).\n  \n- Para saber mais, consulte a [documenta\u00e7\u00e3o](https://the-pocket.github.io/PocketFlow/). Para entender a motiva\u00e7\u00e3o, leia a [hist\u00f3ria](https://zacharyhuang.substack.com/p/i-built-an-llm-framework-in-just).\n  \n- \ud83c\udf89 Junte-se ao nosso [discord](https://discord.gg/hUHHE9Sa6T)!\n\n- \ud83c\udf89 Gra\u00e7as a [@zvictor](https://www.github.com/zvictor), [@jackylee941130](https://www.github.com/jackylee941130) e [@ZebraRoy](https://www.github.com/ZebraRoy), agora temos uma [vers\u00e3o TypeScript](https://github.com/The-Pocket/PocketFlow-Typescript)!\n\n## Por que Pocket Flow?\n\nOs frameworks LLM atuais s\u00e3o inchados... Voc\u00ea s\u00f3 precisa de 100 linhas para um Framework LLM!\n\n<div align=\"center\">\n  <img src=\"https://github.com/The-Pocket/.github/raw/main/assets/meme.jpg\" width=\"400\"/>\n\n\n  |                | **Abstra\u00e7\u00e3o**          | **Wrappers Espec\u00edficos de App**                                      | **Wrappers Espec\u00edficos de Fornecedor**                                    | **Linhas**       | **Tamanho**    |\n|----------------|:-----------------------------: |:-----------------------------------------------------------:|:------------------------------------------------------------:|:---------------:|:----------------------------:|\n| LangChain  | Agente, Cadeia               | Muitos <br><sup><sub>(ex., QA, Sumariza\u00e7\u00e3o)</sub></sup>              | Muitos <br><sup><sub>(ex., OpenAI, Pinecone, etc.)</sub></sup>                   | 405K          | +166MB                     |\n| CrewAI     | Agente, Cadeia            | Muitos <br><sup><sub>(ex., FileReadTool, SerperDevTool)</sub></sup>         | Muitos <br><sup><sub>(ex., OpenAI, Anthropic, Pinecone, etc.)</sub></sup>        | 18K           | +173MB                     |\n| SmolAgent   | Agente                      | Alguns <br><sup><sub>(ex., CodeAgent, VisitWebTool)</sub></sup>         | Alguns <br><sup><sub>(ex., DuckDuckGo, Hugging Face, etc.)</sub></sup>           | 8K            | +198MB                     |\n| LangGraph   | Agente, Grafo           | Alguns <br><sup><sub>(ex., Busca Sem\u00e2ntica)</sub></sup>                     | Alguns <br><sup><sub>(ex., PostgresStore, SqliteSaver, etc.) </sub></sup>        | 37K           | +51MB                      |\n| AutoGen    | Agente                | Alguns <br><sup><sub>(ex., Agente de Ferramentas, Agente de Chat)</sub></sup>              | Muitos <sup><sub>[Opcional]<br> (ex., OpenAI, Pinecone, etc.)</sub></sup>        | 7K <br><sup><sub>(apenas n\u00facleo)</sub></sup>    | +26MB <br><sup><sub>(apenas n\u00facleo)</sub></sup>          |\n| **PocketFlow** | **Grafo**                    | **Nenhum**                                                 | **Nenhum**                                                  | **100**       | **+56KB**                  |\n\n</div>\n\n## Como funciona o Pocket Flow?\n\nAs [100 linhas](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py) capturam a abstra\u00e7\u00e3o central dos frameworks LLM: Grafo!\n<br>\n<div align=\"center\">\n  <img src=\"https://github.com/The-Pocket/.github/raw/main/assets/abstraction.png\" width=\"900\"/>\n</div>\n<br>\n\nA partir da\u00ed, \u00e9 f\u00e1cil implementar padr\u00f5es de design populares como ([Multi-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[Agentes](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [Fluxo de Trabalho](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html), etc.\n<br>\n<div align=\"center\">\n  <img src=\"https://github.com/The-Pocket/.github/raw/main/assets/design.png\" width=\"900\"/>\n</div>\n<br>\n\u2728 Abaixo est\u00e3o tutoriais b\u00e1sicos:\n\n<div align=\"center\">\n  \n|  Nome  | Dificuldade    |  Descri\u00e7\u00e3o  |  \n| :-------------:  | :-------------: | :--------------------- |  \n| [Chat](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat) | \u2606\u2606\u2606 <br> *Iniciante*   | Um chatbot b\u00e1sico com hist\u00f3rico de conversas |\n| [Sa\u00edda Estruturada](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-structured-output) | \u2606\u2606\u2606 <br> *Iniciante* | Extraindo dados estruturados de curr\u00edculos por prompt |\n| [Fluxo de Trabalho](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-workflow) | \u2606\u2606\u2606 <br> *Iniciante*   | Um fluxo de escrita que esbo\u00e7a, escreve conte\u00fado e aplica estilo |\n| [Agente](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-agent) | \u2606\u2606\u2606 <br> *Iniciante*   | Um agente de pesquisa que pode buscar na web e responder perguntas |\n| [RAG](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-rag) | \u2606\u2606\u2606 <br> *Iniciante*   | Um processo simples de Gera\u00e7\u00e3o Aumentada por Recupera\u00e7\u00e3o |\n| [Map-Reduce](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-map-reduce) | \u2606\u2606\u2606 <br> *Iniciante* | Um processador de qualifica\u00e7\u00e3o de curr\u00edculo usando o padr\u00e3o map-reduce para avalia\u00e7\u00e3o em lote |\n| [Streaming](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-llm-streaming) | \u2606\u2606\u2606 <br> *Iniciante*   | Uma demonstra\u00e7\u00e3o de streaming LLM em tempo real com capacidade de interrup\u00e7\u00e3o pelo usu\u00e1rio |\n| [Guarda-rail de Chat](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-guardrail) | \u2606\u2606\u2606 <br> *Iniciante*  | Um chatbot de consultoria de viagens que processa apenas consultas relacionadas a viagens |\n| [Multi-Agente](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-multi-agent) | \u2605\u2606\u2606 <br> *Intermedi\u00e1rio* | Um jogo de palavras Tabu para comunica\u00e7\u00e3o ass\u00edncrona entre dois agentes |\n| [Supervisor](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-supervisor) | \u2605\u2606\u2606 <br> *Intermedi\u00e1rio* | O agente de pesquisa est\u00e1 ficando n\u00e3o confi\u00e1vel... Vamos construir um processo de supervis\u00e3o|\n| [Paralelo](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch) | \u2605\u2606\u2606 <br> *Intermedi\u00e1rio*   | Uma demonstra\u00e7\u00e3o de execu\u00e7\u00e3o paralela que mostra acelera\u00e7\u00e3o de 3x |\n| [Fluxo Paralelo](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch-flow) | \u2605\u2606\u2606 <br> *Intermedi\u00e1rio*   | Uma demonstra\u00e7\u00e3o de processamento de imagem paralela mostrando acelera\u00e7\u00e3o de 8x com m\u00faltiplos filtros |\n| [Voto por Maioria](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-majority-vote) | \u2605\u2606\u2606 <br> *Intermedi\u00e1rio* | Melhore a precis\u00e3o do racioc\u00ednio agregando m\u00faltiplas tentativas de solu\u00e7\u00e3o |\n| [Pensamento](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-thinking) | \u2605\u2606\u2606 <br> *Intermedi\u00e1rio*   | Resolva problemas de racioc\u00ednio complexos atrav\u00e9s da Cadeia de Pensamento |\n| [Mem\u00f3ria](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-memory) | \u2605\u2606\u2606 <br> *Intermedi\u00e1rio* | Um chatbot com mem\u00f3ria de curto e longo prazo |\n| [MCP](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-mcp) | \u2605\u2606\u2606 <br> *Intermedi\u00e1rio* |  Agente usando Protocolo de Contexto de Modelo para opera\u00e7\u00f5es num\u00e9ricas |\n\n</div>\n\n\ud83d\udc40 Quer ver outros tutoriais para iniciantes? [Crie uma issue!](https://github.com/The-Pocket/PocketFlow/issues/new)\n\n## Como Usar Pocket Flow?\n\n\ud83d\ude80 Atrav\u00e9s da **Codifica\u00e7\u00e3o Ag\u00eantica**\u2014o paradigma mais r\u00e1pido de desenvolvimento de aplicativos LLM\u2014onde *humanos projetam* e *agentes codificam*!\n\n<br>\n<div align=\"center\">\n  <a href=\"https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to\" target=\"_blank\">\n    <img src=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F423a39af-49e8-483b-bc5a-88cc764350c6_1050x588.png\" width=\"700\" alt=\"IMAGE ALT TEXT\" style=\"cursor: pointer;\">\n  </a>\n</div>\n<br>\n\n\u2728 Abaixo est\u00e3o exemplos de aplicativos LLM mais complexos:\n\n<div align=\"center\">\n  \n|  Nome do App     |  Dificuldade    | T\u00f3picos  | Design Humano | C\u00f3digo do Agente |\n| :-------------:  | :-------------: | :---------------------: |  :---: |  :---: |\n| [Construir Cursor com Cursor](https://github.com/The-Pocket/Tutorial-Cursor) <br> <sup><sub>Logo chegaremos \u00e0 singularidade ...</sup></sub> | \u2605\u2605\u2605 <br> *Avan\u00e7ado*   | [Agente](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html) | [Doc de Design](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/docs/design.md) | [C\u00f3digo Flow](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/flow.py)\n| [Pergunte ao AI Paul Graham](https://github.com/The-Pocket/Tutorial-YC-Partner) <br> <sup><sub>Pergunte ao AI Paul Graham, caso voc\u00ea n\u00e3o seja aceito</sup></sub> | \u2605\u2605\u2606 <br> *M\u00e9dio*   | [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) <br> [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [TTS](https://the-pocket.github.io/PocketFlow/utility_function/text_to_speech.html) | [Doc de Design](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/docs/design.md) | [C\u00f3digo Flow](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/flow.py)\n| [Resumidor de Youtube](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple)  <br> <sup><sub> Explica v\u00eddeos do YouTube como se voc\u00ea tivesse 5 anos </sup></sub> | \u2605\u2606\u2606 <br> *Iniciante*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) |  [Doc de Design](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/docs/design.md) | [C\u00f3digo Flow](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/flow.py)\n| [Gerador de Aberturas Frias](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization)  <br> <sup><sub> Quebra-gelos instant\u00e2neos que transformam leads frios em quentes </sup></sub> | \u2605\u2606\u2606 <br> *Iniciante*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [Busca Web](https://the-pocket.github.io/PocketFlow/utility_function/websearch.html) |  [Doc de Design](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/docs/design.md) | [C\u00f3digo Flow](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/flow.py)\n\n</div>\n\n- Quer aprender **Codifica\u00e7\u00e3o Ag\u00eantica**?\n\n  - Confira [meu YouTube](https://www.youtube.com/@ZacharyLLM?sub_confirmation=1) para tutorial em v\u00eddeo sobre como alguns aplicativos acima s\u00e3o feitos!\n\n  - Quer construir seu pr\u00f3prio aplicativo LLM? Leia este [post](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)! Comece com [este modelo](https://github.com/The-Pocket/PocketFlow-Template-Python)!\n\n  - Quer aprender os passos detalhados? Leia este [Guia](https://the-pocket.github.io/PocketFlow/guide.html)!\n\n--- File Index 28: cookbook/pocketflow-batch/translations/README_RUSSIAN.md ---\n<div align=\"center\">\n  <img src=\"https://github.com/The-Pocket/.github/raw/main/assets/title.png\" width=\"600\"/>\n</div>\n\n\n![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)\n[![Docs](https://img.shields.io/badge/docs-latest-blue)](https://the-pocket.github.io/PocketFlow/)\n <a href=\"https://discord.gg/hUHHE9Sa6T\">\n    <img src=\"https://img.shields.io/discord/1346833819172601907?logo=discord&style=flat\">\n</a>\n\nPocket Flow \u2014 \u044d\u0442\u043e \u043c\u0438\u043d\u0438\u043c\u0430\u043b\u0438\u0441\u0442\u0438\u0447\u043d\u044b\u0439 \u0444\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a \u0434\u043b\u044f LLM, \u0441\u043e\u0441\u0442\u043e\u044f\u0449\u0438\u0439 \u0432\u0441\u0435\u0433\u043e \u0438\u0437 [100 \u0441\u0442\u0440\u043e\u043a](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py)\n\n- **\u041b\u0435\u0433\u043a\u0438\u0439**: \u0412\u0441\u0435\u0433\u043e 100 \u0441\u0442\u0440\u043e\u043a. \u041d\u0438\u043a\u0430\u043a\u043e\u0433\u043e \u0438\u0437\u0431\u044b\u0442\u043e\u0447\u043d\u043e\u0433\u043e \u043a\u043e\u0434\u0430, \u043d\u0438\u043a\u0430\u043a\u0438\u0445 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0435\u0439, \u043d\u0438\u043a\u0430\u043a\u043e\u0439 \u043f\u0440\u0438\u0432\u044f\u0437\u043a\u0438 \u043a \u043f\u043e\u0441\u0442\u0430\u0432\u0449\u0438\u043a\u0430\u043c.\n  \n- **\u0412\u044b\u0440\u0430\u0437\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0439**: \u0412\u0441\u0451, \u0447\u0442\u043e \u0432\u044b \u043b\u044e\u0431\u0438\u0442\u0435 \u2014 ([\u041c\u0443\u043b\u044c\u0442\u0438-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[\u0410\u0433\u0435\u043d\u0442\u044b](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [\u0420\u0430\u0431\u043e\u0447\u0438\u0435 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u044b](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) \u0438 \u043c\u043d\u043e\u0433\u043e\u0435 \u0434\u0440\u0443\u0433\u043e\u0435.\n\n- **[\u0410\u0433\u0435\u043d\u0442\u043d\u043e\u0435 \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)**: \u041f\u043e\u0437\u0432\u043e\u043b\u044c\u0442\u0435 \u0418\u0418-\u0430\u0433\u0435\u043d\u0442\u0430\u043c (\u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, Cursor AI) \u0441\u043e\u0437\u0434\u0430\u0432\u0430\u0442\u044c \u0434\u0440\u0443\u0433\u0438\u0445 \u0430\u0433\u0435\u043d\u0442\u043e\u0432 \u2014 \u043f\u043e\u0432\u044b\u0448\u0435\u043d\u0438\u0435 \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438 \u0432 10 \u0440\u0430\u0437!\n\n- \u0414\u043b\u044f \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0438 \u0432\u044b\u043f\u043e\u043b\u043d\u0438\u0442\u0435 ```pip install pocketflow``` \u0438\u043b\u0438 \u043f\u0440\u043e\u0441\u0442\u043e \u0441\u043a\u043e\u043f\u0438\u0440\u0443\u0439\u0442\u0435 [\u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0439 \u043a\u043e\u0434](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py) (\u0432\u0441\u0435\u0433\u043e 100 \u0441\u0442\u0440\u043e\u043a).\n  \n- \u0427\u0442\u043e\u0431\u044b \u0443\u0437\u043d\u0430\u0442\u044c \u0431\u043e\u043b\u044c\u0448\u0435, \u043e\u0437\u043d\u0430\u043a\u043e\u043c\u044c\u0442\u0435\u0441\u044c \u0441 [\u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0435\u0439](https://the-pocket.github.io/PocketFlow/). \u0427\u0442\u043e\u0431\u044b \u043f\u043e\u043d\u044f\u0442\u044c \u043c\u043e\u0442\u0438\u0432\u0430\u0446\u0438\u044e, \u043f\u0440\u043e\u0447\u0438\u0442\u0430\u0439\u0442\u0435 [\u0438\u0441\u0442\u043e\u0440\u0438\u044e \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f](https://zacharyhuang.substack.com/p/i-built-an-llm-framework-in-just).\n  \n- \ud83c\udf89 \u041f\u0440\u0438\u0441\u043e\u0435\u0434\u0438\u043d\u044f\u0439\u0442\u0435\u0441\u044c \u043a \u043d\u0430\u0448\u0435\u043c\u0443 [Discord-\u0441\u0435\u0440\u0432\u0435\u0440\u0443](https://discord.gg/hUHHE9Sa6T)!\n\n- \ud83c\udf89 \u0411\u043b\u0430\u0433\u043e\u0434\u0430\u0440\u044f [@zvictor](https://www.github.com/zvictor), [@jackylee941130](https://www.github.com/jackylee941130) \u0438 [@ZebraRoy](https://www.github.com/ZebraRoy), \u0443 \u043d\u0430\u0441 \u0442\u0435\u043f\u0435\u0440\u044c \u0435\u0441\u0442\u044c [\u0432\u0435\u0440\u0441\u0438\u044f \u043d\u0430 TypeScript](https://github.com/The-Pocket/PocketFlow-Typescript)!\n\n## \u041f\u043e\u0447\u0435\u043c\u0443 Pocket Flow?\n\n\u0422\u0435\u043a\u0443\u0449\u0438\u0435 \u0444\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a\u0438 \u0434\u043b\u044f LLM \u043f\u0435\u0440\u0435\u0433\u0440\u0443\u0436\u0435\u043d\u044b... \u0414\u043b\u044f \u0444\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a\u0430 LLM \u0432\u0430\u043c \u043d\u0443\u0436\u043d\u043e \u0432\u0441\u0435\u0433\u043e 100 \u0441\u0442\u0440\u043e\u043a!\n\n<div align=\"center\">\n  <img src=\"https://github.com/The-Pocket/.github/raw/main/assets/meme.jpg\" width=\"400\"/>\n\n\n  |                | **\u0410\u0431\u0441\u0442\u0440\u0430\u043a\u0446\u0438\u044f**          | **\u0421\u043f\u0435\u0446\u0438\u0444\u0438\u0447\u043d\u044b\u0435 \u043e\u0431\u0435\u0440\u0442\u043a\u0438 \u0434\u043b\u044f \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0439**                                      | **\u0421\u043f\u0435\u0446\u0438\u0444\u0438\u0447\u043d\u044b\u0435 \u043e\u0431\u0435\u0440\u0442\u043a\u0438 \u0434\u043b\u044f \u0432\u0435\u043d\u0434\u043e\u0440\u043e\u0432**                                    | **\u0421\u0442\u0440\u043e\u043a \u043a\u043e\u0434\u0430**       | **\u0420\u0430\u0437\u043c\u0435\u0440**    |\n|----------------|:-----------------------------: |:-----------------------------------------------------------:|:------------------------------------------------------------:|:---------------:|:----------------------------:|\n| LangChain  | Agent, Chain               | \u041c\u043d\u043e\u0433\u043e <br><sup><sub>(\u043d\u0430\u043f\u0440., QA, Summarization)</sub></sup>              | \u041c\u043d\u043e\u0433\u043e <br><sup><sub>(\u043d\u0430\u043f\u0440., OpenAI, Pinecone \u0438 \u0442.\u0434.)</sub></sup>                   | 405K          | +166MB                     |\n| CrewAI     | Agent, Chain            | \u041c\u043d\u043e\u0433\u043e <br><sup><sub>(\u043d\u0430\u043f\u0440., FileReadTool, SerperDevTool)</sub></sup>         | \u041c\u043d\u043e\u0433\u043e <br><sup><sub>(\u043d\u0430\u043f\u0440., OpenAI, Anthropic, Pinecone \u0438 \u0442.\u0434.)</sub></sup>        | 18K           | +173MB                     |\n| SmolAgent   | Agent                      | \u041d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e <br><sup><sub>(\u043d\u0430\u043f\u0440., CodeAgent, VisitWebTool)</sub></sup>         | \u041d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e <br><sup><sub>(\u043d\u0430\u043f\u0440., DuckDuckGo, Hugging Face \u0438 \u0442.\u0434.)</sub></sup>           | 8K            | +198MB                     |\n| LangGraph   | Agent, Graph           | \u041d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e <br><sup><sub>(\u043d\u0430\u043f\u0440., Semantic Search)</sub></sup>                     | \u041d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e <br><sup><sub>(\u043d\u0430\u043f\u0440., PostgresStore, SqliteSaver \u0438 \u0442.\u0434.) </sub></sup>        | 37K           | +51MB                      |\n| AutoGen    | Agent                | \u041d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e <br><sup><sub>(\u043d\u0430\u043f\u0440., Tool Agent, Chat Agent)</sub></sup>              | \u041c\u043d\u043e\u0433\u043e <sup><sub>[\u041e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e]<br> (\u043d\u0430\u043f\u0440., OpenAI, Pinecone \u0438 \u0442.\u0434.)</sub></sup>        | 7K <br><sup><sub>(\u0442\u043e\u043b\u044c\u043a\u043e \u044f\u0434\u0440\u043e)</sub></sup>    | +26MB <br><sup><sub>(\u0442\u043e\u043b\u044c\u043a\u043e \u044f\u0434\u0440\u043e)</sub></sup>          |\n| **PocketFlow** | **Graph**                    | **\u041d\u0435\u0442**                                                 | **\u041d\u0435\u0442**                                                  | **100**       | **+56KB**                  |\n\n</div>\n\n## \u041a\u0430\u043a \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 Pocket Flow?\n\n[100 \u0441\u0442\u0440\u043e\u043a](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py) \u043e\u0445\u0432\u0430\u0442\u044b\u0432\u0430\u044e\u0442 \u043e\u0441\u043d\u043e\u0432\u043d\u0443\u044e \u0430\u0431\u0441\u0442\u0440\u0430\u043a\u0446\u0438\u044e \u0444\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a\u043e\u0432 LLM: \u0413\u0440\u0430\u0444!\n<br>\n<div align=\"center\">\n  <img src=\"https://github.com/The-Pocket/.github/raw/main/assets/abstraction.png\" width=\"900\"/>\n</div>\n<br>\n\n\u041e\u0442\u0441\u044e\u0434\u0430 \u043b\u0435\u0433\u043a\u043e \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u044b\u0435 \u0448\u0430\u0431\u043b\u043e\u043d\u044b \u043f\u0440\u043e\u0435\u043a\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f, \u0442\u0430\u043a\u0438\u0435 \u043a\u0430\u043a ([\u041c\u0443\u043b\u044c\u0442\u0438-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[\u0410\u0433\u0435\u043d\u0442\u044b](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [\u0420\u0430\u0431\u043e\u0447\u0438\u0435 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u044b](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) \u0438 \u0442.\u0434.\n<br>\n<div align=\"center\">\n  <img src=\"https://github.com/The-Pocket/.github/raw/main/assets/design.png\" width=\"900\"/>\n</div>\n<br>\n\u2728 \u041d\u0438\u0436\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u044b \u0431\u0430\u0437\u043e\u0432\u044b\u0435 \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u0441\u0442\u0432\u0430:\n\n<div align=\"center\">\n  \n|  \u041d\u0430\u0437\u0432\u0430\u043d\u0438\u0435  | \u0421\u043b\u043e\u0436\u043d\u043e\u0441\u0442\u044c    |  \u041e\u043f\u0438\u0441\u0430\u043d\u0438\u0435  |  \n| :-------------:  | :-------------: | :--------------------- |  \n| [\u0427\u0430\u0442](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat) | \u2606\u2606\u2606 <br> *\u041f\u0440\u043e\u0441\u0442\u0435\u0439\u0448\u0438\u0439*   | \u0411\u0430\u0437\u043e\u0432\u044b\u0439 \u0447\u0430\u0442-\u0431\u043e\u0442 \u0441 \u0438\u0441\u0442\u043e\u0440\u0438\u0435\u0439 \u0440\u0430\u0437\u0433\u043e\u0432\u043e\u0440\u0430 |\n| [\u0421\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0439 \u0432\u044b\u0432\u043e\u0434](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-structured-output) | \u2606\u2606\u2606 <br> *\u041f\u0440\u043e\u0441\u0442\u0435\u0439\u0448\u0438\u0439* | \u0418\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u0435 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438\u0437 \u0440\u0435\u0437\u044e\u043c\u0435 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043f\u0440\u043e\u043c\u043f\u0442\u043e\u0432 |\n| [\u0420\u0430\u0431\u043e\u0447\u0438\u0439 \u043f\u0440\u043e\u0446\u0435\u0441\u0441](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-workflow) | \u2606\u2606\u2606 <br> *\u041f\u0440\u043e\u0441\u0442\u0435\u0439\u0448\u0438\u0439*   | \u0420\u0430\u0431\u043e\u0447\u0438\u0439 \u043f\u0440\u043e\u0446\u0435\u0441\u0441 \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u0442\u0435\u043a\u0441\u0442\u0430, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0441\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442 \u043f\u043b\u0430\u043d, \u043f\u0438\u0448\u0435\u0442 \u043a\u043e\u043d\u0442\u0435\u043d\u0442 \u0438 \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u0442 \u0441\u0442\u0438\u043b\u0438\u0441\u0442\u0438\u043a\u0443 |\n| [\u0410\u0433\u0435\u043d\u0442](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-agent) | \u2606\u2606\u2606 <br> *\u041f\u0440\u043e\u0441\u0442\u0435\u0439\u0448\u0438\u0439*   | \u0418\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u0441\u043a\u0438\u0439 \u0430\u0433\u0435\u043d\u0442, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043c\u043e\u0436\u0435\u0442 \u0438\u0441\u043a\u0430\u0442\u044c \u0432 \u0438\u043d\u0442\u0435\u0440\u043d\u0435\u0442\u0435 \u0438 \u043e\u0442\u0432\u0435\u0447\u0430\u0442\u044c \u043d\u0430 \u0432\u043e\u043f\u0440\u043e\u0441\u044b |\n| [RAG](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-rag) | \u2606\u2606\u2606 <br> *\u041f\u0440\u043e\u0441\u0442\u0435\u0439\u0448\u0438\u0439*   | \u041f\u0440\u043e\u0441\u0442\u043e\u0439 \u043f\u0440\u043e\u0446\u0435\u0441\u0441 \u0433\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u0438 \u0441 \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u0435\u043c (Retrieval-augmented Generation) |\n| [Map-Reduce](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-map-reduce) | \u2606\u2606\u2606 <br> *\u041f\u0440\u043e\u0441\u0442\u0435\u0439\u0448\u0438\u0439* | \u041e\u0431\u0440\u0430\u0431\u043e\u0442\u0447\u0438\u043a \u043a\u0432\u0430\u043b\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0439 \u0440\u0435\u0437\u044e\u043c\u0435 \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u043f\u0430\u0442\u0442\u0435\u0440\u043d\u0430 map-reduce \u0434\u043b\u044f \u043f\u0430\u043a\u0435\u0442\u043d\u043e\u0439 \u043e\u0446\u0435\u043d\u043a\u0438 |\n| [\u041f\u043e\u0442\u043e\u043a\u043e\u0432\u0430\u044f \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-llm-streaming) | \u2606\u2606\u2606 <br> *\u041f\u0440\u043e\u0441\u0442\u0435\u0439\u0448\u0438\u0439*   | \u0414\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0430\u0446\u0438\u044f \u043f\u043e\u0442\u043e\u043a\u043e\u0432\u043e\u0439 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 LLM \u0432 \u0440\u0435\u0430\u043b\u044c\u043d\u043e\u043c \u0432\u0440\u0435\u043c\u0435\u043d\u0438 \u0441 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c\u044e \u043f\u0440\u0435\u0440\u044b\u0432\u0430\u043d\u0438\u044f |\n| [\u041e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u0438\u044f \u0447\u0430\u0442\u0430](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-guardrail) | \u2606\u2606\u2606 <br> *\u041f\u0440\u043e\u0441\u0442\u0435\u0439\u0448\u0438\u0439*  | \u0427\u0430\u0442-\u0431\u043e\u0442 \u0434\u043b\u044f \u043f\u0443\u0442\u0435\u0448\u0435\u0441\u0442\u0432\u0438\u0439, \u043e\u0431\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u044e\u0449\u0438\u0439 \u0442\u043e\u043b\u044c\u043a\u043e \u0437\u0430\u043f\u0440\u043e\u0441\u044b, \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0435 \u0441 \u043f\u0443\u0442\u0435\u0448\u0435\u0441\u0442\u0432\u0438\u044f\u043c\u0438 |\n| [\u041c\u0443\u043b\u044c\u0442\u0438-\u0430\u0433\u0435\u043d\u0442](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-multi-agent) | \u2605\u2606\u2606 <br> *\u041d\u0430\u0447\u0430\u043b\u044c\u043d\u044b\u0439* | \u0418\u0433\u0440\u0430 \u0432 \u0422\u0430\u0431\u0443 \u0434\u043b\u044f \u0430\u0441\u0438\u043d\u0445\u0440\u043e\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0449\u0435\u043d\u0438\u044f \u043c\u0435\u0436\u0434\u0443 \u0434\u0432\u0443\u043c\u044f \u0430\u0433\u0435\u043d\u0442\u0430\u043c\u0438 |\n| [\u0421\u0443\u043f\u0435\u0440\u0432\u0438\u0437\u043e\u0440](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-supervisor) | \u2605\u2606\u2606 <br> *\u041d\u0430\u0447\u0430\u043b\u044c\u043d\u044b\u0439* | \u0418\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u0441\u043a\u0438\u0439 \u0430\u0433\u0435\u043d\u0442 \u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442\u0441\u044f \u043d\u0435\u043d\u0430\u0434\u0435\u0436\u043d\u044b\u043c... \u041f\u043e\u0441\u0442\u0440\u043e\u0438\u043c \u043f\u0440\u043e\u0446\u0435\u0441\u0441 \u043d\u0430\u0434\u0437\u043e\u0440\u0430 |\n| [\u041f\u0430\u0440\u0430\u043b\u043b\u0435\u043b\u044c\u043d\u043e\u0435 \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u0435](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch) | \u2605\u2606\u2606 <br> *\u041d\u0430\u0447\u0430\u043b\u044c\u043d\u044b\u0439*   | \u0414\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0430\u0446\u0438\u044f \u043f\u0430\u0440\u0430\u043b\u043b\u0435\u043b\u044c\u043d\u043e\u0433\u043e \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f \u0441 \u0443\u0441\u043a\u043e\u0440\u0435\u043d\u0438\u0435\u043c \u0432 3 \u0440\u0430\u0437\u0430 |\n| [\u041f\u0430\u0440\u0430\u043b\u043b\u0435\u043b\u044c\u043d\u044b\u0439 \u043f\u043e\u0442\u043e\u043a](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch-flow) | \u2605\u2606\u2606 <br> *\u041d\u0430\u0447\u0430\u043b\u044c\u043d\u044b\u0439*   | \u0414\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0430\u0446\u0438\u044f \u043f\u0430\u0440\u0430\u043b\u043b\u0435\u043b\u044c\u043d\u043e\u0439 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439 \u0441 \u0443\u0441\u043a\u043e\u0440\u0435\u043d\u0438\u0435\u043c \u0432 8 \u0440\u0430\u0437 \u043f\u0440\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0438 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u0438\u0445 \u0444\u0438\u043b\u044c\u0442\u0440\u043e\u0432 |\n| [\u0413\u043e\u043b\u043e\u0441\u043e\u0432\u0430\u043d\u0438\u0435 \u0431\u043e\u043b\u044c\u0448\u0438\u043d\u0441\u0442\u0432\u043e\u043c](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-majority-vote) | \u2605\u2606\u2606 <br> *\u041d\u0430\u0447\u0430\u043b\u044c\u043d\u044b\u0439* | \u041f\u043e\u0432\u044b\u0448\u0435\u043d\u0438\u0435 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438 \u0440\u0430\u0441\u0441\u0443\u0436\u0434\u0435\u043d\u0438\u0439 \u043f\u0443\u0442\u0435\u043c \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u0435\u043d\u0438\u044f \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u0438\u0445 \u043f\u043e\u043f\u044b\u0442\u043e\u043a \u0440\u0435\u0448\u0435\u043d\u0438\u044f |\n| [\u041c\u044b\u0448\u043b\u0435\u043d\u0438\u0435](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-thinking) | \u2605\u2606\u2606 <br> *\u041d\u0430\u0447\u0430\u043b\u044c\u043d\u044b\u0439*   | \u0420\u0435\u0448\u0435\u043d\u0438\u0435 \u0441\u043b\u043e\u0436\u043d\u044b\u0445 \u0437\u0430\u0434\u0430\u0447 \u0440\u0430\u0441\u0441\u0443\u0436\u0434\u0435\u043d\u0438\u044f \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0446\u0435\u043f\u043e\u0447\u043a\u0438 \u043c\u044b\u0441\u043b\u0435\u0439 (Chain-of-Thought) |\n| [\u041f\u0430\u043c\u044f\u0442\u044c](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-memory) | \u2605\u2606\u2606 <br> *\u041d\u0430\u0447\u0430\u043b\u044c\u043d\u044b\u0439* | \u0427\u0430\u0442-\u0431\u043e\u0442 \u0441 \u043a\u0440\u0430\u0442\u043a\u043e\u0441\u0440\u043e\u0447\u043d\u043e\u0439 \u0438 \u0434\u043e\u043b\u0433\u043e\u0441\u0440\u043e\u0447\u043d\u043e\u0439 \u043f\u0430\u043c\u044f\u0442\u044c\u044e |\n| [MCP](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-mcp) | \u2605\u2606\u2606 <br> *\u041d\u0430\u0447\u0430\u043b\u044c\u043d\u044b\u0439* | \u0410\u0433\u0435\u043d\u0442 \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u043f\u0440\u043e\u0442\u043e\u043a\u043e\u043b\u0430 \u043a\u043e\u043d\u0442\u0435\u043a\u0441\u0442\u0430 \u043c\u043e\u0434\u0435\u043b\u0438 (Model Context Protocol) \u0434\u043b\u044f \u0447\u0438\u0441\u043b\u043e\u0432\u044b\u0445 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0439 |\n\n</div>\n\n\ud83d\udc40 \u0425\u043e\u0442\u0438\u0442\u0435 \u0443\u0432\u0438\u0434\u0435\u0442\u044c \u0434\u0440\u0443\u0433\u0438\u0435 \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u0441\u0442\u0432\u0430 \u0434\u043b\u044f \u043d\u0430\u0447\u0438\u043d\u0430\u044e\u0449\u0438\u0445? [\u0421\u043e\u0437\u0434\u0430\u0439\u0442\u0435 \u0437\u0430\u0434\u0430\u0447\u0443!](https://github.com/The-Pocket/PocketFlow/issues/new)\n\n## \u041a\u0430\u043a \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c Pocket Flow?\n\n\ud83d\ude80 \u0427\u0435\u0440\u0435\u0437 **\u0410\u0433\u0435\u043d\u0442\u043d\u043e\u0435 \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435** \u2014 \u0441\u0430\u043c\u0443\u044e \u0431\u044b\u0441\u0442\u0440\u0443\u044e \u043f\u0430\u0440\u0430\u0434\u0438\u0433\u043c\u0443 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0438 LLM-\u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0439, \u0433\u0434\u0435 *\u043b\u044e\u0434\u0438 \u043f\u0440\u043e\u0435\u043a\u0442\u0438\u0440\u0443\u044e\u0442*, \u0430 *\u0430\u0433\u0435\u043d\u0442\u044b \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0438\u0440\u0443\u044e\u0442*!\n\n<br>\n<div align=\"center\">\n  <a href=\"https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to\" target=\"_blank\">\n    <img src=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F423a39af-49e8-483b-bc5a-88cc764350c6_1050x588.png\" width=\"700\" alt=\"IMAGE ALT TEXT\" style=\"cursor: pointer;\">\n  </a>\n</div>\n<br>\n\n\u2728 \u041d\u0438\u0436\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u044b \u043f\u0440\u0438\u043c\u0435\u0440\u044b \u0431\u043e\u043b\u0435\u0435 \u0441\u043b\u043e\u0436\u043d\u044b\u0445 LLM-\u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0439:\n\n<div align=\"center\">\n  \n|  \u041d\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f     |  \u0421\u043b\u043e\u0436\u043d\u043e\u0441\u0442\u044c    | \u0422\u0435\u043c\u044b  | \u0414\u0438\u0437\u0430\u0439\u043d \u043e\u0442 \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430 | \u041a\u043e\u0434 \u043e\u0442 \u0430\u0433\u0435\u043d\u0442\u0430 |\n| :-------------:  | :-------------: | :---------------------: |  :---: |  :---: |\n| [\u0421\u043e\u0437\u0434\u0430\u0435\u043c Cursor \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e Cursor](https://github.com/The-Pocket/Tutorial-Cursor) <br> <sup><sub>\u0421\u043a\u043e\u0440\u043e \u0434\u043e\u0441\u0442\u0438\u0433\u043d\u0435\u043c \u0441\u0438\u043d\u0433\u0443\u043b\u044f\u0440\u043d\u043e\u0441\u0442\u0438 ...</sup></sub> | \u2605\u2605\u2605 <br> *\u041f\u0440\u043e\u0434\u0432\u0438\u043d\u0443\u0442\u044b\u0439*   | [\u0410\u0433\u0435\u043d\u0442](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html) | [\u0414\u0438\u0437\u0430\u0439\u043d-\u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/docs/design.md) | [\u041a\u043e\u0434 \u043f\u043e\u0442\u043e\u043a\u0430](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/flow.py)\n| [\u0421\u043f\u0440\u043e\u0441\u0438 AI \u041f\u043e\u043b\u0430 \u0413\u0440\u044d\u043c\u0430](https://github.com/The-Pocket/Tutorial-YC-Partner) <br> <sup><sub>\u0421\u043f\u0440\u043e\u0441\u0438 AI \u041f\u043e\u043b\u0430 \u0413\u0440\u044d\u043c\u0430, \u0435\u0441\u043b\u0438 \u043d\u0435 \u043f\u043e\u043f\u0430\u043b \u0432 \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0443</sup></sub> | \u2605\u2605\u2606 <br> *\u0421\u0440\u0435\u0434\u043d\u0438\u0439*   | [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) <br> [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [TTS](https://the-pocket.github.io/PocketFlow/utility_function/text_to_speech.html) | [\u0414\u0438\u0437\u0430\u0439\u043d-\u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/docs/design.md) | [\u041a\u043e\u0434 \u043f\u043e\u0442\u043e\u043a\u0430](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/flow.py)\n| [Youtube Summarizer](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple)  <br> <sup><sub> \u041e\u0431\u044a\u044f\u0441\u043d\u044f\u0435\u0442 YouTube-\u0432\u0438\u0434\u0435\u043e \u0442\u0430\u043a, \u043a\u0430\u043a \u0431\u0443\u0434\u0442\u043e \u0432\u0430\u043c 5 \u043b\u0435\u0442 </sup></sub> | \u2605\u2606\u2606 <br> *\u041d\u0430\u0447\u0430\u043b\u044c\u043d\u044b\u0439*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) |  [\u0414\u0438\u0437\u0430\u0439\u043d-\u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/docs/design.md) | [\u041a\u043e\u0434 \u043f\u043e\u0442\u043e\u043a\u0430](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/flow.py)\n| [\u0413\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440 \"\u0445\u043e\u043b\u043e\u0434\u043d\u044b\u0445\" \u043e\u0442\u043a\u0440\u044b\u0442\u0438\u0439](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization)  <br> <sup><sub> \u041c\u0433\u043d\u043e\u0432\u0435\u043d\u043d\u044b\u0435 \u043b\u0435\u0434\u043e\u043a\u043e\u043b\u044b, \u043f\u0440\u0435\u0432\u0440\u0430\u0449\u0430\u044e\u0449\u0438\u0435 \u0445\u043e\u043b\u043e\u0434\u043d\u044b\u0435 \u043a\u043e\u043d\u0442\u0430\u043a\u0442\u044b \u0432 \u0433\u043e\u0440\u044f\u0447\u0438\u0435 </sup></sub> | \u2605\u2606\u2606 <br> *\u041d\u0430\u0447\u0430\u043b\u044c\u043d\u044b\u0439*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [\u0412\u0435\u0431-\u043f\u043e\u0438\u0441\u043a](https://the-pocket.github.io/PocketFlow/utility_function/websearch.html) |  [\u0414\u0438\u0437\u0430\u0439\u043d-\u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/docs/design.md) | [\u041a\u043e\u0434 \u043f\u043e\u0442\u043e\u043a\u0430](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/flow.py)\n\n</div>\n\n- \u0425\u043e\u0442\u0438\u0442\u0435 \u0438\u0437\u0443\u0447\u0438\u0442\u044c **\u0410\u0433\u0435\u043d\u0442\u043d\u043e\u0435 \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435**?\n\n  - \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u0442\u0435 [\u043c\u043e\u0439 YouTube-\u043a\u0430\u043d\u0430\u043b](https://www.youtube.com/@ZacharyLLM?sub_confirmation=1) \u0434\u043b\u044f \u0432\u0438\u0434\u0435\u043e\u0443\u0440\u043e\u043a\u043e\u0432 \u043e \u0442\u043e\u043c, \u043a\u0430\u043a \u0431\u044b\u043b\u0438 \u0441\u043e\u0437\u0434\u0430\u043d\u044b \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0432\u044b\u0448\u0435\u0443\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f!\n\n  - \u0425\u043e\u0442\u0438\u0442\u0435 \u0441\u043e\u0437\u0434\u0430\u0442\u044c \u0441\u0432\u043e\u0435 \u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0435 LLM-\u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435? \u041f\u0440\u043e\u0447\u0438\u0442\u0430\u0439\u0442\u0435 \u044d\u0442\u0443 [\u0441\u0442\u0430\u0442\u044c\u044e](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)! \u041d\u0430\u0447\u043d\u0438\u0442\u0435 \u0441 [\u044d\u0442\u043e\u0433\u043e \u0448\u0430\u0431\u043b\u043e\u043d\u0430](https://github.com/The-Pocket/PocketFlow-Template-Python)!\n\n  - \u0425\u043e\u0442\u0438\u0442\u0435 \u0443\u0437\u043d\u0430\u0442\u044c \u043f\u043e\u0434\u0440\u043e\u0431\u043d\u044b\u0435 \u0448\u0430\u0433\u0438? \u041f\u0440\u043e\u0447\u0438\u0442\u0430\u0439\u0442\u0435 \u044d\u0442\u043e [\u0420\u0443\u043a\u043e\u0432\u043e\u0434\u0441\u0442\u0432\u043e](https://the-pocket.github.io/PocketFlow/guide.html)!\n\n--- File Index 29: cookbook/pocketflow-batch/translations/README_SPANISH.md ---\n<div align=\"center\">\n  <img src=\"https://github.com/The-Pocket/.github/raw/main/assets/title.png\" width=\"600\"/>\n</div>\n\n\n![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)\n[![Docs](https://img.shields.io/badge/docs-latest-blue)](https://the-pocket.github.io/PocketFlow/)\n <a href=\"https://discord.gg/hUHHE9Sa6T\">\n    <img src=\"https://img.shields.io/discord/1346833819172601907?logo=discord&style=flat\">\n</a>\n\nPocket Flow es un framework minimalista para LLM de [100 l\u00edneas](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py)\n\n- **Ligero**: Solo 100 l\u00edneas. Cero redundancia, cero dependencias, cero bloqueo de proveedor.\n  \n- **Expresivo**: Todo lo que te gusta\u2014([Multi-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[Agentes](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [Flujo de trabajo](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html), y m\u00e1s.\n\n- **[Programaci\u00f3n Ag\u00e9ntica](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)**: Deja que los Agentes de IA (por ejemplo, Cursor AI) construyan Agentes\u2014\u00a1potencia tu productividad 10 veces!\n\n- Para instalar, ```pip install pocketflow``` o simplemente copia el [c\u00f3digo fuente](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py) (solo 100 l\u00edneas).\n  \n- Para saber m\u00e1s, consulta la [documentaci\u00f3n](https://the-pocket.github.io/PocketFlow/). Para conocer la motivaci\u00f3n, lee la [historia](https://zacharyhuang.substack.com/p/i-built-an-llm-framework-in-just).\n  \n- \ud83c\udf89 \u00a1\u00danete a nuestro [discord](https://discord.gg/hUHHE9Sa6T)!\n\n- \ud83c\udf89 Gracias a [@zvictor](https://www.github.com/zvictor), [@jackylee941130](https://www.github.com/jackylee941130) y [@ZebraRoy](https://www.github.com/ZebraRoy), \u00a1ahora tenemos una [versi\u00f3n TypeScript](https://github.com/The-Pocket/PocketFlow-Typescript)!\n\n## \u00bfPor qu\u00e9 Pocket Flow?\n\nLos frameworks LLM actuales est\u00e1n sobrecargados... \u00a1Solo necesitas 100 l\u00edneas para un Framework LLM!\n\n<div align=\"center\">\n  <img src=\"https://github.com/The-Pocket/.github/raw/main/assets/meme.jpg\" width=\"400\"/>\n\n\n  |                | **Abstracci\u00f3n**          | **Envoltorios espec\u00edficos de aplicaci\u00f3n**                                      | **Envoltorios espec\u00edficos de proveedor**                                    | **L\u00edneas**       | **Tama\u00f1o**    |\n|----------------|:-----------------------------: |:-----------------------------------------------------------:|:------------------------------------------------------------:|:---------------:|:----------------------------:|\n| LangChain  | Agente, Cadena               | Muchos <br><sup><sub>(p.ej., QA, Resumen)</sub></sup>              | Muchos <br><sup><sub>(p.ej., OpenAI, Pinecone, etc.)</sub></sup>                   | 405K          | +166MB                     |\n| CrewAI     | Agente, Cadena            | Muchos <br><sup><sub>(p.ej., FileReadTool, SerperDevTool)</sub></sup>         | Muchos <br><sup><sub>(p.ej., OpenAI, Anthropic, Pinecone, etc.)</sub></sup>        | 18K           | +173MB                     |\n| SmolAgent   | Agente                      | Algunos <br><sup><sub>(p.ej., CodeAgent, VisitWebTool)</sub></sup>         | Algunos <br><sup><sub>(p.ej., DuckDuckGo, Hugging Face, etc.)</sub></sup>           | 8K            | +198MB                     |\n| LangGraph   | Agente, Grafo           | Algunos <br><sup><sub>(p.ej., B\u00fasqueda Sem\u00e1ntica)</sub></sup>                     | Algunos <br><sup><sub>(p.ej., PostgresStore, SqliteSaver, etc.) </sub></sup>        | 37K           | +51MB                      |\n| AutoGen    | Agente                | Algunos <br><sup><sub>(p.ej., Tool Agent, Chat Agent)</sub></sup>              | Muchos <sup><sub>[Opcional]<br> (p.ej., OpenAI, Pinecone, etc.)</sub></sup>        | 7K <br><sup><sub>(solo n\u00facleo)</sub></sup>    | +26MB <br><sup><sub>(solo n\u00facleo)</sub></sup>          |\n| **PocketFlow** | **Grafo**                    | **Ninguno**                                                 | **Ninguno**                                                  | **100**       | **+56KB**                  |\n\n</div>\n\n## \u00bfC\u00f3mo funciona Pocket Flow?\n\nLas [100 l\u00edneas](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py) capturan la abstracci\u00f3n central de los frameworks LLM: \u00a1Grafo!\n<br>\n<div align=\"center\">\n  <img src=\"https://github.com/The-Pocket/.github/raw/main/assets/abstraction.png\" width=\"900\"/>\n</div>\n<br>\n\nA partir de ah\u00ed, es f\u00e1cil implementar patrones de dise\u00f1o populares como ([Multi-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[Agentes](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [Flujo de trabajo](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html), etc.\n<br>\n<div align=\"center\">\n  <img src=\"https://github.com/The-Pocket/.github/raw/main/assets/design.png\" width=\"900\"/>\n</div>\n<br>\n\u2728 A continuaci\u00f3n se presentan tutoriales b\u00e1sicos:\n\n<div align=\"center\">\n  \n|  Nombre  | Dificultad    |  Descripci\u00f3n  |  \n| :-------------:  | :-------------: | :--------------------- |  \n| [Chat](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat) | \u2606\u2606\u2606 <br> *Novato*   | Un bot de chat b\u00e1sico con historial de conversaci\u00f3n |\n| [Salida Estructurada](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-structured-output) | \u2606\u2606\u2606 <br> *Novato* | Extracci\u00f3n de datos estructurados de curr\u00edculums mediante prompts |\n| [Flujo de Trabajo](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-workflow) | \u2606\u2606\u2606 <br> *Novato*   | Un flujo de escritura que esquematiza, escribe contenido y aplica estilo |\n| [Agente](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-agent) | \u2606\u2606\u2606 <br> *Novato*   | Un agente de investigaci\u00f3n que puede buscar en la web y responder preguntas |\n| [RAG](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-rag) | \u2606\u2606\u2606 <br> *Novato*   | Un proceso simple de Generaci\u00f3n aumentada por Recuperaci\u00f3n |\n| [Map-Reduce](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-map-reduce) | \u2606\u2606\u2606 <br> *Novato* | Un procesador de calificaci\u00f3n de curr\u00edculums usando el patr\u00f3n map-reduce para evaluaci\u00f3n por lotes |\n| [Streaming](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-llm-streaming) | \u2606\u2606\u2606 <br> *Novato*   | Una demo de streaming LLM en tiempo real con capacidad de interrupci\u00f3n por el usuario |\n| [Chat con Barreras](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-guardrail) | \u2606\u2606\u2606 <br> *Novato*  | Un chatbot asesor de viajes que solo procesa consultas relacionadas con viajes |\n| [Multi-Agente](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-multi-agent) | \u2605\u2606\u2606 <br> *Principiante* | Un juego de palabras tab\u00fa para comunicaci\u00f3n as\u00edncrona entre dos agentes |\n| [Supervisor](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-supervisor) | \u2605\u2606\u2606 <br> *Principiante* | El agente de investigaci\u00f3n se est\u00e1 volviendo poco fiable... \u00a1Construyamos un proceso de supervisi\u00f3n! |\n| [Paralelo](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch) | \u2605\u2606\u2606 <br> *Principiante*   | Una demo de ejecuci\u00f3n paralela que muestra una aceleraci\u00f3n de 3x |\n| [Flujo Paralelo](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch-flow) | \u2605\u2606\u2606 <br> *Principiante*   | Una demo de procesamiento de im\u00e1genes en paralelo que muestra una aceleraci\u00f3n de 8x con m\u00faltiples filtros |\n| [Voto por Mayor\u00eda](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-majority-vote) | \u2605\u2606\u2606 <br> *Principiante* | Mejora la precisi\u00f3n del razonamiento agregando m\u00faltiples intentos de soluci\u00f3n |\n| [Pensamiento](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-thinking) | \u2605\u2606\u2606 <br> *Principiante*   | Resuelve problemas de razonamiento complejos a trav\u00e9s de Cadena de Pensamiento |\n| [Memoria](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-memory) | \u2605\u2606\u2606 <br> *Principiante* | Un bot de chat con memoria a corto y largo plazo |\n| [MCP](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-mcp) | \u2605\u2606\u2606 <br> *Principiante* | Agente que usa el Protocolo de Contexto de Modelo para operaciones num\u00e9ricas |\n\n</div>\n\n\ud83d\udc40 \u00bfQuieres ver otros tutoriales para novatos? [\u00a1Crea un issue!](https://github.com/The-Pocket/PocketFlow/issues/new)\n\n## \u00bfC\u00f3mo usar Pocket Flow?\n\n\ud83d\ude80 A trav\u00e9s de **Programaci\u00f3n Ag\u00e9ntica** \u2014 el paradigma de desarrollo de aplicaciones LLM m\u00e1s r\u00e1pido \u2014 donde *los humanos dise\u00f1an* y *los agentes programan*!\n\n<br>\n<div align=\"center\">\n  <a href=\"https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to\" target=\"_blank\">\n    <img src=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F423a39af-49e8-483b-bc5a-88cc764350c6_1050x588.png\" width=\"700\" alt=\"IMAGE ALT TEXT\" style=\"cursor: pointer;\">\n  </a>\n</div>\n<br>\n\n\u2728 A continuaci\u00f3n hay ejemplos de aplicaciones LLM m\u00e1s complejas:\n\n<div align=\"center\">\n  \n|  Nombre de la App     |  Dificultad    | Temas  | Dise\u00f1o Humano | C\u00f3digo de Agente |\n| :-------------:  | :-------------: | :---------------------: |  :---: |  :---: |\n| [Construir Cursor con Cursor](https://github.com/The-Pocket/Tutorial-Cursor) <br> <sup><sub>Pronto llegaremos a la singularidad...</sup></sub> | \u2605\u2605\u2605 <br> *Avanzado*   | [Agente](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html) | [Doc de Dise\u00f1o](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/docs/design.md) | [C\u00f3digo de Flujo](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/flow.py)\n| [Pregunta a IA Paul Graham](https://github.com/The-Pocket/Tutorial-YC-Partner) <br> <sup><sub>Pregunta a IA Paul Graham, en caso de que no entres</sup></sub> | \u2605\u2605\u2606 <br> *Medio*   | [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) <br> [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [TTS](https://the-pocket.github.io/PocketFlow/utility_function/text_to_speech.html) | [Doc de Dise\u00f1o](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/docs/design.md) | [C\u00f3digo de Flujo](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/flow.py)\n| [Resumidor de Youtube](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple)  <br> <sup><sub> Te explica videos de YouTube como si tuvieras 5 a\u00f1os </sup></sub> | \u2605\u2606\u2606 <br> *Principiante*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) |  [Doc de Dise\u00f1o](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/docs/design.md) | [C\u00f3digo de Flujo](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/flow.py)\n| [Generador de Introducci\u00f3n Fr\u00eda](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization)  <br> <sup><sub> Rompehielos instant\u00e1neos que calientan contactos fr\u00edos </sup></sub> | \u2605\u2606\u2606 <br> *Principiante*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [B\u00fasqueda Web](https://the-pocket.github.io/PocketFlow/utility_function/websearch.html) |  [Doc de Dise\u00f1o](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/docs/design.md) | [C\u00f3digo de Flujo](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/flow.py)\n\n</div>\n\n- \u00bfQuieres aprender **Programaci\u00f3n Ag\u00e9ntica**?\n\n  - \u00a1Consulta [mi YouTube](https://www.youtube.com/@ZacharyLLM?sub_confirmation=1) para tutoriales en video sobre c\u00f3mo se hicieron algunas aplicaciones anteriores!\n\n  - \u00bfQuieres construir tu propia aplicaci\u00f3n LLM? \u00a1Lee este [post](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)! \u00a1Comienza con [esta plantilla](https://github.com/The-Pocket/PocketFlow-Template-Python)!\n\n  - \u00bfQuieres aprender los pasos detallados? \u00a1Lee esta [Gu\u00eda](https://the-pocket.github.io/PocketFlow/guide.html)!\n\n--- File Index 30: cookbook/pocketflow-batch/utils.py ---\nfrom anthropic import Anthropic\nimport os\n\ndef call_llm(prompt):\n    client = Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\", \"your-api-key\"))\n    response = client.messages.create(\n        model=\"claude-3-7-sonnet-20250219\",\n        max_tokens=10000,\n        messages=[\n            {\"role\": \"user\", \"content\": prompt}\n        ]\n    )\n    return response.content[0].text\n\nif __name__ == \"__main__\":\n    print(\"## Testing call_llm\")\n    prompt = \"In a few words, what is the meaning of life?\"\n    print(f\"## Prompt: {prompt}\")\n    response = call_llm(prompt)\n    print(f\"## Response: {response}\")\n\n--- File Index 31: cookbook/pocketflow-chat-guardrail/README.md ---\n#  Travel Advisor Chat with Guardrails\n\nA travel-focused chat application using PocketFlow with OpenAI's GPT-4o model, enhanced with input validation to ensure only travel-related queries are processed.\n\n## Features\n\n- Travel advisor chatbot that answers questions about destinations, planning, accommodations, etc.\n- **Topic-specific guardrails** to ensure only travel-related queries are accepted\n\n## Run It\n\n1. Make sure your OpenAI API key is set:\n    ```bash\n    export OPENAI_API_KEY=\"your-api-key-here\"\n    ```\n    Alternatively, you can edit the `utils.py` file to include your API key directly.\n\n2. Install requirements and run the application:\n    ```bash\n    pip install -r requirements.txt\n    python main.py\n    ```\n\n## How It Works\n\n```mermaid\nflowchart LR\n    user[UserInputNode] -->|validate| guardrail[GuardrailNode]\n    guardrail -->|retry| user\n    guardrail -->|process| llm[LLMNode]\n    llm -->|continue| user\n```\n\nThe chat application uses:\n- A `UserInputNode` that collects user input in its `exec` method\n- A `GuardrailNode` that validates if the query is travel-related using:\n  - Basic validation checks (empty input, too short)\n  - LLM-based validation to determine if the query relates to travel\n- An `LLMNode` that processes valid travel queries using GPT-4o with a travel advisor system prompt\n- Flow connections that route inputs through validation before processing and handle retries for non-travel related queries\n\n## Files\n\n- [`main.py`](./main.py): Implementation of the nodes and chat flow\n- [`utils.py`](./utils.py): Utilities for calling the OpenAI API\n\n## Example Outputs\n\n``` \nWelcome to the Travel Advisor Chat! Type 'exit' to end the conversation.\n\nYou: Plan my trip to Thailand\n\nTravel Advisor: Thailand offers a blend of vibrant cities, serene beaches, and rich culture. Begin in Bangkok to explore the Grand Palace and local markets. Head north to Chiang Mai for temples and elephant sanctuaries. Fly south to the islands; Phuket or Koh Samui for beaches, diving, and nightlife. Consider visiting during the cool season (November-February) for pleasant weather. Accommodation ranges from budget hostels to luxury resorts. Internal flights, trains, and buses connect major destinations. Don\u2019t miss local cuisine, from street food to fine dining. Ensure your passport is valid for six months and consider travel insurance for peace of mind. Enjoy!\n\nYou: exit\n\nGoodbye! Safe travels!\n```\n\n```\nWelcome to the Travel Advisor Chat! Type 'exit' to end the conversation.\n\nYou: How to study large language models?\n\nTravel Advisor: The query is not related to travel advice, destinations, planning, or other travel topics. It is about studying large language models, which is a topic related to artificial intelligence and machine learning.\n\nYou: exit\n\nGoodbye! Safe travels!\n```\n\n--- File Index 32: cookbook/pocketflow-chat-guardrail/main.py ---\nfrom pocketflow import Node, Flow\nfrom utils import call_llm\n\nclass UserInputNode(Node):\n    def prep(self, shared):\n        # Initialize messages if this is the first run\n        if \"messages\" not in shared:\n            shared[\"messages\"] = []\n            print(\"Welcome to the Travel Advisor Chat! Type 'exit' to end the conversation.\")\n        \n        return None\n\n    def exec(self, _):\n        # Get user input\n        user_input = input(\"\\nYou: \")\n        return user_input\n\n    def post(self, shared, prep_res, exec_res):\n        user_input = exec_res\n        \n        # Check if user wants to exit\n        if user_input and user_input.lower() == 'exit':\n            print(\"\\nGoodbye! Safe travels!\")\n            return None  # End the conversation\n        \n        # Store user input in shared\n        shared[\"user_input\"] = user_input\n        \n        # Move to guardrail validation\n        return \"validate\"\n\nclass GuardrailNode(Node):\n    def prep(self, shared):\n        # Get the user input from shared data\n        user_input = shared.get(\"user_input\", \"\")\n        return user_input\n    \n    def exec(self, user_input):\n        # Basic validation checks\n        if not user_input or user_input.strip() == \"\":\n            return False, \"Your query is empty. Please provide a travel-related question.\"\n        \n        if len(user_input.strip()) < 3:\n            return False, \"Your query is too short. Please provide more details about your travel question.\"\n        \n        # LLM-based validation for travel topics\n        prompt = f\"\"\"\nEvaluate if the following user query is related to travel advice, destinations, planning, or other travel topics.\nThe chat should ONLY answer travel-related questions and reject any off-topic, harmful, or inappropriate queries.\nUser query: {user_input}\nReturn your evaluation in YAML format:\n```yaml\nvalid: true/false\nreason: [Explain why the query is valid or invalid]\n```\"\"\"\n        \n        # Call LLM with the validation prompt\n        messages = [{\"role\": \"user\", \"content\": prompt}]\n        response = call_llm(messages)\n        \n        # Extract YAML content\n        yaml_content = response.split(\"```yaml\")[1].split(\"```\")[0].strip() if \"```yaml\" in response else response\n        \n        import yaml\n        result = yaml.safe_load(yaml_content)\n        assert result is not None, \"Error: Invalid YAML format\"\n        assert \"valid\" in result and \"reason\" in result, \"Error: Invalid YAML format\"\n        is_valid = result.get(\"valid\", False)\n        reason = result.get(\"reason\", \"Missing reason in YAML response\")\n        \n        return is_valid, reason\n    \n    def post(self, shared, prep_res, exec_res):\n        is_valid, message = exec_res\n        \n        if not is_valid:\n            # Display error message to user\n            print(f\"\\nTravel Advisor: {message}\")\n            # Skip LLM call and go back to user input\n            return \"retry\"\n        \n        # Valid input, add to message history\n        shared[\"messages\"].append({\"role\": \"user\", \"content\": shared[\"user_input\"]})\n        # Proceed to LLM processing\n        return \"process\"\n\nclass LLMNode(Node):\n    def prep(self, shared):\n        # Add system message if not present\n        if not any(msg.get(\"role\") == \"system\" for msg in shared[\"messages\"]):\n            shared[\"messages\"].insert(0, {\n                \"role\": \"system\", \n                \"content\": \"You are a helpful travel advisor that provides information about destinations, travel planning, accommodations, transportation, activities, and other travel-related topics. Only respond to travel-related queries and keep responses informative and friendly. Your response are concise in 100 words.\"\n            })\n        \n        # Return all messages for the LLM\n        return shared[\"messages\"]\n\n    def exec(self, messages):\n        # Call LLM with the entire conversation history\n        response = call_llm(messages)\n        return response\n\n    def post(self, shared, prep_res, exec_res):\n        # Print the assistant's response\n        print(f\"\\nTravel Advisor: {exec_res}\")\n        \n        # Add assistant message to history\n        shared[\"messages\"].append({\"role\": \"assistant\", \"content\": exec_res})\n        \n        # Loop back to continue the conversation\n        return \"continue\"\n\n# Create the flow with nodes and connections\nuser_input_node = UserInputNode()\nguardrail_node = GuardrailNode()\nllm_node = LLMNode()\n\n# Create flow connections\nuser_input_node - \"validate\" >> guardrail_node\nguardrail_node - \"retry\" >> user_input_node  # Loop back if input is invalid\nguardrail_node - \"process\" >> llm_node\nllm_node - \"continue\" >> user_input_node     # Continue conversation\n\nflow = Flow(start=user_input_node)\n\n# Start the chat\nif __name__ == \"__main__\":\n    shared = {}\n    flow.run(shared)\n\n\n--- File Index 33: cookbook/pocketflow-chat-guardrail/utils.py ---\nfrom openai import OpenAI\nimport os\n\ndef call_llm(messages):\n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n    \n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=messages,\n        temperature=0.7\n    )\n    \n    return response.choices[0].message.content\n\nif __name__ == \"__main__\":\n    # Test the LLM call\n    messages = [{\"role\": \"user\", \"content\": \"In a few words, what's the meaning of life?\"}]\n    response = call_llm(messages)\n    print(f\"Prompt: {messages[0]['content']}\")\n    print(f\"Response: {response}\")\n\n\n--- File Index 34: cookbook/pocketflow-chat-memory/README.md ---\n# PocketFlow Chat with Memory\n\nA chat application with memory retrieval using PocketFlow. This example maintains a sliding window of recent conversations while retrieving relevant past conversations based on context. \n\nThis implementation is based directly on the tutorial: [Build AI Agent Memory From Scratch \u2014 Tutorial For Dummies](https://zacharyhuang.substack.com/p/build-ai-agent-memory-from-scratch).\n\n## Features\n\n- Maintains a window of 3 most recent conversation pairs\n- Archives older conversations with embeddings\n- Uses vector similarity to retrieve the most relevant past conversation\n- Combines recent context (3 pairs) with retrieved context (1 pair) for better responses\n\n## Run It\n\n1. Make sure your OpenAI API key is set:\n    ```bash\n    export OPENAI_API_KEY=\"your-api-key-here\"\n    ```\n\n2. Install requirements and run the application:\n    ```bash\n    pip install -r requirements.txt\n    python main.py\n    ```\n    \n## How It Works\n\n```mermaid\nflowchart LR\n    Question[GetUserQuestionNode] -->|retrieve| Retrieve[RetrieveNode]\n    Retrieve -->|answer| Answer[AnswerNode]\n    Answer -->|question| Question\n    Answer -->|embed| Embed[EmbedNode]\n    Embed -->|question| Question\n```\n\nThe chat application uses:\n- Four specialized nodes:\n  - `GetUserQuestionNode`: Handles interactive user input\n  - `RetrieveNode`: Finds relevant past conversations using vector similarity\n  - `AnswerNode`: Generates responses using both recent and retrieved context\n  - `EmbedNode`: Archives older conversations with embeddings\n- A sliding window approach that maintains only the 3 most recent conversation pairs in active context\n\n## Files\n\n- [`nodes.py`](./nodes.py): Four node implementations with clear separation of concerns\n- [`flow.py`](./flow.py): Chat flow structure definition\n- [`main.py`](./main.py): Entry point for running the demo\n- [`utils/`](./utils/): Utility functions for embeddings, LLM calls, and vector operations\n\n\n## Example Output\n\n```\n==================================================\nPocketFlow Chat with Memory\n==================================================\nThis chat keeps your 3 most recent conversations\nand brings back relevant past conversations when helpful\nType 'exit' to end the conversation\n==================================================\nWelcome to the interactive chat! Type 'exit' to end the conversation.\n\nYou: remember, my cat name is Whiskers and she only drinks from running water taps\n\nAssistant: Got it! Whiskers only drinks from running water taps. If you have any questions or need advice related to her, feel free to ask!\n\nYou: remember, I have a peanut allergy but I can eat almonds and cashews without problems\n\nAssistant: Understood! You have a peanut allergy but can safely eat almonds and cashews. If you need any assistance or information related to your dietary preferences or restrictions, just let me know!\n\nYou: remember, my anniversary is June 17th and I always forget to buy a gift until the last minute\n\nAssistant: Got it! Your anniversary is on June 17th, and you often remember to buy a gift at the last minute. If you'd like, I can help you brainstorm gift ideas or set reminders as the date approaches!\n\nYou: remember, I lived in Portugal for 3 years when I was a child and still speak some Portuguese\n\nAssistant: That's great to know! You lived in Portugal for three years during your childhood and still speak some Portuguese. If you ever want to practice or need help with anything related to Portuguese language or culture, feel free to ask!\n\u2705 Added conversation to index at position 0\n\u2705 Index now contains 1 conversations\n\nYou: remember, my favorite dessert is tiramisu but only if it's made with real mascarpone cheese\n\ud83d\udd0d Finding relevant conversation for: remember, my favorite dessert ...\n\ud83d\udcc4 Retrieved conversation (distance: 0.5008)\n\nAssistant: Got it! Your favorite dessert is tiramisu, but only when it's made with real mascarpone cheese. If you ever need recommendations or recipes, just let me know!\n\u2705 Added conversation to index at position 1\n\u2705 Index now contains 2 conversations\n\nYou: remember, I collect vintage mechanical watches and my most valuable one is a 1965 Omega Seamaster\n\ud83d\udd0d Finding relevant conversation for: remember, I collect vintage me...\n\ud83d\udcc4 Retrieved conversation (distance: 0.5374)\n\nAssistant: Got it! You collect vintage mechanical watches, and your most valuable piece is a 1965 Omega Seamaster. If you have questions about watches or need assistance with your collection, feel free to reach out!\n\u2705 Added conversation to index at position 2\n\u2705 Index now contains 3 conversations\n\nYou: what's my cat name?\n\ud83d\udd0d Finding relevant conversation for: what's my cat name?...\n\ud83d\udcc4 Retrieved conversation (distance: 0.3643)\n\nAssistant: Your cat's name is Whiskers.\n\u2705 Added conversation to index at position 3\n\u2705 Index now contains 4 conversations\n```\n\n\n--- File Index 35: cookbook/pocketflow-chat-memory/flow.py ---\nfrom pocketflow import Flow\nfrom nodes import GetUserQuestionNode, RetrieveNode, AnswerNode, EmbedNode\n\ndef create_chat_flow():\n    # Create the nodes\n    question_node = GetUserQuestionNode()\n    retrieve_node = RetrieveNode()\n    answer_node = AnswerNode()\n    embed_node = EmbedNode()\n    \n    # Connect the flow:\n    # 1. Start with getting a question\n    # 2. Retrieve relevant conversations\n    # 3. Generate an answer\n    # 4. Optionally embed old conversations\n    # 5. Loop back to get the next question\n\n    # Main flow path\n    question_node - \"retrieve\" >> retrieve_node\n    retrieve_node - \"answer\" >> answer_node\n    \n    # When we need to embed old conversations\n    answer_node - \"embed\" >> embed_node\n    \n    # Loop back for next question\n    answer_node - \"question\" >> question_node\n    embed_node - \"question\" >> question_node\n    \n    # Create the flow starting with question node\n    return Flow(start=question_node)\n\n# Initialize the flow\nchat_flow = create_chat_flow() \n\n--- File Index 36: cookbook/pocketflow-chat-memory/main.py ---\nfrom flow import chat_flow\n\ndef run_chat_memory_demo():\n    \"\"\"\n    Run an interactive chat interface with memory retrieval.\n    \n    Features:\n    1. Maintains a window of the 3 most recent conversation pairs\n    2. Archives older conversations with embeddings\n    3. Retrieves 1 relevant past conversation when needed\n    4. Total context to LLM: 3 recent pairs + 1 retrieved pair\n    \"\"\"\n    \n    print(\"=\" * 50)\n    print(\"PocketFlow Chat with Memory\")\n    print(\"=\" * 50)\n    print(\"This chat keeps your 3 most recent conversations\")\n    print(\"and brings back relevant past conversations when helpful\")\n    print(\"Type 'exit' to end the conversation\")\n    print(\"=\" * 50)\n    \n    # Run the chat flow\n    chat_flow.run({})\n\nif __name__ == \"__main__\":\n    run_chat_memory_demo()\n\n--- File Index 37: cookbook/pocketflow-chat-memory/nodes.py ---\nfrom pocketflow import Node\nfrom utils.vector_index import create_index, add_vector, search_vectors\nfrom utils.call_llm import call_llm\nfrom utils.get_embedding import get_embedding\n\nclass GetUserQuestionNode(Node):\n    def prep(self, shared):\n        \"\"\"Initialize messages if first run\"\"\"\n        if \"messages\" not in shared:\n            shared[\"messages\"] = []\n            print(\"Welcome to the interactive chat! Type 'exit' to end the conversation.\")\n        \n        return None\n    \n    def exec(self, _):\n        \"\"\"Get user input interactively\"\"\"\n        # Get interactive input from user\n        user_input = input(\"\\nYou: \")\n            \n        # Check if user wants to exit\n        if user_input.lower() == 'exit':\n            return None\n            \n        return user_input\n    \n    def post(self, shared, prep_res, exec_res):\n        # If exec_res is None, the user wants to exit\n        if exec_res is None:\n            print(\"\\nGoodbye!\")\n            return None  # End the conversation\n            \n        # Add user message to current messages\n        shared[\"messages\"].append({\"role\": \"user\", \"content\": exec_res})\n        \n        return \"retrieve\"\n\nclass AnswerNode(Node):\n    def prep(self, shared):\n        \"\"\"Prepare context for the LLM\"\"\"\n        if not shared.get(\"messages\"):\n            return None\n            \n        # 1. Get the last 3 conversation pairs (or fewer if not available)\n        recent_messages = shared[\"messages\"][-6:] if len(shared[\"messages\"]) > 6 else shared[\"messages\"]\n        \n        # 2. Add the retrieved relevant conversation if available\n        context = []\n        if shared.get(\"retrieved_conversation\"):\n            # Add a system message to indicate this is a relevant past conversation\n            context.append({\n                \"role\": \"system\", \n                \"content\": \"The following is a relevant past conversation that may help with the current query:\"\n            })\n            context.extend(shared[\"retrieved_conversation\"])\n            context.append({\n                \"role\": \"system\", \n                \"content\": \"Now continue the current conversation:\"\n            })\n        \n        # 3. Add the recent messages\n        context.extend(recent_messages)\n        \n        return context\n    \n    def exec(self, messages):\n        \"\"\"Generate a response using the LLM\"\"\"\n        if messages is None:\n            return None\n        \n        # Call LLM with the context\n        response = call_llm(messages)\n        return response\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Process the LLM response\"\"\"\n        if prep_res is None or exec_res is None:\n            return None  # End the conversation\n        \n        # Print the assistant's response\n        print(f\"\\nAssistant: {exec_res}\")\n        \n        # Add assistant message to history\n        shared[\"messages\"].append({\"role\": \"assistant\", \"content\": exec_res})\n        \n        # If we have more than 6 messages (3 conversation pairs), archive the oldest pair\n        if len(shared[\"messages\"]) > 6:\n            return \"embed\"\n        \n        # We only end if the user explicitly typed 'exit'\n        # Even if last_question is set, we continue in interactive mode\n        return \"question\"\n\nclass EmbedNode(Node):\n    def prep(self, shared):\n        \"\"\"Extract the oldest conversation pair for embedding\"\"\"\n        if len(shared[\"messages\"]) <= 6:\n            return None\n            \n        # Extract the oldest user-assistant pair\n        oldest_pair = shared[\"messages\"][:2]\n        # Remove them from current messages\n        shared[\"messages\"] = shared[\"messages\"][2:]\n        \n        return oldest_pair\n    \n    def exec(self, conversation):\n        \"\"\"Embed a conversation\"\"\"\n        if not conversation:\n            return None\n            \n        # Combine user and assistant messages into a single text for embedding\n        user_msg = next((msg for msg in conversation if msg[\"role\"] == \"user\"), {\"content\": \"\"})\n        assistant_msg = next((msg for msg in conversation if msg[\"role\"] == \"assistant\"), {\"content\": \"\"})\n        combined = f\"User: {user_msg['content']} Assistant: {assistant_msg['content']}\"\n        \n        # Generate embedding\n        embedding = get_embedding(combined)\n        \n        return {\n            \"conversation\": conversation,\n            \"embedding\": embedding\n        }\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Store the embedding and add to index\"\"\"\n        if not exec_res:\n            # If there's nothing to embed, just continue with the next question\n            return \"question\"\n            \n        # Initialize vector index if not exist\n        if \"vector_index\" not in shared:\n            shared[\"vector_index\"] = create_index()\n            shared[\"vector_items\"] = []  # Track items separately\n            \n        # Add the embedding to the index and store the conversation\n        position = add_vector(shared[\"vector_index\"], exec_res[\"embedding\"])\n        shared[\"vector_items\"].append(exec_res[\"conversation\"])\n        \n        print(f\"\u2705 Added conversation to index at position {position}\")\n        print(f\"\u2705 Index now contains {len(shared['vector_items'])} conversations\")\n        \n        # Continue with the next question\n        return \"question\"\n\nclass RetrieveNode(Node):\n    def prep(self, shared):\n        \"\"\"Get the current query for retrieval\"\"\"\n        if not shared.get(\"messages\"):\n            return None\n            \n        # Get the latest user message for searching\n        latest_user_msg = next((msg for msg in reversed(shared[\"messages\"]) \n                                if msg[\"role\"] == \"user\"), {\"content\": \"\"})\n        \n        # Check if we have a vector index with items\n        if (\"vector_index\" not in shared or \n            \"vector_items\" not in shared or \n            len(shared[\"vector_items\"]) == 0):\n            return None\n            \n        return {\n            \"query\": latest_user_msg[\"content\"],\n            \"vector_index\": shared[\"vector_index\"],\n            \"vector_items\": shared[\"vector_items\"]\n        }\n    \n    def exec(self, inputs):\n        \"\"\"Find the most relevant past conversation\"\"\"\n        if not inputs:\n            return None\n            \n        query = inputs[\"query\"]\n        vector_index = inputs[\"vector_index\"]\n        vector_items = inputs[\"vector_items\"]\n        \n        print(f\"\ud83d\udd0d Finding relevant conversation for: {query[:30]}...\")\n        \n        # Create embedding for the query\n        query_embedding = get_embedding(query)\n        \n        # Search for the most similar conversation\n        indices, distances = search_vectors(vector_index, query_embedding, k=1)\n        \n        if not indices:\n            return None\n            \n        # Get the corresponding conversation\n        conversation = vector_items[indices[0]]\n        \n        return {\n            \"conversation\": conversation,\n            \"distance\": distances[0]\n        }\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Store the retrieved conversation\"\"\"\n        if exec_res is not None:\n            shared[\"retrieved_conversation\"] = exec_res[\"conversation\"]\n            print(f\"\ud83d\udcc4 Retrieved conversation (distance: {exec_res['distance']:.4f})\")\n        else:\n            shared[\"retrieved_conversation\"] = None\n        \n        return \"answer\"\n\n--- File Index 38: cookbook/pocketflow-chat-memory/utils/__init__.py ---\n\n\n\n--- File Index 39: cookbook/pocketflow-chat-memory/utils/call_llm.py ---\nimport os\nfrom openai import OpenAI\n\ndef call_llm(messages):\n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n    \n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=messages,\n        temperature=0.7\n    )\n    \n    return response.choices[0].message.content\n\nif __name__ == \"__main__\":\n    # Test the LLM call\n    messages = [{\"role\": \"user\", \"content\": \"In a few words, what's the meaning of life?\"}]\n    response = call_llm(messages)\n    print(f\"Prompt: {messages[0]['content']}\")\n    print(f\"Response: {response}\") \n\n--- File Index 40: cookbook/pocketflow-chat-memory/utils/get_embedding.py ---\nimport os\nimport numpy as np\nfrom openai import OpenAI\n\ndef get_embedding(text):\n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"YOUR_API_KEY\"))\n    \n    response = client.embeddings.create(\n        model=\"text-embedding-ada-002\",\n        input=text\n    )\n    \n    # Extract the embedding vector from the response\n    embedding = response.data[0].embedding\n    \n    # Convert to numpy array for consistency with other embedding functions\n    return np.array(embedding, dtype=np.float32)\n\n\nif __name__ == \"__main__\":\n    # Test the embedding function\n    text1 = \"The quick brown fox jumps over the lazy dog.\"\n    text2 = \"Python is a popular programming language for data science.\"\n    \n    emb1 = get_embedding(text1)\n    emb2 = get_embedding(text2)\n    \n    print(f\"Embedding 1 shape: {emb1.shape}\")\n    print(f\"Embedding 2 shape: {emb2.shape}\")\n    \n    # Calculate similarity (dot product)\n    similarity = np.dot(emb1, emb2)\n    print(f\"Similarity between texts: {similarity:.4f}\") \n\n--- File Index 41: cookbook/pocketflow-chat-memory/utils/vector_index.py ---\nimport numpy as np\nimport faiss\n\ndef create_index(dimension=1536):\n    return faiss.IndexFlatL2(dimension)\n\ndef add_vector(index, vector):\n    # Make sure the vector is a numpy array with the right shape for FAISS\n    vector = np.array(vector).reshape(1, -1).astype(np.float32)\n    \n    # Add the vector to the index\n    index.add(vector)\n    \n    # Return the position (index.ntotal is the total number of vectors in the index)\n    return index.ntotal - 1\n\ndef search_vectors(index, query_vector, k=1):\n    \"\"\"Search for the k most similar vectors to the query vector\n    \n    Args:\n        index: The FAISS index\n        query_vector: The query vector (numpy array or list)\n        k: Number of results to return (default: 1)\n        \n    Returns:\n        tuple: (indices, distances) where:\n            - indices is a list of positions in the index\n            - distances is a list of the corresponding distances\n    \"\"\"\n    # Make sure we don't try to retrieve more vectors than exist in the index\n    k = min(k, index.ntotal)\n    if k == 0:\n        return [], []\n        \n    # Make sure the query is a numpy array with the right shape for FAISS\n    query_vector = np.array(query_vector).reshape(1, -1).astype(np.float32)\n    \n    # Search the index\n    distances, indices = index.search(query_vector, k)\n    \n    return indices[0].tolist(), distances[0].tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a new index\n    index = create_index(dimension=3)\n    \n    # Add some random vectors and track them separately\n    items = []\n    for i in range(5):\n        vector = np.random.random(3)\n        position = add_vector(index, vector)\n        items.append(f\"Item {i}\")\n        print(f\"Added vector at position {position}\")\n        \n    print(f\"Index contains {index.ntotal} vectors\")\n    \n    # Search for a similar vector\n    query = np.random.random(3)\n    indices, distances = search_vectors(index, query, k=2)\n    \n    print(\"Query:\", query)\n    print(\"Found indices:\", indices)\n    print(\"Distances:\", distances)\n    print(\"Retrieved items:\", [items[idx] for idx in indices]) \n\n--- File Index 42: cookbook/pocketflow-chat/README.md ---\n#  Simple PocketFlow Chat\n\nA basic chat application using PocketFlow with OpenAI's GPT-4o model.\n\n## Features\n\n- Conversational chat interface in the terminal\n- Maintains full conversation history for context\n- Simple implementation demonstrating PocketFlow's node and flow concepts\n\n## Run It\n\n1. Make sure your OpenAI API key is set:\n    ```bash\n    export OPENAI_API_KEY=\"your-api-key-here\"\n    ```\n    Alternatively, you can edit the `utils.py` file to include your API key directly.\n\n2. Install requirements and run the application:\n    ```bash\n    pip install -r requirements.txt\n    python main.py\n    ```\n\n## How It Works\n\n```mermaid\nflowchart LR\n    chat[ChatNode] -->|continue| chat\n```\n\nThe chat application uses:\n- A single `ChatNode` with a self-loop that:\n  - Takes user input in the `prep` method\n  - Sends the complete conversation history to GPT-4o\n  - Adds responses to the conversation history\n  - Loops back to continue the chat until the user types 'exit'\n\n\n## Files\n\n- [`main.py`](./main.py): Implementation of the ChatNode and chat flow\n- [`utils.py`](./utils.py): Simple wrapper for calling the OpenAI API\n \n\n--- File Index 43: cookbook/pocketflow-chat/main.py ---\nfrom pocketflow import Node, Flow\nfrom utils import call_llm\n\nclass ChatNode(Node):\n    def prep(self, shared):\n        # Initialize messages if this is the first run\n        if \"messages\" not in shared:\n            shared[\"messages\"] = []\n            print(\"Welcome to the chat! Type 'exit' to end the conversation.\")\n        \n        # Get user input\n        user_input = input(\"\\nYou: \")\n        \n        # Check if user wants to exit\n        if user_input.lower() == 'exit':\n            return None\n        \n        # Add user message to history\n        shared[\"messages\"].append({\"role\": \"user\", \"content\": user_input})\n        \n        # Return all messages for the LLM\n        return shared[\"messages\"]\n\n    def exec(self, messages):\n        if messages is None:\n            return None\n        \n        # Call LLM with the entire conversation history\n        response = call_llm(messages)\n        return response\n\n    def post(self, shared, prep_res, exec_res):\n        if prep_res is None or exec_res is None:\n            print(\"\\nGoodbye!\")\n            return None  # End the conversation\n        \n        # Print the assistant's response\n        print(f\"\\nAssistant: {exec_res}\")\n        \n        # Add assistant message to history\n        shared[\"messages\"].append({\"role\": \"assistant\", \"content\": exec_res})\n        \n        # Loop back to continue the conversation\n        return \"continue\"\n\n# Create the flow with self-loop\nchat_node = ChatNode()\nchat_node - \"continue\" >> chat_node  # Loop back to continue conversation\n\nflow = Flow(start=chat_node)\n\n# Start the chat\nif __name__ == \"__main__\":\n    shared = {}\n    flow.run(shared)\n\n\n--- File Index 44: cookbook/pocketflow-chat/utils.py ---\nfrom openai import OpenAI\nimport os\n\ndef call_llm(messages):\n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n    \n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=messages,\n        temperature=0.7\n    )\n    \n    return response.choices[0].message.content\n\nif __name__ == \"__main__\":\n    # Test the LLM call\n    messages = [{\"role\": \"user\", \"content\": \"In a few words, what's the meaning of life?\"}]\n    response = call_llm(messages)\n    print(f\"Prompt: {messages[0]['content']}\")\n    print(f\"Response: {response}\")\n\n\n\n--- File Index 45: cookbook/pocketflow-communication/README.md ---\n# PocketFlow Communication Example\n\nThis example demonstrates the [Communication](https://the-pocket.github.io/PocketFlow/communication.html) concept in PocketFlow, specifically focusing on the Shared Store pattern.\n\n## Overview\n\nThe example implements a simple word counter that shows how nodes can communicate using a shared store. It demonstrates:\n\n- How to initialize and structure a shared store\n- How nodes can read from and write to the shared store\n- How to maintain state across multiple node executions\n- Best practices for shared store usage\n\n## Project Structure\n\n```\npocketflow-communication/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 main.py\n\u251c\u2500\u2500 flow.py\n\u2514\u2500\u2500 nodes.py\n```\n\n## Installation\n\n```bash\npip install -r requirements.txt\n```\n\n## Usage\n\n```bash\npython main.py\n```\n\nEnter text when prompted. The program will:\n1. Count words in the text\n2. Store statistics in the shared store\n3. Display running statistics (total texts, total words, average)\n\nEnter 'q' to quit.\n\n## How it Works\n\nThe example uses three nodes:\n\n1. `TextInput`: Reads user input and initializes the shared store\n2. `WordCounter`: Counts words and updates statistics in the shared store\n3. `ShowStats`: Displays statistics from the shared store\n\nThis demonstrates how nodes can share and maintain state using the shared store pattern. \n\n--- File Index 46: cookbook/pocketflow-communication/flow.py ---\n\"\"\"Flow configuration for the communication example.\"\"\"\n\nfrom pocketflow import Flow\nfrom nodes import TextInput, WordCounter, ShowStats, EndNode\n\ndef create_flow():\n    \"\"\"Create and configure the flow with all nodes.\"\"\"\n    # Create nodes\n    text_input = TextInput()\n    word_counter = WordCounter()\n    show_stats = ShowStats()\n    end_node = EndNode()\n    \n    # Configure transitions\n    text_input - \"count\" >> word_counter\n    word_counter - \"show\" >> show_stats\n    show_stats - \"continue\" >> text_input\n    text_input - \"exit\" >> end_node\n    \n    # Create and return flow\n    return Flow(start=text_input) \n\n--- File Index 47: cookbook/pocketflow-communication/main.py ---\nfrom flow import create_flow\n\ndef main():\n    \"\"\"Run the communication example.\"\"\"\n    flow = create_flow()\n    shared = {}\n    flow.run(shared)\n\nif __name__ == \"__main__\":\n    main() \n\n--- File Index 48: cookbook/pocketflow-communication/nodes.py ---\n\"\"\"Node implementations for the communication example.\"\"\"\n\nfrom pocketflow import Node\n\nclass EndNode(Node):\n    \"\"\"Node that handles flow termination.\"\"\"\n    pass\n\nclass TextInput(Node):\n    \"\"\"Node that reads text input and initializes the shared store.\"\"\"\n    \n    def prep(self, shared):\n        \"\"\"Get user input and ensure shared store is initialized.\"\"\"\n        return input(\"Enter text (or 'q' to quit): \")\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Store text and initialize/update statistics.\"\"\"\n        if prep_res == 'q':\n            return \"exit\"\n        \n        # Store the text\n        shared[\"text\"] = prep_res\n        \n        # Initialize statistics if they don't exist\n        if \"stats\" not in shared:\n            shared[\"stats\"] = {\n                \"total_texts\": 0,\n                \"total_words\": 0\n            }\n        shared[\"stats\"][\"total_texts\"] += 1\n        \n        return \"count\"\n\nclass WordCounter(Node):\n    \"\"\"Node that counts words in the text.\"\"\"\n    \n    def prep(self, shared):\n        \"\"\"Get text from shared store.\"\"\"\n        return shared[\"text\"]\n    \n    def exec(self, text):\n        \"\"\"Count words in the text.\"\"\"\n        return len(text.split())\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Update word count statistics.\"\"\"\n        shared[\"stats\"][\"total_words\"] += exec_res\n        return \"show\"\n\nclass ShowStats(Node):\n    \"\"\"Node that displays statistics from the shared store.\"\"\"\n    \n    def prep(self, shared):\n        \"\"\"Get statistics from shared store.\"\"\"\n        return shared[\"stats\"]\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Display statistics and continue the flow.\"\"\"\n        stats = prep_res\n        print(f\"\\nStatistics:\")\n        print(f\"- Texts processed: {stats['total_texts']}\")\n        print(f\"- Total words: {stats['total_words']}\")\n        print(f\"- Average words per text: {stats['total_words'] / stats['total_texts']:.1f}\\n\")\n        return \"continue\" \n\n--- File Index 49: cookbook/pocketflow-flow/README.md ---\n# Text Converter Flow\n\nThis project demonstrates an interactive text transformation tool built with PocketFlow.\n\n## Features\n\n- Convert text to UPPERCASE\n- Convert text to lowercase\n- Reverse text\n- Remove extra spaces\n- Interactive command-line interface\n- Continuous flow with option to process multiple texts\n\n## Getting Started\n\n1. Install the required dependencies:\n\n```bash\npip install -r requirements.txt\n```\n\n2. Run the application:\n\n```bash\npython main.py\n```\n\n## How It Works\n\nThe workflow features an interactive loop with branching paths:\n\n```mermaid\ngraph TD\n    Input[TextInput Node] -->|transform| Transform[TextTransform Node]\n    Transform -->|input| Input\n    Transform -->|exit| End[End]\n    Input -->|exit| End\n```\n\nHere's what each part does:\n1. **TextInput Node**: Collects text input and handles menu choices\n2. **TextTransform Node**: Applies the selected transformation to the text\n\n## Example Output\n\n```\nWelcome to Text Converter!\n=========================\n\nEnter text to convert: Pocket Flow is a 100-line LLM framework\n\nChoose transformation:\n1. Convert to UPPERCASE\n2. Convert to lowercase\n3. Reverse text\n4. Remove extra spaces\n5. Exit\n\nYour choice (1-5): 1\n\nResult: POCKET FLOW IS A 100-LINE LLM FRAMEWORK\n\nConvert another text? (y/n): n\n\nThank you for using Text Converter!\n```\n\n## Files\n\n- [`main.py`](./main.py): Main entry point for running the text converter\n- [`flow.py`](./flow.py): Defines the nodes and flow for text transformation\n- [`requirements.txt`](./requirements.txt): Lists the required dependencies\n\n\n--- File Index 50: cookbook/pocketflow-flow/flow.py ---\nfrom pocketflow import Node, Flow\n\nclass TextInput(Node):\n    def prep(self, shared):\n        \"\"\"Get text input from user.\"\"\"\n        if \"text\" not in shared:\n            text = input(\"\\nEnter text to convert: \")\n            shared[\"text\"] = text\n        return shared[\"text\"]\n\n    def post(self, shared, prep_res, exec_res):\n        print(\"\\nChoose transformation:\")\n        print(\"1. Convert to UPPERCASE\")\n        print(\"2. Convert to lowercase\")\n        print(\"3. Reverse text\")\n        print(\"4. Remove extra spaces\")\n        print(\"5. Exit\")\n        \n        choice = input(\"\\nYour choice (1-5): \")\n        \n        if choice == \"5\":\n            return \"exit\"\n        \n        shared[\"choice\"] = choice\n        return \"transform\"\n\nclass TextTransform(Node):\n    def prep(self, shared):\n        return shared[\"text\"], shared[\"choice\"]\n    \n    def exec(self, inputs):\n        text, choice = inputs\n        \n        if choice == \"1\":\n            return text.upper()\n        elif choice == \"2\":\n            return text.lower()\n        elif choice == \"3\":\n            return text[::-1]\n        elif choice == \"4\":\n            return \" \".join(text.split())\n        else:\n            return \"Invalid option!\"\n    \n    def post(self, shared, prep_res, exec_res):\n        print(\"\\nResult:\", exec_res)\n        \n        if input(\"\\nConvert another text? (y/n): \").lower() == 'y':\n            shared.pop(\"text\", None)  # Remove previous text\n            return \"input\"\n        return \"exit\"\n\nclass EndNode(Node):\n    pass\n\n# Create nodes\ntext_input = TextInput()\ntext_transform = TextTransform()\nend_node = EndNode()\n\n# Connect nodes\ntext_input - \"transform\" >> text_transform\ntext_transform - \"input\" >> text_input\ntext_transform - \"exit\" >> end_node\n\n# Create flow\nflow = Flow(start=text_input) \n\n--- File Index 51: cookbook/pocketflow-flow/main.py ---\nfrom flow import flow\n\ndef main():\n    print(\"\\nWelcome to Text Converter!\")\n    print(\"=========================\")\n    \n    # Initialize shared store\n    shared = {}\n    \n    # Run the flow\n    flow.run(shared)\n    \n    print(\"\\nThank you for using Text Converter!\")\n\nif __name__ == \"__main__\":\n    main() \n\n--- File Index 52: cookbook/pocketflow-hello-world/README.md ---\n# PocketFlow Hello World\n\nYour first PocketFlow application! This simple example demonstrates how to create a basic PocketFlow app from scratch.\n\n## Project Structure\n\n```\n.\n\u251c\u2500\u2500 docs/          # Documentation files\n\u251c\u2500\u2500 utils/         # Utility functions\n\u251c\u2500\u2500 flow.py        # PocketFlow implementation\n\u251c\u2500\u2500 main.py        # Main application entry point\n\u2514\u2500\u2500 README.md      # Project documentation\n```\n\n## Setup\n\n1. Create a virtual environment:\n```bash\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n```\n\n2. Install dependencies:\n```bash\npip install -r requirements.txt\n```\n\n3. Run the example:\n```bash\npython main.py\n```\n\n## What This Example Demonstrates\n\n- How to create your first PocketFlow application\n- Basic PocketFlow concepts and usage\n- Simple example of PocketFlow's capabilities\n\n## Additional Resources\n\n- [PocketFlow Documentation](https://the-pocket.github.io/PocketFlow/) \n\n--- File Index 53: cookbook/pocketflow-hello-world/docs/design.md ---\n# Your Project Title\n\n## Project Requirements\nA description of the project requirements. \n\n## Utility Functions\n\n1. **Call LLM** (`utils/call_llm.py`)\n\n## Flow Design\n\n1. **First Node**\n2. **Second Node**\n3. **Third Node**\n\n### Flow Diagram\n\n```mermaid\nflowchart TD\n    firstNode[First Node] --> secondNode[Second Node]\n    secondNode --> thirdNode[Third Node]\n```\n\n## Data Structure\n\nThe shared memory structure will be organized as follows:\n\n```python\nshared = {\n    \"key\": \"value\"\n}\n```\n\n## Node Designs\n\n### 1. First Node\n- **Purpose**: What the node does\n- **Design**: Regular Node (no Batch/Async)\n- **Data Access**: \n  - Read: \"key\" from shared store\n  - Write: \"key\" to shared store\n\n### 2. Second Node\n...\n\n### 3. Third Node\n\n\n--- File Index 54: cookbook/pocketflow-hello-world/flow.py ---\nfrom pocketflow import Node, Flow\nfrom utils.call_llm import call_llm\n\n# An example node and flow\n# Please replace this with your own node and flow\nclass AnswerNode(Node):\n    def prep(self, shared):\n        # Read question from shared\n        return shared[\"question\"]\n    \n    def exec(self, question):\n        return call_llm(question)\n    \n    def post(self, shared, prep_res, exec_res):\n        # Store the answer in shared\n        shared[\"answer\"] = exec_res\n\nanswer_node = AnswerNode()\nqa_flow = Flow(start=answer_node)\n\n--- File Index 55: cookbook/pocketflow-hello-world/main.py ---\nfrom flow import qa_flow\n\n# Example main function\n# Please replace this with your own main function\ndef main():\n    shared = {\n        \"question\": \"In one sentence, what's the end of universe?\",\n        \"answer\": None\n    }\n\n    qa_flow.run(shared)\n    print(\"Question:\", shared[\"question\"])\n    print(\"Answer:\", shared[\"answer\"])\n\nif __name__ == \"__main__\":\n    main()\n\n--- File Index 56: cookbook/pocketflow-hello-world/utils/__init__.py ---\n\n\n--- File Index 57: cookbook/pocketflow-hello-world/utils/call_llm.py ---\nfrom openai import OpenAI\n\ndef call_llm(prompt):    \n    client = OpenAI(api_key=\"YOUR_API_KEY_HERE\")\n    r = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return r.choices[0].message.content\n    \nif __name__ == \"__main__\":\n    prompt = \"What is the meaning of life?\"\n    print(call_llm(prompt))\n\n--- File Index 58: cookbook/pocketflow-llm-streaming/README.md ---\n#  LLM Streaming and Interruption\n\nDemonstrates real-time LLM response streaming with user interrupt capability.\n\n## Features\n\n- Real-time display of LLM responses as they're generated\n- User interrupt with ENTER key at any time\n\n## Run It\n\n```bash\npip install -r requirements.txt\npython main.py\n```\n\n## How It Works\n\nStreamNode:\n1. Creates interrupt listener thread\n2. Fetches content chunks from LLM\n3. Displays chunks in real-time\n4. Handles user interruption\n\n## API Key\n\nBy default, demo uses fake streaming responses. To use real OpenAI streaming:\n\n1. Edit main.py to replace the fake_stream_llm with stream_llm:\n```python\n# Change this line:\nchunks = fake_stream_llm(prompt)\n# To this:\nchunks = stream_llm(prompt)\n```\n\n2. Make sure your OpenAI API key is set:\n```bash\nexport OPENAI_API_KEY=\"your-api-key-here\"\n```\n\n## Files\n\n- `main.py`: StreamNode implementation\n- `utils.py`: Real and fake LLM streaming functions\n \n\n--- File Index 59: cookbook/pocketflow-llm-streaming/main.py ---\nimport time\nimport threading\nfrom pocketflow import Node, Flow\nfrom utils import fake_stream_llm, stream_llm\n\nclass StreamNode(Node):\n    def prep(self, shared):\n        # Create interrupt event\n        interrupt_event = threading.Event()\n\n        # Start a thread to listen for user interrupt\n        def wait_for_interrupt():\n            input(\"Press ENTER at any time to interrupt streaming...\\n\")\n            interrupt_event.set()\n        listener_thread = threading.Thread(target=wait_for_interrupt)\n        listener_thread.start()\n        \n        # Get prompt from shared store\n        prompt = shared[\"prompt\"]\n        # Get chunks from LLM function\n        chunks = fake_stream_llm(prompt)\n        return chunks, interrupt_event, listener_thread\n\n    def exec(self, prep_res):\n        chunks, interrupt_event, listener_thread = prep_res\n        for chunk in chunks:\n            if interrupt_event.is_set():\n                print(\"User interrupted streaming.\")\n                break\n            \n            if hasattr(chunk.choices[0].delta, 'content') and chunk.choices[0].delta.content is not None:\n                chunk_content = chunk.choices[0].delta.content\n                print(chunk_content, end=\"\", flush=True)\n                time.sleep(0.1)  # simulate latency\n        return interrupt_event, listener_thread\n\n    def post(self, shared, prep_res, exec_res):\n        interrupt_event, listener_thread = exec_res\n        # Join the interrupt listener so it doesn't linger\n        interrupt_event.set()\n        listener_thread.join()\n        return \"default\"\n\n# Usage:\nnode = StreamNode()\nflow = Flow(start=node)\n\nshared = {\"prompt\": \"What's the meaning of life?\"}\nflow.run(shared)\n\n\n--- File Index 60: cookbook/pocketflow-llm-streaming/utils.py ---\nfrom openai import OpenAI\nimport os\n\ndef stream_llm(prompt):\n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n\n    # Make a streaming chat completion request\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        temperature=0.7,\n        stream=True  # Enable streaming\n    )\n    return response\n\ndef fake_stream_llm(prompt, predefined_text=\"This is a fake response. Today is a sunny day. The sun is shining. The birds are singing. The flowers are blooming. The bees are buzzing. The wind is blowing. The clouds are drifting. The sky is blue. The grass is green. The trees are tall. The water is clear. The fish are swimming. The sun is shining. The birds are singing. The flowers are blooming. The bees are buzzing. The wind is blowing. The clouds are drifting. The sky is blue. The grass is green. The trees are tall. The water is clear. The fish are swimming.\"):\n    \"\"\"\n    Returns a list of simple objects that mimic the structure needed\n    for OpenAI streaming responses.\n    \"\"\"\n    # Split text into small chunks\n    chunk_size = 10\n    chunks = []\n    \n    # Create the chunks using a simple class outside the nested structure\n    class SimpleObject:\n        def __init__(self, **kwargs):\n            for key, value in kwargs.items():\n                setattr(self, key, value)\n    \n    # Build the chunks\n    for i in range(0, len(predefined_text), chunk_size):\n        text_chunk = predefined_text[i:i+chunk_size]\n        \n        # Create the nested structure using simple objects\n        delta = SimpleObject(content=text_chunk)\n        choice = SimpleObject(delta=delta)\n        chunk = SimpleObject(choices=[choice])\n        \n        chunks.append(chunk)\n    \n    return chunks\n\nif __name__ == \"__main__\":\n    print(\"## Testing streaming LLM\")\n    prompt = \"What's the meaning of life?\"\n    print(f\"## Prompt: {prompt}\")\n    # response = fake_stream_llm(prompt)\n    response = stream_llm(prompt)\n    print(f\"## Response: \")\n    for chunk in response:\n        if hasattr(chunk.choices[0].delta, 'content') and chunk.choices[0].delta.content is not None:\n            chunk_content = chunk.choices[0].delta.content\n            # Print the incoming text without a newline (simulate real-time streaming)\n            print(chunk_content, end=\"\", flush=True)\n\n\n\n--- File Index 61: cookbook/pocketflow-majority-vote/README.md ---\n# Majority Vote Reasoning\n\nThis project demonstrates a majority vote implementation that enables LLMs to solve complex reasoning problems by aggregating multiple independent attempts. It's designed to improve problem-solving accuracy through consensus-based reasoning.\n\n## Features\n\n- Improves model reliability on complex problems through multiple attempts\n- Works with models like Claude 3.7 Sonnet\n- Solves problems that single attempts often fail on\n- Provides detailed reasoning traces for verification\n- Uses a consensus approach to reduce the impact of occasional reasoning errors\n\n## Getting Started\n\n1. Install the required packages:\n```bash\npip install -r requirements.txt\n```\n\n2. Set up your API key:\n```bash\nexport ANTHROPIC_API_KEY=\"your-api-key-here\"\n```\n\n3. Run a test problem to see majority voting in action:\n```bash\npython main.py\n```\n\n4. Try your own reasoning problem:\n```bash\npython main.py --problem \"Your complex reasoning problem here\" --tries 5\n```\n\n## How It Works\n\nThe implementation uses a MajorityVoteNode that processes multiple attempts and finds consensus:\n\n```mermaid\nflowchart LR\n    mv[MajorityVoteNode] \n```\n\nThe MajorityVoteNode:\n1. Makes multiple independent attempts to solve the same problem\n2. Collects structured answers from each attempt\n3. Determines the most frequent answer as the final solution\n4. Returns the consensus answer\n\nThis approach helps overcome occasional reasoning errors that might occur in individual attempts.\n\n## Example Problem\n\nExample Problem from [Quant Interview](https://www.youtube.com/watch?v=SCP7JptxPU0):\n\n```\nYou work at a shoe factory. In front of you, there are three pairs of shoes (six individual shoes) with the following sizes: two size 4s, two size 5s, and two size 6s. The factory defines an \"acceptable pair\" as two shoes that differ in size by a maximum of one size (e.g., a size 5 and a size 6 would be an acceptable pair). If you close your eyes and randomly pick three pairs of shoes without replacement, what is the probability that you end up drawing three acceptable pairs?\n```\n\nBelow is an example of how the majority vote approach uses Claude 3.7 Sonnet to solve this complex problem:\n\n```\n========================\nAll structured answers: ['0.333', '0.333', '0.333', '0.6', '0.333']\nMajority vote => 0.333\nFrequency => 4\n========================\n\n=== Final Answer ===\n0.333\n====================\n```\n\nThis shows that 4 out of 5 attempts yielded the same answer (0.333), which is chosen as the final solution.\n\n## Files\n\n- [`main.py`](./main.py): Implementation of the majority vote node and flow\n- [`utils.py`](./utils.py): Simple wrapper for calling the Anthropic model\n\n--- File Index 62: cookbook/pocketflow-majority-vote/main.py ---\nimport argparse\nfrom pocketflow import BatchNode, Flow\nimport collections\nfrom utils import call_llm\nimport yaml\n\nclass MajorityVoteNode(BatchNode):\n    def prep(self, shared):\n        question = shared.get(\"question\", \"(No question provided)\")\n        attempts_count = shared.get(\"num_tries\", 3)\n        return [question for _ in range(attempts_count)]\n\n    def exec(self, single_question: str):\n        prompt = f\"\"\"\nYou are a helpful assistant. Please answer the user's question below.\nQuestion: {single_question}\n\nReturn strictly using the following YAML structure:\n```yaml\nthinking: |\n    (Your thinking process here)\nanswer: 0.123 # Final answer as a decimal with 3 decimal places\n```\"\"\"\n        raw_response = call_llm(prompt)\n        yaml_part = raw_response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n        parsed = yaml.safe_load(yaml_part)\n\n        # Validate we have at least 'answer' field\n        if not isinstance(parsed, dict) or 'answer' not in parsed:\n            raise RuntimeError(f\"Missing 'answer' in YAML: {parsed}\")\n\n        # Return only the 'answer' field for the majority vote.\n        return str(parsed['answer'])\n    \n    def exec_fallback(self, prep_res, exc):\n        return None\n\n    def post(self, shared, prep_res, exec_res_list):\n        # Count frequency for non-None answers\n        exec_res_list = [res for res in exec_res_list if res is not None]\n        counter = collections.Counter(exec_res_list)\n        best_answer, freq = counter.most_common(1)[0]\n\n        # Store final\n        shared[\"majority_answer\"] = best_answer\n\n        print(\"========================\")\n        print(\"All structured answers:\", exec_res_list)\n        print(\"Majority vote =>\", best_answer)\n        print(\"Frequency =>\", freq)\n        print(\"========================\")\n\n        # End the flow\n        return \"end\"\n\nif __name__ == \"__main__\":\n    # Set up argument parser\n    parser = argparse.ArgumentParser(description=\"Run majority vote reasoning on a problem\")\n    parser.add_argument(\"--problem\", type=str, help=\"Your reasoning problem to solve\")\n    parser.add_argument(\"--tries\", type=int, default=5, help=\"Number of attempts to make (default: 5)\")\n    args = parser.parse_args()\n    \n    # Default problem if none provided\n    default_problem = \"\"\"You work at a shoe factory. In front of you, there are three pairs of shoes (six individual shoes) with the following sizes: two size 4s, two size 5s, and two size 6s. The factory defines an \"acceptable pair\" as two shoes that differ in size by a maximum of one size (e.g., a size 5 and a size 6 would be an acceptable pair). If you close your eyes and randomly pick three pairs of shoes without replacement, what is the probability that you end up drawing three acceptable pairs?\"\"\"\n    \n    shared = {\n        \"question\": args.problem if args.problem else default_problem,\n        \"num_tries\": args.tries\n    }\n\n    majority_node = MajorityVoteNode()\n    flow = Flow(start=majority_node)\n    flow.run(shared)\n\n    print(\"\\n=== Final Answer ===\")\n    print(shared[\"majority_answer\"])\n    print(\"====================\")\n\n--- File Index 63: cookbook/pocketflow-majority-vote/utils.py ---\nfrom anthropic import Anthropic\nimport os\n\ndef call_llm(prompt):\n    client = Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\", \"your-api-key\"))\n    response = client.messages.create(\n        model=\"claude-3-7-sonnet-20250219\",\n        max_tokens=10000,\n        messages=[\n            {\"role\": \"user\", \"content\": prompt}\n        ]\n    )\n    return response.content[0].text\n\nif __name__ == \"__main__\":\n    print(\"## Testing call_llm\")\n    prompt = \"In a few words, what is the meaning of life?\"\n    print(f\"## Prompt: {prompt}\")\n    response = call_llm(prompt)\n    print(f\"## Response: {response}\")\n\n--- File Index 64: cookbook/pocketflow-map-reduce/README.md ---\n# Resume Qualification - Map Reduce Example\n\nA PocketFlow example that demonstrates how to implement a Map-Reduce pattern for processing and evaluating resumes.\n\n## Features\n\n- Read and process multiple resume files using a Map-Reduce pattern\n- Evaluate each resume individually using an LLM with structured YAML output\n- Determine if candidates qualify for technical roles based on specific criteria\n- Aggregate results to generate qualification statistics and summaries\n\n## Getting Started\n\n1. Install the required dependencies:\n\n```bash\npip install -r requirements.txt\n```\n\n2. Set your OpenAI API key as an environment variable:\n\n```bash\nexport OPENAI_API_KEY=your_api_key_here\n```\n\n3. Run the application:\n\n```bash\npython main.py\n```\n\n## How It Works\n\nThe workflow follows a classic Map-Reduce pattern with three sequential nodes:\n\n```mermaid\nflowchart LR\n    ReadResumes[Map: Read Resumese] --> EvaluateResumes[Batch: Evaluate Resumes]\n    EvaluateResumes --> ReduceResults[Reduce: Aggregate Results]\n```\n\nHere's what each node does:\n\n1. **ReadResumesNode (Map Phase)**: Reads all resume files from the data directory and stores them in the shared data store\n2. **EvaluateResumesNode (Batch Processing)**: Processes each resume individually using an LLM to determine if candidates qualify\n3. **ReduceResultsNode (Reduce Phase)**: Aggregates evaluation results and produces a summary of qualified candidates\n\n## Files\n\n- [`main.py`](./main.py): Main entry point for running the resume qualification workflow\n- [`flow.py`](./flow.py): Defines the flow that connects the nodes\n- [`nodes.py`](./nodes.py): Contains the node classes for each step in the workflow\n- [`utils.py`](./utils.py): Utility functions including the LLM wrapper\n- [`requirements.txt`](./requirements.txt): Lists the required dependencies\n- [`data/`](./data/): Directory containing sample resume files for evaluation\n\n## Example Output\n\n```\nStarting resume qualification processing...\n\n===== Resume Qualification Summary =====\nTotal candidates evaluated: 5\nQualified candidates: 2 (40.0%)\n\nQualified candidates:\n- Emily Johnson\n- John Smith\n\nDetailed evaluation results:\n\u2717 Michael Williams (resume3.txt)\n\u2713 Emily Johnson (resume2.txt)\n\u2717 Lisa Chen (resume4.txt)\n\u2717 Robert Taylor (resume5.txt)\n\u2713 John Smith (resume1.txt)\n\nResume processing complete!\n```\n\n--- File Index 65: cookbook/pocketflow-map-reduce/flow.py ---\nfrom pocketflow import Flow\nfrom nodes import ReadResumesNode, EvaluateResumesNode, ReduceResultsNode\n\ndef create_resume_processing_flow():\n    \"\"\"Create a map-reduce flow for processing resumes.\"\"\"\n    # Create nodes\n    read_resumes_node = ReadResumesNode()\n    evaluate_resumes_node = EvaluateResumesNode()\n    reduce_results_node = ReduceResultsNode()\n    \n    # Connect nodes\n    read_resumes_node >> evaluate_resumes_node >> reduce_results_node\n    \n    # Create flow\n    return Flow(start=read_resumes_node)\n\n--- File Index 66: cookbook/pocketflow-map-reduce/main.py ---\nfrom flow import create_resume_processing_flow\n\ndef main():\n    # Initialize shared store\n    shared = {}\n    \n    # Create the resume processing flow\n    resume_flow = create_resume_processing_flow()\n    \n    # Run the flow\n    print(\"Starting resume qualification processing...\")\n    resume_flow.run(shared)\n    \n    # Display final summary information (additional to what's already printed in ReduceResultsNode)\n    if \"summary\" in shared:\n        print(\"\\nDetailed evaluation results:\")\n        for filename, evaluation in shared.get(\"evaluations\", {}).items():\n            qualified = \"\u2713\" if evaluation.get(\"qualifies\", False) else \"\u2717\"\n            name = evaluation.get(\"candidate_name\", \"Unknown\")\n            print(f\"{qualified} {name} ({filename})\")\n    \n    print(\"\\nResume processing complete!\")\n\nif __name__ == \"__main__\":\n    main()\n\n--- File Index 67: cookbook/pocketflow-map-reduce/nodes.py ---\nfrom pocketflow import Node, BatchNode\nfrom utils import call_llm\nimport yaml\nimport os\n\nclass ReadResumesNode(Node):\n    \"\"\"Map phase: Read all resumes from the data directory into shared storage.\"\"\"\n    \n    def exec(self, _):\n        resume_files = {}\n        data_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"data\")\n        \n        for filename in os.listdir(data_dir):\n            if filename.endswith(\".txt\"):\n                file_path = os.path.join(data_dir, filename)\n                with open(file_path, 'r', encoding='utf-8') as file:\n                    resume_files[filename] = file.read()\n        \n        return resume_files\n    \n    def post(self, shared, prep_res, exec_res):\n        shared[\"resumes\"] = exec_res\n        return \"default\"\n\n\nclass EvaluateResumesNode(BatchNode):\n    \"\"\"Batch processing: Evaluate each resume to determine if the candidate qualifies.\"\"\"\n    \n    def prep(self, shared):\n        return list(shared[\"resumes\"].items())\n    \n    def exec(self, resume_item):\n        \"\"\"Evaluate a single resume.\"\"\"\n        filename, content = resume_item\n        \n        prompt = f\"\"\"\nEvaluate the following resume and determine if the candidate qualifies for an advanced technical role.\nCriteria for qualification:\n- At least a bachelor's degree in a relevant field\n- At least 3 years of relevant work experience\n- Strong technical skills relevant to the position\n\nResume:\n{content}\n\nReturn your evaluation in YAML format:\n```yaml\ncandidate_name: [Name of the candidate]\nqualifies: [true/false]\nreasons:\n  - [First reason for qualification/disqualification]\n  - [Second reason, if applicable]\n```\n\"\"\"\n        response = call_llm(prompt)\n        \n        # Extract YAML content\n        yaml_content = response.split(\"```yaml\")[1].split(\"```\")[0].strip() if \"```yaml\" in response else response\n        result = yaml.safe_load(yaml_content)\n        \n        return (filename, result)\n\n    def post(self, shared, prep_res, exec_res_list):\n        shared[\"evaluations\"] = {filename: result for filename, result in exec_res_list}\n        return \"default\"\n\n\nclass ReduceResultsNode(Node):\n    \"\"\"Reduce node: Count and print out how many candidates qualify.\"\"\"\n    \n    def prep(self, shared):\n        return shared[\"evaluations\"]\n    \n    def exec(self, evaluations):\n        qualified_count = 0\n        total_count = len(evaluations)\n        qualified_candidates = []\n        \n        for filename, evaluation in evaluations.items():\n            if evaluation.get(\"qualifies\", False):\n                qualified_count += 1\n                qualified_candidates.append(evaluation.get(\"candidate_name\", \"Unknown\"))\n        \n        summary = {\n            \"total_candidates\": total_count,\n            \"qualified_count\": qualified_count,\n            \"qualified_percentage\": round(qualified_count / total_count * 100, 1) if total_count > 0 else 0,\n            \"qualified_names\": qualified_candidates\n        }\n        \n        return summary\n    \n    def post(self, shared, prep_res, exec_res):\n        shared[\"summary\"] = exec_res\n        \n        print(\"\\n===== Resume Qualification Summary =====\")\n        print(f\"Total candidates evaluated: {exec_res['total_candidates']}\")\n        print(f\"Qualified candidates: {exec_res['qualified_count']} ({exec_res['qualified_percentage']}%)\")\n        \n        if exec_res['qualified_names']:\n            print(\"\\nQualified candidates:\")\n            for name in exec_res['qualified_names']:\n                print(f\"- {name}\")\n        \n        return \"default\" \n\n--- File Index 68: cookbook/pocketflow-map-reduce/utils.py ---\nimport os\nfrom openai import OpenAI\n\ndef call_llm(prompt):    \n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n    r = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return r.choices[0].message.content\n\n# Example usage\nif __name__ == \"__main__\":\n    print(call_llm(\"Tell me a short joke\")) \n\n--- File Index 69: cookbook/pocketflow-mcp/README.md ---\n# PocketFlow MCP Demo\n\nThis project shows how to build an agent that performs addition using PocketFlow and Model Context Protocol (MCP). It presents a comparison between using MCP and basic function calling approaches.\n\nThis implementation is based  on the tutorial: [MCP Simply Explained: Function Calling Rebranded or Genuine Breakthrough?](https://zacharyhuang.substack.com/p/mcp-simply-explained-function-calling)\n\n## Features\n\n- Mathematical operation tools through a simple terminal interface\n- Integration with Model Context Protocol (MCP)\n- Comparison between MCP and direct function calling\n- **Simple toggle** between MCP and local function calling\n\n## How to Run\n\n1. Set your API key:\n   ```bash\n   export OPENAI_API_KEY=\"your-api-key-here\"\n   ```\n   Or update it directly in `utils.py`\n\n2. Install and run:\n   ```bash\n   pip install -r requirements.txt\n   python main.py\n   ```\n\n## MCP vs Function Calling\n\nTo compare both approaches, this demo provides local function alternatives that don't require MCP:\n\n- **Toggle with a simple flag:** Set `MCP = True` or `MCP = False` at the top of `utils.py` to switch between MCP and local implementations.\n- No code changes needed! The application automatically uses either:\n  - MCP server tools when `MCP = True`\n  - Local function implementations when `MCP = False`\n\nThis allows you to see the difference between the two approaches while keeping the same workflow.\n\n### Function Calling\n- Functions are directly embedded in application code\n- Each new tool requires modifying the application\n- Tools are defined within the application itself\n\n### MCP Approach\n- Tools live in separate MCP servers\n- Standard protocol for all tool interactions\n- New tools can be added without changing the agent\n- AI can interact with tools through a consistent interface\n\n## How It Works\n\n```mermaid\nflowchart LR\n    tools[GetToolsNode] -->|decide| decide[DecideToolNode]\n    decide -->|execute| execute[ExecuteToolNode]\n```\n\nThe agent uses PocketFlow to create a workflow where:\n1. It takes user input about numbers\n2. Connects to the MCP server for mathematical operations (or uses local functions based on the `MCP` flag)\n3. Returns the result\n\n## Files\n\n- [`main.py`](./main.py): Implementation of the addition agent using PocketFlow\n- [`utils.py`](./utils.py): Helper functions for API calls and MCP integration\n- [`simple_server.py`](./simple_server.py): MCP server that provides the addition tool\n\n\n--- File Index 70: cookbook/pocketflow-mcp/main.py ---\nfrom pocketflow import Node, Flow\nfrom utils import call_llm, get_tools, call_tool\nimport yaml\nimport sys\n\nclass GetToolsNode(Node):\n    def prep(self, shared):\n        \"\"\"Initialize and get tools\"\"\"\n        # The question is now passed from main via shared\n        print(\"\ud83d\udd0d Getting available tools...\")\n        return \"simple_server.py\"\n\n    def exec(self, server_path):\n        \"\"\"Retrieve tools from the MCP server\"\"\"\n        tools = get_tools(server_path)\n        return tools\n\n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Store tools and process to decision node\"\"\"\n        tools = exec_res\n        shared[\"tools\"] = tools\n        \n        # Format tool information for later use\n        tool_info = []\n        for i, tool in enumerate(tools, 1):\n            properties = tool.inputSchema.get('properties', {})\n            required = tool.inputSchema.get('required', [])\n            \n            params = []\n            for param_name, param_info in properties.items():\n                param_type = param_info.get('type', 'unknown')\n                req_status = \"(Required)\" if param_name in required else \"(Optional)\"\n                params.append(f\"    - {param_name} ({param_type}): {req_status}\")\n            \n            tool_info.append(f\"[{i}] {tool.name}\\n  Description: {tool.description}\\n  Parameters:\\n\" + \"\\n\".join(params))\n        \n        shared[\"tool_info\"] = \"\\n\".join(tool_info)\n        return \"decide\"\n\nclass DecideToolNode(Node):\n    def prep(self, shared):\n        \"\"\"Prepare the prompt for LLM to process the question\"\"\"\n        tool_info = shared[\"tool_info\"]\n        question = shared[\"question\"]\n        \n        prompt = f\"\"\"\n### CONTEXT\nYou are an assistant that can use tools via Model Context Protocol (MCP).\n\n### ACTION SPACE\n{tool_info}\n\n### TASK\nAnswer this question: \"{question}\"\n\n## NEXT ACTION\nAnalyze the question, extract any numbers or parameters, and decide which tool to use.\nReturn your response in this format:\n\n```yaml\nthinking: |\n    <your step-by-step reasoning about what the question is asking and what numbers to extract>\ntool: <name of the tool to use>\nreason: <why you chose this tool>\nparameters:\n    <parameter_name>: <parameter_value>\n    <parameter_name>: <parameter_value>\n```\nIMPORTANT: \n1. Extract numbers from the question properly\n2. Use proper indentation (4 spaces) for multi-line fields\n3. Use the | character for multi-line text fields\n\"\"\"\n        return prompt\n\n    def exec(self, prompt):\n        \"\"\"Call LLM to process the question and decide which tool to use\"\"\"\n        print(\"\ud83e\udd14 Analyzing question and deciding which tool to use...\")\n        response = call_llm(prompt)\n        return response\n\n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Extract decision from YAML and save to shared context\"\"\"\n        try:\n            yaml_str = exec_res.split(\"```yaml\")[1].split(\"```\")[0].strip()\n            decision = yaml.safe_load(yaml_str)\n            \n            shared[\"tool_name\"] = decision[\"tool\"]\n            shared[\"parameters\"] = decision[\"parameters\"]\n            shared[\"thinking\"] = decision.get(\"thinking\", \"\")\n            \n            print(f\"\ud83d\udca1 Selected tool: {decision['tool']}\")\n            print(f\"\ud83d\udd22 Extracted parameters: {decision['parameters']}\")\n            \n            return \"execute\"\n        except Exception as e:\n            print(f\"\u274c Error parsing LLM response: {e}\")\n            print(\"Raw response:\", exec_res)\n            return None\n\nclass ExecuteToolNode(Node):\n    def prep(self, shared):\n        \"\"\"Prepare tool execution parameters\"\"\"\n        return shared[\"tool_name\"], shared[\"parameters\"]\n\n    def exec(self, inputs):\n        \"\"\"Execute the chosen tool\"\"\"\n        tool_name, parameters = inputs\n        print(f\"\ud83d\udd27 Executing tool '{tool_name}' with parameters: {parameters}\")\n        result = call_tool(\"simple_server.py\", tool_name, parameters)\n        return result\n\n    def post(self, shared, prep_res, exec_res):\n        print(f\"\\n\u2705 Final Answer: {exec_res}\")\n        return \"done\"\n\n\nif __name__ == \"__main__\":\n    # Default question\n    default_question = \"What is 982713504867129384651 plus 73916582047365810293746529?\"\n    \n    # Get question from command line if provided with --\n    question = default_question\n    for arg in sys.argv[1:]:\n        if arg.startswith(\"--\"):\n            question = arg[2:]\n            break\n    \n    print(f\"\ud83e\udd14 Processing question: {question}\")\n    \n    # Create nodes\n    get_tools_node = GetToolsNode()\n    decide_node = DecideToolNode()\n    execute_node = ExecuteToolNode()\n    \n    # Connect nodes\n    get_tools_node - \"decide\" >> decide_node\n    decide_node - \"execute\" >> execute_node\n    \n    # Create and run flow\n    flow = Flow(start=get_tools_node)\n    shared = {\"question\": question}\n    flow.run(shared)\n\n--- File Index 71: cookbook/pocketflow-mcp/simple_server.py ---\nfrom fastmcp import FastMCP\n\n# Create a named server\nmcp = FastMCP(\"Math Operations Server\")\n\n# Define mathematical operation tools\n@mcp.tool()\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two numbers together\"\"\"\n    return a + b\n\n@mcp.tool()\ndef subtract(a: int, b: int) -> int:\n    \"\"\"Subtract b from a\"\"\"\n    return a - b\n\n@mcp.tool()\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiply two numbers together\"\"\"\n    return a * b\n\n@mcp.tool()\ndef divide(a: int, b: int) -> float:\n    \"\"\"Divide a by b\"\"\"\n    if b == 0:\n        raise ValueError(\"Division by zero is not allowed\")\n    return a / b\n\n# Start the server\nif __name__ == \"__main__\":\n    mcp.run()\n\n--- File Index 72: cookbook/pocketflow-mcp/utils.py ---\nfrom openai import OpenAI\nimport os\nimport asyncio\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\n# Global flag to control whether to use MCP or local implementation\nMCP = False\n\ndef call_llm(prompt):    \n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n    r = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return r.choices[0].message.content\n\ndef get_tools(server_script_path=None):\n    \"\"\"Get available tools, either from MCP server or locally based on MCP global setting.\"\"\"\n    if MCP:\n        return mcp_get_tools(server_script_path)\n    else:\n        return local_get_tools(server_script_path)\n    \ndef mcp_get_tools(server_script_path):\n    \"\"\"Get available tools from an MCP server.\n    \"\"\"\n    async def _get_tools():\n        server_params = StdioServerParameters(\n            command=\"python\",\n            args=[server_script_path]\n        )\n        \n        async with stdio_client(server_params) as (read, write):\n            async with ClientSession(read, write) as session:\n                await session.initialize()\n                tools_response = await session.list_tools()\n                return tools_response.tools\n    \n    return asyncio.run(_get_tools())\n\ndef local_get_tools(server_script_path=None):\n    \"\"\"A simple dummy implementation of get_tools without MCP.\"\"\"\n    tools = [\n        {\n            \"name\": \"add\",\n            \"description\": \"Add two numbers together\",\n            \"inputSchema\": {\n                \"properties\": {\n                    \"a\": {\"type\": \"integer\"},\n                    \"b\": {\"type\": \"integer\"}\n                },\n                \"required\": [\"a\", \"b\"]\n            }\n        },\n        {\n            \"name\": \"subtract\",\n            \"description\": \"Subtract b from a\",\n            \"inputSchema\": {\n                \"properties\": {\n                    \"a\": {\"type\": \"integer\"},\n                    \"b\": {\"type\": \"integer\"}\n                },\n                \"required\": [\"a\", \"b\"]\n            }\n        },\n        {\n            \"name\": \"multiply\",\n            \"description\": \"Multiply two numbers together\",\n            \"inputSchema\": {\n                \"properties\": {\n                    \"a\": {\"type\": \"integer\"},\n                    \"b\": {\"type\": \"integer\"}\n                },\n                \"required\": [\"a\", \"b\"]\n            }\n        },\n        {\n            \"name\": \"divide\",\n            \"description\": \"Divide a by b\",\n            \"inputSchema\": {\n                \"properties\": {\n                    \"a\": {\"type\": \"integer\"},\n                    \"b\": {\"type\": \"integer\"}\n                },\n                \"required\": [\"a\", \"b\"]\n            }\n        }\n    ]\n\n    class DictObject(dict):\n        \"\"\"A simple class that behaves both as a dictionary and as an object with attributes.\"\"\"\n        def __init__(self, data):\n            super().__init__(data)\n            for key, value in data.items():\n                if isinstance(value, dict):\n                    self[key] = DictObject(value)\n                elif isinstance(value, list) and value and isinstance(value[0], dict):\n                    self[key] = [DictObject(item) for item in value]\n        \n        def __getattr__(self, key):\n            try:\n                return self[key]\n            except KeyError:\n                raise AttributeError(f\"'DictObject' object has no attribute '{key}'\")\n\n    return [DictObject(tool) for tool in tools]\n\ndef call_tool(server_script_path=None, tool_name=None, arguments=None):\n    \"\"\"Call a tool, either from MCP server or locally based on MCP global setting.\"\"\"\n    if MCP:\n        return mcp_call_tool(server_script_path, tool_name, arguments)\n    else:\n        return local_call_tool(server_script_path, tool_name, arguments)\n    \ndef mcp_call_tool(server_script_path=None, tool_name=None, arguments=None):\n    \"\"\"Call a tool on an MCP server.\n    \"\"\"\n    async def _call_tool():\n        server_params = StdioServerParameters(\n            command=\"python\",\n            args=[server_script_path]\n        )\n        \n        async with stdio_client(server_params) as (read, write):\n            async with ClientSession(read, write) as session:\n                await session.initialize()\n                result = await session.call_tool(tool_name, arguments)\n                return result.content[0].text\n    \n    return asyncio.run(_call_tool())\n\ndef local_call_tool(server_script_path=None, tool_name=None, arguments=None):\n    \"\"\"A simple dummy implementation of call_tool without MCP.\"\"\"\n    # Simple implementation of tools\n    if tool_name == \"add\":\n        if \"a\" in arguments and \"b\" in arguments:\n            return arguments[\"a\"] + arguments[\"b\"]\n        else:\n            return \"Error: Missing required arguments 'a' or 'b'\"\n    elif tool_name == \"subtract\":\n        if \"a\" in arguments and \"b\" in arguments:\n            return arguments[\"a\"] - arguments[\"b\"]\n        else:\n            return \"Error: Missing required arguments 'a' or 'b'\"\n    elif tool_name == \"multiply\":\n        if \"a\" in arguments and \"b\" in arguments:\n            return arguments[\"a\"] * arguments[\"b\"]\n        else:\n            return \"Error: Missing required arguments 'a' or 'b'\"\n    elif tool_name == \"divide\":\n        if \"a\" in arguments and \"b\" in arguments:\n            if arguments[\"b\"] == 0:\n                return \"Error: Division by zero is not allowed\"\n            return arguments[\"a\"] / arguments[\"b\"]\n        else:\n            return \"Error: Missing required arguments 'a' or 'b'\"\n    else:\n        return f\"Error: Unknown tool '{tool_name}'\"\n\nif __name__ == \"__main__\":\n    print(\"=== Testing call_llm ===\")\n    prompt = \"In a few words, what is the meaning of life?\"\n    print(f\"Prompt: {prompt}\")\n    response = call_llm(prompt)\n    print(f\"Response: {response}\")\n\n        # Find available tools\n    print(\"=== Finding available tools ===\")\n    tools = get_tools(\"simple_server.py\")\n    \n    # Print tool information nicely formatted\n    for i, tool in enumerate(tools, 1):\n        print(f\"\\nTool {i}: {tool.name}\")\n        print(\"=\" * (len(tool.name) + 8))\n        print(f\"Description: {tool.description}\")\n        \n        # Parameters section\n        print(\"Parameters:\")\n        properties = tool.inputSchema.get('properties', {})\n        required = tool.inputSchema.get('required', [])\n        \n        # No parameters case\n        if not properties:\n            print(\"  None\")\n        \n        # Print each parameter with its details\n        for param_name, param_info in properties.items():\n            param_type = param_info.get('type', 'unknown')\n            req_status = \"(Required)\" if param_name in required else \"(Optional)\"\n            print(f\"  \u2022 {param_name}: {param_type} {req_status}\")\n    \n    # Call a tool\n    print(\"\\n=== Calling the add tool ===\")\n    a, b = 5, 3\n    result = call_tool(\"simple_server.py\", \"add\", {\"a\": a, \"b\": b})\n    print(f\"Result of {a} + {b} = {result}\")\n    \n    # You can easily call with different parameters\n    a, b = 10, 20\n    result = call_tool(\"simple_server.py\", \"add\", {\"a\": a, \"b\": b})\n    print(f\"Result of {a} + {b} = {result}\")\n\n\n\n--- File Index 73: cookbook/pocketflow-multi-agent/README.md ---\n# Multi-Agent Taboo Game\n\nA PocketFlow example that demonstrates how to implement asynchronous multi-agent communication using the Taboo word guessing game.\n\n## Features\n\n- Implement asynchronous communication between two AI agents (Hinter and Guesser)\n- Use AsyncNode for non-blocking agent interactions\n- Create dynamic conversation flow through asyncio message queues\n- Demonstrate complex turn-based game mechanics with LLMs\n- Automatically terminate the game when the correct word is guessed\n\n## Getting Started\n\n1. Install the required dependencies:\n\n```bash\npip install -r requirements.txt\n```\n\n2. Set your OpenAI API key as an environment variable:\n\n```bash\nexport OPENAI_API_KEY=your_api_key_here\n```\n\n3. Run the application:\n\n```bash\npython main.py\n```\n\n## How It Works\n\nThe workflow follows an asynchronous multi-agent communication pattern:\n\n```mermaid\nflowchart LR\n    AsyncHinter[AsyncHinter Node] <--> MessageQueue{Message Queue}\n    MessageQueue <--> AsyncGuesser[AsyncGuesser Node]\n```\n\nHere's what each component does:\n\n1. **AsyncHinter Node**: Generates hints about the target word while avoiding forbidden words\n2. **AsyncGuesser Node**: Makes guesses based on the hints received from the Hinter\n3. **Message Queue**: Facilitates asynchronous communication between the agents\n\n## Files\n\n- [`main.py`](./main.py): Main entry point implementing the AsyncHinter and AsyncGuesser nodes and game flow\n- [`utils.py`](./utils.py): Utility functions including LLM wrappers for generating hints and guesses\n- [`requirements.txt`](./requirements.txt): Lists the required dependencies\n\n## Example Output\n\n```\n=========== Taboo Game Starting! ===========\nTarget word: nostalgic\nForbidden words: ['memory', 'past', 'remember', 'feeling', 'longing']\n============================================\n\nHinter: Here's your hint - Sentiment for earlier times.\nGuesser: I guess it's - Nostalgia\n\nHinter: Here's your hint - Sentiment for earlier times.\nGuesser: I guess it's - Reminiscence\n\nHinter: Here's your hint - Yearning for days gone by.\nGuesser: I guess it's - Sentimentality\n\nHinter: Here's your hint - Reliving cherished moments or experiences.\nGuesser: I guess it's - Memories\n\nHinter: Here's your hint - Recollection of cherished experiences.\nGuesser: I guess it's - Reflection\n\nHinter: Here's your hint - Yearning for earlier times.\nGuesser: I guess it's - Longing\n\nHinter: Here's your hint - Sentiment for earlier times.\nGuesser: I guess it's - Nostalgic\nGame Over - Correct guess!\n\n--- File Index 74: cookbook/pocketflow-multi-agent/main.py ---\nimport asyncio\nfrom pocketflow import AsyncNode, AsyncFlow\nfrom utils import call_llm\n\nclass AsyncHinter(AsyncNode):\n    async def prep_async(self, shared):\n        # Wait for message from guesser (or empty string at start)\n        guess = await shared[\"hinter_queue\"].get()\n        if guess == \"GAME_OVER\":\n            return None\n        return shared[\"target_word\"], shared[\"forbidden_words\"], shared.get(\"past_guesses\", [])\n\n    async def exec_async(self, inputs):\n        if inputs is None:\n            return None\n        target, forbidden, past_guesses = inputs\n        prompt = f\"Generate hint for '{target}'\\nForbidden words: {forbidden}\"\n        if past_guesses:\n            prompt += f\"\\nPrevious wrong guesses: {past_guesses}\\nMake hint more specific.\"\n        prompt += \"\\nUse at most 5 words.\"\n        \n        hint = call_llm(prompt)\n        print(f\"\\nHinter: Here's your hint - {hint}\")\n        return hint\n\n    async def post_async(self, shared, prep_res, exec_res):\n        if exec_res is None:\n            return \"end\"\n        # Send hint to guesser\n        await shared[\"guesser_queue\"].put(exec_res)\n        return \"continue\"\n\nclass AsyncGuesser(AsyncNode):\n    async def prep_async(self, shared):\n        # Wait for hint from hinter\n        hint = await shared[\"guesser_queue\"].get()\n        return hint, shared.get(\"past_guesses\", [])\n\n    async def exec_async(self, inputs):\n        hint, past_guesses = inputs\n        prompt = f\"Given hint: {hint}, past wrong guesses: {past_guesses}, make a new guess. Directly reply a single word:\"\n        guess = call_llm(prompt)\n        print(f\"Guesser: I guess it's - {guess}\")\n        return guess\n\n    async def post_async(self, shared, prep_res, exec_res):\n        # Check if guess is correct\n        if exec_res.lower() == shared[\"target_word\"].lower():\n            print(\"Game Over - Correct guess!\")\n            await shared[\"hinter_queue\"].put(\"GAME_OVER\")\n            return \"end\"\n            \n        # Store the guess in shared state\n        if \"past_guesses\" not in shared:\n            shared[\"past_guesses\"] = []\n        shared[\"past_guesses\"].append(exec_res)\n        \n        # Send guess to hinter\n        await shared[\"hinter_queue\"].put(exec_res)\n        return \"continue\"\n\nasync def main():\n    # Set up game\n    shared = {\n        \"target_word\": \"nostalgic\",\n        \"forbidden_words\": [\"memory\", \"past\", \"remember\", \"feeling\", \"longing\"],\n        \"hinter_queue\": asyncio.Queue(),\n        \"guesser_queue\": asyncio.Queue()\n    }\n    \n    print(\"=========== Taboo Game Starting! ===========\")\n    print(f\"Target word: {shared['target_word']}\")\n    print(f\"Forbidden words: {shared['forbidden_words']}\")\n    print(\"============================================\")\n\n    # Initialize by sending empty guess to hinter\n    await shared[\"hinter_queue\"].put(\"\")\n\n    # Create nodes and flows\n    hinter = AsyncHinter()\n    guesser = AsyncGuesser()\n\n    # Set up flows\n    hinter_flow = AsyncFlow(start=hinter)\n    guesser_flow = AsyncFlow(start=guesser)\n\n    # Connect nodes to themselves for looping\n    hinter - \"continue\" >> hinter\n    guesser - \"continue\" >> guesser\n\n    # Run both agents concurrently\n    await asyncio.gather(\n        hinter_flow.run_async(shared),\n        guesser_flow.run_async(shared)\n    )\n    \n    print(\"=========== Game Complete! ===========\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n\n--- File Index 75: cookbook/pocketflow-multi-agent/utils.py ---\nimport os\nfrom openai import OpenAI\n\ndef call_llm(prompt):    \n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n    r = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return r.choices[0].message.content\n\n# Example usage\nif __name__ == \"__main__\":\n    print(call_llm(\"Tell me a short joke\")) \n\n--- File Index 76: cookbook/pocketflow-nested-batch/README.md ---\n# PocketFlow Nested BatchFlow Example\n\nThis example demonstrates Nested BatchFlow using a simple school grades calculator.\n\n## What this Example Does\n\nCalculates average grades for:\n1. Each student in a class\n2. Each class in the school\n\n## Structure\n```\nschool/\n\u251c\u2500\u2500 class_a/\n\u2502   \u251c\u2500\u2500 student1.txt  (grades: 7.5, 8.0, 9.0)\n\u2502   \u2514\u2500\u2500 student2.txt  (grades: 8.5, 7.0, 9.5)\n\u2514\u2500\u2500 class_b/\n    \u251c\u2500\u2500 student3.txt  (grades: 6.5, 8.5, 7.0)\n    \u2514\u2500\u2500 student4.txt  (grades: 9.0, 9.5, 8.0)\n```\n\n## How it Works\n\n1. **Outer BatchFlow (SchoolBatchFlow)**\n   - Processes each class folder\n   - Returns parameters like: `{\"class\": \"class_a\"}`\n\n2. **Inner BatchFlow (ClassBatchFlow)**\n   - Processes each student file in a class\n   - Returns parameters like: `{\"student\": \"student1.txt\"}`\n\n3. **Base Flow**\n   - Loads student grades\n   - Calculates average\n   - Saves result\n\n## Running the Example\n\n```bash\npip install -r requirements.txt\npython main.py\n```\n\n## Expected Output\n\n```\nProcessing class_a...\n- student1: Average = 8.2\n- student2: Average = 8.3\nClass A Average: 8.25\n\nProcessing class_b...\n- student3: Average = 7.3\n- student4: Average = 8.8\nClass B Average: 8.05\n\nSchool Average: 8.15\n```\n\n## Key Concepts\n\n1. **Nested BatchFlow**: One BatchFlow inside another\n2. **Parameter Inheritance**: Inner flow gets parameters from outer flow\n3. **Hierarchical Processing**: Process data in a tree-like structure \n\n--- File Index 77: cookbook/pocketflow-nested-batch/flow.py ---\nimport os\nfrom pocketflow import Flow, BatchFlow\nfrom nodes import LoadGrades, CalculateAverage\n\ndef create_base_flow():\n    \"\"\"Create base flow for processing one student's grades.\"\"\"\n    # Create nodes\n    load = LoadGrades()\n    calc = CalculateAverage()\n    \n    # Connect nodes\n    load - \"calculate\" >> calc\n    \n    # Create and return flow\n    return Flow(start=load)\n\nclass ClassBatchFlow(BatchFlow):\n    \"\"\"BatchFlow for processing all students in a class.\"\"\"\n    \n    def prep(self, shared):\n        \"\"\"Generate parameters for each student in the class.\"\"\"\n        # Get class folder from parameters\n        class_folder = self.params[\"class\"]\n        \n        # List all student files\n        class_path = os.path.join(\"school\", class_folder)\n        students = [f for f in os.listdir(class_path) if f.endswith(\".txt\")]\n        \n        # Return parameters for each student\n        return [{\"student\": student} for student in students]\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Calculate and print class average.\"\"\"\n        class_name = self.params[\"class\"]\n        class_results = shared[\"results\"][class_name]\n        class_average = sum(class_results.values()) / len(class_results)\n        \n        print(f\"Class {class_name.split('_')[1].upper()} Average: {class_average:.2f}\\n\")\n        return \"default\"\n\nclass SchoolBatchFlow(BatchFlow):\n    \"\"\"BatchFlow for processing all classes in the school.\"\"\"\n    \n    def prep(self, shared):\n        \"\"\"Generate parameters for each class.\"\"\"\n        # List all class folders\n        classes = [d for d in os.listdir(\"school\") if os.path.isdir(os.path.join(\"school\", d))]\n        \n        # Return parameters for each class\n        return [{\"class\": class_name} for class_name in classes]\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Calculate and print school average.\"\"\"\n        all_grades = []\n        for class_results in shared[\"results\"].values():\n            all_grades.extend(class_results.values())\n            \n        school_average = sum(all_grades) / len(all_grades)\n        print(f\"School Average: {school_average:.2f}\")\n        return \"default\"\n\ndef create_flow():\n    \"\"\"Create the complete nested batch processing flow.\"\"\"\n    # Create base flow for single student\n    base_flow = create_base_flow()\n    \n    # Wrap in ClassBatchFlow for processing all students in a class\n    class_flow = ClassBatchFlow(start=base_flow)\n    \n    # Wrap in SchoolBatchFlow for processing all classes\n    school_flow = SchoolBatchFlow(start=class_flow)\n    \n    return school_flow \n\n--- File Index 78: cookbook/pocketflow-nested-batch/main.py ---\nimport os\nfrom flow import create_flow\n\ndef create_sample_data():\n    \"\"\"Create sample grade files.\"\"\"\n    # Create directory structure\n    os.makedirs(\"school/class_a\", exist_ok=True)\n    os.makedirs(\"school/class_b\", exist_ok=True)\n    \n    # Sample grades\n    data = {\n        \"class_a\": {\n            \"student1.txt\": [7.5, 8.0, 9.0],\n            \"student2.txt\": [8.5, 7.0, 9.5]\n        },\n        \"class_b\": {\n            \"student3.txt\": [6.5, 8.5, 7.0],\n            \"student4.txt\": [9.0, 9.5, 8.0]\n        }\n    }\n    \n    # Create files\n    for class_name, students in data.items():\n        for student, grades in students.items():\n            file_path = os.path.join(\"school\", class_name, student)\n            with open(file_path, 'w') as f:\n                for grade in grades:\n                    f.write(f\"{grade}\\n\")\n\ndef main():\n    \"\"\"Run the nested batch example.\"\"\"\n    # Create sample data\n    create_sample_data()\n    \n    print(\"Processing school grades...\\n\")\n    \n    # Create and run flow\n    flow = create_flow()\n    flow.run({})\n\nif __name__ == \"__main__\":\n    main() \n\n--- File Index 79: cookbook/pocketflow-nested-batch/nodes.py ---\nimport os\nfrom pocketflow import Node\n\nclass LoadGrades(Node):\n    \"\"\"Node that loads grades from a student's file.\"\"\"\n    \n    def prep(self, shared):\n        \"\"\"Get file path from parameters.\"\"\"\n        class_name = self.params[\"class\"]\n        student_file = self.params[\"student\"]\n        return os.path.join(\"school\", class_name, student_file)\n    \n    def exec(self, file_path):\n        \"\"\"Load and parse grades from file.\"\"\"\n        with open(file_path, 'r') as f:\n            # Each line is a grade\n            grades = [float(line.strip()) for line in f]\n        return grades\n    \n    def post(self, shared, prep_res, grades):\n        \"\"\"Store grades in shared store.\"\"\"\n        shared[\"grades\"] = grades\n        return \"calculate\"\n\nclass CalculateAverage(Node):\n    \"\"\"Node that calculates average grade.\"\"\"\n    \n    def prep(self, shared):\n        \"\"\"Get grades from shared store.\"\"\"\n        return shared[\"grades\"]\n    \n    def exec(self, grades):\n        \"\"\"Calculate average.\"\"\"\n        return sum(grades) / len(grades)\n    \n    def post(self, shared, prep_res, average):\n        \"\"\"Store and print result.\"\"\"\n        # Store in results dictionary\n        if \"results\" not in shared:\n            shared[\"results\"] = {}\n        \n        class_name = self.params[\"class\"]\n        student = self.params[\"student\"]\n        \n        if class_name not in shared[\"results\"]:\n            shared[\"results\"][class_name] = {}\n            \n        shared[\"results\"][class_name][student] = average\n        \n        # Print individual result\n        print(f\"- {student}: Average = {average:.1f}\")\n        return \"default\" \n\n--- File Index 80: cookbook/pocketflow-node/README.md ---\n# PocketFlow Summarize\n\nA practical example demonstrating how to use PocketFlow to build a robust text summarization tool with error handling and retries. This example showcases core PocketFlow concepts in a real-world application.\n\n## Features\n\n- Text summarization using LLMs (Large Language Models)\n- Automatic retry mechanism (up to 3 attempts) on API failures\n- Graceful error handling with fallback responses\n- Clean separation of concerns using PocketFlow's Node architecture\n\n## Project Structure\n\n```\n.\n\u251c\u2500\u2500 docs/          # Documentation files\n\u251c\u2500\u2500 utils/         # Utility functions (LLM API wrapper)\n\u251c\u2500\u2500 flow.py        # PocketFlow implementation with Summarize Node\n\u251c\u2500\u2500 main.py        # Main application entry point\n\u2514\u2500\u2500 README.md      # Project documentation\n```\n\n## Implementation Details\n\nThe example implements a simple but robust text summarization workflow:\n\n1. **Summarize Node** (`flow.py`):\n   - `prep()`: Retrieves text from the shared store\n   - `exec()`: Calls LLM to summarize text in 10 words\n   - `exec_fallback()`: Provides graceful error handling\n   - `post()`: Stores the summary back in shared store\n\n2. **Flow Structure**:\n   - Single node flow for demonstration\n   - Configured with 3 retries for reliability\n   - Uses shared store for data passing\n\n## Setup\n\n1. Create a virtual environment:\n```bash\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n```\n\n2. Install dependencies:\n```bash\npip install -r requirements.txt\n```\n\n3. Configure your environment:\n   - Set up your LLM API key (check utils/call_llm.py for configuration)\n\n4. Run the example:\n```bash\npython main.py\n```\n\n## Example Usage\n\nThe example comes with a sample text about PocketFlow, but you can modify `main.py` to summarize your own text:\n\n```python\nshared = {\"data\": \"Your text to summarize here...\"}\nflow.run(shared)\nprint(\"Summary:\", shared[\"summary\"])\n```\n\n## What You'll Learn\n\nThis example demonstrates several key PocketFlow concepts:\n\n- **Node Architecture**: How to structure LLM tasks using prep/exec/post pattern\n- **Error Handling**: Implementing retry mechanisms and fallbacks\n- **Shared Store**: Using shared storage for data flow between steps\n- **Flow Creation**: Setting up a basic PocketFlow workflow\n\n## Additional Resources\n\n- [PocketFlow Documentation](https://the-pocket.github.io/PocketFlow/)\n- [Node Concept Guide](https://the-pocket.github.io/PocketFlow/node.html)\n- [Flow Design Patterns](https://the-pocket.github.io/PocketFlow/flow.html) \n\n--- File Index 81: cookbook/pocketflow-node/flow.py ---\nfrom pocketflow import Node, Flow\nfrom utils.call_llm import call_llm\n\nclass Summarize(Node):\n    def prep(self, shared):\n        \"\"\"Read and preprocess data from shared store.\"\"\"\n        return shared[\"data\"]\n\n    def exec(self, prep_res):\n        \"\"\"Execute the summarization using LLM.\"\"\"\n        if not prep_res:\n            return \"Empty text\"\n        prompt = f\"Summarize this text in 10 words: {prep_res}\"\n        summary = call_llm(prompt)  # might fail\n        return summary\n\n    def exec_fallback(self, shared, prep_res, exc):\n        \"\"\"Provide a simple fallback instead of crashing.\"\"\"\n        return \"There was an error processing your request.\"\n\n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Store the summary in shared store.\"\"\"\n        shared[\"summary\"] = exec_res\n        # Return \"default\" by not returning\n\n# Create the flow\nsummarize_node = Summarize(max_retries=3)\nflow = Flow(start=summarize_node) \n\n--- File Index 82: cookbook/pocketflow-node/main.py ---\nfrom flow import flow\n\ndef main():\n    # Example text to summarize\n    text = \"\"\"\n    PocketFlow is a minimalist LLM framework that models workflows as a Nested Directed Graph.\n    Nodes handle simple LLM tasks, connecting through Actions for Agents.\n    Flows orchestrate these nodes for Task Decomposition, and can be nested.\n    It also supports Batch processing and Async execution.\n    \"\"\"\n\n    # Initialize shared store\n    shared = {\"data\": text}\n    \n    # Run the flow\n    flow.run(shared)\n    \n    # Print result\n    print(\"\\nInput text:\", text)\n    print(\"\\nSummary:\", shared[\"summary\"])\n\nif __name__ == \"__main__\":\n    main() \n\n--- File Index 83: cookbook/pocketflow-node/utils/call_llm.py ---\nfrom openai import OpenAI\n\ndef call_llm(prompt):    \n    client = OpenAI(api_key=\"YOUR_API_KEY_HERE\")\n    r = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return r.choices[0].message.content\n    \nif __name__ == \"__main__\":\n    prompt = \"What is the meaning of life?\"\n    print(call_llm(prompt))\n\n--- File Index 84: cookbook/pocketflow-parallel-batch-flow/README.md ---\n# Parallel Image Processor\n\nDemonstrates how AsyncParallelBatchFlow processes multiple images with multiple filters >8x faster than sequential processing.\n\n## Features\n\n  ```mermaid\n  graph TD\n      subgraph AsyncParallelBatchFlow[Image Processing Flow]\n          subgraph AsyncFlow[Per Image-Filter Flow]\n              A[Load Image] --> B[Apply Filter]\n              B --> C[Save Image]\n          end\n      end\n  ```\n  \n- Processes images with multiple filters in parallel\n- Applies three different filters (grayscale, blur, sepia)\n- Shows significant speed improvement over sequential processing\n- Manages system resources with semaphores\n\n## Run It\n\n```bash\npip install -r requirements.txt\npython main.py\n```\n\n## Output\n\n```=== Processing Images in Parallel ===\nParallel Image Processor\n------------------------------\nFound 3 images:\n- images/bird.jpg\n- images/cat.jpg\n- images/dog.jpg\n\nRunning sequential batch flow...\nProcessing 3 images with 3 filters...\nTotal combinations: 9\nLoading image: images/bird.jpg\nApplying grayscale filter...\nSaved: output/bird_grayscale.jpg\n...etc\n\nTiming Results:\nSequential batch processing: 13.76 seconds\nParallel batch processing: 1.71 seconds\nSpeedup: 8.04x\n\nProcessing complete! Check the output/ directory for results.\n```\n\n## Key Points\n\n- **Sequential**: Total time = sum of all item times\n  - Good for: Rate-limited APIs, maintaining order\n\n- **Parallel**: Total time \u2248 longest single item time\n  - Good for: I/O-bound tasks, independent operations \n\n\n--- File Index 85: cookbook/pocketflow-parallel-batch-flow/flow.py ---\n\"\"\"Flow definitions for parallel image processing.\"\"\"\n\nfrom pocketflow import AsyncFlow, AsyncParallelBatchFlow, AsyncBatchFlow\nfrom nodes import LoadImage, ApplyFilter, SaveImage, NoOp\n\ndef create_base_flow():\n    \"\"\"Create flow for processing a single image with one filter.\"\"\"\n    # Create nodes\n    load = LoadImage()\n    apply_filter = ApplyFilter()\n    save = SaveImage()\n    noop = NoOp()\n    \n    # Connect nodes\n    load - \"apply_filter\" >> apply_filter\n    apply_filter - \"save\" >> save\n    save - \"default\" >> noop\n    \n    # Create flow\n    return load\n\nclass ImageBatchFlow(AsyncBatchFlow):\n    \"\"\"Flow that processes multiple images with multiple filters in batch.\"\"\"\n    \n    async def prep_async(self, shared):\n        \"\"\"Generate parameters for each image-filter combination.\"\"\"\n        # Get list of images and filters\n        images = shared.get(\"images\", [])\n        filters = [\"grayscale\", \"blur\", \"sepia\"]\n        \n        # Create parameter combinations\n        params = []\n        for image_path in images:\n            for filter_type in filters:\n                params.append({\n                    \"image_path\": image_path,\n                    \"filter\": filter_type\n                })\n        \n        print(f\"Processing {len(images)} images with {len(filters)} filters...\")\n        print(f\"Total combinations: {len(params)}\")\n        return params\n\nclass ImageParallelBatchFlow(AsyncParallelBatchFlow):\n    \"\"\"Flow that processes multiple images with multiple filters in parallel.\"\"\"\n\n    async def prep_async(self, shared):\n        \"\"\"Generate parameters for each image-filter combination.\"\"\"\n        # Get list of images and filters\n        images = shared.get(\"images\", [])\n        filters = [\"grayscale\", \"blur\", \"sepia\"]\n        \n        # Create parameter combinations\n        params = []\n        for image_path in images:\n            for filter_type in filters:\n                params.append({\n                    \"image_path\": image_path,\n                    \"filter\": filter_type\n                })\n        \n        print(f\"Processing {len(images)} images with {len(filters)} filters...\")\n        print(f\"Total combinations: {len(params)}\")\n        return params\n\ndef create_flows():\n    \"\"\"Create the complete parallel processing flow.\"\"\"\n    # Create base flow for single image processing\n    base_flow = create_base_flow()\n    \n    # Wrap in parallel batch flow\n    return ImageBatchFlow(start=base_flow), ImageParallelBatchFlow(start=base_flow)\n\n--- File Index 86: cookbook/pocketflow-parallel-batch-flow/main.py ---\nimport os\nimport asyncio\nimport time\nfrom flow import create_flows\n\ndef get_image_paths():\n    \"\"\"Get paths of existing images in the images directory.\"\"\"\n    images_dir = \"images\"\n    if not os.path.exists(images_dir):\n        raise ValueError(f\"Directory '{images_dir}' not found!\")\n    \n    # List all jpg files in the images directory\n    image_paths = []\n    for filename in os.listdir(images_dir):\n        if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n            image_paths.append(os.path.join(images_dir, filename))\n    \n    if not image_paths:\n        raise ValueError(f\"No images found in '{images_dir}' directory!\")\n    \n    print(f\"Found {len(image_paths)} images:\")\n    for path in image_paths:\n        print(f\"- {path}\")\n    \n    return image_paths\n\nasync def main():\n    \"\"\"Run the parallel image processing example.\"\"\"\n    print(\"Parallel Image Processor\")\n    print(\"-\" * 30)\n    \n    # Get existing image paths\n    image_paths = get_image_paths()\n    \n    # Create shared store with image paths\n    shared = {\"images\": image_paths}\n    \n    # Create both flows\n    batch_flow, parallel_batch_flow = create_flows()\n    \n    # Run and time batch flow\n    start_time = time.time()\n    print(\"\\nRunning sequential batch flow...\")\n    await batch_flow.run_async(shared)\n    batch_time = time.time() - start_time\n    \n    # Run and time parallel batch flow\n    start_time = time.time()\n    print(\"\\nRunning parallel batch flow...\")\n    await parallel_batch_flow.run_async(shared)\n    parallel_time = time.time() - start_time\n    \n    # Print timing results\n    print(\"\\nTiming Results:\")\n    print(f\"Sequential batch processing: {batch_time:.2f} seconds\")\n    print(f\"Parallel batch processing: {parallel_time:.2f} seconds\")\n    print(f\"Speedup: {batch_time/parallel_time:.2f}x\")\n    \n    print(\"\\nProcessing complete! Check the output/ directory for results.\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main()) \n\n--- File Index 87: cookbook/pocketflow-parallel-batch-flow/nodes.py ---\n\"\"\"AsyncNode implementations for image processing.\"\"\"\nimport os\nimport asyncio\nfrom PIL import Image, ImageFilter\nimport numpy as np\nfrom pocketflow import AsyncNode\n\nclass LoadImage(AsyncNode):\n    \"\"\"Node that loads an image from file.\"\"\"\n    async def prep_async(self, shared):\n        \"\"\"Get image path from parameters.\"\"\"\n        image_path = self.params[\"image_path\"]\n        print(f\"Loading image: {image_path}\")\n        return image_path\n    \n    async def exec_async(self, image_path):\n        \"\"\"Load image using PIL.\"\"\"\n        # Simulate I/O delay\n        await asyncio.sleep(0.5)\n        return Image.open(image_path)\n    \n    async def post_async(self, shared, prep_res, exec_res):\n        \"\"\"Store image in shared store.\"\"\"\n        shared[\"image\"] = exec_res\n        return \"apply_filter\"\n\nclass ApplyFilter(AsyncNode):\n    \"\"\"Node that applies a filter to an image.\"\"\"\n    async def prep_async(self, shared):\n        \"\"\"Get image and filter type.\"\"\"\n        image = shared[\"image\"]\n        filter_type = self.params[\"filter\"]\n        print(f\"Applying {filter_type} filter...\")\n        return image, filter_type\n    \n    async def exec_async(self, inputs):\n        \"\"\"Apply the specified filter.\"\"\"\n        image, filter_type = inputs\n        \n        # Simulate processing delay\n        await asyncio.sleep(0.5)\n        \n        if filter_type == \"grayscale\":\n            return image.convert(\"L\")\n        elif filter_type == \"blur\":\n            return image.filter(ImageFilter.BLUR)\n        elif filter_type == \"sepia\":\n            # Convert to array for sepia calculation\n            img_array = np.array(image)\n            sepia_matrix = np.array([\n                [0.393, 0.769, 0.189],\n                [0.349, 0.686, 0.168],\n                [0.272, 0.534, 0.131]\n            ])\n            sepia_array = img_array.dot(sepia_matrix.T)\n            sepia_array = np.clip(sepia_array, 0, 255).astype(np.uint8)\n            return Image.fromarray(sepia_array)\n        else:\n            raise ValueError(f\"Unknown filter: {filter_type}\")\n    \n    async def post_async(self, shared, prep_res, exec_res):\n        \"\"\"Store filtered image.\"\"\"\n        shared[\"filtered_image\"] = exec_res\n        return \"save\"\n\nclass SaveImage(AsyncNode):\n    \"\"\"Node that saves the processed image.\"\"\"\n    async def prep_async(self, shared):\n        \"\"\"Prepare output path.\"\"\"\n        image = shared[\"filtered_image\"]\n        base_name = os.path.splitext(os.path.basename(self.params[\"image_path\"]))[0]\n        filter_type = self.params[\"filter\"]\n        output_path = f\"output/{base_name}_{filter_type}.jpg\"\n        \n        # Create output directory if needed\n        os.makedirs(\"output\", exist_ok=True)\n        \n        return image, output_path\n    \n    async def exec_async(self, inputs):\n        \"\"\"Save the image.\"\"\"\n        image, output_path = inputs\n        \n        # Simulate I/O delay\n        await asyncio.sleep(0.5)\n        \n        image.save(output_path)\n        return output_path\n    \n    async def post_async(self, shared, prep_res, exec_res):\n        \"\"\"Print success message.\"\"\"\n        print(f\"Saved: {exec_res}\")\n        return \"default\" \n\n--- File Index 88: cookbook/pocketflow-parallel-batch/README.md ---\n# Sequential vs Parallel Processing\n\nDemonstrates how AsyncParallelBatchNode accelerates processing by 3x over AsyncBatchNode.\n\n## Features\n\n- Processes identical tasks with two approaches\n- Compares sequential vs parallel execution time\n- Shows 3x speed improvement with parallel processing\n\n## Run It\n\n```bash\npip install pocketflow\npython main.py\n```\n\n## Output\n\n```\n=== Running Sequential (AsyncBatchNode) ===\n[Sequential] Summarizing file1.txt...\n[Sequential] Summarizing file2.txt...\n[Sequential] Summarizing file3.txt...\n\n=== Running Parallel (AsyncParallelBatchNode) ===\n[Parallel] Summarizing file1.txt...\n[Parallel] Summarizing file2.txt...\n[Parallel] Summarizing file3.txt...\n\nSequential took: 3.00 seconds\nParallel took:   1.00 seconds\n```\n\n## Key Points\n\n- **Sequential**: Total time = sum of all item times\n  - Good for: Rate-limited APIs, maintaining order\n\n- **Parallel**: Total time \u2248 longest single item time\n  - Good for: I/O-bound tasks, independent operations \n\n## Tech Dive Deep\n\n- **Python's GIL** prevents true CPU-bound parallelism, but LLM calls are I/O-bound\n- **Async/await** overlaps waiting time between requests\n  - Example: `await client.chat.completions.create(...)`\n  - See: [OpenAI's async usage](https://github.com/openai/openai-python?tab=readme-ov-file#async-usage)\n\nFor maximum performance and cost efficiency, consider using batch APIs:\n- [OpenAI's Batch API](https://platform.openai.com/docs/guides/batch) lets you process multiple prompts in a single request\n- Reduces overhead and can be more cost-effective for large workloads \n\n--- File Index 89: cookbook/pocketflow-parallel-batch/main.py ---\nimport asyncio\nimport time\n\nfrom pocketflow import AsyncBatchNode, AsyncParallelBatchNode, AsyncFlow\n\n####################################\n# Dummy async function (1s delay)\n####################################\nasync def dummy_llm_summarize(text):\n    \"\"\"Simulates an async LLM call that takes 1 second.\"\"\"\n    await asyncio.sleep(1)\n    return f\"Summarized({len(text)} chars)\"\n\n###############################################\n# 1) AsyncBatchNode (sequential) version\n###############################################\n\nclass SummariesAsyncNode(AsyncBatchNode):\n    \"\"\"\n    Processes items sequentially in an async manner.\n    The next item won't start until the previous item has finished.\n    \"\"\"\n\n    async def prep_async(self, shared):\n        # Return a list of items to process.\n        # Each item is (filename, content).\n        return list(shared[\"data\"].items())\n\n    async def exec_async(self, item):\n        filename, content = item\n        print(f\"[Sequential] Summarizing {filename}...\")\n        summary = await dummy_llm_summarize(content)\n        return (filename, summary)\n\n    async def post_async(self, shared, prep_res, exec_res_list):\n        # exec_res_list is a list of (filename, summary)\n        shared[\"sequential_summaries\"] = dict(exec_res_list)\n        return \"done_sequential\"\n\n###############################################\n# 2) AsyncParallelBatchNode (concurrent) version\n###############################################\n\nclass SummariesAsyncParallelNode(AsyncParallelBatchNode):\n    \"\"\"\n    Processes items in parallel. Many LLM calls start at once.\n    \"\"\"\n\n    async def prep_async(self, shared):\n        return list(shared[\"data\"].items())\n\n    async def exec_async(self, item):\n        filename, content = item\n        print(f\"[Parallel] Summarizing {filename}...\")\n        summary = await dummy_llm_summarize(content)\n        return (filename, summary)\n\n    async def post_async(self, shared, prep_res, exec_res_list):\n        shared[\"parallel_summaries\"] = dict(exec_res_list)\n        return \"done_parallel\"\n\n###############################################\n# Demo comparing the two approaches\n###############################################\n\nasync def main():\n    # We'll use the same data for both flows\n    shared_data = {\n        \"data\": {\n            \"file1.txt\": \"Hello world 1\",\n            \"file2.txt\": \"Hello world 2\",\n            \"file3.txt\": \"Hello world 3\",\n        }\n    }\n\n    # 1) Run the sequential version\n    seq_node = SummariesAsyncNode()\n    seq_flow = AsyncFlow(start=seq_node)\n\n    print(\"\\n=== Running Sequential (AsyncBatchNode) ===\")\n    t0 = time.time()\n    await seq_flow.run_async(shared_data)\n    t1 = time.time()\n\n    # 2) Run the parallel version\n    par_node = SummariesAsyncParallelNode()\n    par_flow = AsyncFlow(start=par_node)\n\n    print(\"\\n=== Running Parallel (AsyncParallelBatchNode) ===\")\n    t2 = time.time()\n    await par_flow.run_async(shared_data)\n    t3 = time.time()\n\n    # Show times\n    print(\"\\n--- Results ---\")\n    print(f\"Sequential Summaries: {shared_data.get('sequential_summaries')}\")\n    print(f\"Parallel Summaries:   {shared_data.get('parallel_summaries')}\")\n\n    print(f\"Sequential took: {t1 - t0:.2f} seconds\")\n    print(f\"Parallel took:   {t3 - t2:.2f} seconds\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n--- File Index 90: cookbook/pocketflow-rag/README.md ---\n# Retrieval Augmented Generation (RAG)\n\nThis project demonstrates a simplified RAG system that retrieves relevant documents based on user queries and generates answers using an LLM. This implementation is based directly on the tutorial: [Retrieval Augmented Generation (RAG) from Scratch \u2014 Tutorial For Dummies](https://zacharyhuang.substack.com/p/retrieval-augmented-generation-rag).\n\n\n## Features\n\n- Document chunking for processing long texts\n- FAISS-powered vector-based document retrieval\n- LLM-powered answer generation\n\n## How to Run\n\n1. Set your API key:\n   ```bash\n   export OPENAI_API_KEY=\"your-api-key-here\"\n   ```\n   Or update it directly in `utils.py`\n\n2. Install and run with the default query:\n   ```bash\n   pip install -r requirements.txt\n   python main.py\n   ```\n\n3. Run the application with a sample query:\n\n   ```bash\n   python main.py --\"How does the Q-Mesh protocol achieve high transaction speeds?\"\n   ```\n\n## How It Works\n\nThe magic happens through a two-phase pipeline implemented with PocketFlow:\n\n```mermaid\ngraph TD\n    subgraph OfflineFlow[Offline Document Indexing]\n        ChunkDocs[ChunkDocumentsNode] --> EmbedDocs[EmbedDocumentsNode] --> CreateIndex[CreateIndexNode]\n    end\n    \n    subgraph OnlineFlow[Online Processing]\n        EmbedQuery[EmbedQueryNode] --> RetrieveDoc[RetrieveDocumentNode] --> GenerateAnswer[GenerateAnswerNode]\n    end\n```\n\nHere's what each part does:\n1. **ChunkDocumentsNode**: Breaks documents into smaller chunks for better retrieval\n2. **EmbedDocumentsNode**: Converts document chunks into vector representations\n3. **CreateIndexNode**: Creates a searchable FAISS index from embeddings\n4. **EmbedQueryNode**: Converts user query into the same vector space\n5. **RetrieveDocumentNode**: Finds the most similar document using vector search\n6. **GenerateAnswerNode**: Uses an LLM to generate an answer based on the retrieved content\n\n## Example Output\n\n```\n\u2705 Created 5 chunks from 5 documents\n\u2705 Created 5 document embeddings\n\ud83d\udd0d Creating search index...\n\u2705 Index created with 5 vectors\n\ud83d\udd0d Embedding query: How to install PocketFlow?\n\ud83d\udd0e Searching for relevant documents...\n\ud83d\udcc4 Retrieved document (index: 0, distance: 0.3427)\n\ud83d\udcc4 Most relevant text: \"Pocket Flow is a 100-line minimalist LLM framework\n        Lightweight: Just 100 lines. Zero bloat, zero dependencies, zero vendor lock-in.\n        Expressive: Everything you love\u2014(Multi-)Agents, Workflow, RAG, and more.\n        Agentic Coding: Let AI Agents (e.g., Cursor AI) build Agents\u201410x productivity boost!\n        To install, pip install pocketflow or just copy the source code (only 100 lines).\"\n\n\ud83e\udd16 Generated Answer:\nTo install PocketFlow, use the command `pip install pocketflow` or simply copy its 100 lines of source code.\n```\n\n\n--- File Index 91: cookbook/pocketflow-rag/flow.py ---\nfrom pocketflow import Flow\nfrom nodes import EmbedDocumentsNode, CreateIndexNode, EmbedQueryNode, RetrieveDocumentNode, ChunkDocumentsNode, GenerateAnswerNode\n\ndef get_offline_flow():\n    # Create offline flow for document indexing\n    chunk_docs_node = ChunkDocumentsNode()\n    embed_docs_node = EmbedDocumentsNode()\n    create_index_node = CreateIndexNode()\n    \n    # Connect the nodes\n    chunk_docs_node >> embed_docs_node >> create_index_node\n    \n    offline_flow = Flow(start=chunk_docs_node)\n    return offline_flow\n\ndef get_online_flow():\n    # Create online flow for document retrieval and answer generation\n    embed_query_node = EmbedQueryNode()\n    retrieve_doc_node = RetrieveDocumentNode()\n    generate_answer_node = GenerateAnswerNode()\n    \n    # Connect the nodes\n    embed_query_node >> retrieve_doc_node >> generate_answer_node\n    \n    online_flow = Flow(start=embed_query_node)\n    return online_flow\n\n# Initialize flows\noffline_flow = get_offline_flow()\nonline_flow = get_online_flow()\n\n--- File Index 92: cookbook/pocketflow-rag/main.py ---\nimport sys\nfrom flow import offline_flow, online_flow\n\ndef run_rag_demo():\n    \"\"\"\n    Run a demonstration of the RAG system.\n    \n    This function:\n    1. Indexes a set of sample documents (offline flow)\n    2. Takes a query from the command line\n    3. Retrieves the most relevant document (online flow)\n    4. Generates an answer using an LLM\n    \"\"\"\n\n    # Sample texts - specialized/fictional content that benefits from RAG\n    texts = [\n        # PocketFlow framework\n        \"\"\"Pocket Flow is a 100-line minimalist LLM framework\n        Lightweight: Just 100 lines. Zero bloat, zero dependencies, zero vendor lock-in.\n        Expressive: Everything you love\u2014(Multi-)Agents, Workflow, RAG, and more.\n        Agentic Coding: Let AI Agents (e.g., Cursor AI) build Agents\u201410x productivity boost!\n        To install, pip install pocketflow or just copy the source code (only 100 lines).\"\"\",\n        \n        # Fictional medical device\n        \"\"\"NeurAlign M7 is a revolutionary non-invasive neural alignment device.\n        Targeted magnetic resonance technology increases neuroplasticity in specific brain regions.\n        Clinical trials showed 72% improvement in PTSD treatment outcomes.\n        Developed by Cortex Medical in 2024 as an adjunct to standard cognitive therapy.\n        Portable design allows for in-home use with remote practitioner monitoring.\"\"\",\n        \n        # Made-up historical event\n        \"\"\"The Velvet Revolution of Caldonia (1967-1968) ended Generalissimo Verak's 40-year rule.\n        Led by poet Eliza Markovian through underground literary societies.\n        Culminated in the Great Silence Protest with 300,000 silent protesters.\n        First democratic elections held in March 1968 with 94% voter turnout.\n        Became a model for non-violent political transitions in neighboring regions.\"\"\",\n        \n        # Fictional technology \n        \"\"\"Q-Mesh is QuantumLeap Technologies' instantaneous data synchronization protocol.\n        Utilizes directed acyclic graph consensus for 500,000 transactions per second.\n        Consumes 95% less energy than traditional blockchain systems.\n        Adopted by three central banks for secure financial data transfer.\n        Released in February 2024 after five years of development in stealth mode.\"\"\",\n        \n        # Made-up scientific research\n        \"\"\"Harlow Institute's Mycelium Strain HI-271 removes 99.7% of PFAS from contaminated soil.\n        Engineered fungi create symbiotic relationships with native soil bacteria.\n        Breaks down \"forever chemicals\" into non-toxic compounds within 60 days.\n        Field tests successfully remediated previously permanently contaminated industrial sites.\n        Deployment costs 80% less than traditional chemical extraction methods.\"\"\"\n    ]\n    \n    print(\"=\" * 50)\n    print(\"PocketFlow RAG Document Retrieval\")\n    print(\"=\" * 50)\n    \n    # Default query about the fictional technology\n    default_query = \"How to install PocketFlow?\"\n    \n    # Get query from command line if provided with --\n    query = default_query\n    for arg in sys.argv[1:]:\n        if arg.startswith(\"--\"):\n            query = arg[2:]\n            break\n    \n    # Single shared store for both flows\n    shared = {\n        \"texts\": texts,\n        \"embeddings\": None,\n        \"index\": None,\n        \"query\": query,\n        \"query_embedding\": None,\n        \"retrieved_document\": None,\n        \"generated_answer\": None\n    }\n    \n    # Initialize and run the offline flow (document indexing)\n    offline_flow.run(shared)\n    \n    # Run the online flow to retrieve the most relevant document and generate an answer\n    online_flow.run(shared)\n\n\nif __name__ == \"__main__\":\n    run_rag_demo()\n\n--- File Index 93: cookbook/pocketflow-rag/nodes.py ---\nfrom pocketflow import Node, Flow, BatchNode\nimport numpy as np\nimport faiss\nfrom utils import call_llm, get_embedding, get_simple_embedding, fixed_size_chunk\n\n# Nodes for the offline flow\nclass ChunkDocumentsNode(BatchNode):\n    def prep(self, shared):\n        \"\"\"Read texts from shared store\"\"\"\n        return shared[\"texts\"]\n    \n    def exec(self, text):\n        \"\"\"Chunk a single text into smaller pieces\"\"\"\n        return fixed_size_chunk(text)\n    \n    def post(self, shared, prep_res, exec_res_list):\n        \"\"\"Store chunked texts in the shared store\"\"\"\n        # Flatten the list of lists into a single list of chunks\n        all_chunks = []\n        for chunks in exec_res_list:\n            all_chunks.extend(chunks)\n        \n        # Replace the original texts with the flat list of chunks\n        shared[\"texts\"] = all_chunks\n        \n        print(f\"\u2705 Created {len(all_chunks)} chunks from {len(prep_res)} documents\")\n        return \"default\"\n    \nclass EmbedDocumentsNode(BatchNode):\n    def prep(self, shared):\n        \"\"\"Read texts from shared store and return as an iterable\"\"\"\n        return shared[\"texts\"]\n    \n    def exec(self, text):\n        \"\"\"Embed a single text\"\"\"\n        return get_embedding(text)\n    \n    def post(self, shared, prep_res, exec_res_list):\n        \"\"\"Store embeddings in the shared store\"\"\"\n        embeddings = np.array(exec_res_list, dtype=np.float32)\n        shared[\"embeddings\"] = embeddings\n        print(f\"\u2705 Created {len(embeddings)} document embeddings\")\n        return \"default\"\n\nclass CreateIndexNode(Node):\n    def prep(self, shared):\n        \"\"\"Get embeddings from shared store\"\"\"\n        return shared[\"embeddings\"]\n    \n    def exec(self, embeddings):\n        \"\"\"Create FAISS index and add embeddings\"\"\"\n        print(\"\ud83d\udd0d Creating search index...\")\n        dimension = embeddings.shape[1]\n        \n        # Create a flat L2 index\n        index = faiss.IndexFlatL2(dimension)\n        \n        # Add the embeddings to the index\n        index.add(embeddings)\n        \n        return index\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Store the index in shared store\"\"\"\n        shared[\"index\"] = exec_res\n        print(f\"\u2705 Index created with {exec_res.ntotal} vectors\")\n        return \"default\"\n\n# Nodes for the online flow\nclass EmbedQueryNode(Node):\n    def prep(self, shared):\n        \"\"\"Get query from shared store\"\"\"\n        return shared[\"query\"]\n    \n    def exec(self, query):\n        \"\"\"Embed the query\"\"\"\n        print(f\"\ud83d\udd0d Embedding query: {query}\")\n        query_embedding = get_embedding(query)\n        return np.array([query_embedding], dtype=np.float32)\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Store query embedding in shared store\"\"\"\n        shared[\"query_embedding\"] = exec_res\n        return \"default\"\n\nclass RetrieveDocumentNode(Node):\n    def prep(self, shared):\n        \"\"\"Get query embedding, index, and texts from shared store\"\"\"\n        return shared[\"query_embedding\"], shared[\"index\"], shared[\"texts\"]\n    \n    def exec(self, inputs):\n        \"\"\"Search the index for similar documents\"\"\"\n        print(\"\ud83d\udd0e Searching for relevant documents...\")\n        query_embedding, index, texts = inputs\n        \n        # Search for the most similar document\n        distances, indices = index.search(query_embedding, k=1)\n        \n        # Get the index of the most similar document\n        best_idx = indices[0][0]\n        distance = distances[0][0]\n        \n        # Get the corresponding text\n        most_relevant_text = texts[best_idx]\n        \n        return {\n            \"text\": most_relevant_text,\n            \"index\": best_idx,\n            \"distance\": distance\n        }\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Store retrieved document in shared store\"\"\"\n        shared[\"retrieved_document\"] = exec_res\n        print(f\"\ud83d\udcc4 Retrieved document (index: {exec_res['index']}, distance: {exec_res['distance']:.4f})\")\n        print(f\"\ud83d\udcc4 Most relevant text: \\\"{exec_res['text']}\\\"\")\n        return \"default\"\n    \nclass GenerateAnswerNode(Node):\n    def prep(self, shared):\n        \"\"\"Get query, retrieved document, and any other context needed\"\"\"\n        return shared[\"query\"], shared[\"retrieved_document\"]\n    \n    def exec(self, inputs):\n        \"\"\"Generate an answer using the LLM\"\"\"\n        query, retrieved_doc = inputs\n        \n        prompt = f\"\"\"\nBriefly answer the following question based on the context provided:\nQuestion: {query}\nContext: {retrieved_doc['text']}\nAnswer:\n\"\"\"\n        \n        answer = call_llm(prompt)\n        return answer\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Store generated answer in shared store\"\"\"\n        shared[\"generated_answer\"] = exec_res\n        print(\"\\n\ud83e\udd16 Generated Answer:\")\n        print(exec_res)\n        return \"default\"\n\n--- File Index 94: cookbook/pocketflow-rag/utils.py ---\nimport os\nimport numpy as np\nfrom openai import OpenAI\n\ndef call_llm(prompt):    \n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n    r = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return r.choices[0].message.content\n\ndef get_embedding(text):\n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n    \n    response = client.embeddings.create(\n        model=\"text-embedding-ada-002\",\n        input=text\n    )\n    \n    # Extract the embedding vector from the response\n    embedding = response.data[0].embedding\n    \n    # Convert to numpy array for consistency with other embedding functions\n    return np.array(embedding, dtype=np.float32)\n\ndef fixed_size_chunk(text, chunk_size=2000):\n    chunks = []\n    for i in range(0, len(text), chunk_size):\n        chunks.append(text[i : i + chunk_size])\n    return chunks\n\nif __name__ == \"__main__\":\n    print(\"=== Testing call_llm ===\")\n    prompt = \"In a few words, what is the meaning of life?\"\n    print(f\"Prompt: {prompt}\")\n    response = call_llm(prompt)\n    print(f\"Response: {response}\")\n\n    print(\"=== Testing embedding function ===\")\n    \n    text1 = \"The quick brown fox jumps over the lazy dog.\"\n    text2 = \"Python is a popular programming language for data science.\"\n    \n    oai_emb1 = get_embedding(text1)\n    oai_emb2 = get_embedding(text2)\n    print(f\"OpenAI Embedding 1 shape: {oai_emb1.shape}\")\n    oai_similarity = np.dot(oai_emb1, oai_emb2)\n    print(f\"OpenAI similarity between texts: {oai_similarity:.4f}\")\n\n--- File Index 95: cookbook/pocketflow-structured-output/README.md ---\n# Structured Output Demo\n\nA minimal demo application showing how to use PocketFlow to extract structured data from a resume using direct prompting and YAML formatting. Why YAML? Check out the [doc](https://the-pocket.github.io/PocketFlow/design_pattern/structure.html).\n\n## Features\n\n- Extracts structured data using prompt engineering\n- Validates output structure before processing\n\n## Run It\n\n1. Make sure your OpenAI API key is set:\n    ```bash\n    export OPENAI_API_KEY=\"your-api-key-here\"\n    ```\n    Alternatively, you can edit the `utils.py` file to include your API key directly.\n\n2. Edit data.txt with the resume you want to parse (a sample resume is already included)\n\n3. Install requirements and run the application:\n    ```bash\n    pip install -r requirements.txt\n    python main.py\n    ```\n\n## How It Works\n\n```mermaid\nflowchart LR\n    parser[ResumeParserNode]\n```\n\nThe Resume Parser application uses a single node that:\n1. Takes resume text from the shared state (loaded from data.txt)\n2. Sends the resume to an LLM with a prompt that requests YAML formatted output\n3. Extracts and validates the structured YAML data\n4. Outputs the structured result\n\n## Files\n\n- [`main.py`](./main.py): Implementation of the ResumeParserNode\n- [`utils.py`](./utils.py): LLM utilities\n- [`data.txt`](./data.txt): Sample resume text file\n \n## Example Output\n\n```\n=== STRUCTURED RESUME DATA ===\n\nname: John Smith\nemail: johnsmtih1983@gnail.com\nexperience:\n- title: Sales Manager\n  company: ABC Corporation\n- title: Assistant Manager\n  company: XYZ Industries\n- title: Customer Service Representative\n  company: Fast Solutions Inc\nskills:\n- Microsoft Office: Excel, Word, PowerPoint (Advanced)\n- Customer relationship management (CRM) software\n- Team leadership & management\n- Project management\n- Public speaking\n- Time management\n\n============================\n```\n\n\n--- File Index 96: cookbook/pocketflow-structured-output/main.py ---\nfrom pocketflow import Node, Flow\nfrom utils import call_llm\nimport yaml\n\nclass ResumeParserNode(Node):\n    def prep(self, shared):\n        \"\"\"Return resume text from shared state\"\"\"\n        return shared[\"resume_text\"]\n    \n    def exec(self, resume_text):\n        \"\"\"Extract structured data from resume using prompt engineering\"\"\"\n        prompt = f\"\"\"\nPlease extract the following information from this resume and format it as YAML:\n- name\n- email\n- experience (list of positions with title and company)\n- skills (list of skills)\n\n{resume_text}\n\nNow, output:\n```yaml\nname: John Doe\nemail: john@example.com\nexperience:\n  - title: Software Engineer\n    company: Tech Company\n  - title: Developer\n    company: Another Company\nskills:\n  - Python\n  - JavaScript\n  - HTML/CSS\n```\"\"\"\n        \n        response = call_llm(prompt)\n        \n        # Extract YAML content from markdown code blocks\n        yaml_str = response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n        structured_result = yaml.safe_load(yaml_str)\n        \n        # Validate structure\n        assert \"name\" in structured_result\n        assert \"experience\" in structured_result\n        assert isinstance(structured_result[\"experience\"], list)\n        assert \"skills\" in structured_result\n        assert isinstance(structured_result[\"skills\"], list)\n        \n        return structured_result\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Store and display structured resume data in YAML\"\"\"\n        shared[\"structured_data\"] = exec_res\n        \n        # Print structured data in YAML format\n        print(\"\\n=== STRUCTURED RESUME DATA ===\\n\")\n        print(yaml.dump(exec_res, sort_keys=False))\n        print(\"\\n============================\\n\")\n        \n        print(\"\u2705 Extracted basic resume information\")\n        \n# Create and run the flow\nif __name__ == \"__main__\":\n    print(\"=== Simple Resume Parser - YAML Output ===\\n\")\n    \n    # Read resume text from file\n    shared = {}\n    with open('data.txt', 'r') as file:\n        resume_text = file.read()\n    shared[\"resume_text\"] = resume_text\n\n    \n    flow = Flow(start=ResumeParserNode())\n    flow.run(shared)\n\n\n--- File Index 97: cookbook/pocketflow-structured-output/utils.py ---\nimport os\nfrom openai import OpenAI\n\ndef call_llm(prompt):    \n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n    r = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return r.choices[0].message.content\n\n# Example usage\nif __name__ == \"__main__\":\n    print(call_llm(\"Tell me a short joke\")) \n\n--- File Index 98: cookbook/pocketflow-supervisor/README.md ---\n# Research Supervisor\n\nThis project demonstrates a supervisor that oversees an unreliable [research agent](../pocketflow-agent) to ensure high-quality answers.\n\n## Features\n\n- Evaluates responses for quality and relevance\n- Rejects nonsensical or unreliable answers\n- Requests new answers until a quality response is produced\n\n## Getting Started\n\n1. Install the packages you need with this simple command:\n```bash\npip install -r requirements.txt\n```\n\n2. Let's get your OpenAI API key ready:\n\n```bash\nexport OPENAI_API_KEY=\"your-api-key-here\"\n```\n\n3. Let's do a quick check to make sure your API key is working properly:\n\n```bash\npython utils.py\n```\n\nThis will test both the LLM call and web search features. If you see responses, you're good to go!\n\n4. Try out the agent with the default question (about Nobel Prize winners):\n\n```bash\npython main.py\n```\n\n5. Got a burning question? Ask anything you want by using the `--` prefix:\n\n```bash\npython main.py --\"What is quantum computing?\"\n```\n\n## How It Works?\n\nThe magic happens through a simple but powerful graph structure with these main components:\n\n```mermaid\ngraph TD\n    subgraph InnerAgent[Inner Research Agent]\n        DecideAction -->|\"search\"| SearchWeb\n        DecideAction -->|\"answer\"| UnreliableAnswerNode\n        SearchWeb -->|\"decide\"| DecideAction\n    end\n    \n    InnerAgent --> SupervisorNode\n    SupervisorNode -->|\"retry\"| InnerAgent\n```\n\nHere's what each part does:\n1. **DecideAction**: The brain that figures out whether to search or answer based on current context\n2. **SearchWeb**: The researcher that goes out and finds information using web search\n3. **UnreliableAnswerNode**: Generates answers (with a 50% chance of being unreliable)\n4. **SupervisorNode**: Quality control that validates answers and rejects nonsensical ones\n\n## Example Output\n\n```\n\ud83e\udd14 Processing question: Who won the Nobel Prize in Physics 2024?\n\ud83e\udd14 Agent deciding what to do next...\n\ud83d\udd0d Agent decided to search for: Nobel Prize in Physics 2024 winner\n\ud83c\udf10 Searching the web for: Nobel Prize in Physics 2024 winner\n\ud83d\udcda Found information, analyzing results...\n\ud83e\udd14 Agent deciding what to do next...\n\ud83d\udca1 Agent decided to answer the question\n\ud83e\udd2a Generating unreliable dummy answer...\n\u2705 Answer generated successfully\n    \ud83d\udd0d Supervisor checking answer quality...\n    \u274c Supervisor rejected answer: Answer appears to be nonsensical or unhelpful\n\ud83e\udd14 Agent deciding what to do next...\n\ud83d\udca1 Agent decided to answer the question\n\u270d\ufe0f Crafting final answer...\n\u2705 Answer generated successfully\n    \ud83d\udd0d Supervisor checking answer quality...\n    \u2705 Supervisor approved answer: Answer appears to be legitimate\n\n\ud83c\udfaf Final Answer:\nThe Nobel Prize in Physics for 2024 was awarded jointly to John J. Hopfield and Geoffrey Hinton. They were recognized \"for foundational discoveries and inventions that enable machine learning with artificial neural networks.\" Their work has been pivotal in the field of artificial intelligence, specifically in developing the theories and technologies that support machine learning using artificial neural networks. John Hopfield is associated with Princeton University, while Geoffrey Hinton is connected to the University of Toronto. Their achievements have laid essential groundwork for advancements in AI and its widespread application across various domains.\n```\n\n## Files\n\n- [`main.py`](./main.py): The starting point - runs the whole show!\n- [`flow.py`](./flow.py): Connects everything together into a smart agent with supervision\n- [`nodes.py`](./nodes.py): The building blocks that make decisions, take actions, and validate answers\n- [`utils.py`](./utils.py): Helper functions for talking to the LLM and searching the web\n\n\n--- File Index 99: cookbook/pocketflow-supervisor/flow.py ---\nfrom pocketflow import Flow\nfrom nodes import DecideAction, SearchWeb, UnreliableAnswerNode, SupervisorNode\n\ndef create_agent_inner_flow():\n    \"\"\"\n    Create the inner research agent flow without supervision.\n    \n    This flow handles the research cycle:\n    1. DecideAction node decides whether to search or answer\n    2. If search, go to SearchWeb node and return to decide\n    3. If answer, go to UnreliableAnswerNode\n    \n    Returns:\n        Flow: A research agent flow\n    \"\"\"\n    # Create instances of each node\n    decide = DecideAction()\n    search = SearchWeb()\n    answer = UnreliableAnswerNode()\n    \n    # Connect the nodes\n    # If DecideAction returns \"search\", go to SearchWeb\n    decide - \"search\" >> search\n    \n    # If DecideAction returns \"answer\", go to UnreliableAnswerNode\n    decide - \"answer\" >> answer\n    \n    # After SearchWeb completes and returns \"decide\", go back to DecideAction\n    search - \"decide\" >> decide\n    \n    # Create and return the inner flow, starting with the DecideAction node\n    return Flow(start=decide)\n\ndef create_agent_flow():\n    \"\"\"\n    Create a supervised agent flow by treating the entire agent flow as a node\n    and placing the supervisor outside of it.\n    \n    The flow works like this:\n    1. Inner agent flow does research and generates an answer\n    2. SupervisorNode checks if the answer is valid\n    3. If answer is valid, flow completes\n    4. If answer is invalid, restart the inner agent flow\n    \n    Returns:\n        Flow: A complete research agent flow with supervision\n    \"\"\"\n    # Create the inner flow\n    agent_flow = create_agent_inner_flow()\n    \n    # Create the supervisor node\n    supervisor = SupervisorNode()\n    \n    # Connect the components\n    # After agent_flow completes, go to supervisor\n    agent_flow >> supervisor\n    \n    # If supervisor rejects the answer, go back to agent_flow\n    supervisor - \"retry\" >> agent_flow\n    \n    # Create and return the outer flow, starting with the agent_flow\n    return Flow(start=agent_flow) \n\n--- File Index 100: cookbook/pocketflow-supervisor/main.py ---\nimport sys\nfrom flow import create_agent_flow\n\ndef main():\n    \"\"\"Simple function to process a question with supervised answers.\"\"\"\n    # Default question\n    default_question = \"Who won the Nobel Prize in Physics 2024?\"\n    \n    # Get question from command line if provided with --\n    question = default_question\n    for arg in sys.argv[1:]:\n        if arg.startswith(\"--\"):\n            question = arg[2:]\n            break\n    \n    # Create the agent flow with supervision\n    agent_flow = create_agent_flow()\n    \n    # Process the question\n    shared = {\"question\": question}\n    print(f\"\ud83e\udd14 Processing question: {question}\")\n    agent_flow.run(shared)\n    print(\"\\n\ud83c\udfaf Final Answer:\")\n    print(shared.get(\"answer\", \"No answer found\"))\n\nif __name__ == \"__main__\":\n    main()\n\n--- File Index 101: cookbook/pocketflow-supervisor/nodes.py ---\nfrom pocketflow import Node\nfrom utils import call_llm, search_web\nimport yaml\nimport random\n\nclass DecideAction(Node):\n    def prep(self, shared):\n        \"\"\"Prepare the context and question for the decision-making process.\"\"\"\n        # Get the current context (default to \"No previous search\" if none exists)\n        context = shared.get(\"context\", \"No previous search\")\n        # Get the question from the shared store\n        question = shared[\"question\"]\n        # Return both for the exec step\n        return question, context\n        \n    def exec(self, inputs):\n        \"\"\"Call the LLM to decide whether to search or answer.\"\"\"\n        question, context = inputs\n        \n        print(f\"\ud83e\udd14 Agent deciding what to do next...\")\n        \n        # Create a prompt to help the LLM decide what to do next\n        prompt = f\"\"\"\n### CONTEXT\nYou are a research assistant that can search the web.\nQuestion: {question}\nPrevious Research: {context}\n\n### ACTION SPACE\n[1] search\n  Description: Look up more information on the web\n  Parameters:\n    - query (str): What to search for\n\n[2] answer\n  Description: Answer the question with current knowledge\n  Parameters:\n    - answer (str): Final answer to the question\n\n## NEXT ACTION\nDecide the next action based on the context and available actions.\nReturn your response in this format:\n\n```yaml\nthinking: |\n    <your step-by-step reasoning process>\naction: search OR answer\nreason: <why you chose this action>\nsearch_query: <specific search query if action is search>\n```\"\"\"\n        \n        # Call the LLM to make a decision\n        response = call_llm(prompt)\n        \n        # Parse the response to get the decision\n        yaml_str = response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n        decision = yaml.safe_load(yaml_str)\n        \n        return decision\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Save the decision and determine the next step in the flow.\"\"\"\n        # If LLM decided to search, save the search query\n        if exec_res[\"action\"] == \"search\":\n            shared[\"search_query\"] = exec_res[\"search_query\"]\n            print(f\"\ud83d\udd0d Agent decided to search for: {exec_res['search_query']}\")\n        else:\n            print(f\"\ud83d\udca1 Agent decided to answer the question\")\n        \n        # Return the action to determine the next node in the flow\n        return exec_res[\"action\"]\n\nclass SearchWeb(Node):\n    def prep(self, shared):\n        \"\"\"Get the search query from the shared store.\"\"\"\n        return shared[\"search_query\"]\n        \n    def exec(self, search_query):\n        \"\"\"Search the web for the given query.\"\"\"\n        # Call the search utility function\n        print(f\"\ud83c\udf10 Searching the web for: {search_query}\")\n        results = search_web(search_query)\n        return results\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Save the search results and go back to the decision node.\"\"\"\n        # Add the search results to the context in the shared store\n        previous = shared.get(\"context\", \"\")\n        shared[\"context\"] = previous + \"\\n\\nSEARCH: \" + shared[\"search_query\"] + \"\\nRESULTS: \" + exec_res\n        \n        print(f\"\ud83d\udcda Found information, analyzing results...\")\n        \n        # Always go back to the decision node after searching\n        return \"decide\"\n\nclass UnreliableAnswerNode(Node):\n    def prep(self, shared):\n        \"\"\"Get the question and context for answering.\"\"\"\n        return shared[\"question\"], shared.get(\"context\", \"\")\n        \n    def exec(self, inputs):\n        \"\"\"Call the LLM to generate a final answer with 50% chance of returning a dummy answer.\"\"\"\n        question, context = inputs\n        \n        # 50% chance to return a dummy answer\n        if random.random() < 0.5:\n            print(f\"\ud83e\udd2a Generating unreliable dummy answer...\")\n            return \"Sorry, I'm on a coffee break right now. All information I provide is completely made up anyway. The answer to your question is 42, or maybe purple unicorns. Who knows? Certainly not me!\"\n        \n        print(f\"\u270d\ufe0f Crafting final answer...\")\n        \n        # Create a prompt for the LLM to answer the question\n        prompt = f\"\"\"\n### CONTEXT\nBased on the following information, answer the question.\nQuestion: {question}\nResearch: {context}\n\n## YOUR ANSWER:\nProvide a comprehensive answer using the research results.\n\"\"\"\n        # Call the LLM to generate an answer\n        answer = call_llm(prompt)\n        return answer\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Save the final answer and complete the flow.\"\"\"\n        # Save the answer in the shared store\n        shared[\"answer\"] = exec_res\n        \n        print(f\"\u2705 Answer generated successfully\")\n\nclass SupervisorNode(Node):\n    def prep(self, shared):\n        \"\"\"Get the current answer for evaluation.\"\"\"\n        return shared[\"answer\"]\n    \n    def exec(self, answer):\n        \"\"\"Check if the answer is valid or nonsensical.\"\"\"\n        print(f\"    \ud83d\udd0d Supervisor checking answer quality...\")\n        \n        # Check for obvious markers of the nonsense answers\n        nonsense_markers = [\n            \"coffee break\", \n            \"purple unicorns\", \n            \"made up\", \n            \"42\", \n            \"Who knows?\"\n        ]\n        \n        # Check if the answer contains any nonsense markers\n        is_nonsense = any(marker in answer for marker in nonsense_markers)\n        \n        if is_nonsense:\n            return {\"valid\": False, \"reason\": \"Answer appears to be nonsensical or unhelpful\"}\n        else:\n            return {\"valid\": True, \"reason\": \"Answer appears to be legitimate\"}\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Decide whether to accept the answer or restart the process.\"\"\"\n        if exec_res[\"valid\"]:\n            print(f\"    \u2705 Supervisor approved answer: {exec_res['reason']}\")\n        else:\n            print(f\"    \u274c Supervisor rejected answer: {exec_res['reason']}\")\n            # Clean up the bad answer\n            shared[\"answer\"] = None\n            # Add a note about the rejected answer\n            context = shared.get(\"context\", \"\")\n            shared[\"context\"] = context + \"\\n\\nNOTE: Previous answer attempt was rejected by supervisor.\"\n            return \"retry\" \n\n--- File Index 102: cookbook/pocketflow-supervisor/utils.py ---\nfrom openai import OpenAI\nimport os\nfrom duckduckgo_search import DDGS\n\ndef call_llm(prompt):    \n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n    r = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return r.choices[0].message.content\n\ndef search_web(query):\n    results = DDGS().text(query, max_results=5)\n    # Convert results to a string\n    results_str = \"\\n\\n\".join([f\"Title: {r['title']}\\nURL: {r['href']}\\nSnippet: {r['body']}\" for r in results])\n    return results_str\n    \nif __name__ == \"__main__\":\n    print(\"## Testing call_llm\")\n    prompt = \"In a few words, what is the meaning of life?\"\n    print(f\"## Prompt: {prompt}\")\n    response = call_llm(prompt)\n    print(f\"## Response: {response}\")\n\n    print(\"## Testing search_web\")\n    query = \"Who won the Nobel Prize in Physics 2024?\"\n    print(f\"## Query: {query}\")\n    results = search_web(query)\n    print(f\"## Results: {results}\")\n\n--- File Index 103: cookbook/pocketflow-thinking/README.md ---\n# Extended Thinking\n\nThis project demonstrates an extended thinking mode implementation that enables LLMs to solve complex reasoning problems by thinking step-by-step. It's designed to improve problem-solving accuracy through deliberate reasoning.\n\n## Features\n\n- Improves model reasoning on complex problems\n- Works with models like Claude 3.7 Sonnet that support extended thinking\n- Solves problems that direct prompting often fails on\n- Provides detailed reasoning traces for verification\n\n## Getting Started\n\n1. Install the required packages:\n```bash\npip install -r requirements.txt\n```\n\n2. Set up your API key:\n```bash\nexport ANTHROPIC_API_KEY=\"your-api-key-here\"\n```\n\n3. Run a test problem to see thinking mode in action:\n```bash\npython main.py\n```\n\n4. Try your own reasoning problem:\n```bash\npython main.py --\"Your complex reasoning problem here\"\n```\n\n## How It Works\n\nThe implementation uses a self-looping Chain of Thought node that allows an LLM to think through complex problems step by step:\n\n```mermaid\nflowchart LR\n    cot[ChainOfThoughtNode] -->|\"continue\"| cot\n```\n\nEach time the node loops, it:\n1. Reads the problem and previous thoughts\n2. Generates the next thought or final solution\n3. Decides whether more thinking is needed\n\nThis approach helps LLMs solve problems that would be difficult with a single-pass approach.\n\n## Comparison with Different Approaches\n\n- **Standard prompting**: Telling the AI to \"think step by step\" or providing examples helps, but the thinking is usually not significant enough\n- **Extended thinking models**: Models like Claude 3.7 Sonnet, GPT-4o, and Deepseek R1 natively support extended thinking with much better results\n- **This implementation**: Explores how to achieve extended thinking with non-thinking models\n\n## Example Thinking Process\n\n> **Problem**: Break a stick, then break the longer piece again. What's the probability of forming a triangle?\n\nThis problem demonstrates why extended thinking is valuable:\n\n- **Standard models without thinking**: Get the wrong answer\n- **Models with extended thinking**: Find the correct answer (0.386)\n\nFor comparison:\n- [Claude 3.7 Sonnet (without thinking)](https://claude.ai/share/31bf938c-94dd-42f6-bfac-e82ba3616dbc): Wrong answer\n- [Claude 3.7 Sonnet with thinking](https://claude.ai/share/0863f9fd-ae75-4a0c-84ee-f7443d2fcf4a): Correct answer after 4.5 min\n- [GPT-o1 with thinking](https://chatgpt.com/c/67dcbad0-75c8-8000-a538-ee6df8083832): Correct answer after 0.5 min\n- [GPT-o1 pro with thinking](https://chatgpt.com/share/67dcb1bf-ceb0-8000-823a-8ce894032e37): Correct answer after 1.5 min\n\n\nBelow is an example of how Claude 3.7 Sonnet (without native thinking) to solve this complex problem, and get the correct result:\n\n```\n\ud83e\udd14 Processing question: Break a stick, then break the longer piece again. What's the probability of forming a triangle?\n\nLet me understand the problem. We have a stick of some length, which we can call 1 for simplicity. We break it at a random point, creating two pieces. Then we break the longer piece again at a random point. This gives us three pieces. We want to find the probability that these three pieces can form a triangle.\n\nFor three segments to form a triangle, the sum of the lengths of any two sides must be greater than the length of the third side. So we need to check:\na + b > c\na + c > b\nb + c > a\n\nWhere a, b, and c are the lengths of the three pieces.\n\nNext thought needed: True\nTotal thoughts estimate: 8\n--------------------------------------------------\n\nThought 2: Let me set up the problem mathematically. Let's say the original stick has length 1. \n\nFirst break: We break the stick at a point X, where X is chosen uniformly from [0,1]. This gives us two pieces with lengths X and 1-X.\n\nSecond break: We break the longer piece at a random point. If X \u2264 0.5, then 1-X is the longer piece. If X > 0.5, then X is the longer piece.\n\nCase 1: If X \u2264 0.5, the longer piece is 1-X. We break it at point Y(1-X), where Y is chosen uniformly from [0,1]. This gives pieces of lengths:\n- X (from first break)\n- Y(1-X) (from second break)\n- (1-Y)(1-X) (remaining from second break)\n\nCase 2: If X > 0.5, the longer piece is X. We break it at point YX, where Y is chosen uniformly from [0,1]. This gives pieces of lengths:\n- 1-X (from first break)\n- YX (from second break)\n- (1-Y)X (remaining from second break)\n\nFor each case, I need to determine when these three pieces can form a triangle, then calculate the probability.\n\nNext thought needed: True\nTotal thoughts estimate: 8\n--------------------------------------------------\n\nThought 3: Let's analyze when the three pieces can form a triangle for both cases.\n\nFor a triangle to form, each side must be less than the sum of the other two sides. So we need:\n\nCase 1 (X \u2264 0.5): We have pieces of lengths X, Y(1-X), and (1-Y)(1-X)\n\nThe triangle inequality conditions are:\n- X + Y(1-X) > (1-Y)(1-X)\n- X + (1-Y)(1-X) > Y(1-X)\n- Y(1-X) + (1-Y)(1-X) > X\n\nSimplifying:\n- X + Y(1-X) > (1-Y)(1-X)\n- X + (1-Y)(1-X) > Y(1-X)\n- (1-X) > X  (since Y(1-X) + (1-Y)(1-X) = 1-X)\n\nThe third condition is always true when X \u2264 0.5 (which is our assumption for Case 1).\n\nLet's work on the first condition:\nX + Y(1-X) > (1-Y)(1-X)\nX + Y(1-X) > (1-X) - Y(1-X)\nX + 2Y(1-X) > (1-X)\nX + 2Y(1-X) > 1-X\n2X + 2Y(1-X) > 1\n2X + 2Y - 2YX > 1\n\nSimilarly for the second condition:\nX + (1-Y)(1-X) > Y(1-X)\nX + (1-X) - Y(1-X) > Y(1-X)\nX + (1-X) > 2Y(1-X)\n1 > 2Y(1-X)\n1/(2(1-X)) > Y\n\nSo for Case 1, we need:\n- 2X + 2Y - 2YX > 1\n- Y < 1/(2(1-X))\n\nNext thought needed: True\nTotal thoughts estimate: 8\n--------------------------------------------------\n\nThought 4: I need to continue analyzing Case 1 and then move on to Case 2.\n\nFor Case 1 (X \u2264 0.5), we have the constraints:\n- 2X + 2Y - 2YX > 1\n- Y < 1/(2(1-X))\n\nLet's rewrite the first constraint:\n2X + 2Y - 2YX > 1\n2Y(1-X) > 1 - 2X\nY > (1 - 2X)/(2(1-X))\n\nSo for Case 1, we need:\n- Y > (1 - 2X)/(2(1-X))\n- Y < 1/(2(1-X))\n\nThis means Y must be in the range ((1 - 2X)/(2(1-X)), 1/(2(1-X))). For this range to be valid, we need:\n(1 - 2X)/(2(1-X)) < 1/(2(1-X))\n1 - 2X < 1\n-2X < 0\nX > 0\n\nWhich is true for our domain. So we have a valid range for Y when X \u2264 0.5.\n\nNow let's analyze Case 2 (X > 0.5): We have pieces of lengths (1-X), YX, and (1-Y)X\n\nThe triangle inequality conditions are:\n- (1-X) + YX > (1-Y)X\n- (1-X) + (1-Y)X > YX\n- YX + (1-Y)X > (1-X)\n\nSimplifying:\n- (1-X) + YX > (1-Y)X\n- (1-X) + (1-Y)X > YX\n- X > (1-X)  (since YX + (1-Y)X = X)\n\nThe third condition is always true when X > 0.5 (which is our assumption for Case 2).\n\nNext thought needed: True\nTotal thoughts estimate: 8\n--------------------------------------------------\n\nThought 5: Continuing with Case 2 (X > 0.5), let's simplify the remaining triangle inequality conditions:\n\nFirst condition:\n(1-X) + YX > (1-Y)X\n(1-X) + YX > X - YX\n(1-X) + 2YX > X\n1 - X + 2YX > X\n1 + 2YX > 2X\n2YX > 2X - 1\nY > (2X - 1)/(2X)\n\nSecond condition:\n(1-X) + (1-Y)X > YX\n(1-X) + X - YX > YX\n(1-X) + X > 2YX\n1 > 2YX\n1/(2X) > Y\n\nSo for Case 2, we need:\n- Y > (2X - 1)/(2X)\n- Y < 1/(2X)\n\nThis means Y must be in the range ((2X - 1)/(2X), 1/(2X)). For this range to be valid, we need:\n(2X - 1)/(2X) < 1/(2X)\n2X - 1 < 1\n2X < 2\nX < 1\n\nWhich is true for our domain. So we have a valid range for Y when X > 0.5.\n\nNow I need to calculate the probability. For both cases, I need to find the area of the region in the X-Y plane where the triangle can be formed, divided by the total area.\n\nNext thought needed: True\nTotal thoughts estimate: 8\n--------------------------------------------------\n\nThought 6: Now I need to calculate the probability by finding the area in the X-Y plane where the triangle can be formed.\n\nFor Case 1 (X \u2264 0.5):\n- Y must be between (1 - 2X)/(2(1-X)) and 1/(2(1-X))\n\nThe probability for this case is the area of the region divided by the total area of the X-Y plane for X \u2264 0.5, which is 0.5 \u00d7 1 = 0.5.\n\nArea = \u222b\u2080^(0.5) [1/(2(1-X)) - (1 - 2X)/(2(1-X))] dX\n     = \u222b\u2080^(0.5) [(1 - (1 - 2X))/(2(1-X))] dX\n     = \u222b\u2080^(0.5) [2X/(2(1-X))] dX\n     = \u222b\u2080^(0.5) [X/(1-X)] dX\n\nThis integral equals -X - ln(1-X) evaluated from 0 to 0.5, which is:\n[-0.5 - ln(0.5)] - [0 - ln(1)] = -0.5 - (-0.693) = -0.5 + 0.693 = 0.193\n\nSo the probability for Case 1 is 0.193/0.5 = 0.386.\n\nFor Case 2 (X > 0.5):\n- Y must be between (2X - 1)/(2X) and 1/(2X)\n\nThe probability for this case is the area of the region divided by the total area of the X-Y plane for X > 0.5, which is 0.5 \u00d7 1 = 0.5.\n\nArea = \u222b\u2080.\u2085^1 [1/(2X) - (2X - 1)/(2X)] dX\n     = \u222b\u2080.\u2085^1 [(1 - (2X - 1))/(2X)] dX\n     = \u222b\u2080.\u2085^1 [(2 - 2X)/(2X)] dX\n     = \u222b\u2080.\u2085^1 [(1 - X)/X] dX\n     = \u222b\u2080.\u2085^1 [1/X - 1] dX\n     = [ln|X| - X]\u2080.\u2085^1\n     = [ln(1) - 1] - [ln(0.5) - 0.5] = -1 - (-0.693 - 0.5) = -1 + 0.693 + 0.5 = 0.193\n\nSo the probability for Case 2 is also 0.193/0.5 = 0.386.\n\nThe total probability is the sum of the probabilities for both cases: 0.386 + 0.386 = 0.772.\n\nWait, I should double-check my calculations. Let me reconsider the integrals.\n\nNext thought needed: True\nTotal thoughts estimate: 8\n--------------------------------------------------\n\nThought 7: Let me double-check the calculations for both cases more carefully.\n\nFor Case 1 (X \u2264 0.5), the constraints are:\n- Y > (1 - 2X)/(2(1-X))\n- Y < 1/(2(1-X))\n\nThe area for which the triangle can be formed is:\n\u222b\u2080^(0.5) [1/(2(1-X)) - (1 - 2X)/(2(1-X))] dX\n\nSimplifying the integrand:\n[1 - (1 - 2X)]/(2(1-X)) = 2X/(2(1-X)) = X/(1-X)\n\nSo we have:\n\u222b\u2080^(0.5) X/(1-X) dX\n\nThis can be rewritten as:\n\u222b\u2080^(0.5) [1/(1-X) - 1] dX = \u222b\u2080^(0.5) 1/(1-X) dX - \u222b\u2080^(0.5) 1 dX\n= -ln|1-X|\u2080^(0.5) - X\u2080^(0.5)\n= -ln(0.5) - 0.5 - (-ln(1) - 0)\n= -ln(0.5) - 0.5\n= 0.693 - 0.5\n= 0.193\n\nSo the probability for Case 1 is 0.193/0.5 = 0.386.\n\nFor Case 2 (X > 0.5), the constraints are:\n- Y > (2X - 1)/(2X)\n- Y < 1/(2X)\n\nThe area for which the triangle can be formed is:\n\u222b\u2080.\u2085^1 [1/(2X) - (2X - 1)/(2X)] dX\n\nSimplifying the integrand:\n[1 - (2X - 1)]/(2X) = (2 - 2X)/(2X) = (1 - X)/X\n\nSo we have:\n\u222b\u2080.\u2085^1 (1 - X)/X dX = \u222b\u2080.\u2085^1 (1/X - 1) dX\n= ln|X|\u2080.\u2085^1 - X\u2080.\u2085^1\n= ln(1) - ln(0.5) - (1 - 0.5)\n= 0 - (-0.693) - 0.5\n= 0.693 - 0.5\n= 0.193\n\nSo the probability for Case 2 is also 0.193/0.5 = 0.386.\n\nThe total probability is the sum of both: 0.386 + 0.386 = 0.772.\n\nThis confirms our previous calculation. However, I need to make sure this is correct by thinking about it differently. The total area in the X-Y plane is 1 (as both X and Y range from 0 to 1). The area where triangles can form is 0.193 + 0.193 = 0.386. So the probability should be 0.386/1 = 0.386, not 0.772.\n\nWait, that's not right. I need to be more careful about the sample space...\n\nNext thought needed: True\nTotal thoughts estimate: 8\n--------------------------------------------------\n\n=== FINAL SOLUTION ===\nThought 8: I need to clarify my understanding of the problem and the probability calculation.\n\nThe problem involves two random breaks: first breaking a stick at a random point X, and then breaking the longer piece at a random point Y (proportional to the length of that piece). This gives us a sample space where both X and Y are uniformly distributed between 0 and 1, giving a total area of 1\u00d71 = 1.\n\nI've calculated the areas where triangles can form in two cases:\n- Case 1 (X \u2264 0.5): Area = 0.193\n- Case 2 (X > 0.5): Area = 0.193\n\nThe total area where triangles can form is 0.193 + 0.193 = 0.386.\n\nSince the total sample space has area 1, the probability is 0.386/1 = 0.386.\n\nWait - I see my mistake in Thought 6 and 7. I incorrectly divided by 0.5 (the range of X in each case), but I should divide by the total area of the sample space, which is 1.\n\nSo the final probability is 0.386, or approximately 25/65 \u2248 0.385.\n\nAfter further reflection, let me represent this as ln(2) - 1/2, which equals approximately 0.693 - 0.5 = 0.193 for each case, giving a total probability of 2(ln(2) - 1/2) = 2ln(2) - 1 \u2248 0.386.\n\nTherefore, the probability of forming a triangle is 2ln(2) - 1, which is approximately 0.386 or about 39%.\n\n======================\n```\n\n> Note: Even with thinking mode, models don't always get the right answer, but their accuracy significantly improves on complex reasoning tasks.\n\n\n--- File Index 104: cookbook/pocketflow-thinking/design.md ---\n# Chain of Thought Node\n\n## 1. Requirements\nCreate a self-looping Chain of Thought node that can:\n- Generate thoughts to solve a problem step by step\n- Revise previous thoughts when necessary\n- Branch to explore alternative approaches\n- Track thought numbers and adjust total thoughts dynamically\n- Generate and verify hypotheses\n- Provide a final solution when reasoning is complete\n\n## 2. Flow Design\nThis will be a simple flow with a single node that can call itself repeatedly:\n\n```mermaid\nflowchart LR\n    cot[ChainOfThoughtNode] -->|\"continue\"| cot\n```\n\n## 3. Utilities\nWe'll need one primary utility function:\n- `call_llm`: Call LLM to generate the next thought based on the problem and previous thoughts\n\n## 4. Node Design\n### Shared Store Design\n```python\nshared = {\n    \"problem\": \"The problem statement goes here\",\n    \"thoughts\": [],  # List of thought objects\n    \"current_thought_number\": 0,\n    \"total_thoughts_estimate\": 5,  # Initial estimate, can change\n    \"solution\": None  # Final solution when complete\n}\n```\n\nEach thought in the \"thoughts\" list will be a dictionary with:\n```python\n{\n    \"content\": \"The actual thought text\",\n    \"thought_number\": 1,\n    \"is_revision\": False,\n    \"revises_thought\": None,\n    \"branch_from_thought\": None,\n    \"branch_id\": None,\n    \"next_thought_needed\": True\n}\n```\n\n### Chain of Thought Node\n- `type`: Regular (self-looping)\n- `prep`: Read the problem and all thoughts so far from shared store\n- `exec`: Call LLM to generate next thought or solution\n- `post`: Update shared store with the new thought and decide whether to continue or finish\n\n--- File Index 105: cookbook/pocketflow-thinking/flow.py ---\nfrom pocketflow import Flow\nfrom nodes import ChainOfThoughtNode\n\ndef create_chain_of_thought_flow():\n    # Create a ChainOfThoughtNode\n    cot_node = ChainOfThoughtNode()\n    \n    # Connect the node to itself for the \"continue\" action\n    cot_node - \"continue\" >> cot_node\n    \n    # Create the flow\n    cot_flow = Flow(start=cot_node)\n    return cot_flow\n\n--- File Index 106: cookbook/pocketflow-thinking/main.py ---\nimport sys\nfrom flow import create_chain_of_thought_flow\n\ndef main():\n    # Default question\n    default_question = \"Break a stick, then break the longer piece again. What's the probability of forming a triangle?\"\n    \n    # Get question from command line if provided with --\n    question = default_question\n    for arg in sys.argv[1:]:\n        if arg.startswith(\"--\"):\n            question = arg[2:]\n            break\n    \n    print(f\"\ud83e\udd14 Processing question: {question}\")   \n\n    # Create the flow\n    cot_flow = create_chain_of_thought_flow()\n\n    # Set up shared state\n    shared = {\n        \"problem\": question,\n        \"thoughts\": [],\n        \"current_thought_number\": 0,\n        \"total_thoughts_estimate\": 10,\n        \"solution\": None\n    }\n    \n    # Run the flow\n    cot_flow.run(shared)\n    \nif __name__ == \"__main__\":\n    main()\n\n--- File Index 107: cookbook/pocketflow-thinking/nodes.py ---\nfrom pocketflow import Node\nimport yaml\nfrom utils import call_llm\n\nclass ChainOfThoughtNode(Node):\n    def prep(self, shared):\n        # Gather problem and previous thoughts\n        problem = shared.get(\"problem\", \"\")\n        thoughts = shared.get(\"thoughts\", [])\n        current_thought_number = shared.get(\"current_thought_number\", 0)\n        # Increment the current thought number in the shared store\n        shared[\"current_thought_number\"] = current_thought_number + 1\n        total_thoughts_estimate = shared.get(\"total_thoughts_estimate\", 5)\n        \n        # Format previous thoughts\n        thoughts_text = \"\\n\".join([\n            f\"Thought {t['thought_number']}: {t['content']}\" +\n            (f\" (Revision of Thought {t['revises_thought']})\" if t.get('is_revision') and t.get('revises_thought') else \"\") +\n            (f\" (Branch from Thought {t['branch_from_thought']}, Branch ID: {t['branch_id']})\" \n             if t.get('branch_from_thought') else \"\")\n            for t in thoughts\n        ])\n        \n        return {\n            \"problem\": problem,\n            \"thoughts_text\": thoughts_text,\n            \"thoughts\": thoughts,\n            \"current_thought_number\": current_thought_number + 1, \n            \"total_thoughts_estimate\": total_thoughts_estimate\n        }\n    \n    def exec(self, prep_res):\n        problem = prep_res[\"problem\"]\n        thoughts_text = prep_res[\"thoughts_text\"]\n        current_thought_number = prep_res[\"current_thought_number\"] \n        total_thoughts_estimate = prep_res[\"total_thoughts_estimate\"]\n        \n        # Create the prompt for the LLM\n        prompt = f\"\"\"\nYou are solving a hard problem using Chain of Thought reasoning. Think step-by-step.\n\nProblem: {problem}\n\nPrevious thoughts:\n{thoughts_text if thoughts_text else \"No previous thoughts yet.\"}\n\nPlease generate the next thought (Thought {current_thought_number}). You can:\n1. Continue with the next logical step\n2. Revise a previous thought if needed\n3. Branch into a new line of thinking\n4. Generate a hypothesis if you have enough information\n5. Verify a hypothesis against your reasoning\n6. Provide a final solution if you've reached a conclusion\n\nCurrent thought number: {current_thought_number}\nCurrent estimate of total thoughts needed: {total_thoughts_estimate}\n\nFormat your response as a YAML structure with these fields:\n- content: Your thought content\n- next_thought_needed: true/false (true if more thinking is needed)\n- is_revision: true/false (true if revising a previous thought)\n- revises_thought: null or number (if is_revision is true)\n- branch_from_thought: null or number (if branching from previous thought)\n- branch_id: null or string (a short identifier for this branch)\n- total_thoughts: number (your updated estimate if changed)\n\nOnly set next_thought_needed to false when you have a complete solution and the content explains the solution.\nOutput in YAML format:\n```yaml\ncontent: |\n  # If you have a complete solution, explain the solution here.\n  # If it's a revision, provide the updated thought here.\n  # If it's a branch, provide the new thought here.\nnext_thought_needed: true/false\nis_revision: true/false\nrevises_thought: null or number\nbranch_from_thought: null or number\nbranch_id: null or string\ntotal_thoughts: number\n```\"\"\"\n        \n        response = call_llm(prompt)\n        yaml_str = response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n        thought_data = yaml.safe_load(yaml_str)\n        \n        # Add thought number\n        thought_data[\"thought_number\"] = current_thought_number\n        return thought_data\n\n    \n    def post(self, shared, prep_res, exec_res):\n        # Add the new thought to the list\n        if \"thoughts\" not in shared:\n            shared[\"thoughts\"] = []\n        \n        shared[\"thoughts\"].append(exec_res)\n        \n        # Update total_thoughts_estimate if changed\n        if \"total_thoughts\" in exec_res and exec_res[\"total_thoughts\"] != shared.get(\"total_thoughts_estimate\", 5):\n            shared[\"total_thoughts_estimate\"] = exec_res[\"total_thoughts\"]\n        \n        # If we're done, extract the solution from the last thought\n        if exec_res.get(\"next_thought_needed\", True) == False:\n            shared[\"solution\"] = exec_res[\"content\"]\n            print(\"\\n=== FINAL SOLUTION ===\")\n            print(exec_res[\"content\"])\n            print(\"======================\\n\")\n            return \"end\"\n        \n        # Otherwise, continue the chain\n        print(f\"\\n{exec_res['content']}\")\n        print(f\"Next thought needed: {exec_res.get('next_thought_needed', True)}\")\n        print(f\"Total thoughts estimate: {shared.get('total_thoughts_estimate', 5)}\")\n        print(\"-\" * 50)\n        \n        return \"continue\"  # Continue the chain\n\n--- File Index 108: cookbook/pocketflow-thinking/utils.py ---\nfrom anthropic import Anthropic\nimport os\n\ndef call_llm(prompt):\n    client = Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\", \"your-api-key\"))\n    response = client.messages.create(\n        model=\"claude-3-7-sonnet-20250219\",\n        max_tokens=1000,\n        messages=[\n            {\"role\": \"user\", \"content\": prompt}\n        ]\n    )\n    return response.content[0].text\n\nif __name__ == \"__main__\":\n    print(\"## Testing call_llm\")\n    prompt = \"In a few words, what is the meaning of life?\"\n    print(f\"## Prompt: {prompt}\")\n    response = call_llm(prompt)\n    print(f\"## Response: {response}\")\n\n--- File Index 109: cookbook/pocketflow-tool-crawler/README.md ---\n# Web Crawler with Content Analysis\n\nA web crawler tool built with PocketFlow that crawls websites and analyzes content using LLM.\n\n## Features\n\n- Crawls websites while respecting domain boundaries\n- Extracts text content and links from pages\n- Analyzes content using GPT-4 to generate:\n  - Page summaries\n  - Main topics/keywords\n  - Content type classification\n- Processes pages in batches for efficiency\n- Generates a comprehensive analysis report\n\n## Installation\n\n1. Clone the repository\n2. Install dependencies:\n   ```bash\n   pip install -r requirements.txt\n   ```\n3. Set your OpenAI API key:\n   ```bash\n   export OPENAI_API_KEY='your-api-key'\n   ```\n\n## Usage\n\nRun the crawler:\n```bash\npython main.py\n```\n\nYou will be prompted to:\n1. Enter the website URL to crawl\n2. Specify maximum number of pages to crawl (default: 10)\n\nThe tool will then:\n1. Crawl the specified website\n2. Extract and analyze content using GPT-4\n3. Generate a report with findings\n\n## Project Structure\n\n```\npocketflow-tool-crawler/\n\u251c\u2500\u2500 tools/\n\u2502   \u251c\u2500\u2500 crawler.py     # Web crawling functionality\n\u2502   \u2514\u2500\u2500 parser.py      # Content analysis using LLM\n\u251c\u2500\u2500 utils/\n\u2502   \u2514\u2500\u2500 call_llm.py    # LLM API wrapper\n\u251c\u2500\u2500 nodes.py           # PocketFlow nodes\n\u251c\u2500\u2500 flow.py           # Flow configuration\n\u251c\u2500\u2500 main.py           # Main script\n\u2514\u2500\u2500 requirements.txt   # Dependencies\n```\n\n## Limitations\n\n- Only crawls within the same domain\n- Text content only (no images/media)\n- Rate limited by OpenAI API\n- Basic error handling\n\n## Dependencies\n\n- pocketflow: Flow-based processing\n- requests: HTTP requests\n- beautifulsoup4: HTML parsing\n- openai: GPT-4 API access\n\n\n--- File Index 110: cookbook/pocketflow-tool-crawler/flow.py ---\nfrom pocketflow import Flow\nfrom nodes import CrawlWebsiteNode, AnalyzeContentBatchNode, GenerateReportNode\n\ndef create_flow() -> Flow:\n    \"\"\"Create and configure the crawling flow\n    \n    Returns:\n        Flow: Configured flow ready to run\n    \"\"\"\n    # Create nodes\n    crawl = CrawlWebsiteNode()\n    analyze = AnalyzeContentBatchNode()\n    report = GenerateReportNode()\n    \n    # Connect nodes\n    crawl >> analyze >> report\n    \n    # Create flow starting with crawl\n    return Flow(start=crawl)\n\n\n--- File Index 111: cookbook/pocketflow-tool-crawler/main.py ---\nimport os\nfrom flow import create_flow\n\ndef main():\n    \"\"\"Run the web crawler flow\"\"\"\n    \n    # Get website URL from user\n    url = input(\"Enter website URL to crawl (e.g., https://example.com): \")\n    if not url:\n        print(\"Error: URL is required\")\n        return\n        \n    # Initialize shared data\n    shared = {\n        \"base_url\": url,\n        \"max_pages\": 1\n    }\n    \n    # Create and run flow\n    flow = create_flow()\n    flow.run(shared)\n    \n    # Results are in shared[\"report\"]\n    \nif __name__ == \"__main__\":\n    main()\n\n\n--- File Index 112: cookbook/pocketflow-tool-crawler/nodes.py ---\nfrom pocketflow import Node, BatchNode\nfrom tools.crawler import WebCrawler\nfrom tools.parser import analyze_site\nfrom typing import List, Dict\n\nclass CrawlWebsiteNode(Node):\n    \"\"\"Node to crawl a website and extract content\"\"\"\n    \n    def prep(self, shared):\n        return shared.get(\"base_url\"), shared.get(\"max_pages\", 10)\n        \n    def exec(self, inputs):\n        base_url, max_pages = inputs\n        if not base_url:\n            return []\n            \n        crawler = WebCrawler(base_url, max_pages)\n        return crawler.crawl()\n        \n    def post(self, shared, prep_res, exec_res):\n        shared[\"crawl_results\"] = exec_res\n        return \"default\"\n\nclass AnalyzeContentBatchNode(BatchNode):\n    \"\"\"Node to analyze crawled content in batches\"\"\"\n    \n    def prep(self, shared):\n        results = shared.get(\"crawl_results\", [])\n        # Process in batches of 5 pages\n        batch_size = 5\n        return [results[i:i+batch_size] for i in range(0, len(results), batch_size)]\n        \n    def exec(self, batch):\n        return analyze_site(batch)\n        \n    def post(self, shared, prep_res, exec_res_list):\n        # Flatten results from all batches\n        all_results = []\n        for batch_results in exec_res_list:\n            all_results.extend(batch_results)\n            \n        shared[\"analyzed_results\"] = all_results\n        return \"default\"\n\nclass GenerateReportNode(Node):\n    \"\"\"Node to generate a summary report of the analysis\"\"\"\n    \n    def prep(self, shared):\n        return shared.get(\"analyzed_results\", [])\n        \n    def exec(self, results):\n        if not results:\n            return \"No results to report\"\n            \n        report = []\n        report.append(f\"Analysis Report\\n\")\n        report.append(f\"Total pages analyzed: {len(results)}\\n\")\n        \n        for page in results:\n            report.append(f\"\\nPage: {page['url']}\")\n            report.append(f\"Title: {page['title']}\")\n            \n            analysis = page.get(\"analysis\", {})\n            report.append(f\"Summary: {analysis.get('summary', 'N/A')}\")\n            report.append(f\"Topics: {', '.join(analysis.get('topics', []))}\")\n            report.append(f\"Content Type: {analysis.get('content_type', 'unknown')}\")\n            report.append(\"-\" * 80)\n            \n        return \"\\n\".join(report)\n        \n    def post(self, shared, prep_res, exec_res):\n        shared[\"report\"] = exec_res\n        print(\"\\nReport generated:\")\n        print(exec_res)\n        return \"default\"\n\n\n--- File Index 113: cookbook/pocketflow-tool-crawler/tools/crawler.py ---\nimport requests\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin, urlparse\nfrom typing import Dict, List, Set\n\nclass WebCrawler:\n    \"\"\"Simple web crawler that extracts content and follows links\"\"\"\n    \n    def __init__(self, base_url: str, max_pages: int = 10):\n        self.base_url = base_url\n        self.max_pages = max_pages\n        self.visited: Set[str] = set()\n        \n    def is_valid_url(self, url: str) -> bool:\n        \"\"\"Check if URL belongs to the same domain\"\"\"\n        base_domain = urlparse(self.base_url).netloc\n        url_domain = urlparse(url).netloc\n        return base_domain == url_domain\n        \n    def extract_page_content(self, url: str) -> Dict:\n        \"\"\"Extract content from a single page\"\"\"\n        try:\n            response = requests.get(url)\n            response.raise_for_status()\n            \n            soup = BeautifulSoup(response.text, \"html.parser\")\n            \n            # Extract main content\n            content = {\n                \"url\": url,\n                \"title\": soup.title.string if soup.title else \"\",\n                \"text\": soup.get_text(separator=\"\\n\", strip=True),\n                \"links\": []\n            }\n            \n            # Extract links\n            for link in soup.find_all(\"a\"):\n                href = link.get(\"href\")\n                if href:\n                    absolute_url = urljoin(url, href)\n                    if self.is_valid_url(absolute_url):\n                        content[\"links\"].append(absolute_url)\n            \n            return content\n            \n        except Exception as e:\n            print(f\"Error crawling {url}: {str(e)}\")\n            return None\n    \n    def crawl(self) -> List[Dict]:\n        \"\"\"Crawl website starting from base_url\"\"\"\n        to_visit = [self.base_url]\n        results = []\n        \n        while to_visit and len(self.visited) < self.max_pages:\n            url = to_visit.pop(0)\n            \n            if url in self.visited:\n                continue\n                \n            print(f\"Crawling: {url}\")\n            content = self.extract_page_content(url)\n            \n            if content:\n                self.visited.add(url)\n                results.append(content)\n                \n                # Add new URLs to visit\n                new_urls = [url for url in content[\"links\"] \n                          if url not in self.visited \n                          and url not in to_visit]\n                to_visit.extend(new_urls)\n        \n        return results\n\n\n--- File Index 114: cookbook/pocketflow-tool-crawler/tools/parser.py ---\nfrom typing import Dict, List\nfrom utils.call_llm import call_llm\n\ndef analyze_content(content: Dict) -> Dict:\n    \"\"\"Analyze webpage content using LLM\n    \n    Args:\n        content (Dict): Webpage content with url, title and text\n        \n    Returns:\n        Dict: Analysis results including summary and topics\n    \"\"\"\n    prompt = f\"\"\"\nAnalyze this webpage content:\n\nTitle: {content['title']}\nURL: {content['url']}\nContent: {content['text'][:2000]}  # Limit content length\n\nPlease provide:\n1. A brief summary (2-3 sentences)\n2. Main topics/keywords (up to 5)\n3. Content type (article, product page, etc)\n\nOutput in YAML format:\n```yaml\nsummary: >\n    brief summary here\ntopics:\n    - topic 1\n    - topic 2\ncontent_type: type here\n```\n\"\"\"\n    \n    try:\n        response = call_llm(prompt)\n        # Extract YAML between code fences\n        yaml_str = response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n        \n        import yaml\n        analysis = yaml.safe_load(yaml_str)\n        \n        # Validate required fields\n        assert \"summary\" in analysis\n        assert \"topics\" in analysis\n        assert \"content_type\" in analysis\n        assert isinstance(analysis[\"topics\"], list)\n        \n        return analysis\n        \n    except Exception as e:\n        print(f\"Error analyzing content: {str(e)}\")\n        return {\n            \"summary\": \"Error analyzing content\",\n            \"topics\": [],\n            \"content_type\": \"unknown\"\n        }\n\ndef analyze_site(crawl_results: List[Dict]) -> List[Dict]:\n    \"\"\"Analyze all crawled pages\n    \n    Args:\n        crawl_results (List[Dict]): List of crawled page contents\n        \n    Returns:\n        List[Dict]: Original content with added analysis\n    \"\"\"\n    analyzed_results = []\n    \n    for content in crawl_results:\n        if content and content.get(\"text\"):\n            analysis = analyze_content(content)\n            content[\"analysis\"] = analysis\n            analyzed_results.append(content)\n            \n    return analyzed_results\n\n\n--- File Index 115: cookbook/pocketflow-tool-crawler/utils/__init__.py ---\n\n\n--- File Index 116: cookbook/pocketflow-tool-crawler/utils/call_llm.py ---\nfrom openai import OpenAI\nimport os\n\n# Initialize OpenAI client\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\ndef call_llm(prompt: str) -> str:\n    \"\"\"Call OpenAI API to analyze text\n    \n    Args:\n        prompt (str): Input prompt for the model\n        \n    Returns:\n        str: Model response\n    \"\"\"\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n        return response.choices[0].message.content\n        \n    except Exception as e:\n        print(f\"Error calling LLM API: {str(e)}\")\n        return \"\"\n\nif __name__ == \"__main__\":\n    # Test LLM call\n    response = call_llm(\"What is web crawling?\")\n    print(\"Response:\", response)\n\n\n--- File Index 117: cookbook/pocketflow-tool-database/README.md ---\n# SQLite Database with PocketFlow\n\nThis example demonstrates how to properly integrate SQLite database operations with PocketFlow, focusing on:\n\n1. Clean code organization with separation of concerns:\n   - Tools layer for database operations (`tools/database.py`)\n   - Node implementation for PocketFlow integration (`nodes.py`)\n   - Flow configuration (`flow.py`)\n   - Safe SQL query execution with parameter binding\n\n2. Best practices for database operations:\n   - Connection management with proper closing\n   - SQL injection prevention using parameterized queries\n   - Error handling and resource cleanup\n   - Simple schema management\n\n3. Example task management system:\n   - Database initialization\n   - Task creation\n   - Task listing\n   - Status tracking\n\n## Project Structure\n\n```\npocketflow-tool-database/\n\u251c\u2500\u2500 tools/\n\u2502   \u2514\u2500\u2500 database.py    # SQLite database operations\n\u251c\u2500\u2500 nodes.py          # PocketFlow node implementation\n\u251c\u2500\u2500 flow.py          # Flow configuration\n\u2514\u2500\u2500 main.py          # Example usage\n```\n\n## Setup\n\n1. Create a virtual environment:\n```bash\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n```\n\n2. Install dependencies:\n```bash\npip install -r requirements.txt\n```\n\n## Usage\n\nRun the example:\n```bash\npython main.py\n```\n\nThis will:\n1. Initialize a SQLite database with a tasks table\n2. Create an example task\n3. List all tasks in the database\n4. Display the results\n\n## Key Concepts Demonstrated\n\n1. **Database Operations**\n   - Safe connection handling\n   - Query parameterization\n   - Schema management\n\n2. **Code Organization**\n   - Clear separation between database operations and PocketFlow components\n   - Modular project structure\n   - Type hints and documentation\n\n3. **PocketFlow Integration**\n   - Node implementation with prep->exec->post lifecycle\n   - Flow configuration\n   - Shared store usage for data passing\n\n## Example Output\n\n```\nDatabase Status: Database initialized\nTask Status: Task created successfully\n\nAll Tasks:\n- ID: 1\n  Title: Example Task\n  Description: This is an example task created using PocketFlow\n  Status: pending\n  Created: 2024-03-02 12:34:56\n```\n\n\n--- File Index 118: cookbook/pocketflow-tool-database/flow.py ---\nfrom pocketflow import Flow\nfrom nodes import InitDatabaseNode, CreateTaskNode, ListTasksNode\n\ndef create_database_flow():\n    \"\"\"Create a flow for database operations\"\"\"\n    \n    # Create nodes\n    init_db = InitDatabaseNode()\n    create_task = CreateTaskNode()\n    list_tasks = ListTasksNode()\n    \n    # Connect nodes\n    init_db >> create_task >> list_tasks\n    \n    # Create and return flow\n    return Flow(start=init_db)\n\n\n--- File Index 119: cookbook/pocketflow-tool-database/main.py ---\nfrom flow import create_database_flow\n\ndef main():\n    # Create the flow\n    flow = create_database_flow()\n    \n    # Prepare example task data\n    shared = {\n        \"task_title\": \"Example Task\",\n        \"task_description\": \"This is an example task created using PocketFlow\"\n    }\n    \n    # Run the flow\n    flow.run(shared)\n    \n    # Print results\n    print(\"Database Status:\", shared.get(\"db_status\"))\n    print(\"Task Status:\", shared.get(\"task_status\"))\n    print(\"\\nAll Tasks:\")\n    for task in shared.get(\"tasks\", []):\n        print(f\"- ID: {task[0]}\")\n        print(f\"  Title: {task[1]}\")\n        print(f\"  Description: {task[2]}\")\n        print(f\"  Status: {task[3]}\")\n        print(f\"  Created: {task[4]}\")\n        print()\n\nif __name__ == \"__main__\":\n    main()\n\n\n--- File Index 120: cookbook/pocketflow-tool-database/nodes.py ---\nfrom pocketflow import Node\nfrom tools.database import execute_sql, init_db\n\nclass InitDatabaseNode(Node):\n    \"\"\"Node for initializing the database\"\"\"\n    \n    def exec(self, _):\n        init_db()\n        return \"Database initialized\"\n        \n    def post(self, shared, prep_res, exec_res):\n        shared[\"db_status\"] = exec_res\n        return \"default\"\n\nclass CreateTaskNode(Node):\n    \"\"\"Node for creating a new task\"\"\"\n    \n    def prep(self, shared):\n        return (\n            shared.get(\"task_title\", \"\"),\n            shared.get(\"task_description\", \"\")\n        )\n        \n    def exec(self, inputs):\n        title, description = inputs\n        query = \"INSERT INTO tasks (title, description) VALUES (?, ?)\"\n        execute_sql(query, (title, description))\n        return \"Task created successfully\"\n        \n    def post(self, shared, prep_res, exec_res):\n        shared[\"task_status\"] = exec_res\n        return \"default\"\n\nclass ListTasksNode(Node):\n    \"\"\"Node for listing all tasks\"\"\"\n    \n    def exec(self, _):\n        query = \"SELECT * FROM tasks\"\n        return execute_sql(query)\n        \n    def post(self, shared, prep_res, exec_res):\n        shared[\"tasks\"] = exec_res\n        return \"default\"\n\n\n--- File Index 121: cookbook/pocketflow-tool-database/tools/database.py ---\nimport sqlite3\nfrom typing import List, Tuple, Any\n\ndef execute_sql(query: str, params: Tuple = None) -> List[Tuple[Any, ...]]:\n    \"\"\"Execute a SQL query and return results\n    \n    Args:\n        query (str): SQL query to execute\n        params (tuple, optional): Query parameters to prevent SQL injection\n        \n    Returns:\n        list: Query results as a list of tuples\n    \"\"\"\n    conn = sqlite3.connect(\"example.db\")\n    try:\n        cursor = conn.cursor()\n        if params:\n            cursor.execute(query, params)\n        else:\n            cursor.execute(query)\n        result = cursor.fetchall()\n        conn.commit()\n        return result\n    finally:\n        conn.close()\n\ndef init_db():\n    \"\"\"Initialize database with example table\"\"\"\n    create_table_sql = \"\"\"\n    CREATE TABLE IF NOT EXISTS tasks (\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\n        title TEXT NOT NULL,\n        description TEXT,\n        status TEXT DEFAULT 'pending',\n        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n    )\n    \"\"\"\n    execute_sql(create_table_sql)\n\n\n--- File Index 122: cookbook/pocketflow-tool-database/utils/__init__.py ---\n\n\n--- File Index 123: cookbook/pocketflow-tool-embeddings/README.md ---\n# OpenAI Embeddings with PocketFlow\n\nThis example demonstrates how to properly integrate OpenAI's text embeddings API with PocketFlow, focusing on:\n\n1. Clean code organization with separation of concerns:\n   - Tools layer for API interactions (`tools/embeddings.py`)\n   - Node implementation for PocketFlow integration (`nodes.py`)\n   - Flow configuration (`flow.py`)\n   - Centralized environment configuration (`utils/call_llm.py`)\n\n2. Best practices for API key management:\n   - Using environment variables\n   - Supporting both `.env` files and system environment variables\n   - Secure configuration handling\n\n3. Proper project structure:\n   - Modular code organization\n   - Clear separation between tools and PocketFlow components\n   - Reusable OpenAI client configuration\n\n## Project Structure\n\n```\npocketflow-tool-embeddings/\n\u251c\u2500\u2500 tools/\n\u2502   \u2514\u2500\u2500 embeddings.py     # OpenAI embeddings API wrapper\n\u251c\u2500\u2500 utils/\n\u2502   \u2514\u2500\u2500 call_llm.py      # Centralized OpenAI client configuration\n\u251c\u2500\u2500 nodes.py             # PocketFlow node implementation\n\u251c\u2500\u2500 flow.py             # Flow configuration\n\u2514\u2500\u2500 main.py             # Example usage\n```\n\n## Setup\n\n1. Create a virtual environment:\n```bash\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n```\n\n2. Install dependencies:\n```bash\npip install -r requirements.txt\n```\n\n3. Set up your OpenAI API key in one of two ways:\n   \n   a. Using a `.env` file:\n   ```bash\n   OPENAI_API_KEY=your_api_key_here\n   ```\n   \n   b. Or as a system environment variable:\n   ```bash\n   export OPENAI_API_KEY=your_api_key_here\n   ```\n\n## Usage\n\nRun the example:\n```bash\npython main.py\n```\n\nThis will:\n1. Load the OpenAI API key from environment\n2. Create a PocketFlow node to handle embedding generation\n3. Process a sample text and generate its embedding\n4. Display the embedding dimension and first few values\n\n## Key Concepts Demonstrated\n\n1. **Environment Configuration**\n   - Secure API key handling\n   - Flexible configuration options\n\n2. **Code Organization**\n   - Clear separation between tools and PocketFlow components\n   - Reusable OpenAI client configuration\n   - Modular project structure\n\n3. **PocketFlow Integration**\n   - Node implementation with prep->exec->post lifecycle\n   - Flow configuration\n   - Shared store usage for data passing \n\n--- File Index 124: cookbook/pocketflow-tool-embeddings/flow.py ---\nfrom pocketflow import Flow\nfrom nodes import EmbeddingNode\n\ndef create_embedding_flow():\n    \"\"\"Create a flow for text embedding\"\"\"\n    # Create embedding node\n    embedding = EmbeddingNode()\n    \n    # Create and return flow\n    return Flow(start=embedding) \n\n--- File Index 125: cookbook/pocketflow-tool-embeddings/main.py ---\nfrom flow import create_embedding_flow\n\ndef main():\n    # Create the flow\n    flow = create_embedding_flow()\n    \n    # Example text\n    text = \"What's the meaning of life?\"\n    \n    # Prepare shared data\n    shared = {\"text\": text}\n    \n    # Run the flow\n    flow.run(shared)\n    \n    # Print results\n    print(\"Text:\", text)\n    print(\"Embedding dimension:\", len(shared[\"embedding\"]))\n    print(\"First 5 values:\", shared[\"embedding\"][:5])\n\nif __name__ == \"__main__\":\n    main()\n\n--- File Index 126: cookbook/pocketflow-tool-embeddings/nodes.py ---\nfrom pocketflow import Node\nfrom tools.embeddings import get_embedding\n\nclass EmbeddingNode(Node):\n    \"\"\"Node for getting embeddings from OpenAI API\"\"\"\n    \n    def prep(self, shared):\n        # Get text from shared store\n        return shared.get(\"text\", \"\")\n        \n    def exec(self, text):\n        # Get embedding using tool function\n        return get_embedding(text)\n        \n    def post(self, shared, prep_res, exec_res):\n        # Store embedding in shared store\n        shared[\"embedding\"] = exec_res\n        return \"default\" \n\n--- File Index 127: cookbook/pocketflow-tool-embeddings/tools/embeddings.py ---\nfrom utils.call_llm import client\n\ndef get_embedding(text):\n    response = client.embeddings.create(\n        model=\"text-embedding-ada-002\",\n        input=text\n    )\n    return response.data[0].embedding\n\n--- File Index 128: cookbook/pocketflow-tool-embeddings/utils/__init__.py ---\n\n\n--- File Index 129: cookbook/pocketflow-tool-embeddings/utils/call_llm.py ---\nimport os\nfrom openai import OpenAI\n\n# No need for dotenv if using system environment variables\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\ndef call_llm(prompt):    \n    r = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return r.choices[0].message.content\n    \nif __name__ == \"__main__\":\n    prompt = \"What is the meaning of life?\"\n    print(call_llm(prompt)) \n\n--- File Index 130: cookbook/pocketflow-tool-pdf-vision/README.md ---\n# PocketFlow Tool: PDF Vision\n\nA PocketFlow example project demonstrating PDF processing with OpenAI's Vision API for OCR and text extraction.\n\n## Features\n\n- Convert PDF pages to images while maintaining quality and size limits\n- Extract text from scanned documents using GPT-4 Vision API\n- Support for custom extraction prompts\n- Maintain page order and formatting in extracted text\n- Batch processing of multiple PDFs from a directory\n\n## Installation\n\n1. Clone the repository\n2. Install dependencies:\n   ```bash\n   pip install -r requirements.txt\n   ```\n3. Set your OpenAI API key as an environment variable:\n   ```bash\n   export OPENAI_API_KEY=your_api_key_here\n   ```\n\n## Usage\n\n1. Place your PDF files in the `pdfs` directory\n2. Run the example:\n   ```bash\n   python main.py\n   ```\n   The script will process all PDF files in the `pdfs` directory and output the extracted text for each one.\n\n## Project Structure\n\n```\npocketflow-tool-pdf-vision/\n\u251c\u2500\u2500 pdfs/           # Directory for PDF files to process\n\u251c\u2500\u2500 tools/\n\u2502   \u251c\u2500\u2500 pdf.py     # PDF to image conversion\n\u2502   \u2514\u2500\u2500 vision.py  # Vision API integration\n\u251c\u2500\u2500 utils/\n\u2502   \u2514\u2500\u2500 call_llm.py # OpenAI client config\n\u251c\u2500\u2500 nodes.py       # PocketFlow nodes\n\u251c\u2500\u2500 flow.py        # Flow configuration\n\u2514\u2500\u2500 main.py        # Example usage\n```\n\n## Flow Description\n\n1. **LoadPDFNode**: Loads PDF and converts pages to images\n2. **ExtractTextNode**: Processes images with Vision API\n3. **CombineResultsNode**: Combines extracted text from all pages\n\n## Customization\n\nYou can customize the extraction by modifying the prompt in `shared`:\n\n```python\nshared = {\n    \"pdf_path\": \"your_file.pdf\",\n    \"extraction_prompt\": \"Your custom prompt here\"\n}\n```\n\n## Limitations\n\n- Maximum PDF page size: 2000px (configurable in `tools/pdf.py`)\n- Vision API token limit: 1000 tokens per response\n- Image size limit: 20MB per image for Vision API\n\n## License\n\nMIT\n\n\n--- File Index 131: cookbook/pocketflow-tool-pdf-vision/flow.py ---\nfrom pocketflow import Flow\nfrom nodes import ProcessPDFBatchNode\n\ndef create_vision_flow():\n    \"\"\"Create a flow for batch PDF processing with Vision API\"\"\"\n    return Flow(start=ProcessPDFBatchNode())\n\n\n--- File Index 132: cookbook/pocketflow-tool-pdf-vision/main.py ---\nfrom flow import create_vision_flow\n\ndef main():\n    # Create and run flow\n    flow = create_vision_flow()\n    shared = {}\n    flow.run(shared)\n    \n    # Print results\n    if \"results\" in shared:\n        for result in shared[\"results\"]:\n            print(f\"\\nFile: {result['filename']}\")\n            print(\"-\" * 50)\n            print(result[\"text\"])\n\nif __name__ == \"__main__\":\n    main()\n\n\n--- File Index 133: cookbook/pocketflow-tool-pdf-vision/nodes.py ---\nfrom pocketflow import Node, BatchNode\nfrom tools.pdf import pdf_to_images\nfrom tools.vision import extract_text_from_image\nfrom typing import List, Dict, Any\nfrom pathlib import Path\nimport os\n\nclass ProcessPDFBatchNode(BatchNode):\n    \"\"\"Node for processing multiple PDFs from a directory\"\"\"\n    \n    def prep(self, shared):\n        # Get PDF directory path\n        root_dir = Path(__file__).parent\n        pdf_dir = root_dir / \"pdfs\"\n        \n        # List all PDFs\n        pdf_files = []\n        for file in os.listdir(pdf_dir):\n            if file.lower().endswith('.pdf'):\n                pdf_files.append({\n                    \"pdf_path\": str(pdf_dir / file),\n                    \"extraction_prompt\": shared.get(\"extraction_prompt\", \n                        \"Extract all text from this document, preserving formatting and layout.\")\n                })\n        \n        if not pdf_files:\n            print(\"No PDF files found in 'pdfs' directory!\")\n            return []\n            \n        print(f\"Found {len(pdf_files)} PDF files\")\n        return pdf_files\n    \n    def exec(self, item):\n        # Create flow for single PDF\n        flow = create_single_pdf_flow()\n        \n        # Process PDF\n        print(f\"\\nProcessing: {os.path.basename(item['pdf_path'])}\")\n        print(\"-\" * 50)\n        \n        # Run flow\n        shared = item.copy()\n        flow.run(shared)\n        \n        return {\n            \"filename\": os.path.basename(item[\"pdf_path\"]),\n            \"text\": shared.get(\"final_text\", \"No text extracted\")\n        }\n    \n    def post(self, shared, prep_res, exec_res_list):\n        shared[\"results\"] = exec_res_list\n        return \"default\"\n\nclass LoadPDFNode(Node):\n    \"\"\"Node for loading and converting a single PDF to images\"\"\"\n    \n    def prep(self, shared):\n        return shared.get(\"pdf_path\", \"\")\n        \n    def exec(self, pdf_path):\n        return pdf_to_images(pdf_path)\n        \n    def post(self, shared, prep_res, exec_res):\n        shared[\"page_images\"] = exec_res\n        return \"default\"\n\nclass ExtractTextNode(Node):\n    \"\"\"Node for extracting text from images using Vision API\"\"\"\n    \n    def prep(self, shared):\n        return (\n            shared.get(\"page_images\", []),\n            shared.get(\"extraction_prompt\", None)\n        )\n        \n    def exec(self, inputs):\n        images, prompt = inputs\n        results = []\n        \n        for img, page_num in images:\n            text = extract_text_from_image(img, prompt)\n            results.append({\n                \"page\": page_num,\n                \"text\": text\n            })\n            \n        return results\n        \n    def post(self, shared, prep_res, exec_res):\n        shared[\"extracted_text\"] = exec_res\n        return \"default\"\n\nclass CombineResultsNode(Node):\n    \"\"\"Node for combining and formatting extracted text\"\"\"\n    \n    def prep(self, shared):\n        return shared.get(\"extracted_text\", [])\n        \n    def exec(self, results):\n        # Sort by page number\n        sorted_results = sorted(results, key=lambda x: x[\"page\"])\n        \n        # Combine text with page numbers\n        combined = []\n        for result in sorted_results:\n            combined.append(f\"=== Page {result['page']} ===\\n{result['text']}\\n\")\n            \n        return \"\\n\".join(combined)\n        \n    def post(self, shared, prep_res, exec_res):\n        shared[\"final_text\"] = exec_res\n        return \"default\"\n\ndef create_single_pdf_flow():\n    \"\"\"Create a flow for processing a single PDF\"\"\"\n    from pocketflow import Flow\n    \n    # Create nodes\n    load_pdf = LoadPDFNode()\n    extract_text = ExtractTextNode()\n    combine_results = CombineResultsNode()\n    \n    # Connect nodes\n    load_pdf >> extract_text >> combine_results\n    \n    # Create and return flow\n    return Flow(start=load_pdf)\n\n\n--- File Index 134: cookbook/pocketflow-tool-pdf-vision/tools/pdf.py ---\nimport fitz  # PyMuPDF\nfrom PIL import Image\nimport io\nimport base64\nfrom typing import List, Tuple\n\ndef pdf_to_images(pdf_path: str, max_size: int = 2000) -> List[Tuple[Image.Image, int]]:\n    \"\"\"Convert PDF pages to PIL Images with size limit\n    \n    Args:\n        pdf_path (str): Path to PDF file\n        max_size (int): Maximum dimension (width/height) for images\n        \n    Returns:\n        list: List of tuples (PIL Image, page number)\n    \"\"\"\n    doc = fitz.open(pdf_path)\n    images = []\n    \n    try:\n        for page_num in range(len(doc)):\n            page = doc[page_num]\n            pix = page.get_pixmap()\n            \n            # Convert to PIL Image\n            img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n            \n            # Resize if needed while maintaining aspect ratio\n            if max(img.size) > max_size:\n                ratio = max_size / max(img.size)\n                new_size = tuple(int(dim * ratio) for dim in img.size)\n                img = img.resize(new_size, Image.Resampling.LANCZOS)\n            \n            images.append((img, page_num + 1))\n            \n    finally:\n        doc.close()\n        \n    return images\n\ndef image_to_base64(image: Image.Image) -> str:\n    \"\"\"Convert PIL Image to base64 string\n    \n    Args:\n        image (PIL.Image): Image to convert\n        \n    Returns:\n        str: Base64 encoded image string\n    \"\"\"\n    buffer = io.BytesIO()\n    image.save(buffer, format=\"PNG\")\n    return base64.b64encode(buffer.getvalue()).decode('utf-8')\n\n\n--- File Index 135: cookbook/pocketflow-tool-pdf-vision/tools/vision.py ---\nfrom PIL import Image\nfrom utils.call_llm import client\nfrom tools.pdf import image_to_base64\n\ndef extract_text_from_image(image: Image.Image, prompt: str = None) -> str:\n    \"\"\"Extract text from image using OpenAI Vision API\n    \n    Args:\n        image (PIL.Image): Image to process\n        prompt (str, optional): Custom prompt for extraction. Defaults to general OCR.\n        \n    Returns:\n        str: Extracted text from image\n    \"\"\"\n    # Convert image to base64\n    img_base64 = image_to_base64(image)\n    \n    # Default prompt for general OCR\n    if prompt is None:\n        prompt = \"Please extract all text from this image.\"\n    \n    # Call Vision API\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": prompt},\n                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{img_base64}\"}}\n            ]\n        }]\n    )\n    \n    return response.choices[0].message.content\n\nif __name__ == \"__main__\":\n    # Test vision processing\n    test_image = Image.open(\"example.png\")\n    result = extract_text_from_image(test_image)\n    print(\"Extracted text:\", result)\n\n\n--- File Index 136: cookbook/pocketflow-tool-pdf-vision/utils/__init__.py ---\n\n\n--- File Index 137: cookbook/pocketflow-tool-pdf-vision/utils/call_llm.py ---\nimport os\nfrom openai import OpenAI\nfrom pathlib import Path\n\n# Get the project root directory (parent of utils directory)\nROOT_DIR = Path(__file__).parent.parent\n\n# Initialize OpenAI client with API key from environment\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n\n--- File Index 138: cookbook/pocketflow-tool-search/README.md ---\n# Web Search with Analysis\n\nA web search tool built with PocketFlow that performs searches using SerpAPI and analyzes results using LLM.\n\n## Features\n\n- Web search using Google via SerpAPI\n- Extracts titles, snippets, and links\n- Analyzes search results using GPT-4 to provide:\n  - Result summaries\n  - Key points/facts\n  - Suggested follow-up queries\n- Clean command-line interface\n\n## Installation\n\n1. Clone the repository\n2. Install dependencies:\n   ```bash\n   pip install -r requirements.txt\n   ```\n3. Set required API keys:\n   ```bash\n   export SERPAPI_API_KEY='your-serpapi-key'\n   export OPENAI_API_KEY='your-openai-key'\n   ```\n\n## Usage\n\nRun the search tool:\n```bash\npython main.py\n```\n\nYou will be prompted to:\n1. Enter your search query\n2. Specify number of results to fetch (default: 5)\n\nThe tool will then:\n1. Perform the search using SerpAPI\n2. Analyze results using GPT-4\n3. Present a summary with key points and follow-up queries\n\n## Project Structure\n\n```\npocketflow-tool-search/\n\u251c\u2500\u2500 tools/\n\u2502   \u251c\u2500\u2500 search.py      # SerpAPI search functionality\n\u2502   \u2514\u2500\u2500 parser.py      # Result analysis using LLM\n\u251c\u2500\u2500 utils/\n\u2502   \u2514\u2500\u2500 call_llm.py    # LLM API wrapper\n\u251c\u2500\u2500 nodes.py           # PocketFlow nodes\n\u251c\u2500\u2500 flow.py           # Flow configuration\n\u251c\u2500\u2500 main.py           # Main script\n\u2514\u2500\u2500 requirements.txt   # Dependencies\n```\n\n## Limitations\n\n- Requires SerpAPI subscription\n- Rate limited by both APIs\n- Basic error handling\n- Text results only\n\n## Dependencies\n\n- pocketflow: Flow-based processing\n- google-search-results: SerpAPI client\n- openai: GPT-4 API access\n- pyyaml: YAML processing\n\n\n--- File Index 139: cookbook/pocketflow-tool-search/flow.py ---\nfrom pocketflow import Flow\nfrom nodes import SearchNode, AnalyzeResultsNode\n\ndef create_flow() -> Flow:\n    \"\"\"Create and configure the search flow\n    \n    Returns:\n        Flow: Configured flow ready to run\n    \"\"\"\n    # Create nodes\n    search = SearchNode()\n    analyze = AnalyzeResultsNode()\n    \n    # Connect nodes\n    search >> analyze\n    \n    # Create flow starting with search\n    return Flow(start=search)\n\n\n--- File Index 140: cookbook/pocketflow-tool-search/main.py ---\nimport os\nfrom flow import create_flow\n\ndef main():\n    \"\"\"Run the web search flow\"\"\"\n    \n    # Get search query from user\n    query = input(\"Enter search query: \")\n    if not query:\n        print(\"Error: Query is required\")\n        return\n        \n    # Initialize shared data\n    shared = {\n        \"query\": query,\n        \"num_results\": 5\n    }\n    \n    # Create and run flow\n    flow = create_flow()\n    flow.run(shared)\n    \n    # Results are in shared[\"analysis\"]\n    \nif __name__ == \"__main__\":\n    main()\n\n\n--- File Index 141: cookbook/pocketflow-tool-search/nodes.py ---\nfrom pocketflow import Node\nfrom tools.search import SearchTool\nfrom tools.parser import analyze_results\nfrom typing import List, Dict\n\nclass SearchNode(Node):\n    \"\"\"Node to perform web search using SerpAPI\"\"\"\n    \n    def prep(self, shared):\n        return shared.get(\"query\"), shared.get(\"num_results\", 5)\n        \n    def exec(self, inputs):\n        query, num_results = inputs\n        if not query:\n            return []\n            \n        searcher = SearchTool()\n        return searcher.search(query, num_results)\n        \n    def post(self, shared, prep_res, exec_res):\n        shared[\"search_results\"] = exec_res\n        return \"default\"\n\nclass AnalyzeResultsNode(Node):\n    \"\"\"Node to analyze search results using LLM\"\"\"\n    \n    def prep(self, shared):\n        return shared.get(\"query\"), shared.get(\"search_results\", [])\n        \n    def exec(self, inputs):\n        query, results = inputs\n        if not results:\n            return {\n                \"summary\": \"No search results to analyze\",\n                \"key_points\": [],\n                \"follow_up_queries\": []\n            }\n            \n        return analyze_results(query, results)\n        \n    def post(self, shared, prep_res, exec_res):\n        shared[\"analysis\"] = exec_res\n        \n        # Print analysis\n        print(\"\\nSearch Analysis:\")\n        print(\"\\nSummary:\", exec_res[\"summary\"])\n        \n        print(\"\\nKey Points:\")\n        for point in exec_res[\"key_points\"]:\n            print(f\"- {point}\")\n            \n        print(\"\\nSuggested Follow-up Queries:\")\n        for query in exec_res[\"follow_up_queries\"]:\n            print(f\"- {query}\")\n            \n        return \"default\"\n\n\n--- File Index 142: cookbook/pocketflow-tool-search/tools/parser.py ---\nfrom typing import Dict, List\nfrom utils.call_llm import call_llm\n\ndef analyze_results(query: str, results: List[Dict]) -> Dict:\n    \"\"\"Analyze search results using LLM\n    \n    Args:\n        query (str): Original search query\n        results (List[Dict]): Search results to analyze\n        \n    Returns:\n        Dict: Analysis including summary and key points\n    \"\"\"\n    # Format results for prompt\n    formatted_results = []\n    for i, result in enumerate(results, 1):\n        formatted_results.append(f\"\"\"\nResult {i}:\nTitle: {result['title']}\nSnippet: {result['snippet']}\nURL: {result['link']}\n\"\"\")\n    \n    prompt = f\"\"\"\nAnalyze these search results for the query: \"{query}\"\n\n{'\\n'.join(formatted_results)}\n\nPlease provide:\n1. A concise summary of the findings (2-3 sentences)\n2. Key points or facts (up to 5 bullet points)\n3. Suggested follow-up queries (2-3)\n\nOutput in YAML format:\n```yaml\nsummary: >\n    brief summary here\nkey_points:\n    - point 1\n    - point 2\nfollow_up_queries:\n    - query 1\n    - query 2\n```\n\"\"\"\n    \n    try:\n        response = call_llm(prompt)\n        # Extract YAML between code fences\n        yaml_str = response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n        \n        import yaml\n        analysis = yaml.safe_load(yaml_str)\n        \n        # Validate required fields\n        assert \"summary\" in analysis\n        assert \"key_points\" in analysis\n        assert \"follow_up_queries\" in analysis\n        assert isinstance(analysis[\"key_points\"], list)\n        assert isinstance(analysis[\"follow_up_queries\"], list)\n        \n        return analysis\n        \n    except Exception as e:\n        print(f\"Error analyzing results: {str(e)}\")\n        return {\n            \"summary\": \"Error analyzing results\",\n            \"key_points\": [],\n            \"follow_up_queries\": []\n        }\n\n--- File Index 143: cookbook/pocketflow-tool-search/tools/search.py ---\nimport os\nfrom serpapi import GoogleSearch\nfrom typing import Dict, List, Optional\n\nclass SearchTool:\n    \"\"\"Tool for performing web searches using SerpAPI\"\"\"\n    \n    def __init__(self, api_key: Optional[str] = None):\n        \"\"\"Initialize search tool with API key\n        \n        Args:\n            api_key (str, optional): SerpAPI key. Defaults to env var SERPAPI_API_KEY.\n        \"\"\"\n        self.api_key = api_key or os.getenv(\"SERPAPI_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\"SerpAPI key not found. Set SERPAPI_API_KEY env var.\")\n            \n    def search(self, query: str, num_results: int = 5) -> List[Dict]:\n        \"\"\"Perform Google search via SerpAPI\n        \n        Args:\n            query (str): Search query\n            num_results (int, optional): Number of results to return. Defaults to 5.\n            \n        Returns:\n            List[Dict]: Search results with title, snippet, and link\n        \"\"\"\n        # Configure search parameters\n        params = {\n            \"engine\": \"google\",\n            \"q\": query,\n            \"api_key\": self.api_key,\n            \"num\": num_results\n        }\n        \n        try:\n            # Execute search\n            search = GoogleSearch(params)\n            results = search.get_dict()\n            \n            # Extract organic results\n            if \"organic_results\" not in results:\n                return []\n                \n            processed_results = []\n            for result in results[\"organic_results\"][:num_results]:\n                processed_results.append({\n                    \"title\": result.get(\"title\", \"\"),\n                    \"snippet\": result.get(\"snippet\", \"\"),\n                    \"link\": result.get(\"link\", \"\")\n                })\n                \n            return processed_results\n            \n        except Exception as e:\n            print(f\"Search error: {str(e)}\")\n            return []\n\n\n--- File Index 144: cookbook/pocketflow-tool-search/utils/__init__.py ---\n\n\n--- File Index 145: cookbook/pocketflow-tool-search/utils/call_llm.py ---\nimport os\nfrom openai import OpenAI\nfrom pathlib import Path\n\n# Get the project root directory (parent of utils directory)\nROOT_DIR = Path(__file__).parent.parent\n\n# Initialize OpenAI client with API key from environment\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\ndef call_llm(prompt: str) -> str:\n    \"\"\"Call OpenAI API to analyze text\n    \n    Args:\n        prompt (str): Input prompt for the model\n        \n    Returns:\n        str: Model response\n    \"\"\"\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n        return response.choices[0].message.content\n        \n    except Exception as e:\n        print(f\"Error calling LLM API: {str(e)}\")\n        return \"\"\n\nif __name__ == \"__main__\":\n    # Test LLM call\n    response = call_llm(\"What is web search?\")\n    print(\"Response:\", response)\n\n\n--- File Index 146: cookbook/pocketflow-web-hitl/README.md ---\n# PocketFlow Web Human-in-the-Loop (HITL) Feedback Service\n\nThis project demonstrates a minimal web application for human-in-the-loop workflows using PocketFlow, FastAPI, and Server-Sent Events (SSE). Users can submit text, have it processed (simulated), review the output, and approve or reject it, potentially triggering reprocessing until approved.\n\n<p align=\"center\">\n  <img \n    src=\"./assets/banner.png\" width=\"800\"\n  />\n</p>\n\n## Features\n\n-   **Web UI:** Simple interface for submitting tasks and providing feedback.\n-   **PocketFlow Workflow:** Manages the process -> review -> result/reprocess logic.\n-   **FastAPI Backend:** Serves the UI and handles API requests asynchronously.\n-   **Server-Sent Events (SSE):** Provides real-time status updates to the client without polling.\n\n## How to Run\n\n1.  Install Dependencies:\n    ```bash\n    pip install -r requirements.txt\n    ```\n\n2.  Run the FastAPI Server:\n    Use Uvicorn (or another ASGI server):\n    ```bash\n    uvicorn server:app --reload --port 8000\n    ```\n    *(The `--reload` flag is useful for development.)*\n\n3.  Access the Web UI:\n    Open your web browser and navigate to `http://127.0.0.1:8000`.\n\n4.  Use the Application:\n    *   Enter text into the textarea and click \"Submit\".\n    *   Observe the status updates pushed via SSE.\n    *   When prompted (\"waiting_for_review\"), use the \"Approve\" or \"Reject\" buttons.\n    *   If rejected, the process loops back. If approved, the final result is displayed.\n\n## How It Works\n\nThe application uses PocketFlow to define and execute the feedback loop workflow. FastAPI handles web requests and manages the real-time SSE communication.\n\n**PocketFlow Workflow:**\n\nThe core logic is orchestrated by an `AsyncFlow` defined in `flow.py`:\n\n```mermaid\nflowchart TD\n    subgraph FeedbackFlow[MinimalFeedbackFlow]\n        Process[ProcessNode] -- default --> Review[ReviewNode]\n        Review -- approved --> Result[ResultNode]\n        Review -- rejected --> Process\n    end\n```\n\n1.  **`ProcessNode`**: Receives input text, calls the minimal `process_task` utility, and stores the output.\n2.  **`ReviewNode` (Async)**:\n    *   Pushes a \"waiting_for_review\" status with the processed output to the SSE queue.\n    *   Waits asynchronously for an external signal (triggered by the `/feedback` API endpoint).\n    *   Based on the received feedback (\"approved\" or \"rejected\"), determines the next step in the flow. Stores the result if approved.\n3.  **`ResultNode`**: Logs the final approved result.\n\n**FastAPI & SSE Integration:**\n\n*   The `/submit` endpoint creates a unique task, initializes the PocketFlow `shared` state (including an `asyncio.Event` for review and an `asyncio.Queue` for SSE), and schedules the flow execution using `BackgroundTasks`.\n*   Nodes within the flow (specifically `ReviewNode`'s prep logic) put status updates onto the task-specific `sse_queue`.\n*   The `/stream/{task_id}` endpoint uses `StreamingResponse` to read from the task's `sse_queue` and push formatted status updates to the connected client via Server-Sent Events.\n*   The `/feedback/{task_id}` endpoint receives the human's decision, updates the `shared` state, and sets the `asyncio.Event` to unblock the waiting `ReviewNode`.\n\nThis setup allows for a decoupled workflow logic (PocketFlow) and web interaction layer (FastAPI), with efficient real-time updates pushed to the user.\n\n## Files\n\n-   [`server.py`](./server.py): The main FastAPI application handling HTTP requests, SSE, state management, and background task scheduling.\n-   [`nodes.py`](./nodes.py): Defines the PocketFlow `Node` classes (`ProcessNode`, `ReviewNode`, `ResultNode`) for the workflow steps.\n-   [`flow.py`](./flow.py): Defines the PocketFlow `AsyncFlow` that connects the nodes into the feedback loop.\n-   [`utils/process_task.py`](./utils/process_task.py): Contains the minimal simulation function for task processing.\n-   [`templates/index.html`](./templates/index.html): The HTML structure for the frontend user interface.\n-   [`static/style.css`](./static/style.css): Basic CSS for styling the frontend.\n-   [`requirements.txt`](./requirements.txt): Project dependencies (FastAPI, Uvicorn, Jinja2, PocketFlow).\n\n\n--- File Index 147: cookbook/pocketflow-web-hitl/docs/design.md ---\n#  Human-in-the-Loop Web Service\n\n## 1. Requirements\n\n*   **Goal:** Create a web service for task submission, processing, human review (Approve/Reject loop via UI), and finalization.\n*   **Interface:** Simple web UI (HTML/JS) for input, status display, and feedback buttons.\n*   **Backend:** FastAPI using PocketFlow for workflow management.\n*   **Real-time Updates:** Use Server-Sent Events (SSE) to push status changes (pending, running, waiting_for_review, completed, failed) and intermediate results to the client without page reloads.\n*   **State:** Use in-memory storage for task state (Warning: Not suitable for production).\n\n## 2. Flow Design\n\n*   **Core Pattern:** Workflow with a conditional loop based on human feedback. SSE for asynchronous status communication.\n*   **Nodes:**\n    1.  `ProcessNode` (Regular): Takes input, executes the (simulated) task processing.\n    2.  `ReviewNode` (Async): Waits for human feedback signaled via an `asyncio.Event`. Pushes \"waiting\\_for\\_review\" status to the SSE queue.\n    3.  `ResultNode` (Regular): Marks the task as complete and logs the final result.\n*   **Shared Store (`shared` dict per task):**\n    *   `task_input`: Initial data from user.\n    *   `processed_output`: Result from `ProcessNode`.\n    *   `feedback`: 'approved' or 'rejected' set by the `/feedback` endpoint.\n    *   `review_event`: `asyncio.Event` used by `ReviewNode` to wait and `/feedback` to signal.\n    *   `final_result`: The approved output.\n    *   `current_attempt`: Tracks reprocessing count.\n    *   `task_id`: Unique identifier for the task.\n*   **SSE Communication:** An `asyncio.Queue` (stored alongside the `shared` store in the server's global `tasks` dict, *not directly in PocketFlow's shared store*) is used per task. Nodes (or wrapper code) put status updates onto this queue. The `/stream` endpoint reads from the queue and sends SSE messages.\n*   **Mermaid Diagram:**\n\n```mermaid\nflowchart TD\n    Process[Process Task] -- \"default\" --> Review{Wait for Feedback}\n    Review -- \"approved\" --> Result[Final Result]\n    Review -- \"rejected\" --> Process\n```\n\n## 3. Utilities\n\nFor this specific example, the core \"utility\" is the processing logic itself. Let's simulate it with a simple function. The FastAPI server acts as the external interface.\n\n* `process_task(input_data)`: A placeholder function. In a real scenario, this might call an LLM (`utils/call_llm.py`).\n\n## 4. Node Design (Detailed)\n\n*   **`ProcessNode` (Node):**\n    *   `prep`: Reads `task_input`, `current_attempt` from `shared`.\n    *   `exec`: Calls `utils.process_task.process_task`.\n    *   `post`: Writes `processed_output` to `shared`, increments `current_attempt`. Returns \"default\".\n*   **`ReviewNode` (AsyncNode):**\n    *   `prep_async`: (As modified/wrapped by server.py) Reads `review_event`, `processed_output` from `shared`. **Puts \"waiting\\_for\\_review\" status onto the task's SSE queue.**\n    *   `exec_async`: `await shared[\"review_event\"].wait()`.\n    *   `post_async`: Reads `feedback` from `shared`. Clears the event. Returns \"approved\" or \"rejected\". If approved, stores `processed_output` into `final_result`.\n*   **`ResultNode` (Node):**\n    *   `prep`: Reads `final_result` from `shared`.\n    *   `exec`: Prints/logs the final result.\n    *   `post`: Returns `None` (ends flow).\n\n--- File Index 148: cookbook/pocketflow-web-hitl/flow.py ---\nfrom pocketflow import AsyncFlow\nfrom nodes import ProcessNode, ReviewNode, ResultNode\n\ndef create_feedback_flow():\n    \"\"\"Creates the minimal feedback workflow.\"\"\"\n    process_node = ProcessNode()\n    review_node = ReviewNode()\n    result_node = ResultNode()\n\n    # Define transitions\n    process_node >> review_node\n    review_node - \"approved\" >> result_node\n    review_node - \"rejected\" >> process_node # Loop back\n\n    # Create the AsyncFlow\n    flow = AsyncFlow(start=process_node)\n    print(\"Minimal feedback flow created.\")\n    return flow\n\n--- File Index 149: cookbook/pocketflow-web-hitl/main.py ---\nfrom flow import qa_flow\n\n# Example main function\n# Please replace this with your own main function\ndef main():\n    shared = {\n        \"question\": \"In one sentence, what's the end of universe?\",\n        \"answer\": None\n    }\n\n    qa_flow.run(shared)\n    print(\"Question:\", shared[\"question\"])\n    print(\"Answer:\", shared[\"answer\"])\n\nif __name__ == \"__main__\":\n    main()\n\n--- File Index 150: cookbook/pocketflow-web-hitl/nodes.py ---\nfrom pocketflow import Node, AsyncNode\nfrom utils.process_task import process_task\n\nclass ProcessNode(Node):\n    def prep(self, shared):\n        task_input = shared.get(\"task_input\", \"No input\")\n        print(\"ProcessNode Prep\")\n        return task_input\n\n    def exec(self, prep_res):\n        return process_task(prep_res)\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"processed_output\"] = exec_res\n        print(\"ProcessNode Post: Output stored.\")\n        return \"default\" # Go to ReviewNode\n\nclass ReviewNode(AsyncNode):\n    async def prep_async(self, shared):\n        review_event = shared.get(\"review_event\")\n        queue = shared.get(\"sse_queue\") # Expect queue in shared\n        processed_output = shared.get(\"processed_output\", \"N/A\")\n\n        if not review_event or not queue:\n            print(\"ERROR: ReviewNode Prep - Missing review_event or sse_queue in shared store!\")\n            return None # Signal failure\n\n        # Push status update to SSE queue\n        status_update = {\n            \"status\": \"waiting_for_review\",\n            \"output_to_review\": processed_output\n        }\n        await queue.put(status_update)\n        print(\"ReviewNode Prep: Put 'waiting_for_review' on SSE queue.\")\n\n        return review_event # Return event for exec_async\n\n    async def exec_async(self, prep_res):\n        review_event = prep_res\n        if not review_event:\n            print(\"ReviewNode Exec: Skipping wait (no event from prep).\")\n            return\n        print(\"ReviewNode Exec: Waiting on review_event...\")\n        await review_event.wait()\n        print(\"ReviewNode Exec: review_event set.\")\n\n    async def post_async(self, shared, prep_res, exec_res):\n        feedback = shared.get(\"feedback\")\n        print(f\"ReviewNode Post: Processing feedback '{feedback}'\")\n\n        # Clear the event for potential loops\n        review_event = shared.get(\"review_event\")\n        if review_event:\n            review_event.clear()\n        shared[\"feedback\"] = None # Reset feedback\n\n        if feedback == \"approved\":\n            shared[\"final_result\"] = shared.get(\"processed_output\")\n            print(\"ReviewNode Post: Action=approved\")\n            return \"approved\"\n        else:\n            print(\"ReviewNode Post: Action=rejected\")\n            return \"rejected\"\n\nclass ResultNode(Node):\n     def prep(self, shared):\n         print(\"ResultNode Prep\")\n         return shared.get(\"final_result\", \"No final result.\")\n\n     def exec(self, prep_res):\n         print(f\"--- FINAL RESULT ---\")\n         print(prep_res)\n         print(f\"--------------------\")\n         return prep_res\n\n     def post(self, shared, prep_res, exec_res):\n         print(\"ResultNode Post: Flow finished.\")\n         return None # End flow\n\n--- File Index 151: cookbook/pocketflow-web-hitl/server.py ---\nimport asyncio\nimport uuid\nimport json\nimport os\nfrom fastapi import FastAPI, Request, HTTPException, status, BackgroundTasks # Import BackgroundTasks\nfrom fastapi.responses import HTMLResponse, StreamingResponse\nfrom fastapi.staticfiles import StaticFiles\nfrom fastapi.templating import Jinja2Templates\nfrom pydantic import BaseModel, Field # Import Pydantic for request/response models\nfrom typing import Dict, Any, Literal # For type hinting\n\nfrom flow import create_feedback_flow # PocketFlow imports\n\n# --- Configuration ---\napp = FastAPI(title=\"Minimal Feedback Loop API\")\n\nstatic_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), 'static'))\nif os.path.isdir(static_dir):\n    app.mount(\"/static\", StaticFiles(directory=static_dir), name=\"static\")\nelse:\n    print(f\"Warning: Static directory '{static_dir}' not found.\")\n\ntemplate_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), 'templates'))\nif os.path.isdir(template_dir):\n    templates = Jinja2Templates(directory=template_dir)\nelse:\n    print(f\"Warning: Template directory '{template_dir}' not found.\")\n    templates = None\n\n# --- State Management (In-Memory - NOT FOR PRODUCTION) ---\n# Global dictionary to store task state. In production, use Redis, DB, etc.\ntasks: Dict[str, Dict[str, Any]] = {}\n# Structure: task_id -> {\"shared\": dict, \"status\": str, \"task_obj\": asyncio.Task | None}\n\n\n# --- Background Flow Runner ---\n# This function remains mostly the same, as it defines the work to be done.\n# It will be scheduled by FastAPI's BackgroundTasks now.\nasync def run_flow_background(task_id: str, flow, shared: Dict[str, Any]):\n    \"\"\"Runs the flow in background, uses queue in shared for SSE.\"\"\"\n    # Check if task exists (might have been cancelled/deleted)\n    if task_id not in tasks:\n        print(f\"Background task {task_id}: Task not found, aborting.\")\n        return\n    queue = shared.get(\"sse_queue\")\n    if not queue:\n        print(f\"ERROR: Task {task_id} missing sse_queue in shared store!\")\n        tasks[task_id][\"status\"] = \"failed\"\n        # Cannot report failure via SSE if queue is missing\n        return\n\n    tasks[task_id][\"status\"] = \"running\"\n    await queue.put({\"status\": \"running\"})\n    print(f\"Task {task_id}: Background flow starting.\")\n\n    final_status = \"unknown\"\n    error_message = None\n    try:\n        # Execute the potentially long-running PocketFlow\n        await flow.run_async(shared)\n\n        # Determine final status based on shared state after flow completion\n        if shared.get(\"final_result\") is not None:\n            final_status = \"completed\"\n        else:\n            # If flow ends without setting final_result\n            final_status = \"finished_incomplete\"\n        print(f\"Task {task_id}: Flow finished with status: {final_status}\")\n\n    except Exception as e:\n        final_status = \"failed\"\n        error_message = str(e)\n        print(f\"Task {task_id}: Flow execution failed: {e}\")\n        # Consider logging traceback here in production\n    finally:\n        # Ensure task still exists before updating state\n        if task_id in tasks:\n            tasks[task_id][\"status\"] = final_status\n            final_update = {\"status\": final_status}\n            if final_status == \"completed\":\n                final_update[\"final_result\"] = shared.get(\"final_result\")\n            elif error_message:\n                final_update[\"error\"] = error_message\n            # Put final status update onto the queue\n            await queue.put(final_update)\n\n        # Signal the end of the SSE stream by putting None\n        # Must happen regardless of whether task was deleted mid-run\n        if queue:\n           await queue.put(None)\n        print(f\"Task {task_id}: Background task ended. Final update sentinel put on queue.\")\n        # Remove the reference to the completed/failed asyncio Task object\n        if task_id in tasks:\n            tasks[task_id][\"task_obj\"] = None\n\n# --- Pydantic Models for Request/Response Validation ---\nclass SubmitRequest(BaseModel):\n    data: str = Field(..., min_length=1, description=\"Input data for the task\")\n\nclass SubmitResponse(BaseModel):\n    message: str = \"Task submitted\"\n    task_id: str\n\nclass FeedbackRequest(BaseModel):\n    feedback: Literal[\"approved\", \"rejected\"] # Use Literal for specific choices\n\nclass FeedbackResponse(BaseModel):\n    message: str\n\n# --- FastAPI Routes ---\n@app.get(\"/\", response_class=HTMLResponse, include_in_schema=False)\nasync def get_index(request: Request):\n    \"\"\"Serves the main HTML frontend.\"\"\"\n    if templates is None:\n        raise HTTPException(status_code=500, detail=\"Templates directory not configured.\")\n    return templates.TemplateResponse(\"index.html\", {\"request\": request})\n\n@app.post(\"/submit\", response_model=SubmitResponse, status_code=status.HTTP_202_ACCEPTED)\nasync def submit_task(\n    submit_request: SubmitRequest, # Use Pydantic model for validation\n    background_tasks: BackgroundTasks # Inject BackgroundTasks instance\n):\n    \"\"\"\n    Submits a new task. The actual processing runs in the background.\n    Returns immediately with the task ID.\n    \"\"\"\n    task_id = str(uuid.uuid4())\n    feedback_event = asyncio.Event()\n    status_queue = asyncio.Queue()\n\n    shared = {\n        \"task_input\": submit_request.data,\n        \"processed_output\": None,\n        \"feedback\": None,\n        \"review_event\": feedback_event,\n        \"sse_queue\": status_queue,\n        \"final_result\": None,\n        \"task_id\": task_id\n    }\n\n    flow = create_feedback_flow()\n\n    # Store task state BEFORE scheduling background task\n    tasks[task_id] = {\n        \"shared\": shared,\n        \"status\": \"pending\",\n        \"task_obj\": None # Placeholder for the asyncio Task created by BackgroundTasks\n    }\n\n    await status_queue.put({\"status\": \"pending\", \"task_id\": task_id})\n\n    # Schedule the flow execution using FastAPI's BackgroundTasks\n    # This runs AFTER the response has been sent\n    background_tasks.add_task(run_flow_background, task_id, flow, shared)\n    # Note: We don't get a direct reference to the asyncio Task object this way,\n    # which is fine for this minimal example. If cancellation were needed,\n    # managing asyncio.create_task manually would be necessary.\n\n    print(f\"Task {task_id}: Submitted, scheduled for background execution.\")\n    return SubmitResponse(task_id=task_id)\n\n\n@app.post(\"/feedback/{task_id}\", response_model=FeedbackResponse)\nasync def provide_feedback(task_id: str, feedback_request: FeedbackRequest):\n    \"\"\"Provides feedback (approved/rejected) to potentially unblock a waiting task.\"\"\"\n    if task_id not in tasks:\n        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=\"Task not found\")\n\n    task_info = tasks[task_id]\n    shared = task_info[\"shared\"]\n    queue = shared.get(\"sse_queue\")\n    review_event = shared.get(\"review_event\")\n\n    async def report_error(message, status_code=status.HTTP_400_BAD_REQUEST):\n        # Helper to log, put status on queue, and raise HTTP exception\n        print(f\"Task {task_id}: Feedback error - {message}\")\n        if queue: await queue.put({\"status\": \"feedback_error\", \"error\": message})\n        raise HTTPException(status_code=status_code, detail=message)\n\n    if not review_event:\n        # This indicates an internal setup error if the task exists but has no event\n        await report_error(\"Task not configured for feedback\", status.HTTP_500_INTERNAL_SERVER_ERROR)\n    if review_event.is_set():\n        # Prevent processing feedback multiple times or if the task isn't waiting\n        await report_error(\"Task not awaiting feedback or feedback already sent\", status.HTTP_409_CONFLICT)\n\n    feedback = feedback_request.feedback # Already validated by Pydantic\n    print(f\"Task {task_id}: Received feedback via POST: {feedback}\")\n\n    # Update status *before* setting the event, so client sees 'processing' first\n    if queue: await queue.put({\"status\": \"processing_feedback\", \"feedback_value\": feedback})\n    tasks[task_id][\"status\"] = \"processing_feedback\" # Update central status tracker\n\n    # Store feedback and signal the waiting ReviewNode\n    shared[\"feedback\"] = feedback\n    review_event.set()\n\n    return FeedbackResponse(message=f\"Feedback '{feedback}' received\")\n\n\n# --- SSE Endpoint ---\n@app.get(\"/stream/{task_id}\")\nasync def stream_status(task_id: str):\n    \"\"\"Streams status updates for a given task using Server-Sent Events.\"\"\"\n    if task_id not in tasks or \"sse_queue\" not in tasks[task_id][\"shared\"]:\n         raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=\"Task or queue not found\")\n\n    queue = tasks[task_id][\"shared\"][\"sse_queue\"]\n\n    async def event_generator():\n        \"\"\"Yields SSE messages from the task's queue.\"\"\"\n        print(f\"SSE Stream: Client connected for {task_id}\")\n        try:\n            while True:\n                # Wait for the next status update from the queue\n                update = await queue.get()\n                if update is None: # Sentinel value indicates end of stream\n                    print(f\"SSE Stream: Sentinel received for {task_id}, closing stream.\")\n                    yield f\"data: {json.dumps({'status': 'stream_closed'})}\\n\\n\"\n                    break\n\n                sse_data = json.dumps(update)\n                print(f\"SSE Stream: Sending for {task_id}: {sse_data}\")\n                yield f\"data: {sse_data}\\n\\n\" # SSE format: \"data: <json>\\n\\n\"\n                queue.task_done() # Acknowledge processing the queue item\n\n        except asyncio.CancelledError:\n            # This happens if the client disconnects\n            print(f\"SSE Stream: Client disconnected for {task_id}.\")\n        except Exception as e:\n            # Log unexpected errors during streaming\n            print(f\"SSE Stream: Error in generator for {task_id}: {e}\")\n            # Optionally send an error message to the client if possible\n            try:\n                yield f\"data: {json.dumps({'status': 'stream_error', 'error': str(e)})}\\n\\n\"\n            except Exception: # Catch errors if yield fails (e.g., connection already closed)\n                pass\n        finally:\n            print(f\"SSE Stream: Generator finished for {task_id}.\")\n            # Consider cleanup here (e.g., removing task if no longer needed)\n            # if task_id in tasks: del tasks[task_id]\n\n    # Use FastAPI/Starlette's StreamingResponse for SSE\n    headers = {'Cache-Control': 'no-cache', 'X-Accel-Buffering': 'no'}\n    return StreamingResponse(event_generator(), media_type=\"text/event-stream\", headers=headers)\n\n# --- Main Execution Guard (for running with uvicorn) ---\nif __name__ == \"__main__\":\n    print(\"Starting FastAPI server using Uvicorn is recommended:\")\n    print(\"uvicorn server:app --reload --host 0.0.0.0 --port 8000\")\n    # Example using uvicorn programmatically (less common than CLI)\n    # import uvicorn\n    # uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n\n--- File Index 152: cookbook/pocketflow-web-hitl/utils/__init__.py ---\n\n\n--- File Index 153: cookbook/pocketflow-web-hitl/utils/process_task.py ---\nimport time\n\ndef process_task(input_data):\n    \"\"\"Minimal simulation of processing the input data.\"\"\"\n    print(f\"Processing: '{input_data[:50]}...'\")\n    \n    # Simulate work\n    time.sleep(2)\n\n    processed_result = f\"Processed: {input_data}\"\n    print(f\"Finished processing.\")\n    return processed_result\n\n# We don't need a separate utils/call_llm.py for this minimal example,\n# but you would add it here if ProcessNode used an LLM.\n\n\n\n--- File Index 154: cookbook/pocketflow-workflow/README.md ---\n# Article Writing Workflow\n\nA PocketFlow example that demonstrates an article writing workflow using a sequence of LLM calls.\n\n## Features\n\n- Generate a simple outline with up to 3 main sections using YAML structured output\n- Write concise (100 words max) content for each section in simple terms\n- Apply a conversational, engaging style to the final article\n\n## Getting Started\n\n1. Install the required dependencies:\n\n```bash\npip install -r requirements.txt\n```\n\n2. Set your OpenAI API key as an environment variable:\n\n```bash\nexport OPENAI_API_KEY=your_api_key_here\n```\n\n3. Run the application with a default topic (\"AI Safety\"):\n\n```bash\npython main.py\n```\n\n4. Or specify your own topic:\n\n```bash\npython main.py Climate Change\n```\n\n## How It Works\n\nThe workflow consists of three sequential nodes:\n\n```mermaid\ngraph LR\n    Outline[Generate Outline] --> Write[Write Content]\n    Write --> Style[Apply Style]\n```\n\nHere's what each node does:\n\n1. **Generate Outline**: Creates a simple outline with up to 3 main sections using YAML structured output\n2. **Write Simple Content**: Writes a concise 100-word explanation for each section\n3. **Apply Style**: Rewrites the combined content in a conversational, engaging style\n\n## Files\n\n- [`main.py`](./main.py): Main entry point for running the article workflow\n- [`flow.py`](./flow.py): Defines the flow that connects the nodes\n- [`nodes.py`](./nodes.py): Contains the node classes for each step in the workflow\n- [`utils.py`](./utils.py): Utility functions including the LLM wrapper\n- [`requirements.txt`](./requirements.txt): Lists the required dependencies\n\n## Example Output\n\n```\n=== Starting Article Workflow on Topic: AI Safety ===\n\n\n===== OUTLINE (YAML) =====\n\nsections:\n- Introduction to AI Safety\n- Key Challenges in AI Safety\n- Strategies for Ensuring AI Safety\n\n\n===== PARSED OUTLINE =====\n\n1. Introduction to AI Safety\n2. Key Challenges in AI Safety\n3. Strategies for Ensuring AI Safety\n\n=========================\n\n\n===== SECTION CONTENTS =====\n\n--- Introduction to AI Safety ---\nAI Safety is about making sure that artificial intelligence (AI) systems are helpful and not harmful. Imagine teaching a robot to help with chores. AI Safety is like setting ground rules for the robot so it doesn't accidentally cause trouble, like mistaking a pet for a toy. By ensuring AI systems understand their tasks and limitations, we can trust them to act safely. It's about creating guidelines and checks to ensure AI assists us without unintended consequences.\n\n--- Key Challenges in AI Safety ---\nAI safety is about ensuring that artificial intelligence systems operate in ways that are beneficial and not harmful. One key challenge is making sure AI makes decisions that align with human values. Imagine teaching a robot to fetch coffee, but it ends up knocking things over because it doesn't understand the mess it creates. Similarly, if AI systems don't fully grasp human intentions, they might act in unexpected ways. The task is to make AI smart enough to achieve goals without causing problems, much like training a puppy to follow rules without chewing on your shoes.\n\n--- Strategies for Ensuring AI Safety ---\nEnsuring AI safety is about making sure artificial intelligence behaves as expected and doesn\u2019t cause harm. Imagine AI as a new driver on the road; we need rules and safeguards to prevent accidents. By testing AI systems under different conditions, setting clear rules for their behavior, and keeping human oversight, we can manage risks. For instance, just as cars have brakes to ensure safety, AI systems need to have fail-safes. This helps in building trust and avoiding unexpected issues, keeping both humans and AI on the right track.\n\n===========================\n\n\n===== FINAL ARTICLE =====\n\n# Welcome to the World of AI Safety\n\nHave you ever wondered what it would be like to have your very own robot helping you around the house? Sounds like a dream, right? But let\u2019s hit pause for a moment. What if this robot mistook your fluffy cat for a toy? That\u2019s exactly where AI Safety comes in. Think of AI Safety as setting some friendly ground rules for your household helper, ensuring that it knows the difference between doing chores and causing a bit of chaos. It\u2019s all about making sure our AI allies play by the rules, making life easier without those pesky accidental hiccups.\n\n# Navigating the Maze of AI Challenges\n\nPicture this: you've asked your trusty robot to grab you a cup of coffee. But instead, it sends mugs flying and spills coffee because it doesn\u2019t quite get the concept of a mess. Frustrating, isn\u2019t it? One of the biggest hurdles in AI Safety is aligning AI decisions with our human values and intentions. It\u2019s like training a puppy not to gnaw on your favorite pair of shoes. Our job is to teach AI how to reach its goals without stepping on our toes, all while being as reliable and lovable as a well-trained pup.\n\n# Steering AI Toward Safe Horizons\n\nNow, how do we keep our AI friends on the straight and narrow? Imagine AI as a new driver learning to navigate the roads of life. Just like we teach new drivers the rules of the road and equip cars with brakes for safety, we provide AI with guidelines and fail-safes to prevent any unintended mishaps. Testing AI systems in various scenarios and keeping a watchful human eye on them ensures they don\u2019t veer off track. It\u2019s all about building trust and creating a partnership where both humans and AI are cruising smoothly together.\n\n# Wrapping It Up\n\nAt the end of the day, AI Safety is about creating a harmonious relationship between humans and machines, where we trust our metal companions to support us without the fear of unexpected surprises. By setting boundaries and ensuring understanding, we\u2019re not just building smarter machines\u2014we\u2019re crafting a future where AI and humanity can thrive together. So, next time you\u2019re imagining that helpful robot assistant, rest easy knowing that AI Safety is making sure it's ready to lend a hand without dropping the ball\u2014or your coffee mug!\n\n========================\n\n\n=== Workflow Completed ===\n\nTopic: AI Safety\nOutline Length: 96 characters\nDraft Length: 1690 characters\nFinal Article Length: 2266 characters\n```\n\n\n--- File Index 155: cookbook/pocketflow-workflow/flow.py ---\nfrom pocketflow import Flow\nfrom nodes import GenerateOutline, WriteSimpleContent, ApplyStyle\n\ndef create_article_flow():\n    \"\"\"\n    Create and configure the article writing workflow\n    \"\"\"\n    # Create node instances\n    outline_node = GenerateOutline()\n    write_node = WriteSimpleContent()\n    style_node = ApplyStyle()\n    \n    # Connect nodes in sequence\n    outline_node >> write_node >> style_node\n    \n    # Create flow starting with outline node\n    article_flow = Flow(start=outline_node)\n    \n    return article_flow\n\n--- File Index 156: cookbook/pocketflow-workflow/main.py ---\nfrom flow import create_article_flow\n\ndef run_flow(topic=\"AI Safety\"):\n    \"\"\"\n    Run the article writing workflow with a specific topic\n    \n    Args:\n        topic (str): The topic for the article\n    \"\"\"\n    # Initialize shared data with the topic\n    shared = {\"topic\": topic}\n    \n    # Print starting message\n    print(f\"\\n=== Starting Article Workflow on Topic: {topic} ===\\n\")\n    \n    # Run the flow\n    flow = create_article_flow()\n    flow.run(shared)\n    \n    # Output summary\n    print(\"\\n=== Workflow Completed ===\\n\")\n    print(f\"Topic: {shared['topic']}\")\n    print(f\"Outline Length: {len(shared['outline'])} characters\")\n    print(f\"Draft Length: {len(shared['draft'])} characters\")\n    print(f\"Final Article Length: {len(shared['final_article'])} characters\")\n    \n    return shared\n\nif __name__ == \"__main__\":\n    import sys\n    \n    # Get topic from command line if provided\n    topic = \"AI Safety\"  # Default topic\n    if len(sys.argv) > 1:\n        topic = \" \".join(sys.argv[1:])\n    \n    run_flow(topic)\n\n--- File Index 157: cookbook/pocketflow-workflow/nodes.py ---\nfrom pocketflow import Node, BatchNode\nfrom utils import call_llm\nimport yaml\n\nclass GenerateOutline(Node):\n    def prep(self, shared):\n        return shared[\"topic\"]\n    \n    def exec(self, topic):\n        prompt = f\"\"\"\nCreate a simple outline for an article about {topic}.\nInclude at most 3 main sections (no subsections).\n\nOutput the sections in YAML format as shown below:\n\n```yaml\nsections:\n    - First section \n    - Second section\n    - Third section\n```\"\"\"\n        response = call_llm(prompt)\n        yaml_str = response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n        structured_result = yaml.safe_load(yaml_str)\n        return structured_result\n    \n    def post(self, shared, prep_res, exec_res):\n        # Store the structured data\n        shared[\"outline_yaml\"] = exec_res\n        \n        # Extract sections\n        sections = exec_res[\"sections\"]\n        shared[\"sections\"] = sections\n        \n        # Format for display\n        formatted_outline = \"\\n\".join([f\"{i+1}. {section}\" for i, section in enumerate(sections)])\n        shared[\"outline\"] = formatted_outline\n        \n        # Display the results\n        print(\"\\n===== OUTLINE (YAML) =====\\n\")\n        print(yaml.dump(exec_res, default_flow_style=False))\n        print(\"\\n===== PARSED OUTLINE =====\\n\")\n        print(formatted_outline)\n        print(\"\\n=========================\\n\")\n        \n        return \"default\"\n\nclass WriteSimpleContent(Node):\n    def prep(self, shared):\n        # Get the list of sections to process\n        return shared.get(\"sections\", [])\n    \n    def exec(self, sections):\n        all_sections_content = []\n        section_contents = {}\n        \n        for section in sections:\n            prompt = f\"\"\"\nWrite a short paragraph (MAXIMUM 100 WORDS) about this section:\n\n{section}\n\nRequirements:\n- Explain the idea in simple, easy-to-understand terms\n- Use everyday language, avoiding jargon\n- Keep it very concise (no more than 100 words)\n- Include one brief example or analogy\n\"\"\"\n            content = call_llm(prompt)\n            section_contents[section] = content\n            all_sections_content.append(f\"## {section}\\n\\n{content}\\n\")\n        \n        return sections, section_contents, \"\\n\".join(all_sections_content)\n    \n    def post(self, shared, prep_res, exec_res):\n        sections, section_contents, draft = exec_res\n        \n        # Store the section contents and draft\n        shared[\"section_contents\"] = section_contents\n        shared[\"draft\"] = draft\n        \n        print(\"\\n===== SECTION CONTENTS =====\\n\")\n        for section, content in section_contents.items():\n            print(f\"--- {section} ---\")\n            print(content)\n            print()\n        print(\"===========================\\n\")\n        \n        return \"default\"\n\nclass ApplyStyle(Node):\n    def prep(self, shared):\n        \"\"\"\n        Get the draft from shared data\n        \"\"\"\n        return shared[\"draft\"]\n    \n    def exec(self, draft):\n        \"\"\"\n        Apply a specific style to the article\n        \"\"\"\n        prompt = f\"\"\"\n        Rewrite the following draft in a conversational, engaging style:\n        \n        {draft}\n        \n        Make it:\n        - Conversational and warm in tone\n        - Include rhetorical questions that engage the reader\n        - Add analogies and metaphors where appropriate\n        - Include a strong opening and conclusion\n        \"\"\"\n        return call_llm(prompt)\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"\n        Store the final article in shared data\n        \"\"\"\n        shared[\"final_article\"] = exec_res\n        print(\"\\n===== FINAL ARTICLE =====\\n\")\n        print(exec_res)\n        print(\"\\n========================\\n\")\n        return \"default\" \n\n--- File Index 158: cookbook/pocketflow-workflow/utils.py ---\nimport os\nfrom openai import OpenAI\n\ndef call_llm(prompt):    \n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n    r = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return r.choices[0].message.content\n\n# Example usage\nif __name__ == \"__main__\":\n    print(call_llm(\"Tell me a short joke\")) \n\n--- File Index 159: pocketflow/__init__.py ---\nimport asyncio, warnings, copy, time\n\nclass BaseNode:\n    def __init__(self): self.params,self.successors={},{}\n    def set_params(self,params): self.params=params\n    def add_successor(self,node,action=\"default\"):\n        if action in self.successors: warnings.warn(f\"Overwriting successor for action '{action}'\")\n        self.successors[action]=node;return node\n    def prep(self,shared): pass\n    def exec(self,prep_res): pass\n    def post(self,shared,prep_res,exec_res): pass\n    def _exec(self,prep_res): return self.exec(prep_res)\n    def _run(self,shared): p=self.prep(shared);e=self._exec(p);return self.post(shared,p,e)\n    def run(self,shared): \n        if self.successors: warnings.warn(\"Node won't run successors. Use Flow.\")  \n        return self._run(shared)\n    def __rshift__(self,other): return self.add_successor(other)\n    def __sub__(self,action):\n        if isinstance(action,str): return _ConditionalTransition(self,action)\n        raise TypeError(\"Action must be a string\")\n\nclass _ConditionalTransition:\n    def __init__(self,src,action): self.src,self.action=src,action\n    def __rshift__(self,tgt): return self.src.add_successor(tgt,self.action)\n\nclass Node(BaseNode):\n    def __init__(self,max_retries=1,wait=0): super().__init__();self.max_retries,self.wait=max_retries,wait\n    def exec_fallback(self,prep_res,exc): raise exc\n    def _exec(self,prep_res):\n        for self.cur_retry in range(self.max_retries):\n            try: return self.exec(prep_res)\n            except Exception as e:\n                if self.cur_retry==self.max_retries-1: return self.exec_fallback(prep_res,e)\n                if self.wait>0: time.sleep(self.wait)\n\nclass BatchNode(Node):\n    def _exec(self,items): return [super(BatchNode,self)._exec(i) for i in (items or [])]\n\nclass Flow(BaseNode):\n    def __init__(self,start): super().__init__();self.start=start\n    def get_next_node(self,curr,action):\n        nxt=curr.successors.get(action or \"default\")\n        if not nxt and curr.successors: warnings.warn(f\"Flow ends: '{action}' not found in {list(curr.successors)}\")\n        return nxt\n    def _orch(self,shared,params=None):\n        curr,p=copy.copy(self.start),(params or {**self.params})\n        while curr: curr.set_params(p);c=curr._run(shared);curr=copy.copy(self.get_next_node(curr,c))\n    def _run(self,shared): pr=self.prep(shared);self._orch(shared);return self.post(shared,pr,None)\n    def exec(self,prep_res): raise RuntimeError(\"Flow can't exec.\")\n\nclass BatchFlow(Flow):\n    def _run(self,shared):\n        pr=self.prep(shared) or []\n        for bp in pr: self._orch(shared,{**self.params,**bp})\n        return self.post(shared,pr,None)\n\nclass AsyncNode(Node):\n    def prep(self,shared): raise RuntimeError(\"Use prep_async.\")\n    def exec(self,prep_res): raise RuntimeError(\"Use exec_async.\")\n    def post(self,shared,prep_res,exec_res): raise RuntimeError(\"Use post_async.\")\n    def exec_fallback(self,prep_res,exc): raise RuntimeError(\"Use exec_fallback_async.\")\n    def _run(self,shared): raise RuntimeError(\"Use run_async.\")\n    async def prep_async(self,shared): pass\n    async def exec_async(self,prep_res): pass\n    async def exec_fallback_async(self,prep_res,exc): raise exc\n    async def post_async(self,shared,prep_res,exec_res): pass\n    async def _exec(self,prep_res): \n        for i in range(self.max_retries):\n            try: return await self.exec_async(prep_res)\n            except Exception as e:\n                if i==self.max_retries-1: return await self.exec_fallback_async(prep_res,e)\n                if self.wait>0: await asyncio.sleep(self.wait)\n    async def run_async(self,shared): \n        if self.successors: warnings.warn(\"Node won't run successors. Use AsyncFlow.\")  \n        return await self._run_async(shared)\n    async def _run_async(self,shared): p=await self.prep_async(shared);e=await self._exec(p);return await self.post_async(shared,p,e)\n\nclass AsyncBatchNode(AsyncNode,BatchNode):\n    async def _exec(self,items): return [await super(AsyncBatchNode,self)._exec(i) for i in items]\n\nclass AsyncParallelBatchNode(AsyncNode,BatchNode):\n    async def _exec(self,items): return await asyncio.gather(*(super(AsyncParallelBatchNode,self)._exec(i) for i in items))\n\nclass AsyncFlow(Flow,AsyncNode):\n    async def _orch_async(self,shared,params=None):\n        curr,p=copy.copy(self.start),(params or {**self.params})\n        while curr:curr.set_params(p);c=await curr._run_async(shared) if isinstance(curr,AsyncNode) else curr._run(shared);curr=copy.copy(self.get_next_node(curr,c))\n    async def _run_async(self,shared): p=await self.prep_async(shared);await self._orch_async(shared);return await self.post_async(shared,p,None)\n\nclass AsyncBatchFlow(AsyncFlow,BatchFlow):\n    async def _run_async(self,shared):\n        pr=await self.prep_async(shared) or []\n        for bp in pr: await self._orch_async(shared,{**self.params,**bp})\n        return await self.post_async(shared,pr,None)\n\nclass AsyncParallelBatchFlow(AsyncFlow,BatchFlow):\n    async def _run_async(self,shared):\n        pr=await self.prep_async(shared) or []\n        await asyncio.gather(*(self._orch_async(shared,{**self.params,**bp}) for bp in pr))\n        return await self.post_async(shared,pr,None)\n\n--- File Index 160: setup.py ---\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"pocketflow\",\n    version=\"0.0.1\",\n    packages=find_packages(),\n    author=\"Zachary Huang\",\n    author_email=\"zh2408@columbia.edu\",\n    description=\"Minimalist LLM Framework in 100 Lines. Enable LLMs to Program Themselves.\",\n    url=\"https://github.com/miniLLMFlow/PocketFlow/\",\n)\n\n\n\nAnalyze the codebase context.\nIdentify the top 5-10 core most important abstractions to help those new to the codebase.\n\nFor each abstraction, provide:\n1. A concise `name`.\n2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\nList of file indices and paths present in the context:\n- 0 # README.md\n- 1 # cookbook/README.md\n- 2 # cookbook/pocketflow-agent/README.md\n- 3 # cookbook/pocketflow-agent/flow.py\n- 4 # cookbook/pocketflow-agent/main.py\n- 5 # cookbook/pocketflow-agent/nodes.py\n- 6 # cookbook/pocketflow-agent/utils.py\n- 7 # cookbook/pocketflow-async-basic/README.md\n- 8 # cookbook/pocketflow-async-basic/flow.py\n- 9 # cookbook/pocketflow-async-basic/main.py\n- 10 # cookbook/pocketflow-async-basic/nodes.py\n- 11 # cookbook/pocketflow-async-basic/utils.py\n- 12 # cookbook/pocketflow-batch-flow/README.md\n- 13 # cookbook/pocketflow-batch-flow/flow.py\n- 14 # cookbook/pocketflow-batch-flow/main.py\n- 15 # cookbook/pocketflow-batch-flow/nodes.py\n- 16 # cookbook/pocketflow-batch-node/README.md\n- 17 # cookbook/pocketflow-batch-node/flow.py\n- 18 # cookbook/pocketflow-batch-node/main.py\n- 19 # cookbook/pocketflow-batch-node/nodes.py\n- 20 # cookbook/pocketflow-batch/README.md\n- 21 # cookbook/pocketflow-batch/main.py\n- 22 # cookbook/pocketflow-batch/translations/README_CHINESE.md\n- 23 # cookbook/pocketflow-batch/translations/README_FRENCH.md\n- 24 # cookbook/pocketflow-batch/translations/README_GERMAN.md\n- 25 # cookbook/pocketflow-batch/translations/README_JAPANESE.md\n- 26 # cookbook/pocketflow-batch/translations/README_KOREAN.md\n- 27 # cookbook/pocketflow-batch/translations/README_PORTUGUESE.md\n- 28 # cookbook/pocketflow-batch/translations/README_RUSSIAN.md\n- 29 # cookbook/pocketflow-batch/translations/README_SPANISH.md\n- 30 # cookbook/pocketflow-batch/utils.py\n- 31 # cookbook/pocketflow-chat-guardrail/README.md\n- 32 # cookbook/pocketflow-chat-guardrail/main.py\n- 33 # cookbook/pocketflow-chat-guardrail/utils.py\n- 34 # cookbook/pocketflow-chat-memory/README.md\n- 35 # cookbook/pocketflow-chat-memory/flow.py\n- 36 # cookbook/pocketflow-chat-memory/main.py\n- 37 # cookbook/pocketflow-chat-memory/nodes.py\n- 38 # cookbook/pocketflow-chat-memory/utils/__init__.py\n- 39 # cookbook/pocketflow-chat-memory/utils/call_llm.py\n- 40 # cookbook/pocketflow-chat-memory/utils/get_embedding.py\n- 41 # cookbook/pocketflow-chat-memory/utils/vector_index.py\n- 42 # cookbook/pocketflow-chat/README.md\n- 43 # cookbook/pocketflow-chat/main.py\n- 44 # cookbook/pocketflow-chat/utils.py\n- 45 # cookbook/pocketflow-communication/README.md\n- 46 # cookbook/pocketflow-communication/flow.py\n- 47 # cookbook/pocketflow-communication/main.py\n- 48 # cookbook/pocketflow-communication/nodes.py\n- 49 # cookbook/pocketflow-flow/README.md\n- 50 # cookbook/pocketflow-flow/flow.py\n- 51 # cookbook/pocketflow-flow/main.py\n- 52 # cookbook/pocketflow-hello-world/README.md\n- 53 # cookbook/pocketflow-hello-world/docs/design.md\n- 54 # cookbook/pocketflow-hello-world/flow.py\n- 55 # cookbook/pocketflow-hello-world/main.py\n- 56 # cookbook/pocketflow-hello-world/utils/__init__.py\n- 57 # cookbook/pocketflow-hello-world/utils/call_llm.py\n- 58 # cookbook/pocketflow-llm-streaming/README.md\n- 59 # cookbook/pocketflow-llm-streaming/main.py\n- 60 # cookbook/pocketflow-llm-streaming/utils.py\n- 61 # cookbook/pocketflow-majority-vote/README.md\n- 62 # cookbook/pocketflow-majority-vote/main.py\n- 63 # cookbook/pocketflow-majority-vote/utils.py\n- 64 # cookbook/pocketflow-map-reduce/README.md\n- 65 # cookbook/pocketflow-map-reduce/flow.py\n- 66 # cookbook/pocketflow-map-reduce/main.py\n- 67 # cookbook/pocketflow-map-reduce/nodes.py\n- 68 # cookbook/pocketflow-map-reduce/utils.py\n- 69 # cookbook/pocketflow-mcp/README.md\n- 70 # cookbook/pocketflow-mcp/main.py\n- 71 # cookbook/pocketflow-mcp/simple_server.py\n- 72 # cookbook/pocketflow-mcp/utils.py\n- 73 # cookbook/pocketflow-multi-agent/README.md\n- 74 # cookbook/pocketflow-multi-agent/main.py\n- 75 # cookbook/pocketflow-multi-agent/utils.py\n- 76 # cookbook/pocketflow-nested-batch/README.md\n- 77 # cookbook/pocketflow-nested-batch/flow.py\n- 78 # cookbook/pocketflow-nested-batch/main.py\n- 79 # cookbook/pocketflow-nested-batch/nodes.py\n- 80 # cookbook/pocketflow-node/README.md\n- 81 # cookbook/pocketflow-node/flow.py\n- 82 # cookbook/pocketflow-node/main.py\n- 83 # cookbook/pocketflow-node/utils/call_llm.py\n- 84 # cookbook/pocketflow-parallel-batch-flow/README.md\n- 85 # cookbook/pocketflow-parallel-batch-flow/flow.py\n- 86 # cookbook/pocketflow-parallel-batch-flow/main.py\n- 87 # cookbook/pocketflow-parallel-batch-flow/nodes.py\n- 88 # cookbook/pocketflow-parallel-batch/README.md\n- 89 # cookbook/pocketflow-parallel-batch/main.py\n- 90 # cookbook/pocketflow-rag/README.md\n- 91 # cookbook/pocketflow-rag/flow.py\n- 92 # cookbook/pocketflow-rag/main.py\n- 93 # cookbook/pocketflow-rag/nodes.py\n- 94 # cookbook/pocketflow-rag/utils.py\n- 95 # cookbook/pocketflow-structured-output/README.md\n- 96 # cookbook/pocketflow-structured-output/main.py\n- 97 # cookbook/pocketflow-structured-output/utils.py\n- 98 # cookbook/pocketflow-supervisor/README.md\n- 99 # cookbook/pocketflow-supervisor/flow.py\n- 100 # cookbook/pocketflow-supervisor/main.py\n- 101 # cookbook/pocketflow-supervisor/nodes.py\n- 102 # cookbook/pocketflow-supervisor/utils.py\n- 103 # cookbook/pocketflow-thinking/README.md\n- 104 # cookbook/pocketflow-thinking/design.md\n- 105 # cookbook/pocketflow-thinking/flow.py\n- 106 # cookbook/pocketflow-thinking/main.py\n- 107 # cookbook/pocketflow-thinking/nodes.py\n- 108 # cookbook/pocketflow-thinking/utils.py\n- 109 # cookbook/pocketflow-tool-crawler/README.md\n- 110 # cookbook/pocketflow-tool-crawler/flow.py\n- 111 # cookbook/pocketflow-tool-crawler/main.py\n- 112 # cookbook/pocketflow-tool-crawler/nodes.py\n- 113 # cookbook/pocketflow-tool-crawler/tools/crawler.py\n- 114 # cookbook/pocketflow-tool-crawler/tools/parser.py\n- 115 # cookbook/pocketflow-tool-crawler/utils/__init__.py\n- 116 # cookbook/pocketflow-tool-crawler/utils/call_llm.py\n- 117 # cookbook/pocketflow-tool-database/README.md\n- 118 # cookbook/pocketflow-tool-database/flow.py\n- 119 # cookbook/pocketflow-tool-database/main.py\n- 120 # cookbook/pocketflow-tool-database/nodes.py\n- 121 # cookbook/pocketflow-tool-database/tools/database.py\n- 122 # cookbook/pocketflow-tool-database/utils/__init__.py\n- 123 # cookbook/pocketflow-tool-embeddings/README.md\n- 124 # cookbook/pocketflow-tool-embeddings/flow.py\n- 125 # cookbook/pocketflow-tool-embeddings/main.py\n- 126 # cookbook/pocketflow-tool-embeddings/nodes.py\n- 127 # cookbook/pocketflow-tool-embeddings/tools/embeddings.py\n- 128 # cookbook/pocketflow-tool-embeddings/utils/__init__.py\n- 129 # cookbook/pocketflow-tool-embeddings/utils/call_llm.py\n- 130 # cookbook/pocketflow-tool-pdf-vision/README.md\n- 131 # cookbook/pocketflow-tool-pdf-vision/flow.py\n- 132 # cookbook/pocketflow-tool-pdf-vision/main.py\n- 133 # cookbook/pocketflow-tool-pdf-vision/nodes.py\n- 134 # cookbook/pocketflow-tool-pdf-vision/tools/pdf.py\n- 135 # cookbook/pocketflow-tool-pdf-vision/tools/vision.py\n- 136 # cookbook/pocketflow-tool-pdf-vision/utils/__init__.py\n- 137 # cookbook/pocketflow-tool-pdf-vision/utils/call_llm.py\n- 138 # cookbook/pocketflow-tool-search/README.md\n- 139 # cookbook/pocketflow-tool-search/flow.py\n- 140 # cookbook/pocketflow-tool-search/main.py\n- 141 # cookbook/pocketflow-tool-search/nodes.py\n- 142 # cookbook/pocketflow-tool-search/tools/parser.py\n- 143 # cookbook/pocketflow-tool-search/tools/search.py\n- 144 # cookbook/pocketflow-tool-search/utils/__init__.py\n- 145 # cookbook/pocketflow-tool-search/utils/call_llm.py\n- 146 # cookbook/pocketflow-web-hitl/README.md\n- 147 # cookbook/pocketflow-web-hitl/docs/design.md\n- 148 # cookbook/pocketflow-web-hitl/flow.py\n- 149 # cookbook/pocketflow-web-hitl/main.py\n- 150 # cookbook/pocketflow-web-hitl/nodes.py\n- 151 # cookbook/pocketflow-web-hitl/server.py\n- 152 # cookbook/pocketflow-web-hitl/utils/__init__.py\n- 153 # cookbook/pocketflow-web-hitl/utils/process_task.py\n- 154 # cookbook/pocketflow-workflow/README.md\n- 155 # cookbook/pocketflow-workflow/flow.py\n- 156 # cookbook/pocketflow-workflow/main.py\n- 157 # cookbook/pocketflow-workflow/nodes.py\n- 158 # cookbook/pocketflow-workflow/utils.py\n- 159 # pocketflow/__init__.py\n- 160 # setup.py\n\nFormat the output as a YAML list of dictionaries:\n\n```yaml\n- name: Query Processing\n  description: | \n    Explains what the abstraction does.\n    It's like a central dispatcher routing requests.\n  file_indices:\n    - 0 # path/to/file1.py\n    - 3 # path/to/related.py\n- name: Query Optimization\n  description: |\n    Another core concept, similar to a blueprint for objects.\n  file_indices:\n    - 5 # path/to/another.js\n# ... up to 10 abstractions\n```": "```yaml\n- name: Node\n  description: |\n    A fundamental building block in PocketFlow, like a single function in a program. Each node performs a specific task, such as calling an LLM or processing data. It takes inputs, does something with them, and produces outputs. Nodes are connected together to create complex workflows, similar to how functions are chained together to build a larger application.  Nodes implement `prep`, `exec`, and `post` methods to define their behavior.\n  file_indices:\n    - 5 # cookbook/pocketflow-agent/nodes.py\n    - 150 # cookbook/pocketflow-web-hitl/nodes.py\n    - 159 # pocketflow/__init__.py\n\n- name: Flow\n  description: |\n    The orchestration layer that connects individual Nodes into a functioning workflow, like a conductor leading an orchestra. A flow defines the sequence in which nodes are executed and how data is passed between them. Flows are defined by a starting node and transitions are specified using rshift `>>` or subtraction `-` operators to create a directed graph-like structure for task execution.\n  file_indices:\n    - 3 # cookbook/pocketflow-agent/flow.py\n    - 155 # cookbook/pocketflow-workflow/flow.py\n    - 159 # pocketflow/__init__.py\n\n- name: Shared Store\n  description: |\n    Acts as a central memory or message board where Nodes can communicate and share data. It's like a shared notebook in a group project, where each person can read what others have written and add their own contributions. Data stored in the shared store is available to all nodes in the flow, facilitating seamless data transfer between processing steps.\n  file_indices:\n    - 5 # cookbook/pocketflow-agent/nodes.py\n    - 48 # cookbook/pocketflow-communication/nodes.py\n    - 159 # pocketflow/__init__.py\n\n- name: Prep, Exec, Post (Node Lifecycle)\n  description: |\n    Defines the standard steps for each Node, enabling structured and modular task execution. `prep` prepares the input for the task like gathering materials for cooking. `exec` executes the core functionality, similar to performing the actual cooking. `post` processes the output and sets the next step similar to serving the dish.\n  file_indices:\n    - 5 # cookbook/pocketflow-agent/nodes.py\n    - 159 # pocketflow/__init__.py\n\n- name: BatchNode\n  description: |\n    Enables efficient processing of multiple independent data items in a single node, like an assembly line.  It applies the same `exec` function to each item in a batch, allowing for parallel or sequential processing depending on the underlying implementation. Prep splits the input into chunks and the exec stage processes them in batch.\n  file_indices:\n    - 19 # cookbook/pocketflow-batch-node/nodes.py\n    - 21 # cookbook/pocketflow-batch/main.py\n    - 159 # pocketflow/__init__.py\n\n- name: AsyncNode\n  description: |\n    Handles asynchronous operations, enabling PocketFlow to perform non-blocking calls to external services or APIs. It's like ordering food online; the code doesn't wait idly for the food to arrive but can handle other tasks in the meantime. Makes use of `async` and `await` keywords.\n  file_indices:\n    - 10 # cookbook/pocketflow-async-basic/nodes.py\n    - 159 # pocketflow/__init__.py\n\n- name: AsyncFlow\n  description: |\n    Extends the `Flow` abstraction to support asynchronous operations within the workflow, like a chef juggling multiple dishes simultaneously. It allows you to run nodes concurrently, maximizing resource utilization and improving performance when dealing with tasks that involve waiting for external resources or I/O operations.\n  file_indices:\n    - 8 # cookbook/pocketflow-async-basic/flow.py\n    - 159 # pocketflow/__init__.py\n\n- name: AsyncParallelBatchNode\n  description: |\n     Facilitates highly parallel processing of batch items when used alongside AsyncFlow. It combines the benefits of AsyncNode and BatchNode to execute batch jobs concurrently rather than sequentially. This parallelism dramatically increases the performance of I/O-bound operations.\n  file_indices:\n    - 89 # cookbook/pocketflow-parallel-batch/main.py\n    - 159 # pocketflow/__init__.py\n```", "\nBased on the following abstractions and relevant code snippets from the project `PocketFlow`:\n\nList of Abstraction Indices and Names:\n0 # Node\n1 # Flow\n2 # Shared Store\n3 # Prep, Exec, Post (Node Lifecycle)\n4 # BatchNode\n5 # AsyncNode\n6 # AsyncFlow\n7 # AsyncParallelBatchNode\n\nContext (Abstractions, Descriptions, Code):\nIdentified Abstractions:\n- Index 0: Node (Relevant file indices: [5, 150, 159])\n  Description: A fundamental building block in PocketFlow, like a single function in a program. Each node performs a specific task, such as calling an LLM or processing data. It takes inputs, does something with them, and produces outputs. Nodes are connected together to create complex workflows, similar to how functions are chained together to build a larger application.  Nodes implement `prep`, `exec`, and `post` methods to define their behavior.\n\n- Index 1: Flow (Relevant file indices: [3, 155, 159])\n  Description: The orchestration layer that connects individual Nodes into a functioning workflow, like a conductor leading an orchestra. A flow defines the sequence in which nodes are executed and how data is passed between them. Flows are defined by a starting node and transitions are specified using rshift `>>` or subtraction `-` operators to create a directed graph-like structure for task execution.\n\n- Index 2: Shared Store (Relevant file indices: [5, 48, 159])\n  Description: Acts as a central memory or message board where Nodes can communicate and share data. It's like a shared notebook in a group project, where each person can read what others have written and add their own contributions. Data stored in the shared store is available to all nodes in the flow, facilitating seamless data transfer between processing steps.\n\n- Index 3: Prep, Exec, Post (Node Lifecycle) (Relevant file indices: [5, 159])\n  Description: Defines the standard steps for each Node, enabling structured and modular task execution. `prep` prepares the input for the task like gathering materials for cooking. `exec` executes the core functionality, similar to performing the actual cooking. `post` processes the output and sets the next step similar to serving the dish.\n\n- Index 4: BatchNode (Relevant file indices: [19, 21, 159])\n  Description: Enables efficient processing of multiple independent data items in a single node, like an assembly line.  It applies the same `exec` function to each item in a batch, allowing for parallel or sequential processing depending on the underlying implementation. Prep splits the input into chunks and the exec stage processes them in batch.\n\n- Index 5: AsyncNode (Relevant file indices: [10, 159])\n  Description: Handles asynchronous operations, enabling PocketFlow to perform non-blocking calls to external services or APIs. It's like ordering food online; the code doesn't wait idly for the food to arrive but can handle other tasks in the meantime. Makes use of `async` and `await` keywords.\n\n- Index 6: AsyncFlow (Relevant file indices: [8, 159])\n  Description: Extends the `Flow` abstraction to support asynchronous operations within the workflow, like a chef juggling multiple dishes simultaneously. It allows you to run nodes concurrently, maximizing resource utilization and improving performance when dealing with tasks that involve waiting for external resources or I/O operations.\n\n- Index 7: AsyncParallelBatchNode (Relevant file indices: [89, 159])\n  Description: Facilitates highly parallel processing of batch items when used alongside AsyncFlow. It combines the benefits of AsyncNode and BatchNode to execute batch jobs concurrently rather than sequentially. This parallelism dramatically increases the performance of I/O-bound operations.\n\n\nRelevant File Snippets (Referenced by Index and Path):\n--- File: 3 # cookbook/pocketflow-agent/flow.py ---\nfrom pocketflow import Flow\nfrom nodes import DecideAction, SearchWeb, AnswerQuestion\n\ndef create_agent_flow():\n    \"\"\"\n    Create and connect the nodes to form a complete agent flow.\n    \n    The flow works like this:\n    1. DecideAction node decides whether to search or answer\n    2. If search, go to SearchWeb node\n    3. If answer, go to AnswerQuestion node\n    4. After SearchWeb completes, go back to DecideAction\n    \n    Returns:\n        Flow: A complete research agent flow\n    \"\"\"\n    # Create instances of each node\n    decide = DecideAction()\n    search = SearchWeb()\n    answer = AnswerQuestion()\n    \n    # Connect the nodes\n    # If DecideAction returns \"search\", go to SearchWeb\n    decide - \"search\" >> search\n    \n    # If DecideAction returns \"answer\", go to AnswerQuestion\n    decide - \"answer\" >> answer\n    \n    # After SearchWeb completes and returns \"decide\", go back to DecideAction\n    search - \"decide\" >> decide\n    \n    # Create and return the flow, starting with the DecideAction node\n    return Flow(start=decide) \n\n--- File: 5 # cookbook/pocketflow-agent/nodes.py ---\nfrom pocketflow import Node\nfrom utils import call_llm, search_web\nimport yaml\n\nclass DecideAction(Node):\n    def prep(self, shared):\n        \"\"\"Prepare the context and question for the decision-making process.\"\"\"\n        # Get the current context (default to \"No previous search\" if none exists)\n        context = shared.get(\"context\", \"No previous search\")\n        # Get the question from the shared store\n        question = shared[\"question\"]\n        # Return both for the exec step\n        return question, context\n        \n    def exec(self, inputs):\n        \"\"\"Call the LLM to decide whether to search or answer.\"\"\"\n        question, context = inputs\n        \n        print(f\"\ud83e\udd14 Agent deciding what to do next...\")\n        \n        # Create a prompt to help the LLM decide what to do next with proper yaml formatting\n        prompt = f\"\"\"\n### CONTEXT\nYou are a research assistant that can search the web.\nQuestion: {question}\nPrevious Research: {context}\n\n### ACTION SPACE\n[1] search\n  Description: Look up more information on the web\n  Parameters:\n    - query (str): What to search for\n\n[2] answer\n  Description: Answer the question with current knowledge\n  Parameters:\n    - answer (str): Final answer to the question\n\n## NEXT ACTION\nDecide the next action based on the context and available actions.\nReturn your response in this format:\n\n```yaml\nthinking: |\n    <your step-by-step reasoning process>\naction: search OR answer\nreason: <why you chose this action>\nanswer: <if action is answer>\nsearch_query: <specific search query if action is search>\n```\nIMPORTANT: Make sure to:\n1. Use proper indentation (4 spaces) for all multi-line fields\n2. Use the | character for multi-line text fields\n3. Keep single-line fields without the | character\n\"\"\"\n        \n        # Call the LLM to make a decision\n        response = call_llm(prompt)\n        \n        # Parse the response to get the decision\n        yaml_str = response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n        decision = yaml.safe_load(yaml_str)\n        \n        return decision\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Save the decision and determine the next step in the flow.\"\"\"\n        # If LLM decided to search, save the search query\n        if exec_res[\"action\"] == \"search\":\n            shared[\"search_query\"] = exec_res[\"search_query\"]\n            print(f\"\ud83d\udd0d Agent decided to search for: {exec_res['search_query']}\")\n        else:\n            shared[\"context\"] = exec_res[\"answer\"] #save the context if LLM gives the answer without searching.\n            print(f\"\ud83d\udca1 Agent decided to answer the question\")\n        \n        # Return the action to determine the next node in the flow\n        return exec_res[\"action\"]\n\nclass SearchWeb(Node):\n    def prep(self, shared):\n        \"\"\"Get the search query from the shared store.\"\"\"\n        return shared[\"search_query\"]\n        \n    def exec(self, search_query):\n        \"\"\"Search the web for the given query.\"\"\"\n        # Call the search utility function\n        print(f\"\ud83c\udf10 Searching the web for: {search_query}\")\n        results = search_web(search_query)\n        return results\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Save the search results and go back to the decision node.\"\"\"\n        # Add the search results to the context in the shared store\n        previous = shared.get(\"context\", \"\")\n        shared[\"context\"] = previous + \"\\n\\nSEARCH: \" + shared[\"search_query\"] + \"\\nRESULTS: \" + exec_res\n        \n        print(f\"\ud83d\udcda Found information, analyzing results...\")\n        \n        # Always go back to the decision node after searching\n        return \"decide\"\n\nclass AnswerQuestion(Node):\n    def prep(self, shared):\n        \"\"\"Get the question and context for answering.\"\"\"\n        return shared[\"question\"], shared.get(\"context\", \"\")\n        \n    def exec(self, inputs):\n        \"\"\"Call the LLM to generate a final answer.\"\"\"\n        question, context = inputs\n        \n        print(f\"\u270d\ufe0f Crafting final answer...\")\n        \n        # Create a prompt for the LLM to answer the question\n        prompt = f\"\"\"\n### CONTEXT\nBased on the following information, answer the question.\nQuestion: {question}\nResearch: {context}\n\n## YOUR ANSWER:\nProvide a comprehensive answer using the research results.\n\"\"\"\n        # Call the LLM to generate an answer\n        answer = call_llm(prompt)\n        return answer\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Save the final answer and complete the flow.\"\"\"\n        # Save the answer in the shared store\n        shared[\"answer\"] = exec_res\n        \n        print(f\"\u2705 Answer generated successfully\")\n        \n        # We're done - no need to continue the flow\n        return \"done\" \n\n\n--- File: 8 # cookbook/pocketflow-async-basic/flow.py ---\n\"\"\"AsyncFlow implementation for recipe finder.\"\"\"\n\nfrom pocketflow import AsyncFlow, Node\nfrom nodes import FetchRecipes, SuggestRecipe, GetApproval\n\nclass NoOp(Node):\n    \"\"\"Node that does nothing, used to properly end the flow.\"\"\"\n    pass\n\ndef create_flow():\n    \"\"\"Create and connect nodes into a flow.\"\"\"\n    \n    # Create nodes\n    fetch = FetchRecipes()\n    suggest = SuggestRecipe()\n    approve = GetApproval()\n    end = NoOp()\n    \n    # Connect nodes\n    fetch - \"suggest\" >> suggest\n    suggest - \"approve\" >> approve\n    approve - \"retry\" >> suggest  # Loop back for another suggestion\n    approve - \"accept\" >> end     # Properly end the flow\n    \n    # Create flow starting with fetch\n    flow = AsyncFlow(start=fetch)\n    return flow \n\n--- File: 10 # cookbook/pocketflow-async-basic/nodes.py ---\nfrom pocketflow import AsyncNode\nfrom utils import fetch_recipes, call_llm_async, get_user_input\n\nclass FetchRecipes(AsyncNode):\n    \"\"\"AsyncNode that fetches recipes.\"\"\"\n    \n    async def prep_async(self, shared):\n        \"\"\"Get ingredient from user.\"\"\"\n        ingredient = await get_user_input(\"Enter ingredient: \")\n        return ingredient\n    \n    async def exec_async(self, ingredient):\n        \"\"\"Fetch recipes asynchronously.\"\"\"\n        recipes = await fetch_recipes(ingredient)\n        return recipes\n    \n    async def post_async(self, shared, prep_res, recipes):\n        \"\"\"Store recipes and continue.\"\"\"\n        shared[\"recipes\"] = recipes\n        shared[\"ingredient\"] = prep_res\n        return \"suggest\"\n\nclass SuggestRecipe(AsyncNode):\n    \"\"\"AsyncNode that suggests a recipe using LLM.\"\"\"\n    \n    async def prep_async(self, shared):\n        \"\"\"Get recipes from shared store.\"\"\"\n        return shared[\"recipes\"]\n    \n    async def exec_async(self, recipes):\n        \"\"\"Get suggestion from LLM.\"\"\"\n        suggestion = await call_llm_async(\n            f\"Choose best recipe from: {', '.join(recipes)}\"\n        )\n        return suggestion\n    \n    async def post_async(self, shared, prep_res, suggestion):\n        \"\"\"Store suggestion and continue.\"\"\"\n        shared[\"suggestion\"] = suggestion\n        return \"approve\"\n\nclass GetApproval(AsyncNode):\n    \"\"\"AsyncNode that gets user approval.\"\"\"\n    \n    async def prep_async(self, shared):\n        \"\"\"Get current suggestion.\"\"\"\n        return shared[\"suggestion\"]\n    \n    async def exec_async(self, suggestion):\n        \"\"\"Ask for user approval.\"\"\"\n        answer = await get_user_input(f\"\\nAccept this recipe? (y/n): \")\n        return answer\n    \n    async def post_async(self, shared, prep_res, answer):\n        \"\"\"Handle user's decision.\"\"\"\n        if answer == \"y\":\n            print(\"\\nGreat choice! Here's your recipe...\")\n            print(f\"Recipe: {shared['suggestion']}\")\n            print(f\"Ingredient: {shared['ingredient']}\")\n            return \"accept\"\n        else:\n            print(\"\\nLet's try another recipe...\")\n            return \"retry\" \n\n--- File: 19 # cookbook/pocketflow-batch-node/nodes.py ---\nimport pandas as pd\nfrom pocketflow import BatchNode\n\nclass CSVProcessor(BatchNode):\n    \"\"\"BatchNode that processes a large CSV file in chunks.\"\"\"\n    \n    def __init__(self, chunk_size=1000):\n        \"\"\"Initialize with chunk size.\"\"\"\n        super().__init__()\n        self.chunk_size = chunk_size\n    \n    def prep(self, shared):\n        \"\"\"Split CSV file into chunks.\n        \n        Returns an iterator of DataFrames, each containing chunk_size rows.\n        \"\"\"\n        # Read CSV in chunks\n        chunks = pd.read_csv(\n            shared[\"input_file\"],\n            chunksize=self.chunk_size\n        )\n        return chunks\n    \n    def exec(self, chunk):\n        \"\"\"Process a single chunk of the CSV.\n        \n        Args:\n            chunk: pandas DataFrame containing chunk_size rows\n            \n        Returns:\n            dict: Statistics for this chunk\n        \"\"\"\n        return {\n            \"total_sales\": chunk[\"amount\"].sum(),\n            \"num_transactions\": len(chunk),\n            \"total_amount\": chunk[\"amount\"].sum()\n        }\n    \n    def post(self, shared, prep_res, exec_res_list):\n        \"\"\"Combine results from all chunks.\n        \n        Args:\n            prep_res: Original chunks iterator\n            exec_res_list: List of results from each chunk\n            \n        Returns:\n            str: Action to take next\n        \"\"\"\n        # Combine statistics from all chunks\n        total_sales = sum(res[\"total_sales\"] for res in exec_res_list)\n        total_transactions = sum(res[\"num_transactions\"] for res in exec_res_list)\n        total_amount = sum(res[\"total_amount\"] for res in exec_res_list)\n        \n        # Calculate final statistics\n        shared[\"statistics\"] = {\n            \"total_sales\": total_sales,\n            \"average_sale\": total_amount / total_transactions,\n            \"total_transactions\": total_transactions\n        }\n        \n        return \"show_stats\" \n\n--- File: 21 # cookbook/pocketflow-batch/main.py ---\nimport os\nfrom pocketflow import BatchNode, Flow\nfrom utils import call_llm\n\nclass TranslateTextNode(BatchNode):\n    def prep(self, shared):\n        text = shared.get(\"text\", \"(No text provided)\")\n        languages = shared.get(\"languages\", [\"Chinese\", \"Spanish\", \"Japanese\", \"German\", \n                              \"Russian\", \"Portuguese\", \"French\", \"Korean\"])\n        \n        # Create batches for each language translation\n        return [(text, lang) for lang in languages]\n\n    def exec(self, data_tuple):\n        text, language = data_tuple\n        \n        prompt = f\"\"\"\nPlease translate the following markdown file into {language}. \nBut keep the original markdown format, links and code blocks.\nDirectly return the translated text, without any other text or comments.\n\nOriginal: \n{text}\n\nTranslated:\"\"\"\n        \n        result = call_llm(prompt)\n        \n        print(f\"Translated {language} text\")\n\n        return {\"language\": language, \"translation\": result}\n\n    def post(self, shared, prep_res, exec_res_list):\n        # Create output directory if it doesn't exist\n        output_dir = shared.get(\"output_dir\", \"translations\")\n        os.makedirs(output_dir, exist_ok=True)\n        \n        # Write each translation to a file\n        for result in exec_res_list:\n            language, translation = result[\"language\"], result[\"translation\"]\n            \n            # Write to file\n            filename = os.path.join(output_dir, f\"README_{language.upper()}.md\")\n            with open(filename, \"w\", encoding=\"utf-8\") as f:\n                f.write(translation)\n            \n            print(f\"Saved translation to {filename}\")\n\nif __name__ == \"__main__\":\n    # read the text from ../../README.md\n    with open(\"../../README.md\", \"r\") as f:\n        text = f.read()\n    \n    # Default settings\n    shared = {\n        \"text\": text,\n        \"languages\": [\"Chinese\", \"Spanish\", \"Japanese\", \"German\", \"Russian\", \"Portuguese\", \"French\", \"Korean\"],\n        \"output_dir\": \"translations\"\n    }\n\n    # Run the translation flow\n    translate_node = TranslateTextNode(max_retries=3)\n    flow = Flow(start=translate_node)\n    flow.run(shared)\n\n    print(\"\\n=== Translation Complete ===\")\n    print(f\"Translations saved to: {shared['output_dir']}\")\n    print(\"============================\")\n\n--- File: 48 # cookbook/pocketflow-communication/nodes.py ---\n\"\"\"Node implementations for the communication example.\"\"\"\n\nfrom pocketflow import Node\n\nclass EndNode(Node):\n    \"\"\"Node that handles flow termination.\"\"\"\n    pass\n\nclass TextInput(Node):\n    \"\"\"Node that reads text input and initializes the shared store.\"\"\"\n    \n    def prep(self, shared):\n        \"\"\"Get user input and ensure shared store is initialized.\"\"\"\n        return input(\"Enter text (or 'q' to quit): \")\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Store text and initialize/update statistics.\"\"\"\n        if prep_res == 'q':\n            return \"exit\"\n        \n        # Store the text\n        shared[\"text\"] = prep_res\n        \n        # Initialize statistics if they don't exist\n        if \"stats\" not in shared:\n            shared[\"stats\"] = {\n                \"total_texts\": 0,\n                \"total_words\": 0\n            }\n        shared[\"stats\"][\"total_texts\"] += 1\n        \n        return \"count\"\n\nclass WordCounter(Node):\n    \"\"\"Node that counts words in the text.\"\"\"\n    \n    def prep(self, shared):\n        \"\"\"Get text from shared store.\"\"\"\n        return shared[\"text\"]\n    \n    def exec(self, text):\n        \"\"\"Count words in the text.\"\"\"\n        return len(text.split())\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Update word count statistics.\"\"\"\n        shared[\"stats\"][\"total_words\"] += exec_res\n        return \"show\"\n\nclass ShowStats(Node):\n    \"\"\"Node that displays statistics from the shared store.\"\"\"\n    \n    def prep(self, shared):\n        \"\"\"Get statistics from shared store.\"\"\"\n        return shared[\"stats\"]\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Display statistics and continue the flow.\"\"\"\n        stats = prep_res\n        print(f\"\\nStatistics:\")\n        print(f\"- Texts processed: {stats['total_texts']}\")\n        print(f\"- Total words: {stats['total_words']}\")\n        print(f\"- Average words per text: {stats['total_words'] / stats['total_texts']:.1f}\\n\")\n        return \"continue\" \n\n--- File: 89 # cookbook/pocketflow-parallel-batch/main.py ---\nimport asyncio\nimport time\n\nfrom pocketflow import AsyncBatchNode, AsyncParallelBatchNode, AsyncFlow\n\n####################################\n# Dummy async function (1s delay)\n####################################\nasync def dummy_llm_summarize(text):\n    \"\"\"Simulates an async LLM call that takes 1 second.\"\"\"\n    await asyncio.sleep(1)\n    return f\"Summarized({len(text)} chars)\"\n\n###############################################\n# 1) AsyncBatchNode (sequential) version\n###############################################\n\nclass SummariesAsyncNode(AsyncBatchNode):\n    \"\"\"\n    Processes items sequentially in an async manner.\n    The next item won't start until the previous item has finished.\n    \"\"\"\n\n    async def prep_async(self, shared):\n        # Return a list of items to process.\n        # Each item is (filename, content).\n        return list(shared[\"data\"].items())\n\n    async def exec_async(self, item):\n        filename, content = item\n        print(f\"[Sequential] Summarizing {filename}...\")\n        summary = await dummy_llm_summarize(content)\n        return (filename, summary)\n\n    async def post_async(self, shared, prep_res, exec_res_list):\n        # exec_res_list is a list of (filename, summary)\n        shared[\"sequential_summaries\"] = dict(exec_res_list)\n        return \"done_sequential\"\n\n###############################################\n# 2) AsyncParallelBatchNode (concurrent) version\n###############################################\n\nclass SummariesAsyncParallelNode(AsyncParallelBatchNode):\n    \"\"\"\n    Processes items in parallel. Many LLM calls start at once.\n    \"\"\"\n\n    async def prep_async(self, shared):\n        return list(shared[\"data\"].items())\n\n    async def exec_async(self, item):\n        filename, content = item\n        print(f\"[Parallel] Summarizing {filename}...\")\n        summary = await dummy_llm_summarize(content)\n        return (filename, summary)\n\n    async def post_async(self, shared, prep_res, exec_res_list):\n        shared[\"parallel_summaries\"] = dict(exec_res_list)\n        return \"done_parallel\"\n\n###############################################\n# Demo comparing the two approaches\n###############################################\n\nasync def main():\n    # We'll use the same data for both flows\n    shared_data = {\n        \"data\": {\n            \"file1.txt\": \"Hello world 1\",\n            \"file2.txt\": \"Hello world 2\",\n            \"file3.txt\": \"Hello world 3\",\n        }\n    }\n\n    # 1) Run the sequential version\n    seq_node = SummariesAsyncNode()\n    seq_flow = AsyncFlow(start=seq_node)\n\n    print(\"\\n=== Running Sequential (AsyncBatchNode) ===\")\n    t0 = time.time()\n    await seq_flow.run_async(shared_data)\n    t1 = time.time()\n\n    # 2) Run the parallel version\n    par_node = SummariesAsyncParallelNode()\n    par_flow = AsyncFlow(start=par_node)\n\n    print(\"\\n=== Running Parallel (AsyncParallelBatchNode) ===\")\n    t2 = time.time()\n    await par_flow.run_async(shared_data)\n    t3 = time.time()\n\n    # Show times\n    print(\"\\n--- Results ---\")\n    print(f\"Sequential Summaries: {shared_data.get('sequential_summaries')}\")\n    print(f\"Parallel Summaries:   {shared_data.get('parallel_summaries')}\")\n\n    print(f\"Sequential took: {t1 - t0:.2f} seconds\")\n    print(f\"Parallel took:   {t3 - t2:.2f} seconds\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n--- File: 150 # cookbook/pocketflow-web-hitl/nodes.py ---\nfrom pocketflow import Node, AsyncNode\nfrom utils.process_task import process_task\n\nclass ProcessNode(Node):\n    def prep(self, shared):\n        task_input = shared.get(\"task_input\", \"No input\")\n        print(\"ProcessNode Prep\")\n        return task_input\n\n    def exec(self, prep_res):\n        return process_task(prep_res)\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"processed_output\"] = exec_res\n        print(\"ProcessNode Post: Output stored.\")\n        return \"default\" # Go to ReviewNode\n\nclass ReviewNode(AsyncNode):\n    async def prep_async(self, shared):\n        review_event = shared.get(\"review_event\")\n        queue = shared.get(\"sse_queue\") # Expect queue in shared\n        processed_output = shared.get(\"processed_output\", \"N/A\")\n\n        if not review_event or not queue:\n            print(\"ERROR: ReviewNode Prep - Missing review_event or sse_queue in shared store!\")\n            return None # Signal failure\n\n        # Push status update to SSE queue\n        status_update = {\n            \"status\": \"waiting_for_review\",\n            \"output_to_review\": processed_output\n        }\n        await queue.put(status_update)\n        print(\"ReviewNode Prep: Put 'waiting_for_review' on SSE queue.\")\n\n        return review_event # Return event for exec_async\n\n    async def exec_async(self, prep_res):\n        review_event = prep_res\n        if not review_event:\n            print(\"ReviewNode Exec: Skipping wait (no event from prep).\")\n            return\n        print(\"ReviewNode Exec: Waiting on review_event...\")\n        await review_event.wait()\n        print(\"ReviewNode Exec: review_event set.\")\n\n    async def post_async(self, shared, prep_res, exec_res):\n        feedback = shared.get(\"feedback\")\n        print(f\"ReviewNode Post: Processing feedback '{feedback}'\")\n\n        # Clear the event for potential loops\n        review_event = shared.get(\"review_event\")\n        if review_event:\n            review_event.clear()\n        shared[\"feedback\"] = None # Reset feedback\n\n        if feedback == \"approved\":\n            shared[\"final_result\"] = shared.get(\"processed_output\")\n            print(\"ReviewNode Post: Action=approved\")\n            return \"approved\"\n        else:\n            print(\"ReviewNode Post: Action=rejected\")\n            return \"rejected\"\n\nclass ResultNode(Node):\n     def prep(self, shared):\n         print(\"ResultNode Prep\")\n         return shared.get(\"final_result\", \"No final result.\")\n\n     def exec(self, prep_res):\n         print(f\"--- FINAL RESULT ---\")\n         print(prep_res)\n         print(f\"--------------------\")\n         return prep_res\n\n     def post(self, shared, prep_res, exec_res):\n         print(\"ResultNode Post: Flow finished.\")\n         return None # End flow\n\n--- File: 155 # cookbook/pocketflow-workflow/flow.py ---\nfrom pocketflow import Flow\nfrom nodes import GenerateOutline, WriteSimpleContent, ApplyStyle\n\ndef create_article_flow():\n    \"\"\"\n    Create and configure the article writing workflow\n    \"\"\"\n    # Create node instances\n    outline_node = GenerateOutline()\n    write_node = WriteSimpleContent()\n    style_node = ApplyStyle()\n    \n    # Connect nodes in sequence\n    outline_node >> write_node >> style_node\n    \n    # Create flow starting with outline node\n    article_flow = Flow(start=outline_node)\n    \n    return article_flow\n\n--- File: 159 # pocketflow/__init__.py ---\nimport asyncio, warnings, copy, time\n\nclass BaseNode:\n    def __init__(self): self.params,self.successors={},{}\n    def set_params(self,params): self.params=params\n    def add_successor(self,node,action=\"default\"):\n        if action in self.successors: warnings.warn(f\"Overwriting successor for action '{action}'\")\n        self.successors[action]=node;return node\n    def prep(self,shared): pass\n    def exec(self,prep_res): pass\n    def post(self,shared,prep_res,exec_res): pass\n    def _exec(self,prep_res): return self.exec(prep_res)\n    def _run(self,shared): p=self.prep(shared);e=self._exec(p);return self.post(shared,p,e)\n    def run(self,shared): \n        if self.successors: warnings.warn(\"Node won't run successors. Use Flow.\")  \n        return self._run(shared)\n    def __rshift__(self,other): return self.add_successor(other)\n    def __sub__(self,action):\n        if isinstance(action,str): return _ConditionalTransition(self,action)\n        raise TypeError(\"Action must be a string\")\n\nclass _ConditionalTransition:\n    def __init__(self,src,action): self.src,self.action=src,action\n    def __rshift__(self,tgt): return self.src.add_successor(tgt,self.action)\n\nclass Node(BaseNode):\n    def __init__(self,max_retries=1,wait=0): super().__init__();self.max_retries,self.wait=max_retries,wait\n    def exec_fallback(self,prep_res,exc): raise exc\n    def _exec(self,prep_res):\n        for self.cur_retry in range(self.max_retries):\n            try: return self.exec(prep_res)\n            except Exception as e:\n                if self.cur_retry==self.max_retries-1: return self.exec_fallback(prep_res,e)\n                if self.wait>0: time.sleep(self.wait)\n\nclass BatchNode(Node):\n    def _exec(self,items): return [super(BatchNode,self)._exec(i) for i in (items or [])]\n\nclass Flow(BaseNode):\n    def __init__(self,start): super().__init__();self.start=start\n    def get_next_node(self,curr,action):\n        nxt=curr.successors.get(action or \"default\")\n        if not nxt and curr.successors: warnings.warn(f\"Flow ends: '{action}' not found in {list(curr.successors)}\")\n        return nxt\n    def _orch(self,shared,params=None):\n        curr,p=copy.copy(self.start),(params or {**self.params})\n        while curr: curr.set_params(p);c=curr._run(shared);curr=copy.copy(self.get_next_node(curr,c))\n    def _run(self,shared): pr=self.prep(shared);self._orch(shared);return self.post(shared,pr,None)\n    def exec(self,prep_res): raise RuntimeError(\"Flow can't exec.\")\n\nclass BatchFlow(Flow):\n    def _run(self,shared):\n        pr=self.prep(shared) or []\n        for bp in pr: self._orch(shared,{**self.params,**bp})\n        return self.post(shared,pr,None)\n\nclass AsyncNode(Node):\n    def prep(self,shared): raise RuntimeError(\"Use prep_async.\")\n    def exec(self,prep_res): raise RuntimeError(\"Use exec_async.\")\n    def post(self,shared,prep_res,exec_res): raise RuntimeError(\"Use post_async.\")\n    def exec_fallback(self,prep_res,exc): raise RuntimeError(\"Use exec_fallback_async.\")\n    def _run(self,shared): raise RuntimeError(\"Use run_async.\")\n    async def prep_async(self,shared): pass\n    async def exec_async(self,prep_res): pass\n    async def exec_fallback_async(self,prep_res,exc): raise exc\n    async def post_async(self,shared,prep_res,exec_res): pass\n    async def _exec(self,prep_res): \n        for i in range(self.max_retries):\n            try: return await self.exec_async(prep_res)\n            except Exception as e:\n                if i==self.max_retries-1: return await self.exec_fallback_async(prep_res,e)\n                if self.wait>0: await asyncio.sleep(self.wait)\n    async def run_async(self,shared): \n        if self.successors: warnings.warn(\"Node won't run successors. Use AsyncFlow.\")  \n        return await self._run_async(shared)\n    async def _run_async(self,shared): p=await self.prep_async(shared);e=await self._exec(p);return await self.post_async(shared,p,e)\n\nclass AsyncBatchNode(AsyncNode,BatchNode):\n    async def _exec(self,items): return [await super(AsyncBatchNode,self)._exec(i) for i in items]\n\nclass AsyncParallelBatchNode(AsyncNode,BatchNode):\n    async def _exec(self,items): return await asyncio.gather(*(super(AsyncParallelBatchNode,self)._exec(i) for i in items))\n\nclass AsyncFlow(Flow,AsyncNode):\n    async def _orch_async(self,shared,params=None):\n        curr,p=copy.copy(self.start),(params or {**self.params})\n        while curr:curr.set_params(p);c=await curr._run_async(shared) if isinstance(curr,AsyncNode) else curr._run(shared);curr=copy.copy(self.get_next_node(curr,c))\n    async def _run_async(self,shared): p=await self.prep_async(shared);await self._orch_async(shared);return await self.post_async(shared,p,None)\n\nclass AsyncBatchFlow(AsyncFlow,BatchFlow):\n    async def _run_async(self,shared):\n        pr=await self.prep_async(shared) or []\n        for bp in pr: await self._orch_async(shared,{**self.params,**bp})\n        return await self.post_async(shared,pr,None)\n\nclass AsyncParallelBatchFlow(AsyncFlow,BatchFlow):\n    async def _run_async(self,shared):\n        pr=await self.prep_async(shared) or []\n        await asyncio.gather(*(self._orch_async(shared,{**self.params,**bp}) for bp in pr))\n        return await self.post_async(shared,pr,None)\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n": "```yaml\nsummary: |\n  **PocketFlow** helps you build complex workflows by connecting reusable building blocks called *Nodes*.  These workflows, called *Flows*, allow you to automate tasks, process data in batches, and even interact with external services asynchronously. Data is shared between Nodes through a *Shared Store*, and each Node follows a standard lifecycle of *Prep, Exec, and Post* steps.\n\nrelationships:\n  - from_abstraction: 1 # Flow\n    to_abstraction: 0 # Node\n    label: \"Orchestrates execution\"\n  - from_abstraction: 0 # Node\n    to_abstraction: 2 # Shared Store\n    label: \"Accesses and Updates\"\n  - from_abstraction: 0 # Node\n    to_abstraction: 3 # Prep, Exec, Post (Node Lifecycle)\n    label: \"Implements stages\"\n  - from_abstraction: 4 # BatchNode\n    to_abstraction: 0 # Node\n    label: \"Inherits from\"\n  - from_abstraction: 5 # AsyncNode\n    to_abstraction: 0 # Node\n    label: \"Inherits from\"\n  - from_abstraction: 6 # AsyncFlow\n    to_abstraction: 1 # Flow\n    label: \"Inherits from\"\n  - from_abstraction: 7 # AsyncParallelBatchNode\n    to_abstraction: 5 # AsyncNode\n    label: \"Combines with\"\n  - from_abstraction: 7 # AsyncParallelBatchNode\n    to_abstraction: 4 # BatchNode\n    label: \"Combines with\"\n```", "\nGiven the following project abstractions and their relationships for the project ```` PocketFlow ````:\n\nAbstractions (Index # Name):\n- 0 # Node\n- 1 # Flow\n- 2 # Shared Store\n- 3 # Prep, Exec, Post (Node Lifecycle)\n- 4 # BatchNode\n- 5 # AsyncNode\n- 6 # AsyncFlow\n- 7 # AsyncParallelBatchNode\n\nContext about relationships and project summary:\nProject Summary:\n**PocketFlow** helps you build complex workflows by connecting reusable building blocks called *Nodes*.  These workflows, called *Flows*, allow you to automate tasks, process data in batches, and even interact with external services asynchronously. Data is shared between Nodes through a *Shared Store*, and each Node follows a standard lifecycle of *Prep, Exec, and Post* steps.\n\n\nRelationships (Indices refer to abstractions above):\n- From 1 (Flow) to 0 (Node): Orchestrates execution\n- From 0 (Node) to 2 (Shared Store): Accesses and Updates\n- From 0 (Node) to 3 (Prep, Exec, Post (Node Lifecycle)): Implements stages\n- From 4 (BatchNode) to 0 (Node): Inherits from\n- From 5 (AsyncNode) to 0 (Node): Inherits from\n- From 6 (AsyncFlow) to 1 (Flow): Inherits from\n- From 7 (AsyncParallelBatchNode) to 5 (AsyncNode): Combines with\n- From 7 (AsyncParallelBatchNode) to 4 (BatchNode): Combines with\n\n\nIf you are going to make a tutorial for ```` PocketFlow ````, what is the best order to explain these abstractions, from first to last?\nIdeally, first explain those that are the most important or foundational, perhaps user-facing concepts or entry points. Then move to more detailed, lower-level implementation details or supporting concepts.\n\nOutput the ordered list of abstraction indices, including the name in a comment for clarity. Use the format `idx # AbstractionName`.\n\n```yaml\n- 2 # FoundationalConcept\n- 0 # CoreClassA\n- 1 # CoreClassB (uses CoreClassA)\n- ...\n```\n\nNow, provide the YAML output:\n": "```yaml\n- 1 # Flow (Orchestrates everything)\n- 0 # Node (Building block of Flows)\n- 2 # Shared Store (Data sharing between Nodes)\n- 3 # Prep, Exec, Post (Node Lifecycle) (Details of Node execution)\n- 4 # BatchNode (Specialized Node for batch processing)\n- 5 # AsyncNode (Specialized Node for asynchronous operations)\n- 6 # AsyncFlow (Specialized Flow for asynchronous operations)\n- 7 # AsyncParallelBatchNode (Advanced Node combining async and batch)\n```", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `PocketFlow` about the concept: \"Flow\". This is Chapter 1.\n\nConcept Details:\n- Description:\nThe orchestration layer that connects individual Nodes into a functioning workflow, like a conductor leading an orchestra. A flow defines the sequence in which nodes are executed and how data is passed between them. Flows are defined by a starting node and transitions are specified using rshift `>>` or subtraction `-` operators to create a directed graph-like structure for task execution.\n\n\nComplete Tutorial Structure:\n1. [Flow](01_flow.md)\n2. [Node](02_node.md)\n3. [Shared Store](03_shared_store.md)\n4. [Prep, Exec, Post (Node Lifecycle)](04_prep__exec__post__node_lifecycle_.md)\n5. [BatchNode](05_batchnode.md)\n6. [AsyncNode](06_asyncnode.md)\n7. [AsyncFlow](07_asyncflow.md)\n8. [AsyncParallelBatchNode](08_asyncparallelbatchnode.md)\n\nContext from previous chapters (summary):\nThis is the first chapter.\n\nRelevant Code Snippets:\n--- File: cookbook/pocketflow-agent/flow.py ---\nfrom pocketflow import Flow\nfrom nodes import DecideAction, SearchWeb, AnswerQuestion\n\ndef create_agent_flow():\n    \"\"\"\n    Create and connect the nodes to form a complete agent flow.\n    \n    The flow works like this:\n    1. DecideAction node decides whether to search or answer\n    2. If search, go to SearchWeb node\n    3. If answer, go to AnswerQuestion node\n    4. After SearchWeb completes, go back to DecideAction\n    \n    Returns:\n        Flow: A complete research agent flow\n    \"\"\"\n    # Create instances of each node\n    decide = DecideAction()\n    search = SearchWeb()\n    answer = AnswerQuestion()\n    \n    # Connect the nodes\n    # If DecideAction returns \"search\", go to SearchWeb\n    decide - \"search\" >> search\n    \n    # If DecideAction returns \"answer\", go to AnswerQuestion\n    decide - \"answer\" >> answer\n    \n    # After SearchWeb completes and returns \"decide\", go back to DecideAction\n    search - \"decide\" >> decide\n    \n    # Create and return the flow, starting with the DecideAction node\n    return Flow(start=decide) \n\n--- File: cookbook/pocketflow-workflow/flow.py ---\nfrom pocketflow import Flow\nfrom nodes import GenerateOutline, WriteSimpleContent, ApplyStyle\n\ndef create_article_flow():\n    \"\"\"\n    Create and configure the article writing workflow\n    \"\"\"\n    # Create node instances\n    outline_node = GenerateOutline()\n    write_node = WriteSimpleContent()\n    style_node = ApplyStyle()\n    \n    # Connect nodes in sequence\n    outline_node >> write_node >> style_node\n    \n    # Create flow starting with outline node\n    article_flow = Flow(start=outline_node)\n    \n    return article_flow\n\n--- File: pocketflow/__init__.py ---\nimport asyncio, warnings, copy, time\n\nclass BaseNode:\n    def __init__(self): self.params,self.successors={},{}\n    def set_params(self,params): self.params=params\n    def add_successor(self,node,action=\"default\"):\n        if action in self.successors: warnings.warn(f\"Overwriting successor for action '{action}'\")\n        self.successors[action]=node;return node\n    def prep(self,shared): pass\n    def exec(self,prep_res): pass\n    def post(self,shared,prep_res,exec_res): pass\n    def _exec(self,prep_res): return self.exec(prep_res)\n    def _run(self,shared): p=self.prep(shared);e=self._exec(p);return self.post(shared,p,e)\n    def run(self,shared): \n        if self.successors: warnings.warn(\"Node won't run successors. Use Flow.\")  \n        return self._run(shared)\n    def __rshift__(self,other): return self.add_successor(other)\n    def __sub__(self,action):\n        if isinstance(action,str): return _ConditionalTransition(self,action)\n        raise TypeError(\"Action must be a string\")\n\nclass _ConditionalTransition:\n    def __init__(self,src,action): self.src,self.action=src,action\n    def __rshift__(self,tgt): return self.src.add_successor(tgt,self.action)\n\nclass Node(BaseNode):\n    def __init__(self,max_retries=1,wait=0): super().__init__();self.max_retries,self.wait=max_retries,wait\n    def exec_fallback(self,prep_res,exc): raise exc\n    def _exec(self,prep_res):\n        for self.cur_retry in range(self.max_retries):\n            try: return self.exec(prep_res)\n            except Exception as e:\n                if self.cur_retry==self.max_retries-1: return self.exec_fallback(prep_res,e)\n                if self.wait>0: time.sleep(self.wait)\n\nclass BatchNode(Node):\n    def _exec(self,items): return [super(BatchNode,self)._exec(i) for i in (items or [])]\n\nclass Flow(BaseNode):\n    def __init__(self,start): super().__init__();self.start=start\n    def get_next_node(self,curr,action):\n        nxt=curr.successors.get(action or \"default\")\n        if not nxt and curr.successors: warnings.warn(f\"Flow ends: '{action}' not found in {list(curr.successors)}\")\n        return nxt\n    def _orch(self,shared,params=None):\n        curr,p=copy.copy(self.start),(params or {**self.params})\n        while curr: curr.set_params(p);c=curr._run(shared);curr=copy.copy(self.get_next_node(curr,c))\n    def _run(self,shared): pr=self.prep(shared);self._orch(shared);return self.post(shared,pr,None)\n    def exec(self,prep_res): raise RuntimeError(\"Flow can't exec.\")\n\nclass BatchFlow(Flow):\n    def _run(self,shared):\n        pr=self.prep(shared) or []\n        for bp in pr: self._orch(shared,{**self.params,**bp})\n        return self.post(shared,pr,None)\n\nclass AsyncNode(Node):\n    def prep(self,shared): raise RuntimeError(\"Use prep_async.\")\n    def exec(self,prep_res): raise RuntimeError(\"Use exec_async.\")\n    def post(self,shared,prep_res,exec_res): raise RuntimeError(\"Use post_async.\")\n    def exec_fallback(self,prep_res,exc): raise RuntimeError(\"Use exec_fallback_async.\")\n    def _run(self,shared): raise RuntimeError(\"Use run_async.\")\n    async def prep_async(self,shared): pass\n    async def exec_async(self,prep_res): pass\n    async def exec_fallback_async(self,prep_res,exc): raise exc\n    async def post_async(self,shared,prep_res,exec_res): pass\n    async def _exec(self,prep_res): \n        for i in range(self.max_retries):\n            try: return await self.exec_async(prep_res)\n            except Exception as e:\n                if i==self.max_retries-1: return await self.exec_fallback_async(prep_res,e)\n                if self.wait>0: await asyncio.sleep(self.wait)\n    async def run_async(self,shared): \n        if self.successors: warnings.warn(\"Node won't run successors. Use AsyncFlow.\")  \n        return await self._run_async(shared)\n    async def _run_async(self,shared): p=await self.prep_async(shared);e=await self._exec(p);return await self.post_async(shared,p,e)\n\nclass AsyncBatchNode(AsyncNode,BatchNode):\n    async def _exec(self,items): return [await super(AsyncBatchNode,self)._exec(i) for i in items]\n\nclass AsyncParallelBatchNode(AsyncNode,BatchNode):\n    async def _exec(self,items): return await asyncio.gather(*(super(AsyncParallelBatchNode,self)._exec(i) for i in items))\n\nclass AsyncFlow(Flow,AsyncNode):\n    async def _orch_async(self,shared,params=None):\n        curr,p=copy.copy(self.start),(params or {**self.params})\n        while curr:curr.set_params(p);c=await curr._run_async(shared) if isinstance(curr,AsyncNode) else curr._run(shared);curr=copy.copy(self.get_next_node(curr,c))\n    async def _run_async(self,shared): p=await self.prep_async(shared);await self._orch_async(shared);return await self.post_async(shared,p,None)\n\nclass AsyncBatchFlow(AsyncFlow,BatchFlow):\n    async def _run_async(self,shared):\n        pr=await self.prep_async(shared) or []\n        for bp in pr: await self._orch_async(shared,{**self.params,**bp})\n        return await self.post_async(shared,pr,None)\n\nclass AsyncParallelBatchFlow(AsyncFlow,BatchFlow):\n    async def _run_async(self,shared):\n        pr=await self.prep_async(shared) or []\n        await asyncio.gather(*(self._orch_async(shared,{**self.params,**bp}) for bp in pr))\n        return await self.post_async(shared,pr,None)\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 1: Flow`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "# Chapter 1: Flow\n\nImagine you're directing a play. You have actors, each with specific roles (like answering a question or searching the web). But how do you tell them *when* to act, and *what* to do next?  That's where a **Flow** comes in!\n\nIn PocketFlow, a Flow is like the conductor of an orchestra. It's the orchestration layer that connects individual actions (we call them **Nodes**) into a functioning workflow. It defines the sequence in which these actions are executed and how data is passed between them.\n\n**Our Use Case: A Simple Research Agent**\n\nLet's say we're building a simple research agent that can either search the web or answer a question, depending on what's needed.  The agent will first *decide* what to do. If it needs to search, it will *search the web*. If it needs to answer, it will *answer the question*. After searching, it goes back to the deciding step.\n\nWithout a Flow, we'd have to manually call each action in the right order, which can get messy quickly. A Flow lets us define this process in a clear and organized way.\n\n**Key Concepts of a Flow**\n\n1.  **Nodes:** These are the individual steps or actions in our workflow. Think of them as the actors in our play. We'll dive deep into Nodes in [Chapter 2: Node](02_node.md). For now, just think of them as individual units of work.\n\n2.  **Sequence:** The Flow defines the order in which Nodes are executed.  It's the script of our play.\n\n3.  **Data Passing:** The Flow manages how data is passed between Nodes. This is how the actors communicate and share information.\n\n4.  **Starting Node:** Every flow must have a start node to kick off the process.\n\n5.  **Transitions:**  These define how the Flow moves from one Node to the next. We'll use special operators like `>>` (rshift) and `-` (subtraction) to define these transitions.\n\n**Creating Our Research Agent Flow**\n\nLet's look at how we can create a Flow for our research agent. The code might look like this (from `cookbook/pocketflow-agent/flow.py`):\n\n```python\nfrom pocketflow import Flow\nfrom nodes import DecideAction, SearchWeb, AnswerQuestion\n\ndef create_agent_flow():\n    # Create instances of each node\n    decide = DecideAction()\n    search = SearchWeb()\n    answer = AnswerQuestion()\n\n    # Connect the nodes\n    # If DecideAction returns \"search\", go to SearchWeb\n    decide - \"search\" >> search\n\n    # If DecideAction returns \"answer\", go to AnswerQuestion\n    decide - \"answer\" >> answer\n\n    # After SearchWeb completes and returns \"decide\", go back to DecideAction\n    search - \"decide\" >> decide\n\n    # Create and return the flow, starting with the DecideAction node\n    return Flow(start=decide)\n```\n\nHere's a breakdown:\n\n*   We import `Flow` from the `pocketflow` library.\n*   We import the `Node` types from our nodes file (`nodes.py`). Don't worry about the implementations of `DecideAction`, `SearchWeb`, and `AnswerQuestion` yet.  Just think of them as steps in our process.\n*   We create instances of each Node.\n*   We use the `>>` and `-` operators to define the transitions between Nodes. For example, `decide - \"search\" >> search` means \"If the `decide` Node returns 'search', then go to the `search` Node.\"\n*   We create a `Flow` object, specifying the `decide` Node as the starting point.\n\n**How it Works: Example**\n\nImagine the agent receives the question: \"What is the capital of France?\".\n\n1.  The `Flow` starts at the `DecideAction` Node.\n2.  The `DecideAction` Node determines that we need to search for the answer. It returns the string \"search\".\n3.  The `Flow` sees the transition `decide - \"search\" >> search` and moves to the `SearchWeb` Node.\n4.  The `SearchWeb` Node searches the web and finds the answer: \"Paris\". It stores the answer in memory and returns the string \"decide\".\n5.  The `Flow` sees the transition `search - \"decide\" >> decide` and moves back to the `DecideAction` Node.\n6.  Now `DecideAction` could determine we need to answer the original question. It returns the string \"answer\".\n7.  The `Flow` sees the transition `decide - \"answer\" >> answer` and moves to the `AnswerQuestion` Node.\n8.  The `AnswerQuestion` node crafts the answer using the information in memory and then returns its final response.\n\n**Under the Hood: A Peek Inside**\n\nLet's see what happens inside the `Flow` when we run it.  Here's a simplified sequence diagram:\n\n```mermaid\nsequenceDiagram\n    participant Flow\n    participant DecideAction\n    participant SearchWeb\n    participant AnswerQuestion\n    participant SharedStore\n\n    Flow->>DecideAction: run(shared_store)\n    DecideAction->>SharedStore: access/modify data\n    DecideAction-->>Flow: \"search\"\n    Flow->>SearchWeb: run(shared_store)\n    SearchWeb->>SharedStore: access/modify data\n    SearchWeb-->>Flow: \"decide\"\n    Flow->>DecideAction: run(shared_store)\n    DecideAction->>SharedStore: access/modify data\n    DecideAction-->>Flow: \"answer\"\n    Flow->>AnswerQuestion: run(shared_store)\n    AnswerQuestion->>SharedStore: access/modify data\n    AnswerQuestion-->>Flow: (End)\n```\n\n*   The `Flow` starts by calling the `run` method of the starting Node (`DecideAction` in our example).\n*   Each Node executes its logic and potentially modifies a `SharedStore` ([Chapter 3: Shared Store](03_shared_store.md)) which is used for passing data between the nodes.\n*   The Node returns a value (like \"search\" or \"answer\") that determines the next Node to execute, based on the transitions defined in the Flow.\n*   This process continues until there are no more transitions, or we hit a dead end (no successor node defined for a particular return value).\n\n**Code Dive: `Flow` Class**\n\nLet's look at some of the key parts of the `Flow` class (from `pocketflow/__init__.py`):\n\n```python\nclass Flow(BaseNode):\n    def __init__(self,start): super().__init__();self.start=start\n    def get_next_node(self,curr,action):\n        nxt=curr.successors.get(action or \"default\")\n        if not nxt and curr.successors: warnings.warn(f\"Flow ends: '{action}' not found in {list(curr.successors)}\")\n        return nxt\n    def _orch(self,shared,params=None):\n        curr,p=copy.copy(self.start),(params or {**self.params})\n        while curr: curr.set_params(p);c=curr._run(shared);curr=copy.copy(self.get_next_node(curr,c))\n    def _run(self,shared): pr=self.prep(shared);self._orch(shared);return self.post(shared,pr,None)\n    def exec(self,prep_res): raise RuntimeError(\"Flow can't exec.\")\n```\n\n*   `__init__(self, start)`: The constructor takes the starting Node as an argument.\n*   `get_next_node(self, curr, action)`:  This method determines the next Node to execute based on the return value (`action`) of the current Node (`curr`). It looks up the successor node in the current node's `successors` dictionary.\n*   `_orch(self, shared, params=None)`:  This is the \"orchestration\" method.  It iterates through the Nodes, executing each one in sequence based on the transitions.\n*   `_run(self, shared)`: This method kicks off the orchestration by calling `_orch(self, shared)`.\n\n**Conclusion**\n\nIn this chapter, we learned about the concept of a Flow in PocketFlow. A Flow is an orchestration layer that connects individual Nodes into a functioning workflow. It defines the sequence in which Nodes are executed and how data is passed between them. We saw how to create a Flow for a simple research agent and how it works under the hood.\n\nNow that we understand Flows, let's dive deeper into the building blocks of Flows: **Nodes**. In [Chapter 2: Node](02_node.md), we'll explore what Nodes are, how to create them, and how they interact with Flows.\n", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `PocketFlow` about the concept: \"Node\". This is Chapter 2.\n\nConcept Details:\n- Description:\nA fundamental building block in PocketFlow, like a single function in a program. Each node performs a specific task, such as calling an LLM or processing data. It takes inputs, does something with them, and produces outputs. Nodes are connected together to create complex workflows, similar to how functions are chained together to build a larger application.  Nodes implement `prep`, `exec`, and `post` methods to define their behavior.\n\n\nComplete Tutorial Structure:\n1. [Flow](01_flow.md)\n2. [Node](02_node.md)\n3. [Shared Store](03_shared_store.md)\n4. [Prep, Exec, Post (Node Lifecycle)](04_prep__exec__post__node_lifecycle_.md)\n5. [BatchNode](05_batchnode.md)\n6. [AsyncNode](06_asyncnode.md)\n7. [AsyncFlow](07_asyncflow.md)\n8. [AsyncParallelBatchNode](08_asyncparallelbatchnode.md)\n\nContext from previous chapters (summary):\n# Chapter 1: Flow\n\nImagine you're directing a play. You have actors, each with specific roles (like answering a question or searching the web). But how do you tell them *when* to act, and *what* to do next?  That's where a **Flow** comes in!\n\nIn PocketFlow, a Flow is like the conductor of an orchestra. It's the orchestration layer that connects individual actions (we call them **Nodes**) into a functioning workflow. It defines the sequence in which these actions are executed and how data is passed between them.\n\n**Our Use Case: A Simple Research Agent**\n\nLet's say we're building a simple research agent that can either search the web or answer a question, depending on what's needed.  The agent will first *decide* what to do. If it needs to search, it will *search the web*. If it needs to answer, it will *answer the question*. After searching, it goes back to the deciding step.\n\nWithout a Flow, we'd have to manually call each action in the right order, which can get messy quickly. A Flow lets us define this process in a clear and organized way.\n\n**Key Concepts of a Flow**\n\n1.  **Nodes:** These are the individual steps or actions in our workflow. Think of them as the actors in our play. We'll dive deep into Nodes in [Chapter 2: Node](02_node.md). For now, just think of them as individual units of work.\n\n2.  **Sequence:** The Flow defines the order in which Nodes are executed.  It's the script of our play.\n\n3.  **Data Passing:** The Flow manages how data is passed between Nodes. This is how the actors communicate and share information.\n\n4.  **Starting Node:** Every flow must have a start node to kick off the process.\n\n5.  **Transitions:**  These define how the Flow moves from one Node to the next. We'll use special operators like `>>` (rshift) and `-` (subtraction) to define these transitions.\n\n**Creating Our Research Agent Flow**\n\nLet's look at how we can create a Flow for our research agent. The code might look like this (from `cookbook/pocketflow-agent/flow.py`):\n\n```python\nfrom pocketflow import Flow\nfrom nodes import DecideAction, SearchWeb, AnswerQuestion\n\ndef create_agent_flow():\n    # Create instances of each node\n    decide = DecideAction()\n    search = SearchWeb()\n    answer = AnswerQuestion()\n\n    # Connect the nodes\n    # If DecideAction returns \"search\", go to SearchWeb\n    decide - \"search\" >> search\n\n    # If DecideAction returns \"answer\", go to AnswerQuestion\n    decide - \"answer\" >> answer\n\n    # After SearchWeb completes and returns \"decide\", go back to DecideAction\n    search - \"decide\" >> decide\n\n    # Create and return the flow, starting with the DecideAction node\n    return Flow(start=decide)\n```\n\nHere's a breakdown:\n\n*   We import `Flow` from the `pocketflow` library.\n*   We import the `Node` types from our nodes file (`nodes.py`). Don't worry about the implementations of `DecideAction`, `SearchWeb`, and `AnswerQuestion` yet.  Just think of them as steps in our process.\n*   We create instances of each Node.\n*   We use the `>>` and `-` operators to define the transitions between Nodes. For example, `decide - \"search\" >> search` means \"If the `decide` Node returns 'search', then go to the `search` Node.\"\n*   We create a `Flow` object, specifying the `decide` Node as the starting point.\n\n**How it Works: Example**\n\nImagine the agent receives the question: \"What is the capital of France?\".\n\n1.  The `Flow` starts at the `DecideAction` Node.\n2.  The `DecideAction` Node determines that we need to search for the answer. It returns the string \"search\".\n3.  The `Flow` sees the transition `decide - \"search\" >> search` and moves to the `SearchWeb` Node.\n4.  The `SearchWeb` Node searches the web and finds the answer: \"Paris\". It stores the answer in memory and returns the string \"decide\".\n5.  The `Flow` sees the transition `search - \"decide\" >> decide` and moves back to the `DecideAction` Node.\n6.  Now `DecideAction` could determine we need to answer the original question. It returns the string \"answer\".\n7.  The `Flow` sees the transition `decide - \"answer\" >> answer` and moves to the `AnswerQuestion` Node.\n8.  The `AnswerQuestion` node crafts the answer using the information in memory and then returns its final response.\n\n**Under the Hood: A Peek Inside**\n\nLet's see what happens inside the `Flow` when we run it.  Here's a simplified sequence diagram:\n\n```mermaid\nsequenceDiagram\n    participant Flow\n    participant DecideAction\n    participant SearchWeb\n    participant AnswerQuestion\n    participant SharedStore\n\n    Flow->>DecideAction: run(shared_store)\n    DecideAction->>SharedStore: access/modify data\n    DecideAction-->>Flow: \"search\"\n    Flow->>SearchWeb: run(shared_store)\n    SearchWeb->>SharedStore: access/modify data\n    SearchWeb-->>Flow: \"decide\"\n    Flow->>DecideAction: run(shared_store)\n    DecideAction->>SharedStore: access/modify data\n    DecideAction-->>Flow: \"answer\"\n    Flow->>AnswerQuestion: run(shared_store)\n    AnswerQuestion->>SharedStore: access/modify data\n    AnswerQuestion-->>Flow: (End)\n```\n\n*   The `Flow` starts by calling the `run` method of the starting Node (`DecideAction` in our example).\n*   Each Node executes its logic and potentially modifies a `SharedStore` ([Chapter 3: Shared Store](03_shared_store.md)) which is used for passing data between the nodes.\n*   The Node returns a value (like \"search\" or \"answer\") that determines the next Node to execute, based on the transitions defined in the Flow.\n*   This process continues until there are no more transitions, or we hit a dead end (no successor node defined for a particular return value).\n\n**Code Dive: `Flow` Class**\n\nLet's look at some of the key parts of the `Flow` class (from `pocketflow/__init__.py`):\n\n```python\nclass Flow(BaseNode):\n    def __init__(self,start): super().__init__();self.start=start\n    def get_next_node(self,curr,action):\n        nxt=curr.successors.get(action or \"default\")\n        if not nxt and curr.successors: warnings.warn(f\"Flow ends: '{action}' not found in {list(curr.successors)}\")\n        return nxt\n    def _orch(self,shared,params=None):\n        curr,p=copy.copy(self.start),(params or {**self.params})\n        while curr: curr.set_params(p);c=curr._run(shared);curr=copy.copy(self.get_next_node(curr,c))\n    def _run(self,shared): pr=self.prep(shared);self._orch(shared);return self.post(shared,pr,None)\n    def exec(self,prep_res): raise RuntimeError(\"Flow can't exec.\")\n```\n\n*   `__init__(self, start)`: The constructor takes the starting Node as an argument.\n*   `get_next_node(self, curr, action)`:  This method determines the next Node to execute based on the return value (`action`) of the current Node (`curr`). It looks up the successor node in the current node's `successors` dictionary.\n*   `_orch(self, shared, params=None)`:  This is the \"orchestration\" method.  It iterates through the Nodes, executing each one in sequence based on the transitions.\n*   `_run(self, shared)`: This method kicks off the orchestration by calling `_orch(self, shared)`.\n\n**Conclusion**\n\nIn this chapter, we learned about the concept of a Flow in PocketFlow. A Flow is an orchestration layer that connects individual Nodes into a functioning workflow. It defines the sequence in which Nodes are executed and how data is passed between them. We saw how to create a Flow for a simple research agent and how it works under the hood.\n\nNow that we understand Flows, let's dive deeper into the building blocks of Flows: **Nodes**. In [Chapter 2: Node](02_node.md), we'll explore what Nodes are, how to create them, and how they interact with Flows.\n\n\nRelevant Code Snippets:\n--- File: cookbook/pocketflow-agent/nodes.py ---\nfrom pocketflow import Node\nfrom utils import call_llm, search_web\nimport yaml\n\nclass DecideAction(Node):\n    def prep(self, shared):\n        \"\"\"Prepare the context and question for the decision-making process.\"\"\"\n        # Get the current context (default to \"No previous search\" if none exists)\n        context = shared.get(\"context\", \"No previous search\")\n        # Get the question from the shared store\n        question = shared[\"question\"]\n        # Return both for the exec step\n        return question, context\n        \n    def exec(self, inputs):\n        \"\"\"Call the LLM to decide whether to search or answer.\"\"\"\n        question, context = inputs\n        \n        print(f\"\ud83e\udd14 Agent deciding what to do next...\")\n        \n        # Create a prompt to help the LLM decide what to do next with proper yaml formatting\n        prompt = f\"\"\"\n### CONTEXT\nYou are a research assistant that can search the web.\nQuestion: {question}\nPrevious Research: {context}\n\n### ACTION SPACE\n[1] search\n  Description: Look up more information on the web\n  Parameters:\n    - query (str): What to search for\n\n[2] answer\n  Description: Answer the question with current knowledge\n  Parameters:\n    - answer (str): Final answer to the question\n\n## NEXT ACTION\nDecide the next action based on the context and available actions.\nReturn your response in this format:\n\n```yaml\nthinking: |\n    <your step-by-step reasoning process>\naction: search OR answer\nreason: <why you chose this action>\nanswer: <if action is answer>\nsearch_query: <specific search query if action is search>\n```\nIMPORTANT: Make sure to:\n1. Use proper indentation (4 spaces) for all multi-line fields\n2. Use the | character for multi-line text fields\n3. Keep single-line fields without the | character\n\"\"\"\n        \n        # Call the LLM to make a decision\n        response = call_llm(prompt)\n        \n        # Parse the response to get the decision\n        yaml_str = response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n        decision = yaml.safe_load(yaml_str)\n        \n        return decision\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Save the decision and determine the next step in the flow.\"\"\"\n        # If LLM decided to search, save the search query\n        if exec_res[\"action\"] == \"search\":\n            shared[\"search_query\"] = exec_res[\"search_query\"]\n            print(f\"\ud83d\udd0d Agent decided to search for: {exec_res['search_query']}\")\n        else:\n            shared[\"context\"] = exec_res[\"answer\"] #save the context if LLM gives the answer without searching.\n            print(f\"\ud83d\udca1 Agent decided to answer the question\")\n        \n        # Return the action to determine the next node in the flow\n        return exec_res[\"action\"]\n\nclass SearchWeb(Node):\n    def prep(self, shared):\n        \"\"\"Get the search query from the shared store.\"\"\"\n        return shared[\"search_query\"]\n        \n    def exec(self, search_query):\n        \"\"\"Search the web for the given query.\"\"\"\n        # Call the search utility function\n        print(f\"\ud83c\udf10 Searching the web for: {search_query}\")\n        results = search_web(search_query)\n        return results\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Save the search results and go back to the decision node.\"\"\"\n        # Add the search results to the context in the shared store\n        previous = shared.get(\"context\", \"\")\n        shared[\"context\"] = previous + \"\\n\\nSEARCH: \" + shared[\"search_query\"] + \"\\nRESULTS: \" + exec_res\n        \n        print(f\"\ud83d\udcda Found information, analyzing results...\")\n        \n        # Always go back to the decision node after searching\n        return \"decide\"\n\nclass AnswerQuestion(Node):\n    def prep(self, shared):\n        \"\"\"Get the question and context for answering.\"\"\"\n        return shared[\"question\"], shared.get(\"context\", \"\")\n        \n    def exec(self, inputs):\n        \"\"\"Call the LLM to generate a final answer.\"\"\"\n        question, context = inputs\n        \n        print(f\"\u270d\ufe0f Crafting final answer...\")\n        \n        # Create a prompt for the LLM to answer the question\n        prompt = f\"\"\"\n### CONTEXT\nBased on the following information, answer the question.\nQuestion: {question}\nResearch: {context}\n\n## YOUR ANSWER:\nProvide a comprehensive answer using the research results.\n\"\"\"\n        # Call the LLM to generate an answer\n        answer = call_llm(prompt)\n        return answer\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Save the final answer and complete the flow.\"\"\"\n        # Save the answer in the shared store\n        shared[\"answer\"] = exec_res\n        \n        print(f\"\u2705 Answer generated successfully\")\n        \n        # We're done - no need to continue the flow\n        return \"done\" \n\n\n--- File: cookbook/pocketflow-web-hitl/nodes.py ---\nfrom pocketflow import Node, AsyncNode\nfrom utils.process_task import process_task\n\nclass ProcessNode(Node):\n    def prep(self, shared):\n        task_input = shared.get(\"task_input\", \"No input\")\n        print(\"ProcessNode Prep\")\n        return task_input\n\n    def exec(self, prep_res):\n        return process_task(prep_res)\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"processed_output\"] = exec_res\n        print(\"ProcessNode Post: Output stored.\")\n        return \"default\" # Go to ReviewNode\n\nclass ReviewNode(AsyncNode):\n    async def prep_async(self, shared):\n        review_event = shared.get(\"review_event\")\n        queue = shared.get(\"sse_queue\") # Expect queue in shared\n        processed_output = shared.get(\"processed_output\", \"N/A\")\n\n        if not review_event or not queue:\n            print(\"ERROR: ReviewNode Prep - Missing review_event or sse_queue in shared store!\")\n            return None # Signal failure\n\n        # Push status update to SSE queue\n        status_update = {\n            \"status\": \"waiting_for_review\",\n            \"output_to_review\": processed_output\n        }\n        await queue.put(status_update)\n        print(\"ReviewNode Prep: Put 'waiting_for_review' on SSE queue.\")\n\n        return review_event # Return event for exec_async\n\n    async def exec_async(self, prep_res):\n        review_event = prep_res\n        if not review_event:\n            print(\"ReviewNode Exec: Skipping wait (no event from prep).\")\n            return\n        print(\"ReviewNode Exec: Waiting on review_event...\")\n        await review_event.wait()\n        print(\"ReviewNode Exec: review_event set.\")\n\n    async def post_async(self, shared, prep_res, exec_res):\n        feedback = shared.get(\"feedback\")\n        print(f\"ReviewNode Post: Processing feedback '{feedback}'\")\n\n        # Clear the event for potential loops\n        review_event = shared.get(\"review_event\")\n        if review_event:\n            review_event.clear()\n        shared[\"feedback\"] = None # Reset feedback\n\n        if feedback == \"approved\":\n            shared[\"final_result\"] = shared.get(\"processed_output\")\n            print(\"ReviewNode Post: Action=approved\")\n            return \"approved\"\n        else:\n            print(\"ReviewNode Post: Action=rejected\")\n            return \"rejected\"\n\nclass ResultNode(Node):\n     def prep(self, shared):\n         print(\"ResultNode Prep\")\n         return shared.get(\"final_result\", \"No final result.\")\n\n     def exec(self, prep_res):\n         print(f\"--- FINAL RESULT ---\")\n         print(prep_res)\n         print(f\"--------------------\")\n         return prep_res\n\n     def post(self, shared, prep_res, exec_res):\n         print(\"ResultNode Post: Flow finished.\")\n         return None # End flow\n\n--- File: pocketflow/__init__.py ---\nimport asyncio, warnings, copy, time\n\nclass BaseNode:\n    def __init__(self): self.params,self.successors={},{}\n    def set_params(self,params): self.params=params\n    def add_successor(self,node,action=\"default\"):\n        if action in self.successors: warnings.warn(f\"Overwriting successor for action '{action}'\")\n        self.successors[action]=node;return node\n    def prep(self,shared): pass\n    def exec(self,prep_res): pass\n    def post(self,shared,prep_res,exec_res): pass\n    def _exec(self,prep_res): return self.exec(prep_res)\n    def _run(self,shared): p=self.prep(shared);e=self._exec(p);return self.post(shared,p,e)\n    def run(self,shared): \n        if self.successors: warnings.warn(\"Node won't run successors. Use Flow.\")  \n        return self._run(shared)\n    def __rshift__(self,other): return self.add_successor(other)\n    def __sub__(self,action):\n        if isinstance(action,str): return _ConditionalTransition(self,action)\n        raise TypeError(\"Action must be a string\")\n\nclass _ConditionalTransition:\n    def __init__(self,src,action): self.src,self.action=src,action\n    def __rshift__(self,tgt): return self.src.add_successor(tgt,self.action)\n\nclass Node(BaseNode):\n    def __init__(self,max_retries=1,wait=0): super().__init__();self.max_retries,self.wait=max_retries,wait\n    def exec_fallback(self,prep_res,exc): raise exc\n    def _exec(self,prep_res):\n        for self.cur_retry in range(self.max_retries):\n            try: return self.exec(prep_res)\n            except Exception as e:\n                if self.cur_retry==self.max_retries-1: return self.exec_fallback(prep_res,e)\n                if self.wait>0: time.sleep(self.wait)\n\nclass BatchNode(Node):\n    def _exec(self,items): return [super(BatchNode,self)._exec(i) for i in (items or [])]\n\nclass Flow(BaseNode):\n    def __init__(self,start): super().__init__();self.start=start\n    def get_next_node(self,curr,action):\n        nxt=curr.successors.get(action or \"default\")\n        if not nxt and curr.successors: warnings.warn(f\"Flow ends: '{action}' not found in {list(curr.successors)}\")\n        return nxt\n    def _orch(self,shared,params=None):\n        curr,p=copy.copy(self.start),(params or {**self.params})\n        while curr: curr.set_params(p);c=curr._run(shared);curr=copy.copy(self.get_next_node(curr,c))\n    def _run(self,shared): pr=self.prep(shared);self._orch(shared);return self.post(shared,pr,None)\n    def exec(self,prep_res): raise RuntimeError(\"Flow can't exec.\")\n\nclass BatchFlow(Flow):\n    def _run(self,shared):\n        pr=self.prep(shared) or []\n        for bp in pr: self._orch(shared,{**self.params,**bp})\n        return self.post(shared,pr,None)\n\nclass AsyncNode(Node):\n    def prep(self,shared): raise RuntimeError(\"Use prep_async.\")\n    def exec(self,prep_res): raise RuntimeError(\"Use exec_async.\")\n    def post(self,shared,prep_res,exec_res): raise RuntimeError(\"Use post_async.\")\n    def exec_fallback(self,prep_res,exc): raise RuntimeError(\"Use exec_fallback_async.\")\n    def _run(self,shared): raise RuntimeError(\"Use run_async.\")\n    async def prep_async(self,shared): pass\n    async def exec_async(self,prep_res): pass\n    async def exec_fallback_async(self,prep_res,exc): raise exc\n    async def post_async(self,shared,prep_res,exec_res): pass\n    async def _exec(self,prep_res): \n        for i in range(self.max_retries):\n            try: return await self.exec_async(prep_res)\n            except Exception as e:\n                if i==self.max_retries-1: return await self.exec_fallback_async(prep_res,e)\n                if self.wait>0: await asyncio.sleep(self.wait)\n    async def run_async(self,shared): \n        if self.successors: warnings.warn(\"Node won't run successors. Use AsyncFlow.\")  \n        return await self._run_async(shared)\n    async def _run_async(self,shared): p=await self.prep_async(shared);e=await self._exec(p);return await self.post_async(shared,p,e)\n\nclass AsyncBatchNode(AsyncNode,BatchNode):\n    async def _exec(self,items): return [await super(AsyncBatchNode,self)._exec(i) for i in items]\n\nclass AsyncParallelBatchNode(AsyncNode,BatchNode):\n    async def _exec(self,items): return await asyncio.gather(*(super(AsyncParallelBatchNode,self)._exec(i) for i in items))\n\nclass AsyncFlow(Flow,AsyncNode):\n    async def _orch_async(self,shared,params=None):\n        curr,p=copy.copy(self.start),(params or {**self.params})\n        while curr:curr.set_params(p);c=await curr._run_async(shared) if isinstance(curr,AsyncNode) else curr._run(shared);curr=copy.copy(self.get_next_node(curr,c))\n    async def _run_async(self,shared): p=await self.prep_async(shared);await self._orch_async(shared);return await self.post_async(shared,p,None)\n\nclass AsyncBatchFlow(AsyncFlow,BatchFlow):\n    async def _run_async(self,shared):\n        pr=await self.prep_async(shared) or []\n        for bp in pr: await self._orch_async(shared,{**self.params,**bp})\n        return await self.post_async(shared,pr,None)\n\nclass AsyncParallelBatchFlow(AsyncFlow,BatchFlow):\n    async def _run_async(self,shared):\n        pr=await self.prep_async(shared) or []\n        await asyncio.gather(*(self._orch_async(shared,{**self.params,**bp}) for bp in pr))\n        return await self.post_async(shared,pr,None)\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 2: Node`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "# Chapter 2: Node\n\nIn [Chapter 1: Flow](01_flow.md), we learned about Flows and how they orchestrate different actions in a specific sequence. Now, let's zoom in on the individual *actions* themselves. These actions are called **Nodes**.\n\nImagine you're building a robot that can make a sandwich. The Flow would be the robot's overall plan: \"Get bread -> Get fillings -> Assemble sandwich -> Serve sandwich\".  Each of those steps \u2013 \"Get bread\", \"Get fillings\", etc. \u2013 would be a Node.\n\n**What Problem Do Nodes Solve?**\n\nNodes solve the problem of breaking down a complex task into smaller, manageable pieces. Instead of having one giant, complicated function, we can have a series of simpler Nodes, each responsible for a specific job. This makes our code easier to understand, test, and maintain.\n\n**Our Sandwich-Making Robot Example**\n\nLet's focus on the \"Get fillings\" step. This could be further broken down: \"Open refrigerator -> Check for desired fillings -> Take out fillings\".  Each of *those* could even be a Node!  For our purposes, let's say \"Get fillings\" is a single Node that figures out which fillings to get and retrieves them.\n\n**Key Concepts of a Node**\n\nA Node is like a mini-program that does one thing well. It has three main parts:\n\n1.  **Input(s):** What the Node needs to get its job done. In our \"Get fillings\" Node, the input might be a list of preferred fillings.\n\n2.  **Processing:** What the Node *does* with the input. This is the core logic of the Node.  In our example, it's the code that checks what fillings are available and retrieves them.\n\n3.  **Output(s):** What the Node produces after processing the input.  In our example, the output would be the actual fillings.\n\nIn PocketFlow, these three parts are defined using three special methods: `prep`, `exec`, and `post`. Don't worry too much about these names for now. Just remember the input, processing, and output.  We will discuss [Prep, Exec, Post (Node Lifecycle)](04_prep__exec__post__node_lifecycle_.md) in detail later.\n\n**Creating a Simple Node**\n\nLet's create a simple Node that adds two numbers.\n\n```python\nfrom pocketflow import Node\n\nclass AddNumbers(Node):\n    def prep(self, shared):\n        \"\"\"Prepares the input numbers from the shared store.\"\"\"\n        num1 = shared[\"num1\"]\n        num2 = shared[\"num2\"]\n        return num1, num2\n\n    def exec(self, inputs):\n        \"\"\"Adds the two numbers together.\"\"\"\n        num1, num2 = inputs\n        result = num1 + num2\n        return result\n\n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Stores the result in the shared store.\"\"\"\n        shared[\"result\"] = exec_res\n        return None  # No next node, the Flow can end\n```\n\nHere's what's happening:\n\n*   We import the `Node` class from `pocketflow`.\n*   We create a class called `AddNumbers` that *inherits* from `Node`. This means our `AddNumbers` class *is* a Node and has all the properties of a Node.\n*   `prep(self, shared)`: This method *prepares* the input for the `exec` method. It retrieves the two numbers (`num1` and `num2`) from the `shared` store (we'll talk about the [Shared Store](03_shared_store.md) in the next chapter!). It returns these two numbers as a tuple.\n*   `exec(self, inputs)`: This method *executes* the core logic of the Node. It takes the output of `prep` as `inputs`, adds the two numbers together, and returns the `result`.\n*   `post(self, shared, prep_res, exec_res)`: This method *posts* the result to the `shared` store. It takes the output of `exec` as `exec_res` and stores it in the `shared` store under the key \"result\".  It returns `None` in this case because there's no further action required in the flow after adding the numbers.\n\n**Using the Node in a Flow**\n\nNow, let's see how we can use this Node in a Flow:\n\n```python\nfrom pocketflow import Flow\nfrom nodes import AddNumbers # Assuming the AddNumbers node is in nodes.py\n\n# Create an instance of the AddNumbers node\nadd_node = AddNumbers()\n\n# Create a Flow starting with the AddNumbers node\nflow = Flow(start=add_node)\n\n# Create a shared store with the input numbers\nshared_store = {\"num1\": 5, \"num2\": 3}\n\n# Run the flow\nflow.run(shared_store)\n\n# Print the result\nprint(shared_store[\"result\"])  # Output: 8\n```\n\nHere's what's happening:\n\n*   We import `Flow` from `pocketflow`.\n*   We create an instance of our `AddNumbers` Node.\n*   We create a `Flow` that starts with our `AddNumbers` Node.\n*   We create a `shared_store` (a dictionary) and put the input numbers (`num1` and `num2`) into it.\n*   We run the `Flow` with the `shared_store`.\n*   Finally, we print the result, which is stored in the `shared_store` under the key \"result\".\n\n**Let's Trace the Execution**\n\n1.  The `Flow` starts at the `AddNumbers` Node.\n2.  The `AddNumbers` Node's `prep` method is called. It retrieves `num1` (5) and `num2` (3) from the `shared_store`.\n3.  The `AddNumbers` Node's `exec` method is called with the input (5, 3). It adds them together and returns 8.\n4.  The `AddNumbers` Node's `post` method is called with the result (8). It stores the result in the `shared_store` under the key \"result\".\n5.  The `Flow` finishes because the `AddNumbers` Node returned `None` in the `post` method, indicating there are no more Nodes to execute.\n\n**Under the Hood: A Closer Look**\n\nLet's see a simplified sequence diagram of what happens when we run the `Flow`:\n\n```mermaid\nsequenceDiagram\n    participant Flow\n    participant AddNumbers\n    participant SharedStore\n\n    Flow->>AddNumbers: run(shared_store)\n    AddNumbers->>SharedStore: access num1, num2\n    AddNumbers-->>Flow: num1, num2\n    Flow->>AddNumbers: exec(num1, num2)\n    AddNumbers-->>Flow: result\n    Flow->>AddNumbers: post(shared_store, prep_res, result)\n    AddNumbers->>SharedStore: store result\n    AddNumbers-->>Flow: None\n```\n\n*   The `Flow` calls the `run` method of the `AddNumbers` Node, passing the `shared_store`.\n*   The `AddNumbers` Node accesses the `shared_store` to get the input numbers.\n*   The `AddNumbers` Node executes its logic and returns the result to the `Flow`.\n*   The `AddNumbers` Node then updates the `shared_store` with the result.\n\nNow, let's peek at some of the relevant code from the `pocketflow/__init__.py` file:\n\n```python\nclass Node(BaseNode): #Simplified\n    def prep(self,shared): pass\n    def exec(self,prep_res): pass\n    def post(self,shared,prep_res,exec_res): pass\n    def _exec(self,prep_res): return self.exec(prep_res)\n    def _run(self,shared): p=self.prep(shared);e=self._exec(p);return self.post(shared,p,e)\n```\n\n*   `prep`, `exec`, and `post` are methods that you can override in your own Node classes to define the behavior of the Node.\n*   The `_run` method is responsible for calling `prep`, `exec`, and `post` in sequence.\n\n**Example: Back to the Research Agent**\nLet's look at our Research Agent code from [Chapter 1: Flow](01_flow.md).\n\n```python\nfrom pocketflow import Node\nfrom utils import call_llm, search_web\nimport yaml\n\nclass DecideAction(Node):\n    def prep(self, shared):\n        # Prepare the context and question\n        question = shared[\"question\"]\n        context = shared.get(\"context\", \"No previous search\")\n        return question, context\n\n    def exec(self, inputs):\n        # Call LLM to decide search or answer\n        question, context = inputs\n        prompt = f\"\"\"... (LLM prompt) ...\"\"\" #Skipping prompt for brevity\n        response = call_llm(prompt)\n        yaml_str = response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n        decision = yaml.safe_load(yaml_str)\n        return decision\n\n    def post(self, shared, prep_res, exec_res):\n        #Save search query and determine next step\n        if exec_res[\"action\"] == \"search\":\n            shared[\"search_query\"] = exec_res[\"search_query\"]\n        else:\n            shared[\"context\"] = exec_res[\"answer\"]\n        return exec_res[\"action\"]\n```\n\nIn this example, `DecideAction` Node decides whether to search or answer based on the provided question and context. The `prep` function grabs the values from the `shared` store, the `exec` function decides on the next action, and the `post` function save the search query (if applicable) and specifies the name of the next node to proceed to.\n\n**Different Types of Nodes**\n\nWhile we've focused on a basic Node here, PocketFlow offers different types of Nodes for different purposes, such as [BatchNode](05_batchnode.md) for processing multiple items at once, and [AsyncNode](06_asyncnode.md) for handling asynchronous operations.\n\n**Conclusion**\n\nIn this chapter, we learned about Nodes, the fundamental building blocks of PocketFlow. We saw how Nodes encapsulate individual actions, and how they are connected together in a Flow. We learned about the `prep`, `exec`, and `post` methods that define the behavior of a Node. Now that we understand Nodes, let's move on to [Chapter 3: Shared Store](03_shared_store.md) to see how Nodes can share data with each other.\n", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `PocketFlow` about the concept: \"Shared Store\". This is Chapter 3.\n\nConcept Details:\n- Description:\nActs as a central memory or message board where Nodes can communicate and share data. It's like a shared notebook in a group project, where each person can read what others have written and add their own contributions. Data stored in the shared store is available to all nodes in the flow, facilitating seamless data transfer between processing steps.\n\n\nComplete Tutorial Structure:\n1. [Flow](01_flow.md)\n2. [Node](02_node.md)\n3. [Shared Store](03_shared_store.md)\n4. [Prep, Exec, Post (Node Lifecycle)](04_prep__exec__post__node_lifecycle_.md)\n5. [BatchNode](05_batchnode.md)\n6. [AsyncNode](06_asyncnode.md)\n7. [AsyncFlow](07_asyncflow.md)\n8. [AsyncParallelBatchNode](08_asyncparallelbatchnode.md)\n\nContext from previous chapters (summary):\n# Chapter 1: Flow\n\nImagine you're directing a play. You have actors, each with specific roles (like answering a question or searching the web). But how do you tell them *when* to act, and *what* to do next?  That's where a **Flow** comes in!\n\nIn PocketFlow, a Flow is like the conductor of an orchestra. It's the orchestration layer that connects individual actions (we call them **Nodes**) into a functioning workflow. It defines the sequence in which these actions are executed and how data is passed between them.\n\n**Our Use Case: A Simple Research Agent**\n\nLet's say we're building a simple research agent that can either search the web or answer a question, depending on what's needed.  The agent will first *decide* what to do. If it needs to search, it will *search the web*. If it needs to answer, it will *answer the question*. After searching, it goes back to the deciding step.\n\nWithout a Flow, we'd have to manually call each action in the right order, which can get messy quickly. A Flow lets us define this process in a clear and organized way.\n\n**Key Concepts of a Flow**\n\n1.  **Nodes:** These are the individual steps or actions in our workflow. Think of them as the actors in our play. We'll dive deep into Nodes in [Chapter 2: Node](02_node.md). For now, just think of them as individual units of work.\n\n2.  **Sequence:** The Flow defines the order in which Nodes are executed.  It's the script of our play.\n\n3.  **Data Passing:** The Flow manages how data is passed between Nodes. This is how the actors communicate and share information.\n\n4.  **Starting Node:** Every flow must have a start node to kick off the process.\n\n5.  **Transitions:**  These define how the Flow moves from one Node to the next. We'll use special operators like `>>` (rshift) and `-` (subtraction) to define these transitions.\n\n**Creating Our Research Agent Flow**\n\nLet's look at how we can create a Flow for our research agent. The code might look like this (from `cookbook/pocketflow-agent/flow.py`):\n\n```python\nfrom pocketflow import Flow\nfrom nodes import DecideAction, SearchWeb, AnswerQuestion\n\ndef create_agent_flow():\n    # Create instances of each node\n    decide = DecideAction()\n    search = SearchWeb()\n    answer = AnswerQuestion()\n\n    # Connect the nodes\n    # If DecideAction returns \"search\", go to SearchWeb\n    decide - \"search\" >> search\n\n    # If DecideAction returns \"answer\", go to AnswerQuestion\n    decide - \"answer\" >> answer\n\n    # After SearchWeb completes and returns \"decide\", go back to DecideAction\n    search - \"decide\" >> decide\n\n    # Create and return the flow, starting with the DecideAction node\n    return Flow(start=decide)\n```\n\nHere's a breakdown:\n\n*   We import `Flow` from the `pocketflow` library.\n*   We import the `Node` types from our nodes file (`nodes.py`). Don't worry about the implementations of `DecideAction`, `SearchWeb`, and `AnswerQuestion` yet.  Just think of them as steps in our process.\n*   We create instances of each Node.\n*   We use the `>>` and `-` operators to define the transitions between Nodes. For example, `decide - \"search\" >> search` means \"If the `decide` Node returns 'search', then go to the `search` Node.\"\n*   We create a `Flow` object, specifying the `decide` Node as the starting point.\n\n**How it Works: Example**\n\nImagine the agent receives the question: \"What is the capital of France?\".\n\n1.  The `Flow` starts at the `DecideAction` Node.\n2.  The `DecideAction` Node determines that we need to search for the answer. It returns the string \"search\".\n3.  The `Flow` sees the transition `decide - \"search\" >> search` and moves to the `SearchWeb` Node.\n4.  The `SearchWeb` Node searches the web and finds the answer: \"Paris\". It stores the answer in memory and returns the string \"decide\".\n5.  The `Flow` sees the transition `search - \"decide\" >> decide` and moves back to the `DecideAction` Node.\n6.  Now `DecideAction` could determine we need to answer the original question. It returns the string \"answer\".\n7.  The `Flow` sees the transition `decide - \"answer\" >> answer` and moves to the `AnswerQuestion` Node.\n8.  The `AnswerQuestion` node crafts the answer using the information in memory and then returns its final response.\n\n**Under the Hood: A Peek Inside**\n\nLet's see what happens inside the `Flow` when we run it.  Here's a simplified sequence diagram:\n\n```mermaid\nsequenceDiagram\n    participant Flow\n    participant DecideAction\n    participant SearchWeb\n    participant AnswerQuestion\n    participant SharedStore\n\n    Flow->>DecideAction: run(shared_store)\n    DecideAction->>SharedStore: access/modify data\n    DecideAction-->>Flow: \"search\"\n    Flow->>SearchWeb: run(shared_store)\n    SearchWeb->>SharedStore: access/modify data\n    SearchWeb-->>Flow: \"decide\"\n    Flow->>DecideAction: run(shared_store)\n    DecideAction->>SharedStore: access/modify data\n    DecideAction-->>Flow: \"answer\"\n    Flow->>AnswerQuestion: run(shared_store)\n    AnswerQuestion->>SharedStore: access/modify data\n    AnswerQuestion-->>Flow: (End)\n```\n\n*   The `Flow` starts by calling the `run` method of the starting Node (`DecideAction` in our example).\n*   Each Node executes its logic and potentially modifies a `SharedStore` ([Chapter 3: Shared Store](03_shared_store.md)) which is used for passing data between the nodes.\n*   The Node returns a value (like \"search\" or \"answer\") that determines the next Node to execute, based on the transitions defined in the Flow.\n*   This process continues until there are no more transitions, or we hit a dead end (no successor node defined for a particular return value).\n\n**Code Dive: `Flow` Class**\n\nLet's look at some of the key parts of the `Flow` class (from `pocketflow/__init__.py`):\n\n```python\nclass Flow(BaseNode):\n    def __init__(self,start): super().__init__();self.start=start\n    def get_next_node(self,curr,action):\n        nxt=curr.successors.get(action or \"default\")\n        if not nxt and curr.successors: warnings.warn(f\"Flow ends: '{action}' not found in {list(curr.successors)}\")\n        return nxt\n    def _orch(self,shared,params=None):\n        curr,p=copy.copy(self.start),(params or {**self.params})\n        while curr: curr.set_params(p);c=curr._run(shared);curr=copy.copy(self.get_next_node(curr,c))\n    def _run(self,shared): pr=self.prep(shared);self._orch(shared);return self.post(shared,pr,None)\n    def exec(self,prep_res): raise RuntimeError(\"Flow can't exec.\")\n```\n\n*   `__init__(self, start)`: The constructor takes the starting Node as an argument.\n*   `get_next_node(self, curr, action)`:  This method determines the next Node to execute based on the return value (`action`) of the current Node (`curr`). It looks up the successor node in the current node's `successors` dictionary.\n*   `_orch(self, shared, params=None)`:  This is the \"orchestration\" method.  It iterates through the Nodes, executing each one in sequence based on the transitions.\n*   `_run(self, shared)`: This method kicks off the orchestration by calling `_orch(self, shared)`.\n\n**Conclusion**\n\nIn this chapter, we learned about the concept of a Flow in PocketFlow. A Flow is an orchestration layer that connects individual Nodes into a functioning workflow. It defines the sequence in which Nodes are executed and how data is passed between them. We saw how to create a Flow for a simple research agent and how it works under the hood.\n\nNow that we understand Flows, let's dive deeper into the building blocks of Flows: **Nodes**. In [Chapter 2: Node](02_node.md), we'll explore what Nodes are, how to create them, and how they interact with Flows.\n\n---\n# Chapter 2: Node\n\nIn [Chapter 1: Flow](01_flow.md), we learned about Flows and how they orchestrate different actions in a specific sequence. Now, let's zoom in on the individual *actions* themselves. These actions are called **Nodes**.\n\nImagine you're building a robot that can make a sandwich. The Flow would be the robot's overall plan: \"Get bread -> Get fillings -> Assemble sandwich -> Serve sandwich\".  Each of those steps \u2013 \"Get bread\", \"Get fillings\", etc. \u2013 would be a Node.\n\n**What Problem Do Nodes Solve?**\n\nNodes solve the problem of breaking down a complex task into smaller, manageable pieces. Instead of having one giant, complicated function, we can have a series of simpler Nodes, each responsible for a specific job. This makes our code easier to understand, test, and maintain.\n\n**Our Sandwich-Making Robot Example**\n\nLet's focus on the \"Get fillings\" step. This could be further broken down: \"Open refrigerator -> Check for desired fillings -> Take out fillings\".  Each of *those* could even be a Node!  For our purposes, let's say \"Get fillings\" is a single Node that figures out which fillings to get and retrieves them.\n\n**Key Concepts of a Node**\n\nA Node is like a mini-program that does one thing well. It has three main parts:\n\n1.  **Input(s):** What the Node needs to get its job done. In our \"Get fillings\" Node, the input might be a list of preferred fillings.\n\n2.  **Processing:** What the Node *does* with the input. This is the core logic of the Node.  In our example, it's the code that checks what fillings are available and retrieves them.\n\n3.  **Output(s):** What the Node produces after processing the input.  In our example, the output would be the actual fillings.\n\nIn PocketFlow, these three parts are defined using three special methods: `prep`, `exec`, and `post`. Don't worry too much about these names for now. Just remember the input, processing, and output.  We will discuss [Prep, Exec, Post (Node Lifecycle)](04_prep__exec__post__node_lifecycle_.md) in detail later.\n\n**Creating a Simple Node**\n\nLet's create a simple Node that adds two numbers.\n\n```python\nfrom pocketflow import Node\n\nclass AddNumbers(Node):\n    def prep(self, shared):\n        \"\"\"Prepares the input numbers from the shared store.\"\"\"\n        num1 = shared[\"num1\"]\n        num2 = shared[\"num2\"]\n        return num1, num2\n\n    def exec(self, inputs):\n        \"\"\"Adds the two numbers together.\"\"\"\n        num1, num2 = inputs\n        result = num1 + num2\n        return result\n\n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Stores the result in the shared store.\"\"\"\n        shared[\"result\"] = exec_res\n        return None  # No next node, the Flow can end\n```\n\nHere's what's happening:\n\n*   We import the `Node` class from `pocketflow`.\n*   We create a class called `AddNumbers` that *inherits* from `Node`. This means our `AddNumbers` class *is* a Node and has all the properties of a Node.\n*   `prep(self, shared)`: This method *prepares* the input for the `exec` method. It retrieves the two numbers (`num1` and `num2`) from the `shared` store (we'll talk about the [Shared Store](03_shared_store.md) in the next chapter!). It returns these two numbers as a tuple.\n*   `exec(self, inputs)`: This method *executes* the core logic of the Node. It takes the output of `prep` as `inputs`, adds the two numbers together, and returns the `result`.\n*   `post(self, shared, prep_res, exec_res)`: This method *posts* the result to the `shared` store. It takes the output of `exec` as `exec_res` and stores it in the `shared` store under the key \"result\".  It returns `None` in this case because there's no further action required in the flow after adding the numbers.\n\n**Using the Node in a Flow**\n\nNow, let's see how we can use this Node in a Flow:\n\n```python\nfrom pocketflow import Flow\nfrom nodes import AddNumbers # Assuming the AddNumbers node is in nodes.py\n\n# Create an instance of the AddNumbers node\nadd_node = AddNumbers()\n\n# Create a Flow starting with the AddNumbers node\nflow = Flow(start=add_node)\n\n# Create a shared store with the input numbers\nshared_store = {\"num1\": 5, \"num2\": 3}\n\n# Run the flow\nflow.run(shared_store)\n\n# Print the result\nprint(shared_store[\"result\"])  # Output: 8\n```\n\nHere's what's happening:\n\n*   We import `Flow` from `pocketflow`.\n*   We create an instance of our `AddNumbers` Node.\n*   We create a `Flow` that starts with our `AddNumbers` Node.\n*   We create a `shared_store` (a dictionary) and put the input numbers (`num1` and `num2`) into it.\n*   We run the `Flow` with the `shared_store`.\n*   Finally, we print the result, which is stored in the `shared_store` under the key \"result\".\n\n**Let's Trace the Execution**\n\n1.  The `Flow` starts at the `AddNumbers` Node.\n2.  The `AddNumbers` Node's `prep` method is called. It retrieves `num1` (5) and `num2` (3) from the `shared_store`.\n3.  The `AddNumbers` Node's `exec` method is called with the input (5, 3). It adds them together and returns 8.\n4.  The `AddNumbers` Node's `post` method is called with the result (8). It stores the result in the `shared_store` under the key \"result\".\n5.  The `Flow` finishes because the `AddNumbers` Node returned `None` in the `post` method, indicating there are no more Nodes to execute.\n\n**Under the Hood: A Closer Look**\n\nLet's see a simplified sequence diagram of what happens when we run the `Flow`:\n\n```mermaid\nsequenceDiagram\n    participant Flow\n    participant AddNumbers\n    participant SharedStore\n\n    Flow->>AddNumbers: run(shared_store)\n    AddNumbers->>SharedStore: access num1, num2\n    AddNumbers-->>Flow: num1, num2\n    Flow->>AddNumbers: exec(num1, num2)\n    AddNumbers-->>Flow: result\n    Flow->>AddNumbers: post(shared_store, prep_res, result)\n    AddNumbers->>SharedStore: store result\n    AddNumbers-->>Flow: None\n```\n\n*   The `Flow` calls the `run` method of the `AddNumbers` Node, passing the `shared_store`.\n*   The `AddNumbers` Node accesses the `shared_store` to get the input numbers.\n*   The `AddNumbers` Node executes its logic and returns the result to the `Flow`.\n*   The `AddNumbers` Node then updates the `shared_store` with the result.\n\nNow, let's peek at some of the relevant code from the `pocketflow/__init__.py` file:\n\n```python\nclass Node(BaseNode): #Simplified\n    def prep(self,shared): pass\n    def exec(self,prep_res): pass\n    def post(self,shared,prep_res,exec_res): pass\n    def _exec(self,prep_res): return self.exec(prep_res)\n    def _run(self,shared): p=self.prep(shared);e=self._exec(p);return self.post(shared,p,e)\n```\n\n*   `prep`, `exec`, and `post` are methods that you can override in your own Node classes to define the behavior of the Node.\n*   The `_run` method is responsible for calling `prep`, `exec`, and `post` in sequence.\n\n**Example: Back to the Research Agent**\nLet's look at our Research Agent code from [Chapter 1: Flow](01_flow.md).\n\n```python\nfrom pocketflow import Node\nfrom utils import call_llm, search_web\nimport yaml\n\nclass DecideAction(Node):\n    def prep(self, shared):\n        # Prepare the context and question\n        question = shared[\"question\"]\n        context = shared.get(\"context\", \"No previous search\")\n        return question, context\n\n    def exec(self, inputs):\n        # Call LLM to decide search or answer\n        question, context = inputs\n        prompt = f\"\"\"... (LLM prompt) ...\"\"\" #Skipping prompt for brevity\n        response = call_llm(prompt)\n        yaml_str = response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n        decision = yaml.safe_load(yaml_str)\n        return decision\n\n    def post(self, shared, prep_res, exec_res):\n        #Save search query and determine next step\n        if exec_res[\"action\"] == \"search\":\n            shared[\"search_query\"] = exec_res[\"search_query\"]\n        else:\n            shared[\"context\"] = exec_res[\"answer\"]\n        return exec_res[\"action\"]\n```\n\nIn this example, `DecideAction` Node decides whether to search or answer based on the provided question and context. The `prep` function grabs the values from the `shared` store, the `exec` function decides on the next action, and the `post` function save the search query (if applicable) and specifies the name of the next node to proceed to.\n\n**Different Types of Nodes**\n\nWhile we've focused on a basic Node here, PocketFlow offers different types of Nodes for different purposes, such as [BatchNode](05_batchnode.md) for processing multiple items at once, and [AsyncNode](06_asyncnode.md) for handling asynchronous operations.\n\n**Conclusion**\n\nIn this chapter, we learned about Nodes, the fundamental building blocks of PocketFlow. We saw how Nodes encapsulate individual actions, and how they are connected together in a Flow. We learned about the `prep`, `exec`, and `post` methods that define the behavior of a Node. Now that we understand Nodes, let's move on to [Chapter 3: Shared Store](03_shared_store.md) to see how Nodes can share data with each other.\n\n\nRelevant Code Snippets:\n--- File: cookbook/pocketflow-agent/nodes.py ---\nfrom pocketflow import Node\nfrom utils import call_llm, search_web\nimport yaml\n\nclass DecideAction(Node):\n    def prep(self, shared):\n        \"\"\"Prepare the context and question for the decision-making process.\"\"\"\n        # Get the current context (default to \"No previous search\" if none exists)\n        context = shared.get(\"context\", \"No previous search\")\n        # Get the question from the shared store\n        question = shared[\"question\"]\n        # Return both for the exec step\n        return question, context\n        \n    def exec(self, inputs):\n        \"\"\"Call the LLM to decide whether to search or answer.\"\"\"\n        question, context = inputs\n        \n        print(f\"\ud83e\udd14 Agent deciding what to do next...\")\n        \n        # Create a prompt to help the LLM decide what to do next with proper yaml formatting\n        prompt = f\"\"\"\n### CONTEXT\nYou are a research assistant that can search the web.\nQuestion: {question}\nPrevious Research: {context}\n\n### ACTION SPACE\n[1] search\n  Description: Look up more information on the web\n  Parameters:\n    - query (str): What to search for\n\n[2] answer\n  Description: Answer the question with current knowledge\n  Parameters:\n    - answer (str): Final answer to the question\n\n## NEXT ACTION\nDecide the next action based on the context and available actions.\nReturn your response in this format:\n\n```yaml\nthinking: |\n    <your step-by-step reasoning process>\naction: search OR answer\nreason: <why you chose this action>\nanswer: <if action is answer>\nsearch_query: <specific search query if action is search>\n```\nIMPORTANT: Make sure to:\n1. Use proper indentation (4 spaces) for all multi-line fields\n2. Use the | character for multi-line text fields\n3. Keep single-line fields without the | character\n\"\"\"\n        \n        # Call the LLM to make a decision\n        response = call_llm(prompt)\n        \n        # Parse the response to get the decision\n        yaml_str = response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n        decision = yaml.safe_load(yaml_str)\n        \n        return decision\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Save the decision and determine the next step in the flow.\"\"\"\n        # If LLM decided to search, save the search query\n        if exec_res[\"action\"] == \"search\":\n            shared[\"search_query\"] = exec_res[\"search_query\"]\n            print(f\"\ud83d\udd0d Agent decided to search for: {exec_res['search_query']}\")\n        else:\n            shared[\"context\"] = exec_res[\"answer\"] #save the context if LLM gives the answer without searching.\n            print(f\"\ud83d\udca1 Agent decided to answer the question\")\n        \n        # Return the action to determine the next node in the flow\n        return exec_res[\"action\"]\n\nclass SearchWeb(Node):\n    def prep(self, shared):\n        \"\"\"Get the search query from the shared store.\"\"\"\n        return shared[\"search_query\"]\n        \n    def exec(self, search_query):\n        \"\"\"Search the web for the given query.\"\"\"\n        # Call the search utility function\n        print(f\"\ud83c\udf10 Searching the web for: {search_query}\")\n        results = search_web(search_query)\n        return results\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Save the search results and go back to the decision node.\"\"\"\n        # Add the search results to the context in the shared store\n        previous = shared.get(\"context\", \"\")\n        shared[\"context\"] = previous + \"\\n\\nSEARCH: \" + shared[\"search_query\"] + \"\\nRESULTS: \" + exec_res\n        \n        print(f\"\ud83d\udcda Found information, analyzing results...\")\n        \n        # Always go back to the decision node after searching\n        return \"decide\"\n\nclass AnswerQuestion(Node):\n    def prep(self, shared):\n        \"\"\"Get the question and context for answering.\"\"\"\n        return shared[\"question\"], shared.get(\"context\", \"\")\n        \n    def exec(self, inputs):\n        \"\"\"Call the LLM to generate a final answer.\"\"\"\n        question, context = inputs\n        \n        print(f\"\u270d\ufe0f Crafting final answer...\")\n        \n        # Create a prompt for the LLM to answer the question\n        prompt = f\"\"\"\n### CONTEXT\nBased on the following information, answer the question.\nQuestion: {question}\nResearch: {context}\n\n## YOUR ANSWER:\nProvide a comprehensive answer using the research results.\n\"\"\"\n        # Call the LLM to generate an answer\n        answer = call_llm(prompt)\n        return answer\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Save the final answer and complete the flow.\"\"\"\n        # Save the answer in the shared store\n        shared[\"answer\"] = exec_res\n        \n        print(f\"\u2705 Answer generated successfully\")\n        \n        # We're done - no need to continue the flow\n        return \"done\" \n\n\n--- File: cookbook/pocketflow-communication/nodes.py ---\n\"\"\"Node implementations for the communication example.\"\"\"\n\nfrom pocketflow import Node\n\nclass EndNode(Node):\n    \"\"\"Node that handles flow termination.\"\"\"\n    pass\n\nclass TextInput(Node):\n    \"\"\"Node that reads text input and initializes the shared store.\"\"\"\n    \n    def prep(self, shared):\n        \"\"\"Get user input and ensure shared store is initialized.\"\"\"\n        return input(\"Enter text (or 'q' to quit): \")\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Store text and initialize/update statistics.\"\"\"\n        if prep_res == 'q':\n            return \"exit\"\n        \n        # Store the text\n        shared[\"text\"] = prep_res\n        \n        # Initialize statistics if they don't exist\n        if \"stats\" not in shared:\n            shared[\"stats\"] = {\n                \"total_texts\": 0,\n                \"total_words\": 0\n            }\n        shared[\"stats\"][\"total_texts\"] += 1\n        \n        return \"count\"\n\nclass WordCounter(Node):\n    \"\"\"Node that counts words in the text.\"\"\"\n    \n    def prep(self, shared):\n        \"\"\"Get text from shared store.\"\"\"\n        return shared[\"text\"]\n    \n    def exec(self, text):\n        \"\"\"Count words in the text.\"\"\"\n        return len(text.split())\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Update word count statistics.\"\"\"\n        shared[\"stats\"][\"total_words\"] += exec_res\n        return \"show\"\n\nclass ShowStats(Node):\n    \"\"\"Node that displays statistics from the shared store.\"\"\"\n    \n    def prep(self, shared):\n        \"\"\"Get statistics from shared store.\"\"\"\n        return shared[\"stats\"]\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Display statistics and continue the flow.\"\"\"\n        stats = prep_res\n        print(f\"\\nStatistics:\")\n        print(f\"- Texts processed: {stats['total_texts']}\")\n        print(f\"- Total words: {stats['total_words']}\")\n        print(f\"- Average words per text: {stats['total_words'] / stats['total_texts']:.1f}\\n\")\n        return \"continue\" \n\n--- File: pocketflow/__init__.py ---\nimport asyncio, warnings, copy, time\n\nclass BaseNode:\n    def __init__(self): self.params,self.successors={},{}\n    def set_params(self,params): self.params=params\n    def add_successor(self,node,action=\"default\"):\n        if action in self.successors: warnings.warn(f\"Overwriting successor for action '{action}'\")\n        self.successors[action]=node;return node\n    def prep(self,shared): pass\n    def exec(self,prep_res): pass\n    def post(self,shared,prep_res,exec_res): pass\n    def _exec(self,prep_res): return self.exec(prep_res)\n    def _run(self,shared): p=self.prep(shared);e=self._exec(p);return self.post(shared,p,e)\n    def run(self,shared): \n        if self.successors: warnings.warn(\"Node won't run successors. Use Flow.\")  \n        return self._run(shared)\n    def __rshift__(self,other): return self.add_successor(other)\n    def __sub__(self,action):\n        if isinstance(action,str): return _ConditionalTransition(self,action)\n        raise TypeError(\"Action must be a string\")\n\nclass _ConditionalTransition:\n    def __init__(self,src,action): self.src,self.action=src,action\n    def __rshift__(self,tgt): return self.src.add_successor(tgt,self.action)\n\nclass Node(BaseNode):\n    def __init__(self,max_retries=1,wait=0): super().__init__();self.max_retries,self.wait=max_retries,wait\n    def exec_fallback(self,prep_res,exc): raise exc\n    def _exec(self,prep_res):\n        for self.cur_retry in range(self.max_retries):\n            try: return self.exec(prep_res)\n            except Exception as e:\n                if self.cur_retry==self.max_retries-1: return self.exec_fallback(prep_res,e)\n                if self.wait>0: time.sleep(self.wait)\n\nclass BatchNode(Node):\n    def _exec(self,items): return [super(BatchNode,self)._exec(i) for i in (items or [])]\n\nclass Flow(BaseNode):\n    def __init__(self,start): super().__init__();self.start=start\n    def get_next_node(self,curr,action):\n        nxt=curr.successors.get(action or \"default\")\n        if not nxt and curr.successors: warnings.warn(f\"Flow ends: '{action}' not found in {list(curr.successors)}\")\n        return nxt\n    def _orch(self,shared,params=None):\n        curr,p=copy.copy(self.start),(params or {**self.params})\n        while curr: curr.set_params(p);c=curr._run(shared);curr=copy.copy(self.get_next_node(curr,c))\n    def _run(self,shared): pr=self.prep(shared);self._orch(shared);return self.post(shared,pr,None)\n    def exec(self,prep_res): raise RuntimeError(\"Flow can't exec.\")\n\nclass BatchFlow(Flow):\n    def _run(self,shared):\n        pr=self.prep(shared) or []\n        for bp in pr: self._orch(shared,{**self.params,**bp})\n        return self.post(shared,pr,None)\n\nclass AsyncNode(Node):\n    def prep(self,shared): raise RuntimeError(\"Use prep_async.\")\n    def exec(self,prep_res): raise RuntimeError(\"Use exec_async.\")\n    def post(self,shared,prep_res,exec_res): raise RuntimeError(\"Use post_async.\")\n    def exec_fallback(self,prep_res,exc): raise RuntimeError(\"Use exec_fallback_async.\")\n    def _run(self,shared): raise RuntimeError(\"Use run_async.\")\n    async def prep_async(self,shared): pass\n    async def exec_async(self,prep_res): pass\n    async def exec_fallback_async(self,prep_res,exc): raise exc\n    async def post_async(self,shared,prep_res,exec_res): pass\n    async def _exec(self,prep_res): \n        for i in range(self.max_retries):\n            try: return await self.exec_async(prep_res)\n            except Exception as e:\n                if i==self.max_retries-1: return await self.exec_fallback_async(prep_res,e)\n                if self.wait>0: await asyncio.sleep(self.wait)\n    async def run_async(self,shared): \n        if self.successors: warnings.warn(\"Node won't run successors. Use AsyncFlow.\")  \n        return await self._run_async(shared)\n    async def _run_async(self,shared): p=await self.prep_async(shared);e=await self._exec(p);return await self.post_async(shared,p,e)\n\nclass AsyncBatchNode(AsyncNode,BatchNode):\n    async def _exec(self,items): return [await super(AsyncBatchNode,self)._exec(i) for i in items]\n\nclass AsyncParallelBatchNode(AsyncNode,BatchNode):\n    async def _exec(self,items): return await asyncio.gather(*(super(AsyncParallelBatchNode,self)._exec(i) for i in items))\n\nclass AsyncFlow(Flow,AsyncNode):\n    async def _orch_async(self,shared,params=None):\n        curr,p=copy.copy(self.start),(params or {**self.params})\n        while curr:curr.set_params(p);c=await curr._run_async(shared) if isinstance(curr,AsyncNode) else curr._run(shared);curr=copy.copy(self.get_next_node(curr,c))\n    async def _run_async(self,shared): p=await self.prep_async(shared);await self._orch_async(shared);return await self.post_async(shared,p,None)\n\nclass AsyncBatchFlow(AsyncFlow,BatchFlow):\n    async def _run_async(self,shared):\n        pr=await self.prep_async(shared) or []\n        for bp in pr: await self._orch_async(shared,{**self.params,**bp})\n        return await self.post_async(shared,pr,None)\n\nclass AsyncParallelBatchFlow(AsyncFlow,BatchFlow):\n    async def _run_async(self,shared):\n        pr=await self.prep_async(shared) or []\n        await asyncio.gather(*(self._orch_async(shared,{**self.params,**bp}) for bp in pr))\n        return await self.post_async(shared,pr,None)\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 3: Shared Store`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "# Chapter 3: Shared Store\n\nIn [Chapter 2: Node](02_node.md), we learned about Nodes and how they perform individual tasks. But how do these Nodes *talk* to each other? How does one Node pass its results to the next Node in the [Flow](01_flow.md)? That's where the **Shared Store** comes in!\n\nImagine a relay race. Each runner (Node) has a specific leg to run. The baton (data) needs to be passed from one runner to the next. The Shared Store is like the designated exchange zone where the baton is safely passed.\n\n**The Problem: Passing Information Between Nodes**\n\nWithout a shared space, each Node would operate in isolation.  They wouldn't know what other Nodes have done, and they couldn't build on each other's work.  The Shared Store provides a central location for Nodes to store and retrieve information, enabling collaboration and data flow within the PocketFlow ecosystem.\n\n**Our Research Agent Example (Again!)**\n\nRemember our research agent from [Chapter 1: Flow](01_flow.md)? The `SearchWeb` Node needs to pass the search results to the `AnswerQuestion` Node. Without a Shared Store, the `AnswerQuestion` Node wouldn't know what the web search found!\n\n**What *is* the Shared Store?**\n\nIn PocketFlow, the Shared Store is essentially a **dictionary**.  If you're new to programming, think of a dictionary as a container that holds key-value pairs. You can store data using a specific \"key\" and later retrieve it using that same key.\n\nFor example, we might store the search results with the key \"search_results\":\n\n```python\nshared_store[\"search_results\"] = \"The capital of France is Paris.\"\n```\n\nLater, another Node can access this information:\n\n```python\ncapital = shared_store[\"search_results\"]\nprint(capital) # Output: The capital of France is Paris.\n```\n\n**Key Concepts of the Shared Store**\n\n1.  **Centralized Data:** It's a single place where all Nodes can access and modify data.\n\n2.  **Key-Value Pairs:** Data is stored as key-value pairs, like a Python dictionary.\n\n3.  **Accessibility:** Any Node in the [Flow](01_flow.md) can access the Shared Store.\n\n4.  **Dynamic:** Nodes can add, modify, and delete data in the Shared Store during the [Flow](01_flow.md)'s execution.\n\n**How to Use the Shared Store**\n\nLet's revisit our `AddNumbers` Node from [Chapter 2: Node](02_node.md) and see how it uses the Shared Store:\n\n```python\nfrom pocketflow import Node\n\nclass AddNumbers(Node):\n    def prep(self, shared):\n        \"\"\"Prepares the input numbers from the shared store.\"\"\"\n        num1 = shared[\"num1\"]\n        num2 = shared[\"num2\"]\n        return num1, num2\n\n    def exec(self, inputs):\n        \"\"\"Adds the two numbers together.\"\"\"\n        num1, num2 = inputs\n        result = num1 + num2\n        return result\n\n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Stores the result in the shared store.\"\"\"\n        shared[\"result\"] = exec_res\n        return None  # No next node, the Flow can end\n```\n\n*   In the `prep` method, the Node retrieves the values associated with the keys \"num1\" and \"num2\" from the `shared` store.\n\n*   In the `post` method, the Node stores the calculated `result` in the `shared` store using the key \"result\".\n\n**Example Usage**\n\nHere's how the Shared Store is used when we run the [Flow](01_flow.md):\n\n```python\nfrom pocketflow import Flow\nfrom nodes import AddNumbers # Assuming the AddNumbers node is in nodes.py\n\n# Create an instance of the AddNumbers node\nadd_node = AddNumbers()\n\n# Create a Flow starting with the AddNumbers node\nflow = Flow(start=add_node)\n\n# Create a shared store with the input numbers\nshared_store = {\"num1\": 5, \"num2\": 3}\n\n# Run the flow\nflow.run(shared_store)\n\n# Print the result\nprint(shared_store[\"result\"])  # Output: 8\n```\n\n*   We create a `shared_store` (a dictionary) and initialize it with the input values for \"num1\" and \"num2\". This is how we *seed* the Shared Store with initial data.\n\n*   After the `flow.run(shared_store)` call, the `shared_store` now contains the key \"result\" with the value 8, because the `AddNumbers` Node stored it there.\n\n**Another Example: Passing the Question to the Answer Node**\nLet's look back at the nodes of our research agent in `cookbook/pocketflow-agent/nodes.py`. The `DecideAction` Node checks whether to search or answer the question and it has access to a \"question\" value in the shared store.\n\n```python\nclass DecideAction(Node):\n    def prep(self, shared):\n        # Prepare the context and question\n        question = shared[\"question\"] #Access question value from shared store\n        context = shared.get(\"context\", \"No previous search\")\n        return question, context\n    #Skipping the other functions for brevity.\n```\n\nIn this case, the `prep` function of `DecideAction` node accesses the question value from the shared store using the key \"question\".\n\n**Important Note:**\n\n*   **Key Existence:** It's good practice to check if a key exists in the Shared Store before accessing it, especially if the Node isn't the one who put the value there. You can use `shared.get(\"key\", default_value)` which returns `default_value` if the key doesn't exist. Or `if \"key\" in shared:` to check for a key's existence.\n\n**Under the Hood: How it Works**\n\nWhen you run a [Flow](01_flow.md), the `Flow` object creates and passes a Shared Store object to each Node's `prep`, `exec`, and `post` methods.\n\nHere's a simplified sequence diagram:\n\n```mermaid\nsequenceDiagram\n    participant Flow\n    participant Node1\n    participant Node2\n    participant SharedStore\n\n    Flow->>SharedStore: Create SharedStore\n    Flow->>Node1: run(SharedStore)\n    Node1->>SharedStore: Access/Modify data\n    Node1-->>Flow: (Next Action)\n    Flow->>Node2: run(SharedStore)\n    Node2->>SharedStore: Access/Modify data\n    Node2-->>Flow: (End)\n```\n\n*   The `Flow` creates a single `SharedStore` instance at the beginning.\n\n*   This same `SharedStore` instance is passed to each `Node` in the [Flow](01_flow.md).\n\n*   Each `Node` can then read from and write to the `SharedStore`.\n\n**Code Snippet: Flow Passing the Shared Store (From `pocketflow/__init__.py`)**\nThis is how the [Flow](01_flow.md) passes the Shared Store to each node:\n\n```python\nclass Flow(BaseNode):\n    def __init__(self,start): super().__init__();self.start=start\n    def get_next_node(self,curr,action):\n        nxt=curr.successors.get(action or \"default\")\n        if not nxt and curr.successors: warnings.warn(f\"Flow ends: '{action}' not found in {list(curr.successors)}\")\n        return nxt\n    def _orch(self,shared,params=None):\n        curr,p=copy.copy(self.start),(params or {**self.params})\n        while curr: curr.set_params(p);c=curr._run(shared);curr=copy.copy(self.get_next_node(curr,c))\n    def _run(self,shared): pr=self.prep(shared);self._orch(shared);return self.post(shared,pr,None)\n    def exec(self,prep_res): raise RuntimeError(\"Flow can't exec.\")\n```\nIn the `_orch` function, the shared store is maintained and passed into each subsequent node. Note the line `c=curr._run(shared)` indicating the shared store is passed into the node's run function.\n\n**Conclusion**\n\nIn this chapter, we learned about the Shared Store, a crucial component of PocketFlow that allows Nodes to communicate and share data. We saw how the Shared Store acts as a central memory, enabling seamless data transfer between processing steps. We learned how Nodes access and modify data in the Shared Store, and how the Flow manages the Shared Store behind the scenes.\n\nNow that we understand the Shared Store, we can appreciate how PocketFlow Nodes interact within a Flow to accomplish complex tasks. In the next chapter, [Prep, Exec, Post (Node Lifecycle)](04_prep__exec__post__node_lifecycle_.md), we'll dive deeper into the `prep`, `exec`, and `post` methods and understand the lifecycle of a Node.\n", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `PocketFlow` about the concept: \"Prep, Exec, Post (Node Lifecycle)\". This is Chapter 4.\n\nConcept Details:\n- Description:\nDefines the standard steps for each Node, enabling structured and modular task execution. `prep` prepares the input for the task like gathering materials for cooking. `exec` executes the core functionality, similar to performing the actual cooking. `post` processes the output and sets the next step similar to serving the dish.\n\n\nComplete Tutorial Structure:\n1. [Flow](01_flow.md)\n2. [Node](02_node.md)\n3. [Shared Store](03_shared_store.md)\n4. [Prep, Exec, Post (Node Lifecycle)](04_prep__exec__post__node_lifecycle_.md)\n5. [BatchNode](05_batchnode.md)\n6. [AsyncNode](06_asyncnode.md)\n7. [AsyncFlow](07_asyncflow.md)\n8. [AsyncParallelBatchNode](08_asyncparallelbatchnode.md)\n\nContext from previous chapters (summary):\n# Chapter 1: Flow\n\nImagine you're directing a play. You have actors, each with specific roles (like answering a question or searching the web). But how do you tell them *when* to act, and *what* to do next?  That's where a **Flow** comes in!\n\nIn PocketFlow, a Flow is like the conductor of an orchestra. It's the orchestration layer that connects individual actions (we call them **Nodes**) into a functioning workflow. It defines the sequence in which these actions are executed and how data is passed between them.\n\n**Our Use Case: A Simple Research Agent**\n\nLet's say we're building a simple research agent that can either search the web or answer a question, depending on what's needed.  The agent will first *decide* what to do. If it needs to search, it will *search the web*. If it needs to answer, it will *answer the question*. After searching, it goes back to the deciding step.\n\nWithout a Flow, we'd have to manually call each action in the right order, which can get messy quickly. A Flow lets us define this process in a clear and organized way.\n\n**Key Concepts of a Flow**\n\n1.  **Nodes:** These are the individual steps or actions in our workflow. Think of them as the actors in our play. We'll dive deep into Nodes in [Chapter 2: Node](02_node.md). For now, just think of them as individual units of work.\n\n2.  **Sequence:** The Flow defines the order in which Nodes are executed.  It's the script of our play.\n\n3.  **Data Passing:** The Flow manages how data is passed between Nodes. This is how the actors communicate and share information.\n\n4.  **Starting Node:** Every flow must have a start node to kick off the process.\n\n5.  **Transitions:**  These define how the Flow moves from one Node to the next. We'll use special operators like `>>` (rshift) and `-` (subtraction) to define these transitions.\n\n**Creating Our Research Agent Flow**\n\nLet's look at how we can create a Flow for our research agent. The code might look like this (from `cookbook/pocketflow-agent/flow.py`):\n\n```python\nfrom pocketflow import Flow\nfrom nodes import DecideAction, SearchWeb, AnswerQuestion\n\ndef create_agent_flow():\n    # Create instances of each node\n    decide = DecideAction()\n    search = SearchWeb()\n    answer = AnswerQuestion()\n\n    # Connect the nodes\n    # If DecideAction returns \"search\", go to SearchWeb\n    decide - \"search\" >> search\n\n    # If DecideAction returns \"answer\", go to AnswerQuestion\n    decide - \"answer\" >> answer\n\n    # After SearchWeb completes and returns \"decide\", go back to DecideAction\n    search - \"decide\" >> decide\n\n    # Create and return the flow, starting with the DecideAction node\n    return Flow(start=decide)\n```\n\nHere's a breakdown:\n\n*   We import `Flow` from the `pocketflow` library.\n*   We import the `Node` types from our nodes file (`nodes.py`). Don't worry about the implementations of `DecideAction`, `SearchWeb`, and `AnswerQuestion` yet.  Just think of them as steps in our process.\n*   We create instances of each Node.\n*   We use the `>>` and `-` operators to define the transitions between Nodes. For example, `decide - \"search\" >> search` means \"If the `decide` Node returns 'search', then go to the `search` Node.\"\n*   We create a `Flow` object, specifying the `decide` Node as the starting point.\n\n**How it Works: Example**\n\nImagine the agent receives the question: \"What is the capital of France?\".\n\n1.  The `Flow` starts at the `DecideAction` Node.\n2.  The `DecideAction` Node determines that we need to search for the answer. It returns the string \"search\".\n3.  The `Flow` sees the transition `decide - \"search\" >> search` and moves to the `SearchWeb` Node.\n4.  The `SearchWeb` Node searches the web and finds the answer: \"Paris\". It stores the answer in memory and returns the string \"decide\".\n5.  The `Flow` sees the transition `search - \"decide\" >> decide` and moves back to the `DecideAction` Node.\n6.  Now `DecideAction` could determine we need to answer the original question. It returns the string \"answer\".\n7.  The `Flow` sees the transition `decide - \"answer\" >> answer` and moves to the `AnswerQuestion` Node.\n8.  The `AnswerQuestion` node crafts the answer using the information in memory and then returns its final response.\n\n**Under the Hood: A Peek Inside**\n\nLet's see what happens inside the `Flow` when we run it.  Here's a simplified sequence diagram:\n\n```mermaid\nsequenceDiagram\n    participant Flow\n    participant DecideAction\n    participant SearchWeb\n    participant AnswerQuestion\n    participant SharedStore\n\n    Flow->>DecideAction: run(shared_store)\n    DecideAction->>SharedStore: access/modify data\n    DecideAction-->>Flow: \"search\"\n    Flow->>SearchWeb: run(shared_store)\n    SearchWeb->>SharedStore: access/modify data\n    SearchWeb-->>Flow: \"decide\"\n    Flow->>DecideAction: run(shared_store)\n    DecideAction->>SharedStore: access/modify data\n    DecideAction-->>Flow: \"answer\"\n    Flow->>AnswerQuestion: run(shared_store)\n    AnswerQuestion->>SharedStore: access/modify data\n    AnswerQuestion-->>Flow: (End)\n```\n\n*   The `Flow` starts by calling the `run` method of the starting Node (`DecideAction` in our example).\n*   Each Node executes its logic and potentially modifies a `SharedStore` ([Chapter 3: Shared Store](03_shared_store.md)) which is used for passing data between the nodes.\n*   The Node returns a value (like \"search\" or \"answer\") that determines the next Node to execute, based on the transitions defined in the Flow.\n*   This process continues until there are no more transitions, or we hit a dead end (no successor node defined for a particular return value).\n\n**Code Dive: `Flow` Class**\n\nLet's look at some of the key parts of the `Flow` class (from `pocketflow/__init__.py`):\n\n```python\nclass Flow(BaseNode):\n    def __init__(self,start): super().__init__();self.start=start\n    def get_next_node(self,curr,action):\n        nxt=curr.successors.get(action or \"default\")\n        if not nxt and curr.successors: warnings.warn(f\"Flow ends: '{action}' not found in {list(curr.successors)}\")\n        return nxt\n    def _orch(self,shared,params=None):\n        curr,p=copy.copy(self.start),(params or {**self.params})\n        while curr: curr.set_params(p);c=curr._run(shared);curr=copy.copy(self.get_next_node(curr,c))\n    def _run(self,shared): pr=self.prep(shared);self._orch(shared);return self.post(shared,pr,None)\n    def exec(self,prep_res): raise RuntimeError(\"Flow can't exec.\")\n```\n\n*   `__init__(self, start)`: The constructor takes the starting Node as an argument.\n*   `get_next_node(self, curr, action)`:  This method determines the next Node to execute based on the return value (`action`) of the current Node (`curr`). It looks up the successor node in the current node's `successors` dictionary.\n*   `_orch(self, shared, params=None)`:  This is the \"orchestration\" method.  It iterates through the Nodes, executing each one in sequence based on the transitions.\n*   `_run(self, shared)`: This method kicks off the orchestration by calling `_orch(self, shared)`.\n\n**Conclusion**\n\nIn this chapter, we learned about the concept of a Flow in PocketFlow. A Flow is an orchestration layer that connects individual Nodes into a functioning workflow. It defines the sequence in which Nodes are executed and how data is passed between them. We saw how to create a Flow for a simple research agent and how it works under the hood.\n\nNow that we understand Flows, let's dive deeper into the building blocks of Flows: **Nodes**. In [Chapter 2: Node](02_node.md), we'll explore what Nodes are, how to create them, and how they interact with Flows.\n\n---\n# Chapter 2: Node\n\nIn [Chapter 1: Flow](01_flow.md), we learned about Flows and how they orchestrate different actions in a specific sequence. Now, let's zoom in on the individual *actions* themselves. These actions are called **Nodes**.\n\nImagine you're building a robot that can make a sandwich. The Flow would be the robot's overall plan: \"Get bread -> Get fillings -> Assemble sandwich -> Serve sandwich\".  Each of those steps \u2013 \"Get bread\", \"Get fillings\", etc. \u2013 would be a Node.\n\n**What Problem Do Nodes Solve?**\n\nNodes solve the problem of breaking down a complex task into smaller, manageable pieces. Instead of having one giant, complicated function, we can have a series of simpler Nodes, each responsible for a specific job. This makes our code easier to understand, test, and maintain.\n\n**Our Sandwich-Making Robot Example**\n\nLet's focus on the \"Get fillings\" step. This could be further broken down: \"Open refrigerator -> Check for desired fillings -> Take out fillings\".  Each of *those* could even be a Node!  For our purposes, let's say \"Get fillings\" is a single Node that figures out which fillings to get and retrieves them.\n\n**Key Concepts of a Node**\n\nA Node is like a mini-program that does one thing well. It has three main parts:\n\n1.  **Input(s):** What the Node needs to get its job done. In our \"Get fillings\" Node, the input might be a list of preferred fillings.\n\n2.  **Processing:** What the Node *does* with the input. This is the core logic of the Node.  In our example, it's the code that checks what fillings are available and retrieves them.\n\n3.  **Output(s):** What the Node produces after processing the input.  In our example, the output would be the actual fillings.\n\nIn PocketFlow, these three parts are defined using three special methods: `prep`, `exec`, and `post`. Don't worry too much about these names for now. Just remember the input, processing, and output.  We will discuss [Prep, Exec, Post (Node Lifecycle)](04_prep__exec__post__node_lifecycle_.md) in detail later.\n\n**Creating a Simple Node**\n\nLet's create a simple Node that adds two numbers.\n\n```python\nfrom pocketflow import Node\n\nclass AddNumbers(Node):\n    def prep(self, shared):\n        \"\"\"Prepares the input numbers from the shared store.\"\"\"\n        num1 = shared[\"num1\"]\n        num2 = shared[\"num2\"]\n        return num1, num2\n\n    def exec(self, inputs):\n        \"\"\"Adds the two numbers together.\"\"\"\n        num1, num2 = inputs\n        result = num1 + num2\n        return result\n\n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Stores the result in the shared store.\"\"\"\n        shared[\"result\"] = exec_res\n        return None  # No next node, the Flow can end\n```\n\nHere's what's happening:\n\n*   We import the `Node` class from `pocketflow`.\n*   We create a class called `AddNumbers` that *inherits* from `Node`. This means our `AddNumbers` class *is* a Node and has all the properties of a Node.\n*   `prep(self, shared)`: This method *prepares* the input for the `exec` method. It retrieves the two numbers (`num1` and `num2`) from the `shared` store (we'll talk about the [Shared Store](03_shared_store.md) in the next chapter!). It returns these two numbers as a tuple.\n*   `exec(self, inputs)`: This method *executes* the core logic of the Node. It takes the output of `prep` as `inputs`, adds the two numbers together, and returns the `result`.\n*   `post(self, shared, prep_res, exec_res)`: This method *posts* the result to the `shared` store. It takes the output of `exec` as `exec_res` and stores it in the `shared` store under the key \"result\".  It returns `None` in this case because there's no further action required in the flow after adding the numbers.\n\n**Using the Node in a Flow**\n\nNow, let's see how we can use this Node in a Flow:\n\n```python\nfrom pocketflow import Flow\nfrom nodes import AddNumbers # Assuming the AddNumbers node is in nodes.py\n\n# Create an instance of the AddNumbers node\nadd_node = AddNumbers()\n\n# Create a Flow starting with the AddNumbers node\nflow = Flow(start=add_node)\n\n# Create a shared store with the input numbers\nshared_store = {\"num1\": 5, \"num2\": 3}\n\n# Run the flow\nflow.run(shared_store)\n\n# Print the result\nprint(shared_store[\"result\"])  # Output: 8\n```\n\nHere's what's happening:\n\n*   We import `Flow` from `pocketflow`.\n*   We create an instance of our `AddNumbers` Node.\n*   We create a `Flow` that starts with our `AddNumbers` Node.\n*   We create a `shared_store` (a dictionary) and put the input numbers (`num1` and `num2`) into it.\n*   We run the `Flow` with the `shared_store`.\n*   Finally, we print the result, which is stored in the `shared_store` under the key \"result\".\n\n**Let's Trace the Execution**\n\n1.  The `Flow` starts at the `AddNumbers` Node.\n2.  The `AddNumbers` Node's `prep` method is called. It retrieves `num1` (5) and `num2` (3) from the `shared_store`.\n3.  The `AddNumbers` Node's `exec` method is called with the input (5, 3). It adds them together and returns 8.\n4.  The `AddNumbers` Node's `post` method is called with the result (8). It stores the result in the `shared_store` under the key \"result\".\n5.  The `Flow` finishes because the `AddNumbers` Node returned `None` in the `post` method, indicating there are no more Nodes to execute.\n\n**Under the Hood: A Closer Look**\n\nLet's see a simplified sequence diagram of what happens when we run the `Flow`:\n\n```mermaid\nsequenceDiagram\n    participant Flow\n    participant AddNumbers\n    participant SharedStore\n\n    Flow->>AddNumbers: run(shared_store)\n    AddNumbers->>SharedStore: access num1, num2\n    AddNumbers-->>Flow: num1, num2\n    Flow->>AddNumbers: exec(num1, num2)\n    AddNumbers-->>Flow: result\n    Flow->>AddNumbers: post(shared_store, prep_res, result)\n    AddNumbers->>SharedStore: store result\n    AddNumbers-->>Flow: None\n```\n\n*   The `Flow` calls the `run` method of the `AddNumbers` Node, passing the `shared_store`.\n*   The `AddNumbers` Node accesses the `shared_store` to get the input numbers.\n*   The `AddNumbers` Node executes its logic and returns the result to the `Flow`.\n*   The `AddNumbers` Node then updates the `shared_store` with the result.\n\nNow, let's peek at some of the relevant code from the `pocketflow/__init__.py` file:\n\n```python\nclass Node(BaseNode): #Simplified\n    def prep(self,shared): pass\n    def exec(self,prep_res): pass\n    def post(self,shared,prep_res,exec_res): pass\n    def _exec(self,prep_res): return self.exec(prep_res)\n    def _run(self,shared): p=self.prep(shared);e=self._exec(p);return self.post(shared,p,e)\n```\n\n*   `prep`, `exec`, and `post` are methods that you can override in your own Node classes to define the behavior of the Node.\n*   The `_run` method is responsible for calling `prep`, `exec`, and `post` in sequence.\n\n**Example: Back to the Research Agent**\nLet's look at our Research Agent code from [Chapter 1: Flow](01_flow.md).\n\n```python\nfrom pocketflow import Node\nfrom utils import call_llm, search_web\nimport yaml\n\nclass DecideAction(Node):\n    def prep(self, shared):\n        # Prepare the context and question\n        question = shared[\"question\"]\n        context = shared.get(\"context\", \"No previous search\")\n        return question, context\n\n    def exec(self, inputs):\n        # Call LLM to decide search or answer\n        question, context = inputs\n        prompt = f\"\"\"... (LLM prompt) ...\"\"\" #Skipping prompt for brevity\n        response = call_llm(prompt)\n        yaml_str = response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n        decision = yaml.safe_load(yaml_str)\n        return decision\n\n    def post(self, shared, prep_res, exec_res):\n        #Save search query and determine next step\n        if exec_res[\"action\"] == \"search\":\n            shared[\"search_query\"] = exec_res[\"search_query\"]\n        else:\n            shared[\"context\"] = exec_res[\"answer\"]\n        return exec_res[\"action\"]\n```\n\nIn this example, `DecideAction` Node decides whether to search or answer based on the provided question and context. The `prep` function grabs the values from the `shared` store, the `exec` function decides on the next action, and the `post` function save the search query (if applicable) and specifies the name of the next node to proceed to.\n\n**Different Types of Nodes**\n\nWhile we've focused on a basic Node here, PocketFlow offers different types of Nodes for different purposes, such as [BatchNode](05_batchnode.md) for processing multiple items at once, and [AsyncNode](06_asyncnode.md) for handling asynchronous operations.\n\n**Conclusion**\n\nIn this chapter, we learned about Nodes, the fundamental building blocks of PocketFlow. We saw how Nodes encapsulate individual actions, and how they are connected together in a Flow. We learned about the `prep`, `exec`, and `post` methods that define the behavior of a Node. Now that we understand Nodes, let's move on to [Chapter 3: Shared Store](03_shared_store.md) to see how Nodes can share data with each other.\n\n---\n# Chapter 3: Shared Store\n\nIn [Chapter 2: Node](02_node.md), we learned about Nodes and how they perform individual tasks. But how do these Nodes *talk* to each other? How does one Node pass its results to the next Node in the [Flow](01_flow.md)? That's where the **Shared Store** comes in!\n\nImagine a relay race. Each runner (Node) has a specific leg to run. The baton (data) needs to be passed from one runner to the next. The Shared Store is like the designated exchange zone where the baton is safely passed.\n\n**The Problem: Passing Information Between Nodes**\n\nWithout a shared space, each Node would operate in isolation.  They wouldn't know what other Nodes have done, and they couldn't build on each other's work.  The Shared Store provides a central location for Nodes to store and retrieve information, enabling collaboration and data flow within the PocketFlow ecosystem.\n\n**Our Research Agent Example (Again!)**\n\nRemember our research agent from [Chapter 1: Flow](01_flow.md)? The `SearchWeb` Node needs to pass the search results to the `AnswerQuestion` Node. Without a Shared Store, the `AnswerQuestion` Node wouldn't know what the web search found!\n\n**What *is* the Shared Store?**\n\nIn PocketFlow, the Shared Store is essentially a **dictionary**.  If you're new to programming, think of a dictionary as a container that holds key-value pairs. You can store data using a specific \"key\" and later retrieve it using that same key.\n\nFor example, we might store the search results with the key \"search_results\":\n\n```python\nshared_store[\"search_results\"] = \"The capital of France is Paris.\"\n```\n\nLater, another Node can access this information:\n\n```python\ncapital = shared_store[\"search_results\"]\nprint(capital) # Output: The capital of France is Paris.\n```\n\n**Key Concepts of the Shared Store**\n\n1.  **Centralized Data:** It's a single place where all Nodes can access and modify data.\n\n2.  **Key-Value Pairs:** Data is stored as key-value pairs, like a Python dictionary.\n\n3.  **Accessibility:** Any Node in the [Flow](01_flow.md) can access the Shared Store.\n\n4.  **Dynamic:** Nodes can add, modify, and delete data in the Shared Store during the [Flow](01_flow.md)'s execution.\n\n**How to Use the Shared Store**\n\nLet's revisit our `AddNumbers` Node from [Chapter 2: Node](02_node.md) and see how it uses the Shared Store:\n\n```python\nfrom pocketflow import Node\n\nclass AddNumbers(Node):\n    def prep(self, shared):\n        \"\"\"Prepares the input numbers from the shared store.\"\"\"\n        num1 = shared[\"num1\"]\n        num2 = shared[\"num2\"]\n        return num1, num2\n\n    def exec(self, inputs):\n        \"\"\"Adds the two numbers together.\"\"\"\n        num1, num2 = inputs\n        result = num1 + num2\n        return result\n\n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Stores the result in the shared store.\"\"\"\n        shared[\"result\"] = exec_res\n        return None  # No next node, the Flow can end\n```\n\n*   In the `prep` method, the Node retrieves the values associated with the keys \"num1\" and \"num2\" from the `shared` store.\n\n*   In the `post` method, the Node stores the calculated `result` in the `shared` store using the key \"result\".\n\n**Example Usage**\n\nHere's how the Shared Store is used when we run the [Flow](01_flow.md):\n\n```python\nfrom pocketflow import Flow\nfrom nodes import AddNumbers # Assuming the AddNumbers node is in nodes.py\n\n# Create an instance of the AddNumbers node\nadd_node = AddNumbers()\n\n# Create a Flow starting with the AddNumbers node\nflow = Flow(start=add_node)\n\n# Create a shared store with the input numbers\nshared_store = {\"num1\": 5, \"num2\": 3}\n\n# Run the flow\nflow.run(shared_store)\n\n# Print the result\nprint(shared_store[\"result\"])  # Output: 8\n```\n\n*   We create a `shared_store` (a dictionary) and initialize it with the input values for \"num1\" and \"num2\". This is how we *seed* the Shared Store with initial data.\n\n*   After the `flow.run(shared_store)` call, the `shared_store` now contains the key \"result\" with the value 8, because the `AddNumbers` Node stored it there.\n\n**Another Example: Passing the Question to the Answer Node**\nLet's look back at the nodes of our research agent in `cookbook/pocketflow-agent/nodes.py`. The `DecideAction` Node checks whether to search or answer the question and it has access to a \"question\" value in the shared store.\n\n```python\nclass DecideAction(Node):\n    def prep(self, shared):\n        # Prepare the context and question\n        question = shared[\"question\"] #Access question value from shared store\n        context = shared.get(\"context\", \"No previous search\")\n        return question, context\n    #Skipping the other functions for brevity.\n```\n\nIn this case, the `prep` function of `DecideAction` node accesses the question value from the shared store using the key \"question\".\n\n**Important Note:**\n\n*   **Key Existence:** It's good practice to check if a key exists in the Shared Store before accessing it, especially if the Node isn't the one who put the value there. You can use `shared.get(\"key\", default_value)` which returns `default_value` if the key doesn't exist. Or `if \"key\" in shared:` to check for a key's existence.\n\n**Under the Hood: How it Works**\n\nWhen you run a [Flow](01_flow.md), the `Flow` object creates and passes a Shared Store object to each Node's `prep`, `exec`, and `post` methods.\n\nHere's a simplified sequence diagram:\n\n```mermaid\nsequenceDiagram\n    participant Flow\n    participant Node1\n    participant Node2\n    participant SharedStore\n\n    Flow->>SharedStore: Create SharedStore\n    Flow->>Node1: run(SharedStore)\n    Node1->>SharedStore: Access/Modify data\n    Node1-->>Flow: (Next Action)\n    Flow->>Node2: run(SharedStore)\n    Node2->>SharedStore: Access/Modify data\n    Node2-->>Flow: (End)\n```\n\n*   The `Flow` creates a single `SharedStore` instance at the beginning.\n\n*   This same `SharedStore` instance is passed to each `Node` in the [Flow](01_flow.md).\n\n*   Each `Node` can then read from and write to the `SharedStore`.\n\n**Code Snippet: Flow Passing the Shared Store (From `pocketflow/__init__.py`)**\nThis is how the [Flow](01_flow.md) passes the Shared Store to each node:\n\n```python\nclass Flow(BaseNode):\n    def __init__(self,start): super().__init__();self.start=start\n    def get_next_node(self,curr,action):\n        nxt=curr.successors.get(action or \"default\")\n        if not nxt and curr.successors: warnings.warn(f\"Flow ends: '{action}' not found in {list(curr.successors)}\")\n        return nxt\n    def _orch(self,shared,params=None):\n        curr,p=copy.copy(self.start),(params or {**self.params})\n        while curr: curr.set_params(p);c=curr._run(shared);curr=copy.copy(self.get_next_node(curr,c))\n    def _run(self,shared): pr=self.prep(shared);self._orch(shared);return self.post(shared,pr,None)\n    def exec(self,prep_res): raise RuntimeError(\"Flow can't exec.\")\n```\nIn the `_orch` function, the shared store is maintained and passed into each subsequent node. Note the line `c=curr._run(shared)` indicating the shared store is passed into the node's run function.\n\n**Conclusion**\n\nIn this chapter, we learned about the Shared Store, a crucial component of PocketFlow that allows Nodes to communicate and share data. We saw how the Shared Store acts as a central memory, enabling seamless data transfer between processing steps. We learned how Nodes access and modify data in the Shared Store, and how the Flow manages the Shared Store behind the scenes.\n\nNow that we understand the Shared Store, we can appreciate how PocketFlow Nodes interact within a Flow to accomplish complex tasks. In the next chapter, [Prep, Exec, Post (Node Lifecycle)](04_prep__exec__post__node_lifecycle_.md), we'll dive deeper into the `prep`, `exec`, and `post` methods and understand the lifecycle of a Node.\n\n\nRelevant Code Snippets:\n--- File: cookbook/pocketflow-agent/nodes.py ---\nfrom pocketflow import Node\nfrom utils import call_llm, search_web\nimport yaml\n\nclass DecideAction(Node):\n    def prep(self, shared):\n        \"\"\"Prepare the context and question for the decision-making process.\"\"\"\n        # Get the current context (default to \"No previous search\" if none exists)\n        context = shared.get(\"context\", \"No previous search\")\n        # Get the question from the shared store\n        question = shared[\"question\"]\n        # Return both for the exec step\n        return question, context\n        \n    def exec(self, inputs):\n        \"\"\"Call the LLM to decide whether to search or answer.\"\"\"\n        question, context = inputs\n        \n        print(f\"\ud83e\udd14 Agent deciding what to do next...\")\n        \n        # Create a prompt to help the LLM decide what to do next with proper yaml formatting\n        prompt = f\"\"\"\n### CONTEXT\nYou are a research assistant that can search the web.\nQuestion: {question}\nPrevious Research: {context}\n\n### ACTION SPACE\n[1] search\n  Description: Look up more information on the web\n  Parameters:\n    - query (str): What to search for\n\n[2] answer\n  Description: Answer the question with current knowledge\n  Parameters:\n    - answer (str): Final answer to the question\n\n## NEXT ACTION\nDecide the next action based on the context and available actions.\nReturn your response in this format:\n\n```yaml\nthinking: |\n    <your step-by-step reasoning process>\naction: search OR answer\nreason: <why you chose this action>\nanswer: <if action is answer>\nsearch_query: <specific search query if action is search>\n```\nIMPORTANT: Make sure to:\n1. Use proper indentation (4 spaces) for all multi-line fields\n2. Use the | character for multi-line text fields\n3. Keep single-line fields without the | character\n\"\"\"\n        \n        # Call the LLM to make a decision\n        response = call_llm(prompt)\n        \n        # Parse the response to get the decision\n        yaml_str = response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n        decision = yaml.safe_load(yaml_str)\n        \n        return decision\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Save the decision and determine the next step in the flow.\"\"\"\n        # If LLM decided to search, save the search query\n        if exec_res[\"action\"] == \"search\":\n            shared[\"search_query\"] = exec_res[\"search_query\"]\n            print(f\"\ud83d\udd0d Agent decided to search for: {exec_res['search_query']}\")\n        else:\n            shared[\"context\"] = exec_res[\"answer\"] #save the context if LLM gives the answer without searching.\n            print(f\"\ud83d\udca1 Agent decided to answer the question\")\n        \n        # Return the action to determine the next node in the flow\n        return exec_res[\"action\"]\n\nclass SearchWeb(Node):\n    def prep(self, shared):\n        \"\"\"Get the search query from the shared store.\"\"\"\n        return shared[\"search_query\"]\n        \n    def exec(self, search_query):\n        \"\"\"Search the web for the given query.\"\"\"\n        # Call the search utility function\n        print(f\"\ud83c\udf10 Searching the web for: {search_query}\")\n        results = search_web(search_query)\n        return results\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Save the search results and go back to the decision node.\"\"\"\n        # Add the search results to the context in the shared store\n        previous = shared.get(\"context\", \"\")\n        shared[\"context\"] = previous + \"\\n\\nSEARCH: \" + shared[\"search_query\"] + \"\\nRESULTS: \" + exec_res\n        \n        print(f\"\ud83d\udcda Found information, analyzing results...\")\n        \n        # Always go back to the decision node after searching\n        return \"decide\"\n\nclass AnswerQuestion(Node):\n    def prep(self, shared):\n        \"\"\"Get the question and context for answering.\"\"\"\n        return shared[\"question\"], shared.get(\"context\", \"\")\n        \n    def exec(self, inputs):\n        \"\"\"Call the LLM to generate a final answer.\"\"\"\n        question, context = inputs\n        \n        print(f\"\u270d\ufe0f Crafting final answer...\")\n        \n        # Create a prompt for the LLM to answer the question\n        prompt = f\"\"\"\n### CONTEXT\nBased on the following information, answer the question.\nQuestion: {question}\nResearch: {context}\n\n## YOUR ANSWER:\nProvide a comprehensive answer using the research results.\n\"\"\"\n        # Call the LLM to generate an answer\n        answer = call_llm(prompt)\n        return answer\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Save the final answer and complete the flow.\"\"\"\n        # Save the answer in the shared store\n        shared[\"answer\"] = exec_res\n        \n        print(f\"\u2705 Answer generated successfully\")\n        \n        # We're done - no need to continue the flow\n        return \"done\" \n\n\n--- File: pocketflow/__init__.py ---\nimport asyncio, warnings, copy, time\n\nclass BaseNode:\n    def __init__(self): self.params,self.successors={},{}\n    def set_params(self,params): self.params=params\n    def add_successor(self,node,action=\"default\"):\n        if action in self.successors: warnings.warn(f\"Overwriting successor for action '{action}'\")\n        self.successors[action]=node;return node\n    def prep(self,shared): pass\n    def exec(self,prep_res): pass\n    def post(self,shared,prep_res,exec_res): pass\n    def _exec(self,prep_res): return self.exec(prep_res)\n    def _run(self,shared): p=self.prep(shared);e=self._exec(p);return self.post(shared,p,e)\n    def run(self,shared): \n        if self.successors: warnings.warn(\"Node won't run successors. Use Flow.\")  \n        return self._run(shared)\n    def __rshift__(self,other): return self.add_successor(other)\n    def __sub__(self,action):\n        if isinstance(action,str): return _ConditionalTransition(self,action)\n        raise TypeError(\"Action must be a string\")\n\nclass _ConditionalTransition:\n    def __init__(self,src,action): self.src,self.action=src,action\n    def __rshift__(self,tgt): return self.src.add_successor(tgt,self.action)\n\nclass Node(BaseNode):\n    def __init__(self,max_retries=1,wait=0): super().__init__();self.max_retries,self.wait=max_retries,wait\n    def exec_fallback(self,prep_res,exc): raise exc\n    def _exec(self,prep_res):\n        for self.cur_retry in range(self.max_retries):\n            try: return self.exec(prep_res)\n            except Exception as e:\n                if self.cur_retry==self.max_retries-1: return self.exec_fallback(prep_res,e)\n                if self.wait>0: time.sleep(self.wait)\n\nclass BatchNode(Node):\n    def _exec(self,items): return [super(BatchNode,self)._exec(i) for i in (items or [])]\n\nclass Flow(BaseNode):\n    def __init__(self,start): super().__init__();self.start=start\n    def get_next_node(self,curr,action):\n        nxt=curr.successors.get(action or \"default\")\n        if not nxt and curr.successors: warnings.warn(f\"Flow ends: '{action}' not found in {list(curr.successors)}\")\n        return nxt\n    def _orch(self,shared,params=None):\n        curr,p=copy.copy(self.start),(params or {**self.params})\n        while curr: curr.set_params(p);c=curr._run(shared);curr=copy.copy(self.get_next_node(curr,c))\n    def _run(self,shared): pr=self.prep(shared);self._orch(shared);return self.post(shared,pr,None)\n    def exec(self,prep_res): raise RuntimeError(\"Flow can't exec.\")\n\nclass BatchFlow(Flow):\n    def _run(self,shared):\n        pr=self.prep(shared) or []\n        for bp in pr: self._orch(shared,{**self.params,**bp})\n        return self.post(shared,pr,None)\n\nclass AsyncNode(Node):\n    def prep(self,shared): raise RuntimeError(\"Use prep_async.\")\n    def exec(self,prep_res): raise RuntimeError(\"Use exec_async.\")\n    def post(self,shared,prep_res,exec_res): raise RuntimeError(\"Use post_async.\")\n    def exec_fallback(self,prep_res,exc): raise RuntimeError(\"Use exec_fallback_async.\")\n    def _run(self,shared): raise RuntimeError(\"Use run_async.\")\n    async def prep_async(self,shared): pass\n    async def exec_async(self,prep_res): pass\n    async def exec_fallback_async(self,prep_res,exc): raise exc\n    async def post_async(self,shared,prep_res,exec_res): pass\n    async def _exec(self,prep_res): \n        for i in range(self.max_retries):\n            try: return await self.exec_async(prep_res)\n            except Exception as e:\n                if i==self.max_retries-1: return await self.exec_fallback_async(prep_res,e)\n                if self.wait>0: await asyncio.sleep(self.wait)\n    async def run_async(self,shared): \n        if self.successors: warnings.warn(\"Node won't run successors. Use AsyncFlow.\")  \n        return await self._run_async(shared)\n    async def _run_async(self,shared): p=await self.prep_async(shared);e=await self._exec(p);return await self.post_async(shared,p,e)\n\nclass AsyncBatchNode(AsyncNode,BatchNode):\n    async def _exec(self,items): return [await super(AsyncBatchNode,self)._exec(i) for i in items]\n\nclass AsyncParallelBatchNode(AsyncNode,BatchNode):\n    async def _exec(self,items): return await asyncio.gather(*(super(AsyncParallelBatchNode,self)._exec(i) for i in items))\n\nclass AsyncFlow(Flow,AsyncNode):\n    async def _orch_async(self,shared,params=None):\n        curr,p=copy.copy(self.start),(params or {**self.params})\n        while curr:curr.set_params(p);c=await curr._run_async(shared) if isinstance(curr,AsyncNode) else curr._run(shared);curr=copy.copy(self.get_next_node(curr,c))\n    async def _run_async(self,shared): p=await self.prep_async(shared);await self._orch_async(shared);return await self.post_async(shared,p,None)\n\nclass AsyncBatchFlow(AsyncFlow,BatchFlow):\n    async def _run_async(self,shared):\n        pr=await self.prep_async(shared) or []\n        for bp in pr: await self._orch_async(shared,{**self.params,**bp})\n        return await self.post_async(shared,pr,None)\n\nclass AsyncParallelBatchFlow(AsyncFlow,BatchFlow):\n    async def _run_async(self,shared):\n        pr=await self.prep_async(shared) or []\n        await asyncio.gather(*(self._orch_async(shared,{**self.params,**bp}) for bp in pr))\n        return await self.post_async(shared,pr,None)\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 4: Prep, Exec, Post (Node Lifecycle)`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "# Chapter 4: Prep, Exec, Post (Node Lifecycle)\n\nIn [Chapter 2: Node](02_node.md), we learned about Nodes and how they encapsulate individual actions. We briefly touched upon the `prep`, `exec`, and `post` methods. Now, let's dive deeper into what these methods do and how they define the lifecycle of a Node.\n\nImagine you're a chef preparing a dish.\n\n*   **Prep:** You gather your ingredients, chop vegetables, and prepare your cooking station.\n*   **Exec:** You actually cook the dish \u2013 the main action.\n*   **Post:** You plate the dish, add garnishes, and serve it.\n\nThe `prep`, `exec`, and `post` methods in a Node are similar! They provide a structured way to define the different stages of a Node's operation.\n\n**The Problem: Organizing Node Logic**\n\nWithout a clear structure, the code inside a Node can become disorganized and hard to understand.  The `prep`, `exec`, and `post` pattern provides a consistent way to organize Node logic, making it easier to read, maintain, and debug.\n\n**Our Research Agent Example (One More Time!)**\n\nLet's revisit our research agent and focus on the `SearchWeb` Node. It needs to:\n\n1.  **Prep:** Get the search query from the [Shared Store](03_shared_store.md).\n2.  **Exec:** Actually perform the web search using the query.\n3.  **Post:** Save the search results back into the [Shared Store](03_shared_store.md) and decide what's next.\n\n**Key Concepts: Prep, Exec, Post**\n\n*   **Prep (Preparation):**  This method prepares the input data needed for the main task (`exec`). It typically retrieves data from the [Shared Store](03_shared_store.md) or performs any necessary data transformations.\n    *   **Input:** The [Shared Store](03_shared_store.md).\n    *   **Output:**  Data that the `exec` method needs (can be any Python object).\n\n*   **Exec (Execution):** This is the heart of the Node. It performs the core logic of the Node, using the data prepared by the `prep` method.\n    *   **Input:** The output of the `prep` method.\n    *   **Output:** The result of the Node's main operation (can be any Python object).\n\n*   **Post (Post-processing):** This method processes the output of the `exec` method. It typically saves the results back into the [Shared Store](03_shared_store.md), performs any cleanup, and determines the next step in the [Flow](01_flow.md). It specifies which Node to run next in the flow.\n    *   **Input:** The [Shared Store](03_shared_store.md), the output of `prep`, and the output of `exec`.\n    *   **Output:** A value that determines which Node to execute next in the [Flow](01_flow.md). If it returns `None`, the flow can end.\n\n**Example: Back to the `AddNumbers` Node**\n\nLet's examine the `AddNumbers` Node again:\n\n```python\nfrom pocketflow import Node\n\nclass AddNumbers(Node):\n    def prep(self, shared):\n        \"\"\"Prepares the input numbers from the shared store.\"\"\"\n        num1 = shared[\"num1\"]\n        num2 = shared[\"num2\"]\n        return num1, num2\n\n    def exec(self, inputs):\n        \"\"\"Adds the two numbers together.\"\"\"\n        num1, num2 = inputs\n        result = num1 + num2\n        return result\n\n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Stores the result in the shared store.\"\"\"\n        shared[\"result\"] = exec_res\n        return None  # No next node, the Flow can end\n```\n\n*   **`prep`**: Retrieves `num1` and `num2` from the [Shared Store](03_shared_store.md) and returns them as a tuple.\n*   **`exec`**: Takes the tuple from `prep`, adds the numbers, and returns the `result`.\n*   **`post`**: Stores the `result` in the [Shared Store](03_shared_store.md) and returns `None`, indicating the end of the [Flow](01_flow.md).\n\n**Another Example: `SearchWeb` Node from the Research Agent**\n\n```python\nfrom pocketflow import Node\nfrom utils import search_web # Assuming you have a search_web utility\n\nclass SearchWeb(Node):\n    def prep(self, shared):\n        \"\"\"Get the search query from the shared store.\"\"\"\n        return shared[\"search_query\"]\n        \n    def exec(self, search_query):\n        \"\"\"Search the web for the given query.\"\"\"\n        print(f\"\ud83c\udf10 Searching the web for: {search_query}\")\n        results = search_web(search_query)\n        return results\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Save the search results and go back to the decision node.\"\"\"\n        shared[\"context\"] = \"SEARCH RESULTS: \" + exec_res\n        return \"decide\" # Back to DecideAction node\n```\n\n*   **`prep`**:  Retrieves the `search_query` from the [Shared Store](03_shared_store.md).\n*   **`exec`**:  Performs the web search using `search_web` and returns the `results`.\n*   **`post`**: Saves the `results` into the [Shared Store](03_shared_store.md) under the key \"context\", and returns `\"decide\"` to tell the [Flow](01_flow.md) to go back to `DecideAction` ([Chapter 1: Flow](01_flow.md)).\n\n**How It Works: Example Execution**\n\nLet's trace the execution of the `SearchWeb` Node:\n\n1.  The [Flow](01_flow.md) calls the `run` method of the `SearchWeb` Node, passing the [Shared Store](03_shared_store.md).\n2.  The `prep` method retrieves the `search_query` (e.g., \"capital of France\") from the [Shared Store](03_shared_store.md).\n3.  The `exec` method uses the `search_query` to search the web. Let's say it gets the result \"Paris is the capital of France.\"\n4.  The `post` method saves the search result \"Paris is the capital of France.\" into the [Shared Store](03_shared_store.md). Also the return value is \"decide\".\n5.  The [Flow](01_flow.md) then uses the `\"decide\"` return value to determine the next Node to run.\n\n**Under the Hood: The `_run` Method**\n\nThe `Node` class in `pocketflow/__init__.py` has a `_run` method that orchestrates the `prep`, `exec`, and `post` methods:\n\n```python\nclass Node(BaseNode): #Simplified\n    def prep(self,shared): pass\n    def exec(self,prep_res): pass\n    def post(self,shared,prep_res,exec_res): pass\n    def _exec(self,prep_res): return self.exec(prep_res)\n    def _run(self,shared): p=self.prep(shared);e=self._exec(p);return self.post(shared,p,e)\n```\n\nAs you can see, `_run` calls `prep`, then `exec`, and finally `post` in sequence.  It passes the output of one method as input to the next.\n\nLet's visualize this with a sequence diagram:\n\n```mermaid\nsequenceDiagram\n    participant Node\n    participant SharedStore\n\n    Node->>Node: prep(shared_store)\n    Node-->>Node: prep_result\n    Node->>Node: exec(prep_result)\n    Node-->>Node: exec_result\n    Node->>Node: post(shared_store, prep_result, exec_result)\n    Node-->>Node: next_action\n```\n\n1.  `prep(shared_store)`: The `prep` method is called with the [Shared Store](03_shared_store.md).\n2.  `exec(prep_result)`: The `exec` method is called with the result of `prep`.\n3.  `post(shared_store, prep_result, exec_result)`: The `post` method is called with the [Shared Store](03_shared_store.md), the result of `prep`, and the result of `exec`. The `post` function's return value (next_action) controls what node to run next.\n\n**Why This Structure Matters**\n\n*   **Organization:** Clearly separates data preparation, core logic, and post-processing.\n*   **Testability:**  Each method can be tested independently.\n*   **Reusability:** Nodes become more modular and easier to reuse in different [Flows](01_flow.md).\n*   **Readability:**  The `prep`, `exec`, `post` structure makes the code easier to understand.\n\n**Conclusion**\n\nIn this chapter, we learned about the `prep`, `exec`, and `post` methods and how they define the lifecycle of a Node.  We saw how this structure helps organize Node logic, making it more readable, testable, and reusable. We saw how each function affects the [Shared Store](03_shared_store.md) and how the `post` function returns the next node to execute.\n\nNow that we understand the Node lifecycle, let's move on to [Chapter 5: BatchNode](05_batchnode.md) to see how we can process multiple items in a single Node using `BatchNode`.\n", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `PocketFlow` about the concept: \"BatchNode\". This is Chapter 5.\n\nConcept Details:\n- Description:\nEnables efficient processing of multiple independent data items in a single node, like an assembly line.  It applies the same `exec` function to each item in a batch, allowing for parallel or sequential processing depending on the underlying implementation. Prep splits the input into chunks and the exec stage processes them in batch.\n\n\nComplete Tutorial Structure:\n1. [Flow](01_flow.md)\n2. [Node](02_node.md)\n3. [Shared Store](03_shared_store.md)\n4. [Prep, Exec, Post (Node Lifecycle)](04_prep__exec__post__node_lifecycle_.md)\n5. [BatchNode](05_batchnode.md)\n6. [AsyncNode](06_asyncnode.md)\n7. [AsyncFlow](07_asyncflow.md)\n8. [AsyncParallelBatchNode](08_asyncparallelbatchnode.md)\n\nContext from previous chapters (summary):\n# Chapter 1: Flow\n\nImagine you're directing a play. You have actors, each with specific roles (like answering a question or searching the web). But how do you tell them *when* to act, and *what* to do next?  That's where a **Flow** comes in!\n\nIn PocketFlow, a Flow is like the conductor of an orchestra. It's the orchestration layer that connects individual actions (we call them **Nodes**) into a functioning workflow. It defines the sequence in which these actions are executed and how data is passed between them.\n\n**Our Use Case: A Simple Research Agent**\n\nLet's say we're building a simple research agent that can either search the web or answer a question, depending on what's needed.  The agent will first *decide* what to do. If it needs to search, it will *search the web*. If it needs to answer, it will *answer the question*. After searching, it goes back to the deciding step.\n\nWithout a Flow, we'd have to manually call each action in the right order, which can get messy quickly. A Flow lets us define this process in a clear and organized way.\n\n**Key Concepts of a Flow**\n\n1.  **Nodes:** These are the individual steps or actions in our workflow. Think of them as the actors in our play. We'll dive deep into Nodes in [Chapter 2: Node](02_node.md). For now, just think of them as individual units of work.\n\n2.  **Sequence:** The Flow defines the order in which Nodes are executed.  It's the script of our play.\n\n3.  **Data Passing:** The Flow manages how data is passed between Nodes. This is how the actors communicate and share information.\n\n4.  **Starting Node:** Every flow must have a start node to kick off the process.\n\n5.  **Transitions:**  These define how the Flow moves from one Node to the next. We'll use special operators like `>>` (rshift) and `-` (subtraction) to define these transitions.\n\n**Creating Our Research Agent Flow**\n\nLet's look at how we can create a Flow for our research agent. The code might look like this (from `cookbook/pocketflow-agent/flow.py`):\n\n```python\nfrom pocketflow import Flow\nfrom nodes import DecideAction, SearchWeb, AnswerQuestion\n\ndef create_agent_flow():\n    # Create instances of each node\n    decide = DecideAction()\n    search = SearchWeb()\n    answer = AnswerQuestion()\n\n    # Connect the nodes\n    # If DecideAction returns \"search\", go to SearchWeb\n    decide - \"search\" >> search\n\n    # If DecideAction returns \"answer\", go to AnswerQuestion\n    decide - \"answer\" >> answer\n\n    # After SearchWeb completes and returns \"decide\", go back to DecideAction\n    search - \"decide\" >> decide\n\n    # Create and return the flow, starting with the DecideAction node\n    return Flow(start=decide)\n```\n\nHere's a breakdown:\n\n*   We import `Flow` from the `pocketflow` library.\n*   We import the `Node` types from our nodes file (`nodes.py`). Don't worry about the implementations of `DecideAction`, `SearchWeb`, and `AnswerQuestion` yet.  Just think of them as steps in our process.\n*   We create instances of each Node.\n*   We use the `>>` and `-` operators to define the transitions between Nodes. For example, `decide - \"search\" >> search` means \"If the `decide` Node returns 'search', then go to the `search` Node.\"\n*   We create a `Flow` object, specifying the `decide` Node as the starting point.\n\n**How it Works: Example**\n\nImagine the agent receives the question: \"What is the capital of France?\".\n\n1.  The `Flow` starts at the `DecideAction` Node.\n2.  The `DecideAction` Node determines that we need to search for the answer. It returns the string \"search\".\n3.  The `Flow` sees the transition `decide - \"search\" >> search` and moves to the `SearchWeb` Node.\n4.  The `SearchWeb` Node searches the web and finds the answer: \"Paris\". It stores the answer in memory and returns the string \"decide\".\n5.  The `Flow` sees the transition `search - \"decide\" >> decide` and moves back to the `DecideAction` Node.\n6.  Now `DecideAction` could determine we need to answer the original question. It returns the string \"answer\".\n7.  The `Flow` sees the transition `decide - \"answer\" >> answer` and moves to the `AnswerQuestion` Node.\n8.  The `AnswerQuestion` node crafts the answer using the information in memory and then returns its final response.\n\n**Under the Hood: A Peek Inside**\n\nLet's see what happens inside the `Flow` when we run it.  Here's a simplified sequence diagram:\n\n```mermaid\nsequenceDiagram\n    participant Flow\n    participant DecideAction\n    participant SearchWeb\n    participant AnswerQuestion\n    participant SharedStore\n\n    Flow->>DecideAction: run(shared_store)\n    DecideAction->>SharedStore: access/modify data\n    DecideAction-->>Flow: \"search\"\n    Flow->>SearchWeb: run(shared_store)\n    SearchWeb->>SharedStore: access/modify data\n    SearchWeb-->>Flow: \"decide\"\n    Flow->>DecideAction: run(shared_store)\n    DecideAction->>SharedStore: access/modify data\n    DecideAction-->>Flow: \"answer\"\n    Flow->>AnswerQuestion: run(shared_store)\n    AnswerQuestion->>SharedStore: access/modify data\n    AnswerQuestion-->>Flow: (End)\n```\n\n*   The `Flow` starts by calling the `run` method of the starting Node (`DecideAction` in our example).\n*   Each Node executes its logic and potentially modifies a `SharedStore` ([Chapter 3: Shared Store](03_shared_store.md)) which is used for passing data between the nodes.\n*   The Node returns a value (like \"search\" or \"answer\") that determines the next Node to execute, based on the transitions defined in the Flow.\n*   This process continues until there are no more transitions, or we hit a dead end (no successor node defined for a particular return value).\n\n**Code Dive: `Flow` Class**\n\nLet's look at some of the key parts of the `Flow` class (from `pocketflow/__init__.py`):\n\n```python\nclass Flow(BaseNode):\n    def __init__(self,start): super().__init__();self.start=start\n    def get_next_node(self,curr,action):\n        nxt=curr.successors.get(action or \"default\")\n        if not nxt and curr.successors: warnings.warn(f\"Flow ends: '{action}' not found in {list(curr.successors)}\")\n        return nxt\n    def _orch(self,shared,params=None):\n        curr,p=copy.copy(self.start),(params or {**self.params})\n        while curr: curr.set_params(p);c=curr._run(shared);curr=copy.copy(self.get_next_node(curr,c))\n    def _run(self,shared): pr=self.prep(shared);self._orch(shared);return self.post(shared,pr,None)\n    def exec(self,prep_res): raise RuntimeError(\"Flow can't exec.\")\n```\n\n*   `__init__(self, start)`: The constructor takes the starting Node as an argument.\n*   `get_next_node(self, curr, action)`:  This method determines the next Node to execute based on the return value (`action`) of the current Node (`curr`). It looks up the successor node in the current node's `successors` dictionary.\n*   `_orch(self, shared, params=None)`:  This is the \"orchestration\" method.  It iterates through the Nodes, executing each one in sequence based on the transitions.\n*   `_run(self, shared)`: This method kicks off the orchestration by calling `_orch(self, shared)`.\n\n**Conclusion**\n\nIn this chapter, we learned about the concept of a Flow in PocketFlow. A Flow is an orchestration layer that connects individual Nodes into a functioning workflow. It defines the sequence in which Nodes are executed and how data is passed between them. We saw how to create a Flow for a simple research agent and how it works under the hood.\n\nNow that we understand Flows, let's dive deeper into the building blocks of Flows: **Nodes**. In [Chapter 2: Node](02_node.md), we'll explore what Nodes are, how to create them, and how they interact with Flows.\n\n---\n# Chapter 2: Node\n\nIn [Chapter 1: Flow](01_flow.md), we learned about Flows and how they orchestrate different actions in a specific sequence. Now, let's zoom in on the individual *actions* themselves. These actions are called **Nodes**.\n\nImagine you're building a robot that can make a sandwich. The Flow would be the robot's overall plan: \"Get bread -> Get fillings -> Assemble sandwich -> Serve sandwich\".  Each of those steps \u2013 \"Get bread\", \"Get fillings\", etc. \u2013 would be a Node.\n\n**What Problem Do Nodes Solve?**\n\nNodes solve the problem of breaking down a complex task into smaller, manageable pieces. Instead of having one giant, complicated function, we can have a series of simpler Nodes, each responsible for a specific job. This makes our code easier to understand, test, and maintain.\n\n**Our Sandwich-Making Robot Example**\n\nLet's focus on the \"Get fillings\" step. This could be further broken down: \"Open refrigerator -> Check for desired fillings -> Take out fillings\".  Each of *those* could even be a Node!  For our purposes, let's say \"Get fillings\" is a single Node that figures out which fillings to get and retrieves them.\n\n**Key Concepts of a Node**\n\nA Node is like a mini-program that does one thing well. It has three main parts:\n\n1.  **Input(s):** What the Node needs to get its job done. In our \"Get fillings\" Node, the input might be a list of preferred fillings.\n\n2.  **Processing:** What the Node *does* with the input. This is the core logic of the Node.  In our example, it's the code that checks what fillings are available and retrieves them.\n\n3.  **Output(s):** What the Node produces after processing the input.  In our example, the output would be the actual fillings.\n\nIn PocketFlow, these three parts are defined using three special methods: `prep`, `exec`, and `post`. Don't worry too much about these names for now. Just remember the input, processing, and output.  We will discuss [Prep, Exec, Post (Node Lifecycle)](04_prep__exec__post__node_lifecycle_.md) in detail later.\n\n**Creating a Simple Node**\n\nLet's create a simple Node that adds two numbers.\n\n```python\nfrom pocketflow import Node\n\nclass AddNumbers(Node):\n    def prep(self, shared):\n        \"\"\"Prepares the input numbers from the shared store.\"\"\"\n        num1 = shared[\"num1\"]\n        num2 = shared[\"num2\"]\n        return num1, num2\n\n    def exec(self, inputs):\n        \"\"\"Adds the two numbers together.\"\"\"\n        num1, num2 = inputs\n        result = num1 + num2\n        return result\n\n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Stores the result in the shared store.\"\"\"\n        shared[\"result\"] = exec_res\n        return None  # No next node, the Flow can end\n```\n\nHere's what's happening:\n\n*   We import the `Node` class from `pocketflow`.\n*   We create a class called `AddNumbers` that *inherits* from `Node`. This means our `AddNumbers` class *is* a Node and has all the properties of a Node.\n*   `prep(self, shared)`: This method *prepares* the input for the `exec` method. It retrieves the two numbers (`num1` and `num2`) from the `shared` store (we'll talk about the [Shared Store](03_shared_store.md) in the next chapter!). It returns these two numbers as a tuple.\n*   `exec(self, inputs)`: This method *executes* the core logic of the Node. It takes the output of `prep` as `inputs`, adds the two numbers together, and returns the `result`.\n*   `post(self, shared, prep_res, exec_res)`: This method *posts* the result to the `shared` store. It takes the output of `exec` as `exec_res` and stores it in the `shared` store under the key \"result\".  It returns `None` in this case because there's no further action required in the flow after adding the numbers.\n\n**Using the Node in a Flow**\n\nNow, let's see how we can use this Node in a Flow:\n\n```python\nfrom pocketflow import Flow\nfrom nodes import AddNumbers # Assuming the AddNumbers node is in nodes.py\n\n# Create an instance of the AddNumbers node\nadd_node = AddNumbers()\n\n# Create a Flow starting with the AddNumbers node\nflow = Flow(start=add_node)\n\n# Create a shared store with the input numbers\nshared_store = {\"num1\": 5, \"num2\": 3}\n\n# Run the flow\nflow.run(shared_store)\n\n# Print the result\nprint(shared_store[\"result\"])  # Output: 8\n```\n\nHere's what's happening:\n\n*   We import `Flow` from `pocketflow`.\n*   We create an instance of our `AddNumbers` Node.\n*   We create a `Flow` that starts with our `AddNumbers` Node.\n*   We create a `shared_store` (a dictionary) and put the input numbers (`num1` and `num2`) into it.\n*   We run the `Flow` with the `shared_store`.\n*   Finally, we print the result, which is stored in the `shared_store` under the key \"result\".\n\n**Let's Trace the Execution**\n\n1.  The `Flow` starts at the `AddNumbers` Node.\n2.  The `AddNumbers` Node's `prep` method is called. It retrieves `num1` (5) and `num2` (3) from the `shared_store`.\n3.  The `AddNumbers` Node's `exec` method is called with the input (5, 3). It adds them together and returns 8.\n4.  The `AddNumbers` Node's `post` method is called with the result (8). It stores the result in the `shared_store` under the key \"result\".\n5.  The `Flow` finishes because the `AddNumbers` Node returned `None` in the `post` method, indicating there are no more Nodes to execute.\n\n**Under the Hood: A Closer Look**\n\nLet's see a simplified sequence diagram of what happens when we run the `Flow`:\n\n```mermaid\nsequenceDiagram\n    participant Flow\n    participant AddNumbers\n    participant SharedStore\n\n    Flow->>AddNumbers: run(shared_store)\n    AddNumbers->>SharedStore: access num1, num2\n    AddNumbers-->>Flow: num1, num2\n    Flow->>AddNumbers: exec(num1, num2)\n    AddNumbers-->>Flow: result\n    Flow->>AddNumbers: post(shared_store, prep_res, result)\n    AddNumbers->>SharedStore: store result\n    AddNumbers-->>Flow: None\n```\n\n*   The `Flow` calls the `run` method of the `AddNumbers` Node, passing the `shared_store`.\n*   The `AddNumbers` Node accesses the `shared_store` to get the input numbers.\n*   The `AddNumbers` Node executes its logic and returns the result to the `Flow`.\n*   The `AddNumbers` Node then updates the `shared_store` with the result.\n\nNow, let's peek at some of the relevant code from the `pocketflow/__init__.py` file:\n\n```python\nclass Node(BaseNode): #Simplified\n    def prep(self,shared): pass\n    def exec(self,prep_res): pass\n    def post(self,shared,prep_res,exec_res): pass\n    def _exec(self,prep_res): return self.exec(prep_res)\n    def _run(self,shared): p=self.prep(shared);e=self._exec(p);return self.post(shared,p,e)\n```\n\n*   `prep`, `exec`, and `post` are methods that you can override in your own Node classes to define the behavior of the Node.\n*   The `_run` method is responsible for calling `prep`, `exec`, and `post` in sequence.\n\n**Example: Back to the Research Agent**\nLet's look at our Research Agent code from [Chapter 1: Flow](01_flow.md).\n\n```python\nfrom pocketflow import Node\nfrom utils import call_llm, search_web\nimport yaml\n\nclass DecideAction(Node):\n    def prep(self, shared):\n        # Prepare the context and question\n        question = shared[\"question\"]\n        context = shared.get(\"context\", \"No previous search\")\n        return question, context\n\n    def exec(self, inputs):\n        # Call LLM to decide search or answer\n        question, context = inputs\n        prompt = f\"\"\"... (LLM prompt) ...\"\"\" #Skipping prompt for brevity\n        response = call_llm(prompt)\n        yaml_str = response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n        decision = yaml.safe_load(yaml_str)\n        return decision\n\n    def post(self, shared, prep_res, exec_res):\n        #Save search query and determine next step\n        if exec_res[\"action\"] == \"search\":\n            shared[\"search_query\"] = exec_res[\"search_query\"]\n        else:\n            shared[\"context\"] = exec_res[\"answer\"]\n        return exec_res[\"action\"]\n```\n\nIn this example, `DecideAction` Node decides whether to search or answer based on the provided question and context. The `prep` function grabs the values from the `shared` store, the `exec` function decides on the next action, and the `post` function save the search query (if applicable) and specifies the name of the next node to proceed to.\n\n**Different Types of Nodes**\n\nWhile we've focused on a basic Node here, PocketFlow offers different types of Nodes for different purposes, such as [BatchNode](05_batchnode.md) for processing multiple items at once, and [AsyncNode](06_asyncnode.md) for handling asynchronous operations.\n\n**Conclusion**\n\nIn this chapter, we learned about Nodes, the fundamental building blocks of PocketFlow. We saw how Nodes encapsulate individual actions, and how they are connected together in a Flow. We learned about the `prep`, `exec`, and `post` methods that define the behavior of a Node. Now that we understand Nodes, let's move on to [Chapter 3: Shared Store](03_shared_store.md) to see how Nodes can share data with each other.\n\n---\n# Chapter 3: Shared Store\n\nIn [Chapter 2: Node](02_node.md), we learned about Nodes and how they perform individual tasks. But how do these Nodes *talk* to each other? How does one Node pass its results to the next Node in the [Flow](01_flow.md)? That's where the **Shared Store** comes in!\n\nImagine a relay race. Each runner (Node) has a specific leg to run. The baton (data) needs to be passed from one runner to the next. The Shared Store is like the designated exchange zone where the baton is safely passed.\n\n**The Problem: Passing Information Between Nodes**\n\nWithout a shared space, each Node would operate in isolation.  They wouldn't know what other Nodes have done, and they couldn't build on each other's work.  The Shared Store provides a central location for Nodes to store and retrieve information, enabling collaboration and data flow within the PocketFlow ecosystem.\n\n**Our Research Agent Example (Again!)**\n\nRemember our research agent from [Chapter 1: Flow](01_flow.md)? The `SearchWeb` Node needs to pass the search results to the `AnswerQuestion` Node. Without a Shared Store, the `AnswerQuestion` Node wouldn't know what the web search found!\n\n**What *is* the Shared Store?**\n\nIn PocketFlow, the Shared Store is essentially a **dictionary**.  If you're new to programming, think of a dictionary as a container that holds key-value pairs. You can store data using a specific \"key\" and later retrieve it using that same key.\n\nFor example, we might store the search results with the key \"search_results\":\n\n```python\nshared_store[\"search_results\"] = \"The capital of France is Paris.\"\n```\n\nLater, another Node can access this information:\n\n```python\ncapital = shared_store[\"search_results\"]\nprint(capital) # Output: The capital of France is Paris.\n```\n\n**Key Concepts of the Shared Store**\n\n1.  **Centralized Data:** It's a single place where all Nodes can access and modify data.\n\n2.  **Key-Value Pairs:** Data is stored as key-value pairs, like a Python dictionary.\n\n3.  **Accessibility:** Any Node in the [Flow](01_flow.md) can access the Shared Store.\n\n4.  **Dynamic:** Nodes can add, modify, and delete data in the Shared Store during the [Flow](01_flow.md)'s execution.\n\n**How to Use the Shared Store**\n\nLet's revisit our `AddNumbers` Node from [Chapter 2: Node](02_node.md) and see how it uses the Shared Store:\n\n```python\nfrom pocketflow import Node\n\nclass AddNumbers(Node):\n    def prep(self, shared):\n        \"\"\"Prepares the input numbers from the shared store.\"\"\"\n        num1 = shared[\"num1\"]\n        num2 = shared[\"num2\"]\n        return num1, num2\n\n    def exec(self, inputs):\n        \"\"\"Adds the two numbers together.\"\"\"\n        num1, num2 = inputs\n        result = num1 + num2\n        return result\n\n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Stores the result in the shared store.\"\"\"\n        shared[\"result\"] = exec_res\n        return None  # No next node, the Flow can end\n```\n\n*   In the `prep` method, the Node retrieves the values associated with the keys \"num1\" and \"num2\" from the `shared` store.\n\n*   In the `post` method, the Node stores the calculated `result` in the `shared` store using the key \"result\".\n\n**Example Usage**\n\nHere's how the Shared Store is used when we run the [Flow](01_flow.md):\n\n```python\nfrom pocketflow import Flow\nfrom nodes import AddNumbers # Assuming the AddNumbers node is in nodes.py\n\n# Create an instance of the AddNumbers node\nadd_node = AddNumbers()\n\n# Create a Flow starting with the AddNumbers node\nflow = Flow(start=add_node)\n\n# Create a shared store with the input numbers\nshared_store = {\"num1\": 5, \"num2\": 3}\n\n# Run the flow\nflow.run(shared_store)\n\n# Print the result\nprint(shared_store[\"result\"])  # Output: 8\n```\n\n*   We create a `shared_store` (a dictionary) and initialize it with the input values for \"num1\" and \"num2\". This is how we *seed* the Shared Store with initial data.\n\n*   After the `flow.run(shared_store)` call, the `shared_store` now contains the key \"result\" with the value 8, because the `AddNumbers` Node stored it there.\n\n**Another Example: Passing the Question to the Answer Node**\nLet's look back at the nodes of our research agent in `cookbook/pocketflow-agent/nodes.py`. The `DecideAction` Node checks whether to search or answer the question and it has access to a \"question\" value in the shared store.\n\n```python\nclass DecideAction(Node):\n    def prep(self, shared):\n        # Prepare the context and question\n        question = shared[\"question\"] #Access question value from shared store\n        context = shared.get(\"context\", \"No previous search\")\n        return question, context\n    #Skipping the other functions for brevity.\n```\n\nIn this case, the `prep` function of `DecideAction` node accesses the question value from the shared store using the key \"question\".\n\n**Important Note:**\n\n*   **Key Existence:** It's good practice to check if a key exists in the Shared Store before accessing it, especially if the Node isn't the one who put the value there. You can use `shared.get(\"key\", default_value)` which returns `default_value` if the key doesn't exist. Or `if \"key\" in shared:` to check for a key's existence.\n\n**Under the Hood: How it Works**\n\nWhen you run a [Flow](01_flow.md), the `Flow` object creates and passes a Shared Store object to each Node's `prep`, `exec`, and `post` methods.\n\nHere's a simplified sequence diagram:\n\n```mermaid\nsequenceDiagram\n    participant Flow\n    participant Node1\n    participant Node2\n    participant SharedStore\n\n    Flow->>SharedStore: Create SharedStore\n    Flow->>Node1: run(SharedStore)\n    Node1->>SharedStore: Access/Modify data\n    Node1-->>Flow: (Next Action)\n    Flow->>Node2: run(SharedStore)\n    Node2->>SharedStore: Access/Modify data\n    Node2-->>Flow: (End)\n```\n\n*   The `Flow` creates a single `SharedStore` instance at the beginning.\n\n*   This same `SharedStore` instance is passed to each `Node` in the [Flow](01_flow.md).\n\n*   Each `Node` can then read from and write to the `SharedStore`.\n\n**Code Snippet: Flow Passing the Shared Store (From `pocketflow/__init__.py`)**\nThis is how the [Flow](01_flow.md) passes the Shared Store to each node:\n\n```python\nclass Flow(BaseNode):\n    def __init__(self,start): super().__init__();self.start=start\n    def get_next_node(self,curr,action):\n        nxt=curr.successors.get(action or \"default\")\n        if not nxt and curr.successors: warnings.warn(f\"Flow ends: '{action}' not found in {list(curr.successors)}\")\n        return nxt\n    def _orch(self,shared,params=None):\n        curr,p=copy.copy(self.start),(params or {**self.params})\n        while curr: curr.set_params(p);c=curr._run(shared);curr=copy.copy(self.get_next_node(curr,c))\n    def _run(self,shared): pr=self.prep(shared);self._orch(shared);return self.post(shared,pr,None)\n    def exec(self,prep_res): raise RuntimeError(\"Flow can't exec.\")\n```\nIn the `_orch` function, the shared store is maintained and passed into each subsequent node. Note the line `c=curr._run(shared)` indicating the shared store is passed into the node's run function.\n\n**Conclusion**\n\nIn this chapter, we learned about the Shared Store, a crucial component of PocketFlow that allows Nodes to communicate and share data. We saw how the Shared Store acts as a central memory, enabling seamless data transfer between processing steps. We learned how Nodes access and modify data in the Shared Store, and how the Flow manages the Shared Store behind the scenes.\n\nNow that we understand the Shared Store, we can appreciate how PocketFlow Nodes interact within a Flow to accomplish complex tasks. In the next chapter, [Prep, Exec, Post (Node Lifecycle)](04_prep__exec__post__node_lifecycle_.md), we'll dive deeper into the `prep`, `exec`, and `post` methods and understand the lifecycle of a Node.\n\n---\n# Chapter 4: Prep, Exec, Post (Node Lifecycle)\n\nIn [Chapter 2: Node](02_node.md), we learned about Nodes and how they encapsulate individual actions. We briefly touched upon the `prep`, `exec`, and `post` methods. Now, let's dive deeper into what these methods do and how they define the lifecycle of a Node.\n\nImagine you're a chef preparing a dish.\n\n*   **Prep:** You gather your ingredients, chop vegetables, and prepare your cooking station.\n*   **Exec:** You actually cook the dish \u2013 the main action.\n*   **Post:** You plate the dish, add garnishes, and serve it.\n\nThe `prep`, `exec`, and `post` methods in a Node are similar! They provide a structured way to define the different stages of a Node's operation.\n\n**The Problem: Organizing Node Logic**\n\nWithout a clear structure, the code inside a Node can become disorganized and hard to understand.  The `prep`, `exec`, and `post` pattern provides a consistent way to organize Node logic, making it easier to read, maintain, and debug.\n\n**Our Research Agent Example (One More Time!)**\n\nLet's revisit our research agent and focus on the `SearchWeb` Node. It needs to:\n\n1.  **Prep:** Get the search query from the [Shared Store](03_shared_store.md).\n2.  **Exec:** Actually perform the web search using the query.\n3.  **Post:** Save the search results back into the [Shared Store](03_shared_store.md) and decide what's next.\n\n**Key Concepts: Prep, Exec, Post**\n\n*   **Prep (Preparation):**  This method prepares the input data needed for the main task (`exec`). It typically retrieves data from the [Shared Store](03_shared_store.md) or performs any necessary data transformations.\n    *   **Input:** The [Shared Store](03_shared_store.md).\n    *   **Output:**  Data that the `exec` method needs (can be any Python object).\n\n*   **Exec (Execution):** This is the heart of the Node. It performs the core logic of the Node, using the data prepared by the `prep` method.\n    *   **Input:** The output of the `prep` method.\n    *   **Output:** The result of the Node's main operation (can be any Python object).\n\n*   **Post (Post-processing):** This method processes the output of the `exec` method. It typically saves the results back into the [Shared Store](03_shared_store.md), performs any cleanup, and determines the next step in the [Flow](01_flow.md). It specifies which Node to run next in the flow.\n    *   **Input:** The [Shared Store](03_shared_store.md), the output of `prep`, and the output of `exec`.\n    *   **Output:** A value that determines which Node to execute next in the [Flow](01_flow.md). If it returns `None`, the flow can end.\n\n**Example: Back to the `AddNumbers` Node**\n\nLet's examine the `AddNumbers` Node again:\n\n```python\nfrom pocketflow import Node\n\nclass AddNumbers(Node):\n    def prep(self, shared):\n        \"\"\"Prepares the input numbers from the shared store.\"\"\"\n        num1 = shared[\"num1\"]\n        num2 = shared[\"num2\"]\n        return num1, num2\n\n    def exec(self, inputs):\n        \"\"\"Adds the two numbers together.\"\"\"\n        num1, num2 = inputs\n        result = num1 + num2\n        return result\n\n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Stores the result in the shared store.\"\"\"\n        shared[\"result\"] = exec_res\n        return None  # No next node, the Flow can end\n```\n\n*   **`prep`**: Retrieves `num1` and `num2` from the [Shared Store](03_shared_store.md) and returns them as a tuple.\n*   **`exec`**: Takes the tuple from `prep`, adds the numbers, and returns the `result`.\n*   **`post`**: Stores the `result` in the [Shared Store](03_shared_store.md) and returns `None`, indicating the end of the [Flow](01_flow.md).\n\n**Another Example: `SearchWeb` Node from the Research Agent**\n\n```python\nfrom pocketflow import Node\nfrom utils import search_web # Assuming you have a search_web utility\n\nclass SearchWeb(Node):\n    def prep(self, shared):\n        \"\"\"Get the search query from the shared store.\"\"\"\n        return shared[\"search_query\"]\n        \n    def exec(self, search_query):\n        \"\"\"Search the web for the given query.\"\"\"\n        print(f\"\ud83c\udf10 Searching the web for: {search_query}\")\n        results = search_web(search_query)\n        return results\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Save the search results and go back to the decision node.\"\"\"\n        shared[\"context\"] = \"SEARCH RESULTS: \" + exec_res\n        return \"decide\" # Back to DecideAction node\n```\n\n*   **`prep`**:  Retrieves the `search_query` from the [Shared Store](03_shared_store.md).\n*   **`exec`**:  Performs the web search using `search_web` and returns the `results`.\n*   **`post`**: Saves the `results` into the [Shared Store](03_shared_store.md) under the key \"context\", and returns `\"decide\"` to tell the [Flow](01_flow.md) to go back to `DecideAction` ([Chapter 1: Flow](01_flow.md)).\n\n**How It Works: Example Execution**\n\nLet's trace the execution of the `SearchWeb` Node:\n\n1.  The [Flow](01_flow.md) calls the `run` method of the `SearchWeb` Node, passing the [Shared Store](03_shared_store.md).\n2.  The `prep` method retrieves the `search_query` (e.g., \"capital of France\") from the [Shared Store](03_shared_store.md).\n3.  The `exec` method uses the `search_query` to search the web. Let's say it gets the result \"Paris is the capital of France.\"\n4.  The `post` method saves the search result \"Paris is the capital of France.\" into the [Shared Store](03_shared_store.md). Also the return value is \"decide\".\n5.  The [Flow](01_flow.md) then uses the `\"decide\"` return value to determine the next Node to run.\n\n**Under the Hood: The `_run` Method**\n\nThe `Node` class in `pocketflow/__init__.py` has a `_run` method that orchestrates the `prep`, `exec`, and `post` methods:\n\n```python\nclass Node(BaseNode): #Simplified\n    def prep(self,shared): pass\n    def exec(self,prep_res): pass\n    def post(self,shared,prep_res,exec_res): pass\n    def _exec(self,prep_res): return self.exec(prep_res)\n    def _run(self,shared): p=self.prep(shared);e=self._exec(p);return self.post(shared,p,e)\n```\n\nAs you can see, `_run` calls `prep`, then `exec`, and finally `post` in sequence.  It passes the output of one method as input to the next.\n\nLet's visualize this with a sequence diagram:\n\n```mermaid\nsequenceDiagram\n    participant Node\n    participant SharedStore\n\n    Node->>Node: prep(shared_store)\n    Node-->>Node: prep_result\n    Node->>Node: exec(prep_result)\n    Node-->>Node: exec_result\n    Node->>Node: post(shared_store, prep_result, exec_result)\n    Node-->>Node: next_action\n```\n\n1.  `prep(shared_store)`: The `prep` method is called with the [Shared Store](03_shared_store.md).\n2.  `exec(prep_result)`: The `exec` method is called with the result of `prep`.\n3.  `post(shared_store, prep_result, exec_result)`: The `post` method is called with the [Shared Store](03_shared_store.md), the result of `prep`, and the result of `exec`. The `post` function's return value (next_action) controls what node to run next.\n\n**Why This Structure Matters**\n\n*   **Organization:** Clearly separates data preparation, core logic, and post-processing.\n*   **Testability:**  Each method can be tested independently.\n*   **Reusability:** Nodes become more modular and easier to reuse in different [Flows](01_flow.md).\n*   **Readability:**  The `prep`, `exec`, `post` structure makes the code easier to understand.\n\n**Conclusion**\n\nIn this chapter, we learned about the `prep`, `exec`, and `post` methods and how they define the lifecycle of a Node.  We saw how this structure helps organize Node logic, making it more readable, testable, and reusable. We saw how each function affects the [Shared Store](03_shared_store.md) and how the `post` function returns the next node to execute.\n\nNow that we understand the Node lifecycle, let's move on to [Chapter 5: BatchNode](05_batchnode.md) to see how we can process multiple items in a single Node using `BatchNode`.\n\n\nRelevant Code Snippets:\n--- File: cookbook/pocketflow-batch-node/nodes.py ---\nimport pandas as pd\nfrom pocketflow import BatchNode\n\nclass CSVProcessor(BatchNode):\n    \"\"\"BatchNode that processes a large CSV file in chunks.\"\"\"\n    \n    def __init__(self, chunk_size=1000):\n        \"\"\"Initialize with chunk size.\"\"\"\n        super().__init__()\n        self.chunk_size = chunk_size\n    \n    def prep(self, shared):\n        \"\"\"Split CSV file into chunks.\n        \n        Returns an iterator of DataFrames, each containing chunk_size rows.\n        \"\"\"\n        # Read CSV in chunks\n        chunks = pd.read_csv(\n            shared[\"input_file\"],\n            chunksize=self.chunk_size\n        )\n        return chunks\n    \n    def exec(self, chunk):\n        \"\"\"Process a single chunk of the CSV.\n        \n        Args:\n            chunk: pandas DataFrame containing chunk_size rows\n            \n        Returns:\n            dict: Statistics for this chunk\n        \"\"\"\n        return {\n            \"total_sales\": chunk[\"amount\"].sum(),\n            \"num_transactions\": len(chunk),\n            \"total_amount\": chunk[\"amount\"].sum()\n        }\n    \n    def post(self, shared, prep_res, exec_res_list):\n        \"\"\"Combine results from all chunks.\n        \n        Args:\n            prep_res: Original chunks iterator\n            exec_res_list: List of results from each chunk\n            \n        Returns:\n            str: Action to take next\n        \"\"\"\n        # Combine statistics from all chunks\n        total_sales = sum(res[\"total_sales\"] for res in exec_res_list)\n        total_transactions = sum(res[\"num_transactions\"] for res in exec_res_list)\n        total_amount = sum(res[\"total_amount\"] for res in exec_res_list)\n        \n        # Calculate final statistics\n        shared[\"statistics\"] = {\n            \"total_sales\": total_sales,\n            \"average_sale\": total_amount / total_transactions,\n            \"total_transactions\": total_transactions\n        }\n        \n        return \"show_stats\" \n\n--- File: cookbook/pocketflow-batch/main.py ---\nimport os\nfrom pocketflow import BatchNode, Flow\nfrom utils import call_llm\n\nclass TranslateTextNode(BatchNode):\n    def prep(self, shared):\n        text = shared.get(\"text\", \"(No text provided)\")\n        languages = shared.get(\"languages\", [\"Chinese\", \"Spanish\", \"Japanese\", \"German\", \n                              \"Russian\", \"Portuguese\", \"French\", \"Korean\"])\n        \n        # Create batches for each language translation\n        return [(text, lang) for lang in languages]\n\n    def exec(self, data_tuple):\n        text, language = data_tuple\n        \n        prompt = f\"\"\"\nPlease translate the following markdown file into {language}. \nBut keep the original markdown format, links and code blocks.\nDirectly return the translated text, without any other text or comments.\n\nOriginal: \n{text}\n\nTranslated:\"\"\"\n        \n        result = call_llm(prompt)\n        \n        print(f\"Translated {language} text\")\n\n        return {\"language\": language, \"translation\": result}\n\n    def post(self, shared, prep_res, exec_res_list):\n        # Create output directory if it doesn't exist\n        output_dir = shared.get(\"output_dir\", \"translations\")\n        os.makedirs(output_dir, exist_ok=True)\n        \n        # Write each translation to a file\n        for result in exec_res_list:\n            language, translation = result[\"language\"], result[\"translation\"]\n            \n            # Write to file\n            filename = os.path.join(output_dir, f\"README_{language.upper()}.md\")\n            with open(filename, \"w\", encoding=\"utf-8\") as f:\n                f.write(translation)\n            \n            print(f\"Saved translation to {filename}\")\n\nif __name__ == \"__main__\":\n    # read the text from ../../README.md\n    with open(\"../../README.md\", \"r\") as f:\n        text = f.read()\n    \n    # Default settings\n    shared = {\n        \"text\": text,\n        \"languages\": [\"Chinese\", \"Spanish\", \"Japanese\", \"German\", \"Russian\", \"Portuguese\", \"French\", \"Korean\"],\n        \"output_dir\": \"translations\"\n    }\n\n    # Run the translation flow\n    translate_node = TranslateTextNode(max_retries=3)\n    flow = Flow(start=translate_node)\n    flow.run(shared)\n\n    print(\"\\n=== Translation Complete ===\")\n    print(f\"Translations saved to: {shared['output_dir']}\")\n    print(\"============================\")\n\n--- File: pocketflow/__init__.py ---\nimport asyncio, warnings, copy, time\n\nclass BaseNode:\n    def __init__(self): self.params,self.successors={},{}\n    def set_params(self,params): self.params=params\n    def add_successor(self,node,action=\"default\"):\n        if action in self.successors: warnings.warn(f\"Overwriting successor for action '{action}'\")\n        self.successors[action]=node;return node\n    def prep(self,shared): pass\n    def exec(self,prep_res): pass\n    def post(self,shared,prep_res,exec_res): pass\n    def _exec(self,prep_res): return self.exec(prep_res)\n    def _run(self,shared): p=self.prep(shared);e=self._exec(p);return self.post(shared,p,e)\n    def run(self,shared): \n        if self.successors: warnings.warn(\"Node won't run successors. Use Flow.\")  \n        return self._run(shared)\n    def __rshift__(self,other): return self.add_successor(other)\n    def __sub__(self,action):\n        if isinstance(action,str): return _ConditionalTransition(self,action)\n        raise TypeError(\"Action must be a string\")\n\nclass _ConditionalTransition:\n    def __init__(self,src,action): self.src,self.action=src,action\n    def __rshift__(self,tgt): return self.src.add_successor(tgt,self.action)\n\nclass Node(BaseNode):\n    def __init__(self,max_retries=1,wait=0): super().__init__();self.max_retries,self.wait=max_retries,wait\n    def exec_fallback(self,prep_res,exc): raise exc\n    def _exec(self,prep_res):\n        for self.cur_retry in range(self.max_retries):\n            try: return self.exec(prep_res)\n            except Exception as e:\n                if self.cur_retry==self.max_retries-1: return self.exec_fallback(prep_res,e)\n                if self.wait>0: time.sleep(self.wait)\n\nclass BatchNode(Node):\n    def _exec(self,items): return [super(BatchNode,self)._exec(i) for i in (items or [])]\n\nclass Flow(BaseNode):\n    def __init__(self,start): super().__init__();self.start=start\n    def get_next_node(self,curr,action):\n        nxt=curr.successors.get(action or \"default\")\n        if not nxt and curr.successors: warnings.warn(f\"Flow ends: '{action}' not found in {list(curr.successors)}\")\n        return nxt\n    def _orch(self,shared,params=None):\n        curr,p=copy.copy(self.start),(params or {**self.params})\n        while curr: curr.set_params(p);c=curr._run(shared);curr=copy.copy(self.get_next_node(curr,c))\n    def _run(self,shared): pr=self.prep(shared);self._orch(shared);return self.post(shared,pr,None)\n    def exec(self,prep_res): raise RuntimeError(\"Flow can't exec.\")\n\nclass BatchFlow(Flow):\n    def _run(self,shared):\n        pr=self.prep(shared) or []\n        for bp in pr: self._orch(shared,{**self.params,**bp})\n        return self.post(shared,pr,None)\n\nclass AsyncNode(Node):\n    def prep(self,shared): raise RuntimeError(\"Use prep_async.\")\n    def exec(self,prep_res): raise RuntimeError(\"Use exec_async.\")\n    def post(self,shared,prep_res,exec_res): raise RuntimeError(\"Use post_async.\")\n    def exec_fallback(self,prep_res,exc): raise RuntimeError(\"Use exec_fallback_async.\")\n    def _run(self,shared): raise RuntimeError(\"Use run_async.\")\n    async def prep_async(self,shared): pass\n    async def exec_async(self,prep_res): pass\n    async def exec_fallback_async(self,prep_res,exc): raise exc\n    async def post_async(self,shared,prep_res,exec_res): pass\n    async def _exec(self,prep_res): \n        for i in range(self.max_retries):\n            try: return await self.exec_async(prep_res)\n            except Exception as e:\n                if i==self.max_retries-1: return await self.exec_fallback_async(prep_res,e)\n                if self.wait>0: await asyncio.sleep(self.wait)\n    async def run_async(self,shared): \n        if self.successors: warnings.warn(\"Node won't run successors. Use AsyncFlow.\")  \n        return await self._run_async(shared)\n    async def _run_async(self,shared): p=await self.prep_async(shared);e=await self._exec(p);return await self.post_async(shared,p,e)\n\nclass AsyncBatchNode(AsyncNode,BatchNode):\n    async def _exec(self,items): return [await super(AsyncBatchNode,self)._exec(i) for i in items]\n\nclass AsyncParallelBatchNode(AsyncNode,BatchNode):\n    async def _exec(self,items): return await asyncio.gather(*(super(AsyncParallelBatchNode,self)._exec(i) for i in items))\n\nclass AsyncFlow(Flow,AsyncNode):\n    async def _orch_async(self,shared,params=None):\n        curr,p=copy.copy(self.start),(params or {**self.params})\n        while curr:curr.set_params(p);c=await curr._run_async(shared) if isinstance(curr,AsyncNode) else curr._run(shared);curr=copy.copy(self.get_next_node(curr,c))\n    async def _run_async(self,shared): p=await self.prep_async(shared);await self._orch_async(shared);return await self.post_async(shared,p,None)\n\nclass AsyncBatchFlow(AsyncFlow,BatchFlow):\n    async def _run_async(self,shared):\n        pr=await self.prep_async(shared) or []\n        for bp in pr: await self._orch_async(shared,{**self.params,**bp})\n        return await self.post_async(shared,pr,None)\n\nclass AsyncParallelBatchFlow(AsyncFlow,BatchFlow):\n    async def _run_async(self,shared):\n        pr=await self.prep_async(shared) or []\n        await asyncio.gather(*(self._orch_async(shared,{**self.params,**bp}) for bp in pr))\n        return await self.post_async(shared,pr,None)\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 5: BatchNode`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "# Chapter 5: BatchNode\n\nIn [Chapter 4: Prep, Exec, Post (Node Lifecycle)](04_prep__exec__post__node_lifecycle_.md), we learned about the `prep`, `exec`, and `post` methods and how they structure a Node's lifecycle. Now, let's explore how to efficiently process *multiple* data items within a single Node using the `BatchNode`.\n\nImagine you're running a bakery. Instead of baking one cookie at a time, you bake dozens on a single tray! The `BatchNode` is like that tray \u2013 it lets you process a \"batch\" of items together.\n\n**The Problem: Processing Many Items Efficiently**\n\nSometimes, you need to perform the same operation on a large number of independent data items. Doing this with a regular [Node](02_node.md) would involve iterating through each item individually, which can be slow and inefficient. The `BatchNode` solves this by allowing you to process these items in a batch, making the process much faster.\n\n**Our Use Case: Translating a Document into Multiple Languages**\n\nLet's say you have a document and you want to translate it into several languages (like Chinese, Spanish, French, etc.). Instead of creating a separate [Node](02_node.md) for each language, we can use a `BatchNode` to translate the document into all the languages at once!\n\n**What is a BatchNode?**\n\nA `BatchNode` is a special type of [Node](02_node.md) that's designed to process multiple data items in a single execution. Think of it like an assembly line:\n\n1.  **Prep:** The `prep` method prepares the input data and splits it into chunks. These chunks are individual items or mini-batches within the larger batch.\n2.  **Exec:** The `exec` method is applied to each chunk of data. It performs the same operation on each item in the chunk.\n3.  **Post:** The `post` method combines the results from processing all the chunks and prepares the final output.\n\n**Key Concepts of a BatchNode**\n\n1.  **Batch Processing:** The ability to process multiple independent data items together.\n2.  **Chunking:** Splitting the input data into smaller, manageable chunks.\n3.  **Parallelism (Optional):** The `exec` method can potentially process chunks in parallel (we'll explore this in later chapters). For now, we'll assume it's processed sequentially.\n4.  **Result Aggregation:** Combining the results from processing all the chunks into a final output.\n\n**Creating a BatchNode for Translation**\n\nLet's create a `BatchNode` that translates a given text into multiple languages:\n\n```python\nfrom pocketflow import BatchNode\n\nclass TranslateTextNode(BatchNode):\n    def prep(self, shared):\n        text = shared.get(\"text\", \"(No text provided)\")\n        languages = shared.get(\"languages\", [\"Chinese\", \"Spanish\"]) # Reduced languages for brevity\n        \n        # Create batches for each language translation\n        return [(text, lang) for lang in languages]\n\n    def exec(self, data_tuple):\n        text, language = data_tuple\n        #Simulate Translation for brevity\n        translation=f\"Translated to {language}: {text}\"\n        return {\"language\": language, \"translation\": translation}\n\n    def post(self, shared, prep_res, exec_res_list):\n        # Print each translation result\n        for result in exec_res_list:\n            language, translation = result[\"language\"], result[\"translation\"]\n            print(f\"Translation to {language}: {translation}\")\n        return None\n```\n\nHere's a breakdown:\n\n*   `TranslateTextNode` inherits from `BatchNode`.\n*   `prep(self, shared)`: This method retrieves the text to translate and the list of languages from the [Shared Store](03_shared_store.md). It then creates a list of tuples, where each tuple contains the text and a language. This list becomes the \"batch\" of items to be processed.\n*   `exec(self, data_tuple)`: This method takes a single tuple (text, language) as input. It simulates translation for demonstration purposes by returning a translated version of the text.\n*   `post(self, shared, prep_res, exec_res_list)`: This method receives a list of results, where each result is a dictionary containing the translated text and the language. It prints each translation to the console.\n\n**Using the BatchNode in a Flow**\n\nNow, let's see how to use this `BatchNode` in a [Flow](01_flow.md):\n\n```python\nfrom pocketflow import Flow\nfrom nodes import TranslateTextNode # Assuming the TranslateTextNode is in nodes.py\n\n# Create an instance of the TranslateTextNode\ntranslate_node = TranslateTextNode()\n\n# Create a Flow starting with the TranslateTextNode\nflow = Flow(start=translate_node)\n\n# Create a shared store with the input text and languages\nshared_store = {\n    \"text\": \"Hello, world!\",\n    \"languages\": [\"Chinese\", \"Spanish\"]  # Reduced languages for brevity\n}\n\n# Run the flow\nflow.run(shared_store)\n\n# Expected Output:\n# Translation to Chinese: Translated to Chinese: Hello, world!\n# Translation to Spanish: Translated to Spanish: Hello, world!\n```\n\nHere's what's happening:\n\n1.  We create an instance of the `TranslateTextNode`.\n2.  We create a [Flow](01_flow.md) that starts with the `TranslateTextNode`.\n3.  We create a `shared_store` ([Chapter 3: Shared Store](03_shared_store.md)) and populate it with the text and languages.\n4.  We run the [Flow](01_flow.md). The `TranslateTextNode` will translate the text into each language and print the results.\n\n**Let's Trace the Execution**\n\n1.  The [Flow](01_flow.md) starts at the `TranslateTextNode`.\n2.  The `TranslateTextNode`'s `prep` method is called. It retrieves the text and languages from the `shared_store` and creates a list of tuples: `[(\"Hello, world!\", \"Chinese\"), (\"Hello, world!\", \"Spanish\")]`.\n3.  The `TranslateTextNode`'s `exec` method is called *twice*:\n    *   First, with the tuple `(\"Hello, world!\", \"Chinese\")`. It returns `{\"language\": \"Chinese\", \"translation\": \"Translated to Chinese: Hello, world!\"}`.\n    *   Second, with the tuple `(\"Hello, world!\", \"Spanish\")`. It returns `{\"language\": \"Spanish\", \"translation\": \"Translated to Spanish: Hello, world!\"}`.\n4.  The `TranslateTextNode`'s `post` method is called with the list of results: `[{\"language\": \"Chinese\", \"translation\": \"Translated to Chinese: Hello, world!\"}, {\"language\": \"Spanish\", \"translation\": \"Translated to Spanish: Hello, world!\"}]`. It prints each translation to the console.\n5.  The [Flow](01_flow.md) finishes because the `TranslateTextNode` returned `None` in the `post` method.\n\n**Under the Hood: How BatchNode Works**\n\nWhen a [Flow](01_flow.md) runs a `BatchNode`, the `_exec` method is overridden to process items as a batch.\n\nHere's a simplified sequence diagram:\n\n```mermaid\nsequenceDiagram\n    participant Flow\n    participant BatchNode\n    participant SharedStore\n\n    Flow->>BatchNode: run(shared_store)\n    BatchNode->>SharedStore: access text, languages\n    BatchNode-->>Flow: [(text, lang1), (text, lang2), ...]\n    loop For each item in batch\n        Flow->>BatchNode: exec((text, lang_i))\n        BatchNode-->>Flow: {language: lang_i, translation: ...}\n    end\n    Flow->>BatchNode: post(shared_store, prep_res, exec_res_list)\n    BatchNode->>SharedStore: Store results (optional)\n    BatchNode-->>Flow: None\n```\n\n1.  The [Flow](01_flow.md) calls the `run` method of the `BatchNode`, passing the [Shared Store](03_shared_store.md).\n2.  The `prep` method retrieves the necessary data from the [Shared Store](03_shared_store.md) and prepares the batch of items.\n3.  The `exec` method is called for each item in the batch.\n4.  The `post` method receives a list of results from all the `exec` calls.\n\nLet's look at the relevant code from `pocketflow/__init__.py`:\n\n```python\nclass BatchNode(Node):\n    def _exec(self,items): return [super(BatchNode,self)._exec(i) for i in (items or [])]\n```\n\nThis code snippet shows that the `_exec` method of the `BatchNode` iterates through the `items` (which are the output from the `prep` function) and calls the `exec` method for each item. The results are then collected into a list and returned.\n\n**Benefits of Using BatchNode**\n\n*   **Improved Efficiency:** Processes multiple items in a single Node execution.\n*   **Simplified Code:** Reduces the need for manual iteration within the [Flow](01_flow.md).\n*   **Potential for Parallelism:** Makes it easier to parallelize processing (we'll see this later!).\n\n**Real World example - CSV Processing**\n\nAnother use case is processing CSV files.\n```python\nimport pandas as pd\nfrom pocketflow import BatchNode\n\nclass CSVProcessor(BatchNode):\n    def __init__(self, chunk_size=1000):\n        super().__init__()\n        self.chunk_size = chunk_size\n    \n    def prep(self, shared):\n        chunks = pd.read_csv(\n            shared[\"input_file\"],\n            chunksize=self.chunk_size\n        )\n        return chunks\n    \n    def exec(self, chunk):\n        return {\n            \"total_sales\": chunk[\"amount\"].sum(),\n            \"num_transactions\": len(chunk),\n            \"total_amount\": chunk[\"amount\"].sum()\n        }\n    \n    def post(self, shared, prep_res, exec_res_list):\n        total_sales = sum(res[\"total_sales\"] for res in exec_res_list)\n        shared[\"total_sales\"] = total_sales\n        return None\n```\n\nHere, the `prep` function reads the CSV file in chunks (batches) of `chunk_size`. The `exec` function processes each chunk and calculates sales stats, and the `post` function adds up all of the sales.\n\n**Conclusion**\n\nIn this chapter, we learned about the `BatchNode` and how it allows us to process multiple data items efficiently within a single Node. We saw how to create a `BatchNode` for translating text into multiple languages and how it works under the hood.\n\nNow that we understand `BatchNode`, let's move on to [Chapter 6: AsyncNode](06_asyncnode.md) to see how we can handle asynchronous operations within PocketFlow.\n", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `PocketFlow` about the concept: \"AsyncNode\". This is Chapter 6.\n\nConcept Details:\n- Description:\nHandles asynchronous operations, enabling PocketFlow to perform non-blocking calls to external services or APIs. It's like ordering food online; the code doesn't wait idly for the food to arrive but can handle other tasks in the meantime. Makes use of `async` and `await` keywords.\n\n\nComplete Tutorial Structure:\n1. [Flow](01_flow.md)\n2. [Node](02_node.md)\n3. [Shared Store](03_shared_store.md)\n4. [Prep, Exec, Post (Node Lifecycle)](04_prep__exec__post__node_lifecycle_.md)\n5. [BatchNode](05_batchnode.md)\n6. [AsyncNode](06_asyncnode.md)\n7. [AsyncFlow](07_asyncflow.md)\n8. [AsyncParallelBatchNode](08_asyncparallelbatchnode.md)\n\nContext from previous chapters (summary):\n# Chapter 1: Flow\n\nImagine you're directing a play. You have actors, each with specific roles (like answering a question or searching the web). But how do you tell them *when* to act, and *what* to do next?  That's where a **Flow** comes in!\n\nIn PocketFlow, a Flow is like the conductor of an orchestra. It's the orchestration layer that connects individual actions (we call them **Nodes**) into a functioning workflow. It defines the sequence in which these actions are executed and how data is passed between them.\n\n**Our Use Case: A Simple Research Agent**\n\nLet's say we're building a simple research agent that can either search the web or answer a question, depending on what's needed.  The agent will first *decide* what to do. If it needs to search, it will *search the web*. If it needs to answer, it will *answer the question*. After searching, it goes back to the deciding step.\n\nWithout a Flow, we'd have to manually call each action in the right order, which can get messy quickly. A Flow lets us define this process in a clear and organized way.\n\n**Key Concepts of a Flow**\n\n1.  **Nodes:** These are the individual steps or actions in our workflow. Think of them as the actors in our play. We'll dive deep into Nodes in [Chapter 2: Node](02_node.md). For now, just think of them as individual units of work.\n\n2.  **Sequence:** The Flow defines the order in which Nodes are executed.  It's the script of our play.\n\n3.  **Data Passing:** The Flow manages how data is passed between Nodes. This is how the actors communicate and share information.\n\n4.  **Starting Node:** Every flow must have a start node to kick off the process.\n\n5.  **Transitions:**  These define how the Flow moves from one Node to the next. We'll use special operators like `>>` (rshift) and `-` (subtraction) to define these transitions.\n\n**Creating Our Research Agent Flow**\n\nLet's look at how we can create a Flow for our research agent. The code might look like this (from `cookbook/pocketflow-agent/flow.py`):\n\n```python\nfrom pocketflow import Flow\nfrom nodes import DecideAction, SearchWeb, AnswerQuestion\n\ndef create_agent_flow():\n    # Create instances of each node\n    decide = DecideAction()\n    search = SearchWeb()\n    answer = AnswerQuestion()\n\n    # Connect the nodes\n    # If DecideAction returns \"search\", go to SearchWeb\n    decide - \"search\" >> search\n\n    # If DecideAction returns \"answer\", go to AnswerQuestion\n    decide - \"answer\" >> answer\n\n    # After SearchWeb completes and returns \"decide\", go back to DecideAction\n    search - \"decide\" >> decide\n\n    # Create and return the flow, starting with the DecideAction node\n    return Flow(start=decide)\n```\n\nHere's a breakdown:\n\n*   We import `Flow` from the `pocketflow` library.\n*   We import the `Node` types from our nodes file (`nodes.py`). Don't worry about the implementations of `DecideAction`, `SearchWeb`, and `AnswerQuestion` yet.  Just think of them as steps in our process.\n*   We create instances of each Node.\n*   We use the `>>` and `-` operators to define the transitions between Nodes. For example, `decide - \"search\" >> search` means \"If the `decide` Node returns 'search', then go to the `search` Node.\"\n*   We create a `Flow` object, specifying the `decide` Node as the starting point.\n\n**How it Works: Example**\n\nImagine the agent receives the question: \"What is the capital of France?\".\n\n1.  The `Flow` starts at the `DecideAction` Node.\n2.  The `DecideAction` Node determines that we need to search for the answer. It returns the string \"search\".\n3.  The `Flow` sees the transition `decide - \"search\" >> search` and moves to the `SearchWeb` Node.\n4.  The `SearchWeb` Node searches the web and finds the answer: \"Paris\". It stores the answer in memory and returns the string \"decide\".\n5.  The `Flow` sees the transition `search - \"decide\" >> decide` and moves back to the `DecideAction` Node.\n6.  Now `DecideAction` could determine we need to answer the original question. It returns the string \"answer\".\n7.  The `Flow` sees the transition `decide - \"answer\" >> answer` and moves to the `AnswerQuestion` Node.\n8.  The `AnswerQuestion` node crafts the answer using the information in memory and then returns its final response.\n\n**Under the Hood: A Peek Inside**\n\nLet's see what happens inside the `Flow` when we run it.  Here's a simplified sequence diagram:\n\n```mermaid\nsequenceDiagram\n    participant Flow\n    participant DecideAction\n    participant SearchWeb\n    participant AnswerQuestion\n    participant SharedStore\n\n    Flow->>DecideAction: run(shared_store)\n    DecideAction->>SharedStore: access/modify data\n    DecideAction-->>Flow: \"search\"\n    Flow->>SearchWeb: run(shared_store)\n    SearchWeb->>SharedStore: access/modify data\n    SearchWeb-->>Flow: \"decide\"\n    Flow->>DecideAction: run(shared_store)\n    DecideAction->>SharedStore: access/modify data\n    DecideAction-->>Flow: \"answer\"\n    Flow->>AnswerQuestion: run(shared_store)\n    AnswerQuestion->>SharedStore: access/modify data\n    AnswerQuestion-->>Flow: (End)\n```\n\n*   The `Flow` starts by calling the `run` method of the starting Node (`DecideAction` in our example).\n*   Each Node executes its logic and potentially modifies a `SharedStore` ([Chapter 3: Shared Store](03_shared_store.md)) which is used for passing data between the nodes.\n*   The Node returns a value (like \"search\" or \"answer\") that determines the next Node to execute, based on the transitions defined in the Flow.\n*   This process continues until there are no more transitions, or we hit a dead end (no successor node defined for a particular return value).\n\n**Code Dive: `Flow` Class**\n\nLet's look at some of the key parts of the `Flow` class (from `pocketflow/__init__.py`):\n\n```python\nclass Flow(BaseNode):\n    def __init__(self,start): super().__init__();self.start=start\n    def get_next_node(self,curr,action):\n        nxt=curr.successors.get(action or \"default\")\n        if not nxt and curr.successors: warnings.warn(f\"Flow ends: '{action}' not found in {list(curr.successors)}\")\n        return nxt\n    def _orch(self,shared,params=None):\n        curr,p=copy.copy(self.start),(params or {**self.params})\n        while curr: curr.set_params(p);c=curr._run(shared);curr=copy.copy(self.get_next_node(curr,c))\n    def _run(self,shared): pr=self.prep(shared);self._orch(shared);return self.post(shared,pr,None)\n    def exec(self,prep_res): raise RuntimeError(\"Flow can't exec.\")\n```\n\n*   `__init__(self, start)`: The constructor takes the starting Node as an argument.\n*   `get_next_node(self, curr, action)`:  This method determines the next Node to execute based on the return value (`action`) of the current Node (`curr`). It looks up the successor node in the current node's `successors` dictionary.\n*   `_orch(self, shared, params=None)`:  This is the \"orchestration\" method.  It iterates through the Nodes, executing each one in sequence based on the transitions.\n*   `_run(self, shared)`: This method kicks off the orchestration by calling `_orch(self, shared)`.\n\n**Conclusion**\n\nIn this chapter, we learned about the concept of a Flow in PocketFlow. A Flow is an orchestration layer that connects individual Nodes into a functioning workflow. It defines the sequence in which Nodes are executed and how data is passed between them. We saw how to create a Flow for a simple research agent and how it works under the hood.\n\nNow that we understand Flows, let's dive deeper into the building blocks of Flows: **Nodes**. In [Chapter 2: Node](02_node.md), we'll explore what Nodes are, how to create them, and how they interact with Flows.\n\n---\n# Chapter 2: Node\n\nIn [Chapter 1: Flow](01_flow.md), we learned about Flows and how they orchestrate different actions in a specific sequence. Now, let's zoom in on the individual *actions* themselves. These actions are called **Nodes**.\n\nImagine you're building a robot that can make a sandwich. The Flow would be the robot's overall plan: \"Get bread -> Get fillings -> Assemble sandwich -> Serve sandwich\".  Each of those steps \u2013 \"Get bread\", \"Get fillings\", etc. \u2013 would be a Node.\n\n**What Problem Do Nodes Solve?**\n\nNodes solve the problem of breaking down a complex task into smaller, manageable pieces. Instead of having one giant, complicated function, we can have a series of simpler Nodes, each responsible for a specific job. This makes our code easier to understand, test, and maintain.\n\n**Our Sandwich-Making Robot Example**\n\nLet's focus on the \"Get fillings\" step. This could be further broken down: \"Open refrigerator -> Check for desired fillings -> Take out fillings\".  Each of *those* could even be a Node!  For our purposes, let's say \"Get fillings\" is a single Node that figures out which fillings to get and retrieves them.\n\n**Key Concepts of a Node**\n\nA Node is like a mini-program that does one thing well. It has three main parts:\n\n1.  **Input(s):** What the Node needs to get its job done. In our \"Get fillings\" Node, the input might be a list of preferred fillings.\n\n2.  **Processing:** What the Node *does* with the input. This is the core logic of the Node.  In our example, it's the code that checks what fillings are available and retrieves them.\n\n3.  **Output(s):** What the Node produces after processing the input.  In our example, the output would be the actual fillings.\n\nIn PocketFlow, these three parts are defined using three special methods: `prep`, `exec`, and `post`. Don't worry too much about these names for now. Just remember the input, processing, and output.  We will discuss [Prep, Exec, Post (Node Lifecycle)](04_prep__exec__post__node_lifecycle_.md) in detail later.\n\n**Creating a Simple Node**\n\nLet's create a simple Node that adds two numbers.\n\n```python\nfrom pocketflow import Node\n\nclass AddNumbers(Node):\n    def prep(self, shared):\n        \"\"\"Prepares the input numbers from the shared store.\"\"\"\n        num1 = shared[\"num1\"]\n        num2 = shared[\"num2\"]\n        return num1, num2\n\n    def exec(self, inputs):\n        \"\"\"Adds the two numbers together.\"\"\"\n        num1, num2 = inputs\n        result = num1 + num2\n        return result\n\n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Stores the result in the shared store.\"\"\"\n        shared[\"result\"] = exec_res\n        return None  # No next node, the Flow can end\n```\n\nHere's what's happening:\n\n*   We import the `Node` class from `pocketflow`.\n*   We create a class called `AddNumbers` that *inherits* from `Node`. This means our `AddNumbers` class *is* a Node and has all the properties of a Node.\n*   `prep(self, shared)`: This method *prepares* the input for the `exec` method. It retrieves the two numbers (`num1` and `num2`) from the `shared` store (we'll talk about the [Shared Store](03_shared_store.md) in the next chapter!). It returns these two numbers as a tuple.\n*   `exec(self, inputs)`: This method *executes* the core logic of the Node. It takes the output of `prep` as `inputs`, adds the two numbers together, and returns the `result`.\n*   `post(self, shared, prep_res, exec_res)`: This method *posts* the result to the `shared` store. It takes the output of `exec` as `exec_res` and stores it in the `shared` store under the key \"result\".  It returns `None` in this case because there's no further action required in the flow after adding the numbers.\n\n**Using the Node in a Flow**\n\nNow, let's see how we can use this Node in a Flow:\n\n```python\nfrom pocketflow import Flow\nfrom nodes import AddNumbers # Assuming the AddNumbers node is in nodes.py\n\n# Create an instance of the AddNumbers node\nadd_node = AddNumbers()\n\n# Create a Flow starting with the AddNumbers node\nflow = Flow(start=add_node)\n\n# Create a shared store with the input numbers\nshared_store = {\"num1\": 5, \"num2\": 3}\n\n# Run the flow\nflow.run(shared_store)\n\n# Print the result\nprint(shared_store[\"result\"])  # Output: 8\n```\n\nHere's what's happening:\n\n*   We import `Flow` from `pocketflow`.\n*   We create an instance of our `AddNumbers` Node.\n*   We create a `Flow` that starts with our `AddNumbers` Node.\n*   We create a `shared_store` (a dictionary) and put the input numbers (`num1` and `num2`) into it.\n*   We run the `Flow` with the `shared_store`.\n*   Finally, we print the result, which is stored in the `shared_store` under the key \"result\".\n\n**Let's Trace the Execution**\n\n1.  The `Flow` starts at the `AddNumbers` Node.\n2.  The `AddNumbers` Node's `prep` method is called. It retrieves `num1` (5) and `num2` (3) from the `shared_store`.\n3.  The `AddNumbers` Node's `exec` method is called with the input (5, 3). It adds them together and returns 8.\n4.  The `AddNumbers` Node's `post` method is called with the result (8). It stores the result in the `shared_store` under the key \"result\".\n5.  The `Flow` finishes because the `AddNumbers` Node returned `None` in the `post` method, indicating there are no more Nodes to execute.\n\n**Under the Hood: A Closer Look**\n\nLet's see a simplified sequence diagram of what happens when we run the `Flow`:\n\n```mermaid\nsequenceDiagram\n    participant Flow\n    participant AddNumbers\n    participant SharedStore\n\n    Flow->>AddNumbers: run(shared_store)\n    AddNumbers->>SharedStore: access num1, num2\n    AddNumbers-->>Flow: num1, num2\n    Flow->>AddNumbers: exec(num1, num2)\n    AddNumbers-->>Flow: result\n    Flow->>AddNumbers: post(shared_store, prep_res, result)\n    AddNumbers->>SharedStore: store result\n    AddNumbers-->>Flow: None\n```\n\n*   The `Flow` calls the `run` method of the `AddNumbers` Node, passing the `shared_store`.\n*   The `AddNumbers` Node accesses the `shared_store` to get the input numbers.\n*   The `AddNumbers` Node executes its logic and returns the result to the `Flow`.\n*   The `AddNumbers` Node then updates the `shared_store` with the result.\n\nNow, let's peek at some of the relevant code from the `pocketflow/__init__.py` file:\n\n```python\nclass Node(BaseNode): #Simplified\n    def prep(self,shared): pass\n    def exec(self,prep_res): pass\n    def post(self,shared,prep_res,exec_res): pass\n    def _exec(self,prep_res): return self.exec(prep_res)\n    def _run(self,shared): p=self.prep(shared);e=self._exec(p);return self.post(shared,p,e)\n```\n\n*   `prep`, `exec`, and `post` are methods that you can override in your own Node classes to define the behavior of the Node.\n*   The `_run` method is responsible for calling `prep`, `exec`, and `post` in sequence.\n\n**Example: Back to the Research Agent**\nLet's look at our Research Agent code from [Chapter 1: Flow](01_flow.md).\n\n```python\nfrom pocketflow import Node\nfrom utils import call_llm, search_web\nimport yaml\n\nclass DecideAction(Node):\n    def prep(self, shared):\n        # Prepare the context and question\n        question = shared[\"question\"]\n        context = shared.get(\"context\", \"No previous search\")\n        return question, context\n\n    def exec(self, inputs):\n        # Call LLM to decide search or answer\n        question, context = inputs\n        prompt = f\"\"\"... (LLM prompt) ...\"\"\" #Skipping prompt for brevity\n        response = call_llm(prompt)\n        yaml_str = response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n        decision = yaml.safe_load(yaml_str)\n        return decision\n\n    def post(self, shared, prep_res, exec_res):\n        #Save search query and determine next step\n        if exec_res[\"action\"] == \"search\":\n            shared[\"search_query\"] = exec_res[\"search_query\"]\n        else:\n            shared[\"context\"] = exec_res[\"answer\"]\n        return exec_res[\"action\"]\n```\n\nIn this example, `DecideAction` Node decides whether to search or answer based on the provided question and context. The `prep` function grabs the values from the `shared` store, the `exec` function decides on the next action, and the `post` function save the search query (if applicable) and specifies the name of the next node to proceed to.\n\n**Different Types of Nodes**\n\nWhile we've focused on a basic Node here, PocketFlow offers different types of Nodes for different purposes, such as [BatchNode](05_batchnode.md) for processing multiple items at once, and [AsyncNode](06_asyncnode.md) for handling asynchronous operations.\n\n**Conclusion**\n\nIn this chapter, we learned about Nodes, the fundamental building blocks of PocketFlow. We saw how Nodes encapsulate individual actions, and how they are connected together in a Flow. We learned about the `prep`, `exec`, and `post` methods that define the behavior of a Node. Now that we understand Nodes, let's move on to [Chapter 3: Shared Store](03_shared_store.md) to see how Nodes can share data with each other.\n\n---\n# Chapter 3: Shared Store\n\nIn [Chapter 2: Node](02_node.md), we learned about Nodes and how they perform individual tasks. But how do these Nodes *talk* to each other? How does one Node pass its results to the next Node in the [Flow](01_flow.md)? That's where the **Shared Store** comes in!\n\nImagine a relay race. Each runner (Node) has a specific leg to run. The baton (data) needs to be passed from one runner to the next. The Shared Store is like the designated exchange zone where the baton is safely passed.\n\n**The Problem: Passing Information Between Nodes**\n\nWithout a shared space, each Node would operate in isolation.  They wouldn't know what other Nodes have done, and they couldn't build on each other's work.  The Shared Store provides a central location for Nodes to store and retrieve information, enabling collaboration and data flow within the PocketFlow ecosystem.\n\n**Our Research Agent Example (Again!)**\n\nRemember our research agent from [Chapter 1: Flow](01_flow.md)? The `SearchWeb` Node needs to pass the search results to the `AnswerQuestion` Node. Without a Shared Store, the `AnswerQuestion` Node wouldn't know what the web search found!\n\n**What *is* the Shared Store?**\n\nIn PocketFlow, the Shared Store is essentially a **dictionary**.  If you're new to programming, think of a dictionary as a container that holds key-value pairs. You can store data using a specific \"key\" and later retrieve it using that same key.\n\nFor example, we might store the search results with the key \"search_results\":\n\n```python\nshared_store[\"search_results\"] = \"The capital of France is Paris.\"\n```\n\nLater, another Node can access this information:\n\n```python\ncapital = shared_store[\"search_results\"]\nprint(capital) # Output: The capital of France is Paris.\n```\n\n**Key Concepts of the Shared Store**\n\n1.  **Centralized Data:** It's a single place where all Nodes can access and modify data.\n\n2.  **Key-Value Pairs:** Data is stored as key-value pairs, like a Python dictionary.\n\n3.  **Accessibility:** Any Node in the [Flow](01_flow.md) can access the Shared Store.\n\n4.  **Dynamic:** Nodes can add, modify, and delete data in the Shared Store during the [Flow](01_flow.md)'s execution.\n\n**How to Use the Shared Store**\n\nLet's revisit our `AddNumbers` Node from [Chapter 2: Node](02_node.md) and see how it uses the Shared Store:\n\n```python\nfrom pocketflow import Node\n\nclass AddNumbers(Node):\n    def prep(self, shared):\n        \"\"\"Prepares the input numbers from the shared store.\"\"\"\n        num1 = shared[\"num1\"]\n        num2 = shared[\"num2\"]\n        return num1, num2\n\n    def exec(self, inputs):\n        \"\"\"Adds the two numbers together.\"\"\"\n        num1, num2 = inputs\n        result = num1 + num2\n        return result\n\n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Stores the result in the shared store.\"\"\"\n        shared[\"result\"] = exec_res\n        return None  # No next node, the Flow can end\n```\n\n*   In the `prep` method, the Node retrieves the values associated with the keys \"num1\" and \"num2\" from the `shared` store.\n\n*   In the `post` method, the Node stores the calculated `result` in the `shared` store using the key \"result\".\n\n**Example Usage**\n\nHere's how the Shared Store is used when we run the [Flow](01_flow.md):\n\n```python\nfrom pocketflow import Flow\nfrom nodes import AddNumbers # Assuming the AddNumbers node is in nodes.py\n\n# Create an instance of the AddNumbers node\nadd_node = AddNumbers()\n\n# Create a Flow starting with the AddNumbers node\nflow = Flow(start=add_node)\n\n# Create a shared store with the input numbers\nshared_store = {\"num1\": 5, \"num2\": 3}\n\n# Run the flow\nflow.run(shared_store)\n\n# Print the result\nprint(shared_store[\"result\"])  # Output: 8\n```\n\n*   We create a `shared_store` (a dictionary) and initialize it with the input values for \"num1\" and \"num2\". This is how we *seed* the Shared Store with initial data.\n\n*   After the `flow.run(shared_store)` call, the `shared_store` now contains the key \"result\" with the value 8, because the `AddNumbers` Node stored it there.\n\n**Another Example: Passing the Question to the Answer Node**\nLet's look back at the nodes of our research agent in `cookbook/pocketflow-agent/nodes.py`. The `DecideAction` Node checks whether to search or answer the question and it has access to a \"question\" value in the shared store.\n\n```python\nclass DecideAction(Node):\n    def prep(self, shared):\n        # Prepare the context and question\n        question = shared[\"question\"] #Access question value from shared store\n        context = shared.get(\"context\", \"No previous search\")\n        return question, context\n    #Skipping the other functions for brevity.\n```\n\nIn this case, the `prep` function of `DecideAction` node accesses the question value from the shared store using the key \"question\".\n\n**Important Note:**\n\n*   **Key Existence:** It's good practice to check if a key exists in the Shared Store before accessing it, especially if the Node isn't the one who put the value there. You can use `shared.get(\"key\", default_value)` which returns `default_value` if the key doesn't exist. Or `if \"key\" in shared:` to check for a key's existence.\n\n**Under the Hood: How it Works**\n\nWhen you run a [Flow](01_flow.md), the `Flow` object creates and passes a Shared Store object to each Node's `prep`, `exec`, and `post` methods.\n\nHere's a simplified sequence diagram:\n\n```mermaid\nsequenceDiagram\n    participant Flow\n    participant Node1\n    participant Node2\n    participant SharedStore\n\n    Flow->>SharedStore: Create SharedStore\n    Flow->>Node1: run(SharedStore)\n    Node1->>SharedStore: Access/Modify data\n    Node1-->>Flow: (Next Action)\n    Flow->>Node2: run(SharedStore)\n    Node2->>SharedStore: Access/Modify data\n    Node2-->>Flow: (End)\n```\n\n*   The `Flow` creates a single `SharedStore` instance at the beginning.\n\n*   This same `SharedStore` instance is passed to each `Node` in the [Flow](01_flow.md).\n\n*   Each `Node` can then read from and write to the `SharedStore`.\n\n**Code Snippet: Flow Passing the Shared Store (From `pocketflow/__init__.py`)**\nThis is how the [Flow](01_flow.md) passes the Shared Store to each node:\n\n```python\nclass Flow(BaseNode):\n    def __init__(self,start): super().__init__();self.start=start\n    def get_next_node(self,curr,action):\n        nxt=curr.successors.get(action or \"default\")\n        if not nxt and curr.successors: warnings.warn(f\"Flow ends: '{action}' not found in {list(curr.successors)}\")\n        return nxt\n    def _orch(self,shared,params=None):\n        curr,p=copy.copy(self.start),(params or {**self.params})\n        while curr: curr.set_params(p);c=curr._run(shared);curr=copy.copy(self.get_next_node(curr,c))\n    def _run(self,shared): pr=self.prep(shared);self._orch(shared);return self.post(shared,pr,None)\n    def exec(self,prep_res): raise RuntimeError(\"Flow can't exec.\")\n```\nIn the `_orch` function, the shared store is maintained and passed into each subsequent node. Note the line `c=curr._run(shared)` indicating the shared store is passed into the node's run function.\n\n**Conclusion**\n\nIn this chapter, we learned about the Shared Store, a crucial component of PocketFlow that allows Nodes to communicate and share data. We saw how the Shared Store acts as a central memory, enabling seamless data transfer between processing steps. We learned how Nodes access and modify data in the Shared Store, and how the Flow manages the Shared Store behind the scenes.\n\nNow that we understand the Shared Store, we can appreciate how PocketFlow Nodes interact within a Flow to accomplish complex tasks. In the next chapter, [Prep, Exec, Post (Node Lifecycle)](04_prep__exec__post__node_lifecycle_.md), we'll dive deeper into the `prep`, `exec`, and `post` methods and understand the lifecycle of a Node.\n\n---\n# Chapter 4: Prep, Exec, Post (Node Lifecycle)\n\nIn [Chapter 2: Node](02_node.md), we learned about Nodes and how they encapsulate individual actions. We briefly touched upon the `prep`, `exec`, and `post` methods. Now, let's dive deeper into what these methods do and how they define the lifecycle of a Node.\n\nImagine you're a chef preparing a dish.\n\n*   **Prep:** You gather your ingredients, chop vegetables, and prepare your cooking station.\n*   **Exec:** You actually cook the dish \u2013 the main action.\n*   **Post:** You plate the dish, add garnishes, and serve it.\n\nThe `prep`, `exec`, and `post` methods in a Node are similar! They provide a structured way to define the different stages of a Node's operation.\n\n**The Problem: Organizing Node Logic**\n\nWithout a clear structure, the code inside a Node can become disorganized and hard to understand.  The `prep`, `exec`, and `post` pattern provides a consistent way to organize Node logic, making it easier to read, maintain, and debug.\n\n**Our Research Agent Example (One More Time!)**\n\nLet's revisit our research agent and focus on the `SearchWeb` Node. It needs to:\n\n1.  **Prep:** Get the search query from the [Shared Store](03_shared_store.md).\n2.  **Exec:** Actually perform the web search using the query.\n3.  **Post:** Save the search results back into the [Shared Store](03_shared_store.md) and decide what's next.\n\n**Key Concepts: Prep, Exec, Post**\n\n*   **Prep (Preparation):**  This method prepares the input data needed for the main task (`exec`). It typically retrieves data from the [Shared Store](03_shared_store.md) or performs any necessary data transformations.\n    *   **Input:** The [Shared Store](03_shared_store.md).\n    *   **Output:**  Data that the `exec` method needs (can be any Python object).\n\n*   **Exec (Execution):** This is the heart of the Node. It performs the core logic of the Node, using the data prepared by the `prep` method.\n    *   **Input:** The output of the `prep` method.\n    *   **Output:** The result of the Node's main operation (can be any Python object).\n\n*   **Post (Post-processing):** This method processes the output of the `exec` method. It typically saves the results back into the [Shared Store](03_shared_store.md), performs any cleanup, and determines the next step in the [Flow](01_flow.md). It specifies which Node to run next in the flow.\n    *   **Input:** The [Shared Store](03_shared_store.md), the output of `prep`, and the output of `exec`.\n    *   **Output:** A value that determines which Node to execute next in the [Flow](01_flow.md). If it returns `None`, the flow can end.\n\n**Example: Back to the `AddNumbers` Node**\n\nLet's examine the `AddNumbers` Node again:\n\n```python\nfrom pocketflow import Node\n\nclass AddNumbers(Node):\n    def prep(self, shared):\n        \"\"\"Prepares the input numbers from the shared store.\"\"\"\n        num1 = shared[\"num1\"]\n        num2 = shared[\"num2\"]\n        return num1, num2\n\n    def exec(self, inputs):\n        \"\"\"Adds the two numbers together.\"\"\"\n        num1, num2 = inputs\n        result = num1 + num2\n        return result\n\n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Stores the result in the shared store.\"\"\"\n        shared[\"result\"] = exec_res\n        return None  # No next node, the Flow can end\n```\n\n*   **`prep`**: Retrieves `num1` and `num2` from the [Shared Store](03_shared_store.md) and returns them as a tuple.\n*   **`exec`**: Takes the tuple from `prep`, adds the numbers, and returns the `result`.\n*   **`post`**: Stores the `result` in the [Shared Store](03_shared_store.md) and returns `None`, indicating the end of the [Flow](01_flow.md).\n\n**Another Example: `SearchWeb` Node from the Research Agent**\n\n```python\nfrom pocketflow import Node\nfrom utils import search_web # Assuming you have a search_web utility\n\nclass SearchWeb(Node):\n    def prep(self, shared):\n        \"\"\"Get the search query from the shared store.\"\"\"\n        return shared[\"search_query\"]\n        \n    def exec(self, search_query):\n        \"\"\"Search the web for the given query.\"\"\"\n        print(f\"\ud83c\udf10 Searching the web for: {search_query}\")\n        results = search_web(search_query)\n        return results\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Save the search results and go back to the decision node.\"\"\"\n        shared[\"context\"] = \"SEARCH RESULTS: \" + exec_res\n        return \"decide\" # Back to DecideAction node\n```\n\n*   **`prep`**:  Retrieves the `search_query` from the [Shared Store](03_shared_store.md).\n*   **`exec`**:  Performs the web search using `search_web` and returns the `results`.\n*   **`post`**: Saves the `results` into the [Shared Store](03_shared_store.md) under the key \"context\", and returns `\"decide\"` to tell the [Flow](01_flow.md) to go back to `DecideAction` ([Chapter 1: Flow](01_flow.md)).\n\n**How It Works: Example Execution**\n\nLet's trace the execution of the `SearchWeb` Node:\n\n1.  The [Flow](01_flow.md) calls the `run` method of the `SearchWeb` Node, passing the [Shared Store](03_shared_store.md).\n2.  The `prep` method retrieves the `search_query` (e.g., \"capital of France\") from the [Shared Store](03_shared_store.md).\n3.  The `exec` method uses the `search_query` to search the web. Let's say it gets the result \"Paris is the capital of France.\"\n4.  The `post` method saves the search result \"Paris is the capital of France.\" into the [Shared Store](03_shared_store.md). Also the return value is \"decide\".\n5.  The [Flow](01_flow.md) then uses the `\"decide\"` return value to determine the next Node to run.\n\n**Under the Hood: The `_run` Method**\n\nThe `Node` class in `pocketflow/__init__.py` has a `_run` method that orchestrates the `prep`, `exec`, and `post` methods:\n\n```python\nclass Node(BaseNode): #Simplified\n    def prep(self,shared): pass\n    def exec(self,prep_res): pass\n    def post(self,shared,prep_res,exec_res): pass\n    def _exec(self,prep_res): return self.exec(prep_res)\n    def _run(self,shared): p=self.prep(shared);e=self._exec(p);return self.post(shared,p,e)\n```\n\nAs you can see, `_run` calls `prep`, then `exec`, and finally `post` in sequence.  It passes the output of one method as input to the next.\n\nLet's visualize this with a sequence diagram:\n\n```mermaid\nsequenceDiagram\n    participant Node\n    participant SharedStore\n\n    Node->>Node: prep(shared_store)\n    Node-->>Node: prep_result\n    Node->>Node: exec(prep_result)\n    Node-->>Node: exec_result\n    Node->>Node: post(shared_store, prep_result, exec_result)\n    Node-->>Node: next_action\n```\n\n1.  `prep(shared_store)`: The `prep` method is called with the [Shared Store](03_shared_store.md).\n2.  `exec(prep_result)`: The `exec` method is called with the result of `prep`.\n3.  `post(shared_store, prep_result, exec_result)`: The `post` method is called with the [Shared Store](03_shared_store.md), the result of `prep`, and the result of `exec`. The `post` function's return value (next_action) controls what node to run next.\n\n**Why This Structure Matters**\n\n*   **Organization:** Clearly separates data preparation, core logic, and post-processing.\n*   **Testability:**  Each method can be tested independently.\n*   **Reusability:** Nodes become more modular and easier to reuse in different [Flows](01_flow.md).\n*   **Readability:**  The `prep`, `exec`, `post` structure makes the code easier to understand.\n\n**Conclusion**\n\nIn this chapter, we learned about the `prep`, `exec`, and `post` methods and how they define the lifecycle of a Node.  We saw how this structure helps organize Node logic, making it more readable, testable, and reusable. We saw how each function affects the [Shared Store](03_shared_store.md) and how the `post` function returns the next node to execute.\n\nNow that we understand the Node lifecycle, let's move on to [Chapter 5: BatchNode](05_batchnode.md) to see how we can process multiple items in a single Node using `BatchNode`.\n\n---\n# Chapter 5: BatchNode\n\nIn [Chapter 4: Prep, Exec, Post (Node Lifecycle)](04_prep__exec__post__node_lifecycle_.md), we learned about the `prep`, `exec`, and `post` methods and how they structure a Node's lifecycle. Now, let's explore how to efficiently process *multiple* data items within a single Node using the `BatchNode`.\n\nImagine you're running a bakery. Instead of baking one cookie at a time, you bake dozens on a single tray! The `BatchNode` is like that tray \u2013 it lets you process a \"batch\" of items together.\n\n**The Problem: Processing Many Items Efficiently**\n\nSometimes, you need to perform the same operation on a large number of independent data items. Doing this with a regular [Node](02_node.md) would involve iterating through each item individually, which can be slow and inefficient. The `BatchNode` solves this by allowing you to process these items in a batch, making the process much faster.\n\n**Our Use Case: Translating a Document into Multiple Languages**\n\nLet's say you have a document and you want to translate it into several languages (like Chinese, Spanish, French, etc.). Instead of creating a separate [Node](02_node.md) for each language, we can use a `BatchNode` to translate the document into all the languages at once!\n\n**What is a BatchNode?**\n\nA `BatchNode` is a special type of [Node](02_node.md) that's designed to process multiple data items in a single execution. Think of it like an assembly line:\n\n1.  **Prep:** The `prep` method prepares the input data and splits it into chunks. These chunks are individual items or mini-batches within the larger batch.\n2.  **Exec:** The `exec` method is applied to each chunk of data. It performs the same operation on each item in the chunk.\n3.  **Post:** The `post` method combines the results from processing all the chunks and prepares the final output.\n\n**Key Concepts of a BatchNode**\n\n1.  **Batch Processing:** The ability to process multiple independent data items together.\n2.  **Chunking:** Splitting the input data into smaller, manageable chunks.\n3.  **Parallelism (Optional):** The `exec` method can potentially process chunks in parallel (we'll explore this in later chapters). For now, we'll assume it's processed sequentially.\n4.  **Result Aggregation:** Combining the results from processing all the chunks into a final output.\n\n**Creating a BatchNode for Translation**\n\nLet's create a `BatchNode` that translates a given text into multiple languages:\n\n```python\nfrom pocketflow import BatchNode\n\nclass TranslateTextNode(BatchNode):\n    def prep(self, shared):\n        text = shared.get(\"text\", \"(No text provided)\")\n        languages = shared.get(\"languages\", [\"Chinese\", \"Spanish\"]) # Reduced languages for brevity\n        \n        # Create batches for each language translation\n        return [(text, lang) for lang in languages]\n\n    def exec(self, data_tuple):\n        text, language = data_tuple\n        #Simulate Translation for brevity\n        translation=f\"Translated to {language}: {text}\"\n        return {\"language\": language, \"translation\": translation}\n\n    def post(self, shared, prep_res, exec_res_list):\n        # Print each translation result\n        for result in exec_res_list:\n            language, translation = result[\"language\"], result[\"translation\"]\n            print(f\"Translation to {language}: {translation}\")\n        return None\n```\n\nHere's a breakdown:\n\n*   `TranslateTextNode` inherits from `BatchNode`.\n*   `prep(self, shared)`: This method retrieves the text to translate and the list of languages from the [Shared Store](03_shared_store.md). It then creates a list of tuples, where each tuple contains the text and a language. This list becomes the \"batch\" of items to be processed.\n*   `exec(self, data_tuple)`: This method takes a single tuple (text, language) as input. It simulates translation for demonstration purposes by returning a translated version of the text.\n*   `post(self, shared, prep_res, exec_res_list)`: This method receives a list of results, where each result is a dictionary containing the translated text and the language. It prints each translation to the console.\n\n**Using the BatchNode in a Flow**\n\nNow, let's see how to use this `BatchNode` in a [Flow](01_flow.md):\n\n```python\nfrom pocketflow import Flow\nfrom nodes import TranslateTextNode # Assuming the TranslateTextNode is in nodes.py\n\n# Create an instance of the TranslateTextNode\ntranslate_node = TranslateTextNode()\n\n# Create a Flow starting with the TranslateTextNode\nflow = Flow(start=translate_node)\n\n# Create a shared store with the input text and languages\nshared_store = {\n    \"text\": \"Hello, world!\",\n    \"languages\": [\"Chinese\", \"Spanish\"]  # Reduced languages for brevity\n}\n\n# Run the flow\nflow.run(shared_store)\n\n# Expected Output:\n# Translation to Chinese: Translated to Chinese: Hello, world!\n# Translation to Spanish: Translated to Spanish: Hello, world!\n```\n\nHere's what's happening:\n\n1.  We create an instance of the `TranslateTextNode`.\n2.  We create a [Flow](01_flow.md) that starts with the `TranslateTextNode`.\n3.  We create a `shared_store` ([Chapter 3: Shared Store](03_shared_store.md)) and populate it with the text and languages.\n4.  We run the [Flow](01_flow.md). The `TranslateTextNode` will translate the text into each language and print the results.\n\n**Let's Trace the Execution**\n\n1.  The [Flow](01_flow.md) starts at the `TranslateTextNode`.\n2.  The `TranslateTextNode`'s `prep` method is called. It retrieves the text and languages from the `shared_store` and creates a list of tuples: `[(\"Hello, world!\", \"Chinese\"), (\"Hello, world!\", \"Spanish\")]`.\n3.  The `TranslateTextNode`'s `exec` method is called *twice*:\n    *   First, with the tuple `(\"Hello, world!\", \"Chinese\")`. It returns `{\"language\": \"Chinese\", \"translation\": \"Translated to Chinese: Hello, world!\"}`.\n    *   Second, with the tuple `(\"Hello, world!\", \"Spanish\")`. It returns `{\"language\": \"Spanish\", \"translation\": \"Translated to Spanish: Hello, world!\"}`.\n4.  The `TranslateTextNode`'s `post` method is called with the list of results: `[{\"language\": \"Chinese\", \"translation\": \"Translated to Chinese: Hello, world!\"}, {\"language\": \"Spanish\", \"translation\": \"Translated to Spanish: Hello, world!\"}]`. It prints each translation to the console.\n5.  The [Flow](01_flow.md) finishes because the `TranslateTextNode` returned `None` in the `post` method.\n\n**Under the Hood: How BatchNode Works**\n\nWhen a [Flow](01_flow.md) runs a `BatchNode`, the `_exec` method is overridden to process items as a batch.\n\nHere's a simplified sequence diagram:\n\n```mermaid\nsequenceDiagram\n    participant Flow\n    participant BatchNode\n    participant SharedStore\n\n    Flow->>BatchNode: run(shared_store)\n    BatchNode->>SharedStore: access text, languages\n    BatchNode-->>Flow: [(text, lang1), (text, lang2), ...]\n    loop For each item in batch\n        Flow->>BatchNode: exec((text, lang_i))\n        BatchNode-->>Flow: {language: lang_i, translation: ...}\n    end\n    Flow->>BatchNode: post(shared_store, prep_res, exec_res_list)\n    BatchNode->>SharedStore: Store results (optional)\n    BatchNode-->>Flow: None\n```\n\n1.  The [Flow](01_flow.md) calls the `run` method of the `BatchNode`, passing the [Shared Store](03_shared_store.md).\n2.  The `prep` method retrieves the necessary data from the [Shared Store](03_shared_store.md) and prepares the batch of items.\n3.  The `exec` method is called for each item in the batch.\n4.  The `post` method receives a list of results from all the `exec` calls.\n\nLet's look at the relevant code from `pocketflow/__init__.py`:\n\n```python\nclass BatchNode(Node):\n    def _exec(self,items): return [super(BatchNode,self)._exec(i) for i in (items or [])]\n```\n\nThis code snippet shows that the `_exec` method of the `BatchNode` iterates through the `items` (which are the output from the `prep` function) and calls the `exec` method for each item. The results are then collected into a list and returned.\n\n**Benefits of Using BatchNode**\n\n*   **Improved Efficiency:** Processes multiple items in a single Node execution.\n*   **Simplified Code:** Reduces the need for manual iteration within the [Flow](01_flow.md).\n*   **Potential for Parallelism:** Makes it easier to parallelize processing (we'll see this later!).\n\n**Real World example - CSV Processing**\n\nAnother use case is processing CSV files.\n```python\nimport pandas as pd\nfrom pocketflow import BatchNode\n\nclass CSVProcessor(BatchNode):\n    def __init__(self, chunk_size=1000):\n        super().__init__()\n        self.chunk_size = chunk_size\n    \n    def prep(self, shared):\n        chunks = pd.read_csv(\n            shared[\"input_file\"],\n            chunksize=self.chunk_size\n        )\n        return chunks\n    \n    def exec(self, chunk):\n        return {\n            \"total_sales\": chunk[\"amount\"].sum(),\n            \"num_transactions\": len(chunk),\n            \"total_amount\": chunk[\"amount\"].sum()\n        }\n    \n    def post(self, shared, prep_res, exec_res_list):\n        total_sales = sum(res[\"total_sales\"] for res in exec_res_list)\n        shared[\"total_sales\"] = total_sales\n        return None\n```\n\nHere, the `prep` function reads the CSV file in chunks (batches) of `chunk_size`. The `exec` function processes each chunk and calculates sales stats, and the `post` function adds up all of the sales.\n\n**Conclusion**\n\nIn this chapter, we learned about the `BatchNode` and how it allows us to process multiple data items efficiently within a single Node. We saw how to create a `BatchNode` for translating text into multiple languages and how it works under the hood.\n\nNow that we understand `BatchNode`, let's move on to [Chapter 6: AsyncNode](06_asyncnode.md) to see how we can handle asynchronous operations within PocketFlow.\n\n\nRelevant Code Snippets:\n--- File: cookbook/pocketflow-async-basic/nodes.py ---\nfrom pocketflow import AsyncNode\nfrom utils import fetch_recipes, call_llm_async, get_user_input\n\nclass FetchRecipes(AsyncNode):\n    \"\"\"AsyncNode that fetches recipes.\"\"\"\n    \n    async def prep_async(self, shared):\n        \"\"\"Get ingredient from user.\"\"\"\n        ingredient = await get_user_input(\"Enter ingredient: \")\n        return ingredient\n    \n    async def exec_async(self, ingredient):\n        \"\"\"Fetch recipes asynchronously.\"\"\"\n        recipes = await fetch_recipes(ingredient)\n        return recipes\n    \n    async def post_async(self, shared, prep_res, recipes):\n        \"\"\"Store recipes and continue.\"\"\"\n        shared[\"recipes\"] = recipes\n        shared[\"ingredient\"] = prep_res\n        return \"suggest\"\n\nclass SuggestRecipe(AsyncNode):\n    \"\"\"AsyncNode that suggests a recipe using LLM.\"\"\"\n    \n    async def prep_async(self, shared):\n        \"\"\"Get recipes from shared store.\"\"\"\n        return shared[\"recipes\"]\n    \n    async def exec_async(self, recipes):\n        \"\"\"Get suggestion from LLM.\"\"\"\n        suggestion = await call_llm_async(\n            f\"Choose best recipe from: {', '.join(recipes)}\"\n        )\n        return suggestion\n    \n    async def post_async(self, shared, prep_res, suggestion):\n        \"\"\"Store suggestion and continue.\"\"\"\n        shared[\"suggestion\"] = suggestion\n        return \"approve\"\n\nclass GetApproval(AsyncNode):\n    \"\"\"AsyncNode that gets user approval.\"\"\"\n    \n    async def prep_async(self, shared):\n        \"\"\"Get current suggestion.\"\"\"\n        return shared[\"suggestion\"]\n    \n    async def exec_async(self, suggestion):\n        \"\"\"Ask for user approval.\"\"\"\n        answer = await get_user_input(f\"\\nAccept this recipe? (y/n): \")\n        return answer\n    \n    async def post_async(self, shared, prep_res, answer):\n        \"\"\"Handle user's decision.\"\"\"\n        if answer == \"y\":\n            print(\"\\nGreat choice! Here's your recipe...\")\n            print(f\"Recipe: {shared['suggestion']}\")\n            print(f\"Ingredient: {shared['ingredient']}\")\n            return \"accept\"\n        else:\n            print(\"\\nLet's try another recipe...\")\n            return \"retry\" \n\n--- File: pocketflow/__init__.py ---\nimport asyncio, warnings, copy, time\n\nclass BaseNode:\n    def __init__(self): self.params,self.successors={},{}\n    def set_params(self,params): self.params=params\n    def add_successor(self,node,action=\"default\"):\n        if action in self.successors: warnings.warn(f\"Overwriting successor for action '{action}'\")\n        self.successors[action]=node;return node\n    def prep(self,shared): pass\n    def exec(self,prep_res): pass\n    def post(self,shared,prep_res,exec_res): pass\n    def _exec(self,prep_res): return self.exec(prep_res)\n    def _run(self,shared): p=self.prep(shared);e=self._exec(p);return self.post(shared,p,e)\n    def run(self,shared): \n        if self.successors: warnings.warn(\"Node won't run successors. Use Flow.\")  \n        return self._run(shared)\n    def __rshift__(self,other): return self.add_successor(other)\n    def __sub__(self,action):\n        if isinstance(action,str): return _ConditionalTransition(self,action)\n        raise TypeError(\"Action must be a string\")\n\nclass _ConditionalTransition:\n    def __init__(self,src,action): self.src,self.action=src,action\n    def __rshift__(self,tgt): return self.src.add_successor(tgt,self.action)\n\nclass Node(BaseNode):\n    def __init__(self,max_retries=1,wait=0): super().__init__();self.max_retries,self.wait=max_retries,wait\n    def exec_fallback(self,prep_res,exc): raise exc\n    def _exec(self,prep_res):\n        for self.cur_retry in range(self.max_retries):\n            try: return self.exec(prep_res)\n            except Exception as e:\n                if self.cur_retry==self.max_retries-1: return self.exec_fallback(prep_res,e)\n                if self.wait>0: time.sleep(self.wait)\n\nclass BatchNode(Node):\n    def _exec(self,items): return [super(BatchNode,self)._exec(i) for i in (items or [])]\n\nclass Flow(BaseNode):\n    def __init__(self,start): super().__init__();self.start=start\n    def get_next_node(self,curr,action):\n        nxt=curr.successors.get(action or \"default\")\n        if not nxt and curr.successors: warnings.warn(f\"Flow ends: '{action}' not found in {list(curr.successors)}\")\n        return nxt\n    def _orch(self,shared,params=None):\n        curr,p=copy.copy(self.start),(params or {**self.params})\n        while curr: curr.set_params(p);c=curr._run(shared);curr=copy.copy(self.get_next_node(curr,c))\n    def _run(self,shared): pr=self.prep(shared);self._orch(shared);return self.post(shared,pr,None)\n    def exec(self,prep_res): raise RuntimeError(\"Flow can't exec.\")\n\nclass BatchFlow(Flow):\n    def _run(self,shared):\n        pr=self.prep(shared) or []\n        for bp in pr: self._orch(shared,{**self.params,**bp})\n        return self.post(shared,pr,None)\n\nclass AsyncNode(Node):\n    def prep(self,shared): raise RuntimeError(\"Use prep_async.\")\n    def exec(self,prep_res): raise RuntimeError(\"Use exec_async.\")\n    def post(self,shared,prep_res,exec_res): raise RuntimeError(\"Use post_async.\")\n    def exec_fallback(self,prep_res,exc): raise RuntimeError(\"Use exec_fallback_async.\")\n    def _run(self,shared): raise RuntimeError(\"Use run_async.\")\n    async def prep_async(self,shared): pass\n    async def exec_async(self,prep_res): pass\n    async def exec_fallback_async(self,prep_res,exc): raise exc\n    async def post_async(self,shared,prep_res,exec_res): pass\n    async def _exec(self,prep_res): \n        for i in range(self.max_retries):\n            try: return await self.exec_async(prep_res)\n            except Exception as e:\n                if i==self.max_retries-1: return await self.exec_fallback_async(prep_res,e)\n                if self.wait>0: await asyncio.sleep(self.wait)\n    async def run_async(self,shared): \n        if self.successors: warnings.warn(\"Node won't run successors. Use AsyncFlow.\")  \n        return await self._run_async(shared)\n    async def _run_async(self,shared): p=await self.prep_async(shared);e=await self._exec(p);return await self.post_async(shared,p,e)\n\nclass AsyncBatchNode(AsyncNode,BatchNode):\n    async def _exec(self,items): return [await super(AsyncBatchNode,self)._exec(i) for i in items]\n\nclass AsyncParallelBatchNode(AsyncNode,BatchNode):\n    async def _exec(self,items): return await asyncio.gather(*(super(AsyncParallelBatchNode,self)._exec(i) for i in items))\n\nclass AsyncFlow(Flow,AsyncNode):\n    async def _orch_async(self,shared,params=None):\n        curr,p=copy.copy(self.start),(params or {**self.params})\n        while curr:curr.set_params(p);c=await curr._run_async(shared) if isinstance(curr,AsyncNode) else curr._run(shared);curr=copy.copy(self.get_next_node(curr,c))\n    async def _run_async(self,shared): p=await self.prep_async(shared);await self._orch_async(shared);return await self.post_async(shared,p,None)\n\nclass AsyncBatchFlow(AsyncFlow,BatchFlow):\n    async def _run_async(self,shared):\n        pr=await self.prep_async(shared) or []\n        for bp in pr: await self._orch_async(shared,{**self.params,**bp})\n        return await self.post_async(shared,pr,None)\n\nclass AsyncParallelBatchFlow(AsyncFlow,BatchFlow):\n    async def _run_async(self,shared):\n        pr=await self.prep_async(shared) or []\n        await asyncio.gather(*(self._orch_async(shared,{**self.params,**bp}) for bp in pr))\n        return await self.post_async(shared,pr,None)\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 6: AsyncNode`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "# Chapter 6: AsyncNode\n\nIn [Chapter 5: BatchNode](05_batchnode.md), we learned how to process multiple items efficiently using `BatchNode`. But what if some of the operations inside our Nodes take a *long* time, like waiting for a response from a website or a database? This is where `AsyncNode` comes in!\n\nImagine you're ordering food online. You don't want your whole computer to freeze while waiting for the restaurant to confirm your order, right? You want to be able to do other things while your food is being prepared. `AsyncNode` allows your PocketFlow to do just that - perform non-blocking, asynchronous operations.\n\n**The Problem: Dealing with Time-Consuming Operations**\n\nSome tasks take a while. Network requests, database queries, and complex computations can all block the execution of our [Flow](01_flow.md), making it slow and unresponsive. We want PocketFlow to continue processing other tasks while these long-running operations are in progress.\n\n**Our Use Case: Recipe Generation with an LLM**\n\nLet's say we want to build a simple recipe app. Our [Flow](01_flow.md) will:\n\n1.  Ask the user for an ingredient.\n2.  Fetch recipes containing that ingredient from a website (takes time!).\n3.  Use an LLM (Large Language Model) to suggest the best recipe from the fetched recipes (also takes time!).\n4.  Ask the user if they approve of the recipe.\n\nFetching recipes from a website and interacting with an LLM can take several seconds. We don't want our app to freeze during this time! `AsyncNode` will allow us to handle these operations asynchronously, making our app more responsive.\n\n**What is an AsyncNode?**\n\nAn `AsyncNode` is a special type of [Node](02_node.md) that allows us to perform asynchronous operations using the `async` and `await` keywords in Python.\n\n*   **`async`**:  This keyword declares a function as a *coroutine*. A coroutine is a special type of function that can be paused and resumed.\n*   **`await`**:  This keyword is used inside an `async` function to wait for another coroutine to complete *without blocking* the execution of the rest of the [Flow](01_flow.md).\n\nThink of `await` as telling your program, \"Hey, I'm going to wait for this to finish, but don't just sit there doing nothing! Go do something else in the meantime.\"\n\n**Key Concepts of an AsyncNode**\n\n1.  **Asynchronous Operations:** Performing tasks without blocking the main thread of execution.\n2.  **`async` and `await` Keywords:** Used to define and control asynchronous operations in Python.\n3.  **Non-Blocking:** Allows the [Flow](01_flow.md) to continue processing other tasks while waiting for asynchronous operations to complete.\n\n**Creating an AsyncNode for Fetching Recipes**\n\nLet's create an `AsyncNode` that fetches recipes from a website. We'll simplify things by using a dummy function that simulates a network request:\n\n```python\nimport asyncio\nfrom pocketflow import AsyncNode\n\n# Dummy function to simulate fetching recipes (takes time!)\nasync def fetch_recipes(ingredient):\n    print(f\"\ud83d\ude34 Fetching recipes for {ingredient}... This will take a moment.\")\n    await asyncio.sleep(2)  # Simulate a 2-second delay\n    return [\"Pasta with \" + ingredient, \"Pizza with \" + ingredient]\n\nclass FetchRecipesNode(AsyncNode):\n    async def prep_async(self, shared):\n        # Prep data (can be synchronous or asynchronous)\n        ingredient = shared.get(\"ingredient\", \"default_ingredient\")\n        return ingredient\n\n    async def exec_async(self, ingredient):\n        # Asynchronously fetch recipes\n        recipes = await fetch_recipes(ingredient)\n        return recipes\n\n    async def post_async(self, shared, prep_res, exec_res):\n        # Store the fetched recipes and return a value\n        shared[\"recipes\"] = exec_res\n        return \"suggest\"\n```\n\nHere's what's happening:\n\n*   `FetchRecipesNode` inherits from `AsyncNode`.\n*   `fetch_recipes(ingredient)` is an `async` function that simulates fetching recipes from a website. It uses `await asyncio.sleep(2)` to pause execution for 2 seconds, simulating the delay of a network request.\n*   `prep_async(self, shared)`: This asynchronous `prep` function retrieves the ingredient from the [Shared Store](03_shared_store.md).\n*   `exec_async(self, ingredient)`: This asynchronous `exec` function calls `fetch_recipes(ingredient)` using `await`. This means the `exec` function will pause execution until `fetch_recipes` completes, but *without blocking* the rest of the [Flow](01_flow.md).\n*   `post_async(self, shared, prep_res, exec_res)`: This asynchronous `post` function stores the fetched recipes in the [Shared Store](03_shared_store.md) and returns `\"suggest\"`.\n\n**Important:** Notice that we use `prep_async`, `exec_async`, and `post_async` instead of the regular `prep`, `exec`, and `post` methods. `AsyncNode` requires these `async` versions.\n\n**Using the AsyncNode in a Flow**\n\nNow, let's see how to use this `AsyncNode` in a [Flow](01_flow.md):\n\n```python\nimport asyncio\nfrom pocketflow import Flow\nfrom nodes import FetchRecipesNode # Assuming the FetchRecipesNode is in nodes.py\n\n# Create an instance of the FetchRecipesNode\nfetch_recipes_node = FetchRecipesNode()\n\n# Create a Flow starting with the FetchRecipesNode\nflow = Flow(start=fetch_recipes_node)\n\n# Create a shared store\nshared_store = {\"ingredient\": \"chicken\"}\n\n# Run the flow (using asyncio.run for async flows)\nasync def run_flow():\n    flow.run(shared_store)\n\nasyncio.run(run_flow())\n\n# Expected output:\n# \ud83d\ude34 Fetching recipes for chicken... This will take a moment.\n```\n\nHere's what's happening:\n\n1.  We create an instance of the `FetchRecipesNode`.\n2.  We create a [Flow](01_flow.md) that starts with the `FetchRecipesNode`.\n3.  We create a `shared_store` ([Chapter 3: Shared Store](03_shared_store.md)).\n4.  We define an `async` function `run_flow` that calls `flow.run(shared_store)`.\n5.  We use `asyncio.run(run_flow())` to run the asynchronous [Flow](01_flow.md). *This is crucial!* You need to use `asyncio.run` to properly execute an asynchronous [Flow](01_flow.md).\n\n**Let's Trace the Execution**\n\n1.  The [Flow](01_flow.md) starts at the `FetchRecipesNode`.\n2.  The `FetchRecipesNode`'s `prep_async` method is called. It retrieves the ingredient \"chicken\" from the `shared_store`.\n3.  The `FetchRecipesNode`'s `exec_async` method is called with the ingredient \"chicken\".\n4.  The `exec_async` method calls `await fetch_recipes(\"chicken\")`. The `fetch_recipes` function simulates a network request and pauses execution for 2 seconds.\n5.  *During those 2 seconds, the PocketFlow can theoretically do other things (though in this simple example, there's nothing else to do).*\n6.  After 2 seconds, `fetch_recipes` returns a list of recipes.\n7.  The `exec_async` method returns the list of recipes.\n8.  The `FetchRecipesNode`'s `post_async` method is called. It stores the recipes in the `shared_store` and returns `\"suggest\"`.\n9.  The [Flow](01_flow.md) finishes (or continues to the next node, if there is one).\n\n**Under the Hood: How AsyncNode Works**\n\nThe `AsyncNode` class overrides the regular `prep`, `exec`, and `post` methods to prevent accidental use. It also provides the `_run_async` method, which orchestrates the execution of the `prep_async`, `exec_async`, and `post_async` methods.\n\nHere's a simplified sequence diagram:\n\n```mermaid\nsequenceDiagram\n    participant Flow\n    participant AsyncNode\n    participant SharedStore\n    participant EventLoop\n\n    Flow->>AsyncNode: run_async(shared_store)\n    AsyncNode->>AsyncNode: prep_async(shared_store)\n    AsyncNode-->>Flow: prep_result\n    Flow->>AsyncNode: exec_async(prep_result)\n    AsyncNode->>EventLoop: await fetch_recipes(prep_result)\n    EventLoop-->>AsyncNode: recipes\n    AsyncNode-->>Flow: recipes\n    Flow->>AsyncNode: post_async(shared_store, prep_result, recipes)\n    AsyncNode->>SharedStore: store recipes\n    AsyncNode-->>Flow: next_node\n```\n\n1.  The [Flow](01_flow.md) calls the `run_async` method of the `AsyncNode`, passing the [Shared Store](03_shared_store.md).\n2.  The `prep_async` method retrieves the necessary data from the [Shared Store](03_shared_store.md).\n3.  The `exec_async` method performs the asynchronous operation using `await`. This allows the event loop to execute other tasks while waiting for the operation to complete.\n4.  The `post_async` method processes the result and updates the [Shared Store](03_shared_store.md).\n\nLet's look at some of the relevant code from `pocketflow/__init__.py`:\n\n```python\nclass AsyncNode(Node):\n    def prep(self,shared): raise RuntimeError(\"Use prep_async.\")\n    def exec(self,prep_res): raise RuntimeError(\"Use exec_async.\")\n    def post(self,shared,prep_res,exec_res): raise RuntimeError(\"Use post_async.\")\n    def exec_fallback(self,prep_res,exc): raise RuntimeError(\"Use exec_fallback_async.\")\n    def _run(self,shared): raise RuntimeError(\"Use run_async.\")\n    async def prep_async(self,shared): pass\n    async def exec_async(self,prep_res): pass\n    async def exec_fallback_async(self,prep_res,exc): raise exc\n    async def post_async(self,shared,prep_res,exec_res): pass\n    async def _exec(self,prep_res): \n        for i in range(self.max_retries):\n            try: return await self.exec_async(prep_res)\n            except Exception as e:\n                if i==self.max_retries-1: return await self.exec_fallback_async(prep_res,e)\n                if self.wait>0: await asyncio.sleep(self.wait)\n    async def run_async(self,shared): \n        if self.successors: warnings.warn(\"Node won't run successors. Use AsyncFlow.\")  \n        return await self._run_async(shared)\n    async def _run_async(self,shared): p=await self.prep_async(shared);e=await self._exec(p);return await self.post_async(shared,p,e)\n```\n\nKey takeaways from this code:\n\n*   The regular `prep`, `exec`, `post`, and `_run` functions are overridden to throw `RuntimeError`s if called. This enforces the use of the `async` versions.\n*   The `_exec` function wraps the execution of `exec_async` and takes care of retries. The key line is `return await self.exec_async(prep_res)`.\n*   `run_async` calls the `_run_async` function.\n\n**Conclusion**\n\nIn this chapter, we learned about the `AsyncNode` and how it allows us to perform asynchronous operations within PocketFlow. We saw how to create an `AsyncNode` for fetching recipes from a website and how it works under the hood. Using `AsyncNode` makes the workflow more responsive and enables interaction with time-consuming external services.\n\nNow that we understand `AsyncNode`, let's move on to [Chapter 7: AsyncFlow](07_asyncflow.md) to see how we can build entire asynchronous flows using `AsyncFlow`.\n", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `PocketFlow` about the concept: \"AsyncFlow\". This is Chapter 7.\n\nConcept Details:\n- Description:\nExtends the `Flow` abstraction to support asynchronous operations within the workflow, like a chef juggling multiple dishes simultaneously. It allows you to run nodes concurrently, maximizing resource utilization and improving performance when dealing with tasks that involve waiting for external resources or I/O operations.\n\n\nComplete Tutorial Structure:\n1. [Flow](01_flow.md)\n2. [Node](02_node.md)\n3. [Shared Store](03_shared_store.md)\n4. [Prep, Exec, Post (Node Lifecycle)](04_prep__exec__post__node_lifecycle_.md)\n5. [BatchNode](05_batchnode.md)\n6. [AsyncNode](06_asyncnode.md)\n7. [AsyncFlow](07_asyncflow.md)\n8. [AsyncParallelBatchNode](08_asyncparallelbatchnode.md)\n\nContext from previous chapters (summary):\n# Chapter 1: Flow\n\nImagine you're directing a play. You have actors, each with specific roles (like answering a question or searching the web). But how do you tell them *when* to act, and *what* to do next?  That's where a **Flow** comes in!\n\nIn PocketFlow, a Flow is like the conductor of an orchestra. It's the orchestration layer that connects individual actions (we call them **Nodes**) into a functioning workflow. It defines the sequence in which these actions are executed and how data is passed between them.\n\n**Our Use Case: A Simple Research Agent**\n\nLet's say we're building a simple research agent that can either search the web or answer a question, depending on what's needed.  The agent will first *decide* what to do. If it needs to search, it will *search the web*. If it needs to answer, it will *answer the question*. After searching, it goes back to the deciding step.\n\nWithout a Flow, we'd have to manually call each action in the right order, which can get messy quickly. A Flow lets us define this process in a clear and organized way.\n\n**Key Concepts of a Flow**\n\n1.  **Nodes:** These are the individual steps or actions in our workflow. Think of them as the actors in our play. We'll dive deep into Nodes in [Chapter 2: Node](02_node.md). For now, just think of them as individual units of work.\n\n2.  **Sequence:** The Flow defines the order in which Nodes are executed.  It's the script of our play.\n\n3.  **Data Passing:** The Flow manages how data is passed between Nodes. This is how the actors communicate and share information.\n\n4.  **Starting Node:** Every flow must have a start node to kick off the process.\n\n5.  **Transitions:**  These define how the Flow moves from one Node to the next. We'll use special operators like `>>` (rshift) and `-` (subtraction) to define these transitions.\n\n**Creating Our Research Agent Flow**\n\nLet's look at how we can create a Flow for our research agent. The code might look like this (from `cookbook/pocketflow-agent/flow.py`):\n\n```python\nfrom pocketflow import Flow\nfrom nodes import DecideAction, SearchWeb, AnswerQuestion\n\ndef create_agent_flow():\n    # Create instances of each node\n    decide = DecideAction()\n    search = SearchWeb()\n    answer = AnswerQuestion()\n\n    # Connect the nodes\n    # If DecideAction returns \"search\", go to SearchWeb\n    decide - \"search\" >> search\n\n    # If DecideAction returns \"answer\", go to AnswerQuestion\n    decide - \"answer\" >> answer\n\n    # After SearchWeb completes and returns \"decide\", go back to DecideAction\n    search - \"decide\" >> decide\n\n    # Create and return the flow, starting with the DecideAction node\n    return Flow(start=decide)\n```\n\nHere's a breakdown:\n\n*   We import `Flow` from the `pocketflow` library.\n*   We import the `Node` types from our nodes file (`nodes.py`). Don't worry about the implementations of `DecideAction`, `SearchWeb`, and `AnswerQuestion` yet.  Just think of them as steps in our process.\n*   We create instances of each Node.\n*   We use the `>>` and `-` operators to define the transitions between Nodes. For example, `decide - \"search\" >> search` means \"If the `decide` Node returns 'search', then go to the `search` Node.\"\n*   We create a `Flow` object, specifying the `decide` Node as the starting point.\n\n**How it Works: Example**\n\nImagine the agent receives the question: \"What is the capital of France?\".\n\n1.  The `Flow` starts at the `DecideAction` Node.\n2.  The `DecideAction` Node determines that we need to search for the answer. It returns the string \"search\".\n3.  The `Flow` sees the transition `decide - \"search\" >> search` and moves to the `SearchWeb` Node.\n4.  The `SearchWeb` Node searches the web and finds the answer: \"Paris\". It stores the answer in memory and returns the string \"decide\".\n5.  The `Flow` sees the transition `search - \"decide\" >> decide` and moves back to the `DecideAction` Node.\n6.  Now `DecideAction` could determine we need to answer the original question. It returns the string \"answer\".\n7.  The `Flow` sees the transition `decide - \"answer\" >> answer` and moves to the `AnswerQuestion` Node.\n8.  The `AnswerQuestion` node crafts the answer using the information in memory and then returns its final response.\n\n**Under the Hood: A Peek Inside**\n\nLet's see what happens inside the `Flow` when we run it.  Here's a simplified sequence diagram:\n\n```mermaid\nsequenceDiagram\n    participant Flow\n    participant DecideAction\n    participant SearchWeb\n    participant AnswerQuestion\n    participant SharedStore\n\n    Flow->>DecideAction: run(shared_store)\n    DecideAction->>SharedStore: access/modify data\n    DecideAction-->>Flow: \"search\"\n    Flow->>SearchWeb: run(shared_store)\n    SearchWeb->>SharedStore: access/modify data\n    SearchWeb-->>Flow: \"decide\"\n    Flow->>DecideAction: run(shared_store)\n    DecideAction->>SharedStore: access/modify data\n    DecideAction-->>Flow: \"answer\"\n    Flow->>AnswerQuestion: run(shared_store)\n    AnswerQuestion->>SharedStore: access/modify data\n    AnswerQuestion-->>Flow: (End)\n```\n\n*   The `Flow` starts by calling the `run` method of the starting Node (`DecideAction` in our example).\n*   Each Node executes its logic and potentially modifies a `SharedStore` ([Chapter 3: Shared Store](03_shared_store.md)) which is used for passing data between the nodes.\n*   The Node returns a value (like \"search\" or \"answer\") that determines the next Node to execute, based on the transitions defined in the Flow.\n*   This process continues until there are no more transitions, or we hit a dead end (no successor node defined for a particular return value).\n\n**Code Dive: `Flow` Class**\n\nLet's look at some of the key parts of the `Flow` class (from `pocketflow/__init__.py`):\n\n```python\nclass Flow(BaseNode):\n    def __init__(self,start): super().__init__();self.start=start\n    def get_next_node(self,curr,action):\n        nxt=curr.successors.get(action or \"default\")\n        if not nxt and curr.successors: warnings.warn(f\"Flow ends: '{action}' not found in {list(curr.successors)}\")\n        return nxt\n    def _orch(self,shared,params=None):\n        curr,p=copy.copy(self.start),(params or {**self.params})\n        while curr: curr.set_params(p);c=curr._run(shared);curr=copy.copy(self.get_next_node(curr,c))\n    def _run(self,shared): pr=self.prep(shared);self._orch(shared);return self.post(shared,pr,None)\n    def exec(self,prep_res): raise RuntimeError(\"Flow can't exec.\")\n```\n\n*   `__init__(self, start)`: The constructor takes the starting Node as an argument.\n*   `get_next_node(self, curr, action)`:  This method determines the next Node to execute based on the return value (`action`) of the current Node (`curr`). It looks up the successor node in the current node's `successors` dictionary.\n*   `_orch(self, shared, params=None)`:  This is the \"orchestration\" method.  It iterates through the Nodes, executing each one in sequence based on the transitions.\n*   `_run(self, shared)`: This method kicks off the orchestration by calling `_orch(self, shared)`.\n\n**Conclusion**\n\nIn this chapter, we learned about the concept of a Flow in PocketFlow. A Flow is an orchestration layer that connects individual Nodes into a functioning workflow. It defines the sequence in which Nodes are executed and how data is passed between them. We saw how to create a Flow for a simple research agent and how it works under the hood.\n\nNow that we understand Flows, let's dive deeper into the building blocks of Flows: **Nodes**. In [Chapter 2: Node](02_node.md), we'll explore what Nodes are, how to create them, and how they interact with Flows.\n\n---\n# Chapter 2: Node\n\nIn [Chapter 1: Flow](01_flow.md), we learned about Flows and how they orchestrate different actions in a specific sequence. Now, let's zoom in on the individual *actions* themselves. These actions are called **Nodes**.\n\nImagine you're building a robot that can make a sandwich. The Flow would be the robot's overall plan: \"Get bread -> Get fillings -> Assemble sandwich -> Serve sandwich\".  Each of those steps \u2013 \"Get bread\", \"Get fillings\", etc. \u2013 would be a Node.\n\n**What Problem Do Nodes Solve?**\n\nNodes solve the problem of breaking down a complex task into smaller, manageable pieces. Instead of having one giant, complicated function, we can have a series of simpler Nodes, each responsible for a specific job. This makes our code easier to understand, test, and maintain.\n\n**Our Sandwich-Making Robot Example**\n\nLet's focus on the \"Get fillings\" step. This could be further broken down: \"Open refrigerator -> Check for desired fillings -> Take out fillings\".  Each of *those* could even be a Node!  For our purposes, let's say \"Get fillings\" is a single Node that figures out which fillings to get and retrieves them.\n\n**Key Concepts of a Node**\n\nA Node is like a mini-program that does one thing well. It has three main parts:\n\n1.  **Input(s):** What the Node needs to get its job done. In our \"Get fillings\" Node, the input might be a list of preferred fillings.\n\n2.  **Processing:** What the Node *does* with the input. This is the core logic of the Node.  In our example, it's the code that checks what fillings are available and retrieves them.\n\n3.  **Output(s):** What the Node produces after processing the input.  In our example, the output would be the actual fillings.\n\nIn PocketFlow, these three parts are defined using three special methods: `prep`, `exec`, and `post`. Don't worry too much about these names for now. Just remember the input, processing, and output.  We will discuss [Prep, Exec, Post (Node Lifecycle)](04_prep__exec__post__node_lifecycle_.md) in detail later.\n\n**Creating a Simple Node**\n\nLet's create a simple Node that adds two numbers.\n\n```python\nfrom pocketflow import Node\n\nclass AddNumbers(Node):\n    def prep(self, shared):\n        \"\"\"Prepares the input numbers from the shared store.\"\"\"\n        num1 = shared[\"num1\"]\n        num2 = shared[\"num2\"]\n        return num1, num2\n\n    def exec(self, inputs):\n        \"\"\"Adds the two numbers together.\"\"\"\n        num1, num2 = inputs\n        result = num1 + num2\n        return result\n\n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Stores the result in the shared store.\"\"\"\n        shared[\"result\"] = exec_res\n        return None  # No next node, the Flow can end\n```\n\nHere's what's happening:\n\n*   We import the `Node` class from `pocketflow`.\n*   We create a class called `AddNumbers` that *inherits* from `Node`. This means our `AddNumbers` class *is* a Node and has all the properties of a Node.\n*   `prep(self, shared)`: This method *prepares* the input for the `exec` method. It retrieves the two numbers (`num1` and `num2`) from the `shared` store (we'll talk about the [Shared Store](03_shared_store.md) in the next chapter!). It returns these two numbers as a tuple.\n*   `exec(self, inputs)`: This method *executes* the core logic of the Node. It takes the output of `prep` as `inputs`, adds the two numbers together, and returns the `result`.\n*   `post(self, shared, prep_res, exec_res)`: This method *posts* the result to the `shared` store. It takes the output of `exec` as `exec_res` and stores it in the `shared` store under the key \"result\".  It returns `None` in this case because there's no further action required in the flow after adding the numbers.\n\n**Using the Node in a Flow**\n\nNow, let's see how we can use this Node in a Flow:\n\n```python\nfrom pocketflow import Flow\nfrom nodes import AddNumbers # Assuming the AddNumbers node is in nodes.py\n\n# Create an instance of the AddNumbers node\nadd_node = AddNumbers()\n\n# Create a Flow starting with the AddNumbers node\nflow = Flow(start=add_node)\n\n# Create a shared store with the input numbers\nshared_store = {\"num1\": 5, \"num2\": 3}\n\n# Run the flow\nflow.run(shared_store)\n\n# Print the result\nprint(shared_store[\"result\"])  # Output: 8\n```\n\nHere's what's happening:\n\n*   We import `Flow` from `pocketflow`.\n*   We create an instance of our `AddNumbers` Node.\n*   We create a `Flow` that starts with our `AddNumbers` Node.\n*   We create a `shared_store` (a dictionary) and put the input numbers (`num1` and `num2`) into it.\n*   We run the `Flow` with the `shared_store`.\n*   Finally, we print the result, which is stored in the `shared_store` under the key \"result\".\n\n**Let's Trace the Execution**\n\n1.  The `Flow` starts at the `AddNumbers` Node.\n2.  The `AddNumbers` Node's `prep` method is called. It retrieves `num1` (5) and `num2` (3) from the `shared_store`.\n3.  The `AddNumbers` Node's `exec` method is called with the input (5, 3). It adds them together and returns 8.\n4.  The `AddNumbers` Node's `post` method is called with the result (8). It stores the result in the `shared_store` under the key \"result\".\n5.  The `Flow` finishes because the `AddNumbers` Node returned `None` in the `post` method, indicating there are no more Nodes to execute.\n\n**Under the Hood: A Closer Look**\n\nLet's see a simplified sequence diagram of what happens when we run the `Flow`:\n\n```mermaid\nsequenceDiagram\n    participant Flow\n    participant AddNumbers\n    participant SharedStore\n\n    Flow->>AddNumbers: run(shared_store)\n    AddNumbers->>SharedStore: access num1, num2\n    AddNumbers-->>Flow: num1, num2\n    Flow->>AddNumbers: exec(num1, num2)\n    AddNumbers-->>Flow: result\n    Flow->>AddNumbers: post(shared_store, prep_res, result)\n    AddNumbers->>SharedStore: store result\n    AddNumbers-->>Flow: None\n```\n\n*   The `Flow` calls the `run` method of the `AddNumbers` Node, passing the `shared_store`.\n*   The `AddNumbers` Node accesses the `shared_store` to get the input numbers.\n*   The `AddNumbers` Node executes its logic and returns the result to the `Flow`.\n*   The `AddNumbers` Node then updates the `shared_store` with the result.\n\nNow, let's peek at some of the relevant code from the `pocketflow/__init__.py` file:\n\n```python\nclass Node(BaseNode): #Simplified\n    def prep(self,shared): pass\n    def exec(self,prep_res): pass\n    def post(self,shared,prep_res,exec_res): pass\n    def _exec(self,prep_res): return self.exec(prep_res)\n    def _run(self,shared): p=self.prep(shared);e=self._exec(p);return self.post(shared,p,e)\n```\n\n*   `prep`, `exec`, and `post` are methods that you can override in your own Node classes to define the behavior of the Node.\n*   The `_run` method is responsible for calling `prep`, `exec`, and `post` in sequence.\n\n**Example: Back to the Research Agent**\nLet's look at our Research Agent code from [Chapter 1: Flow](01_flow.md).\n\n```python\nfrom pocketflow import Node\nfrom utils import call_llm, search_web\nimport yaml\n\nclass DecideAction(Node):\n    def prep(self, shared):\n        # Prepare the context and question\n        question = shared[\"question\"]\n        context = shared.get(\"context\", \"No previous search\")\n        return question, context\n\n    def exec(self, inputs):\n        # Call LLM to decide search or answer\n        question, context = inputs\n        prompt = f\"\"\"... (LLM prompt) ...\"\"\" #Skipping prompt for brevity\n        response = call_llm(prompt)\n        yaml_str = response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n        decision = yaml.safe_load(yaml_str)\n        return decision\n\n    def post(self, shared, prep_res, exec_res):\n        #Save search query and determine next step\n        if exec_res[\"action\"] == \"search\":\n            shared[\"search_query\"] = exec_res[\"search_query\"]\n        else:\n            shared[\"context\"] = exec_res[\"answer\"]\n        return exec_res[\"action\"]\n```\n\nIn this example, `DecideAction` Node decides whether to search or answer based on the provided question and context. The `prep` function grabs the values from the `shared` store, the `exec` function decides on the next action, and the `post` function save the search query (if applicable) and specifies the name of the next node to proceed to.\n\n**Different Types of Nodes**\n\nWhile we've focused on a basic Node here, PocketFlow offers different types of Nodes for different purposes, such as [BatchNode](05_batchnode.md) for processing multiple items at once, and [AsyncNode](06_asyncnode.md) for handling asynchronous operations.\n\n**Conclusion**\n\nIn this chapter, we learned about Nodes, the fundamental building blocks of PocketFlow. We saw how Nodes encapsulate individual actions, and how they are connected together in a Flow. We learned about the `prep`, `exec`, and `post` methods that define the behavior of a Node. Now that we understand Nodes, let's move on to [Chapter 3: Shared Store](03_shared_store.md) to see how Nodes can share data with each other.\n\n---\n# Chapter 3: Shared Store\n\nIn [Chapter 2: Node](02_node.md), we learned about Nodes and how they perform individual tasks. But how do these Nodes *talk* to each other? How does one Node pass its results to the next Node in the [Flow](01_flow.md)? That's where the **Shared Store** comes in!\n\nImagine a relay race. Each runner (Node) has a specific leg to run. The baton (data) needs to be passed from one runner to the next. The Shared Store is like the designated exchange zone where the baton is safely passed.\n\n**The Problem: Passing Information Between Nodes**\n\nWithout a shared space, each Node would operate in isolation.  They wouldn't know what other Nodes have done, and they couldn't build on each other's work.  The Shared Store provides a central location for Nodes to store and retrieve information, enabling collaboration and data flow within the PocketFlow ecosystem.\n\n**Our Research Agent Example (Again!)**\n\nRemember our research agent from [Chapter 1: Flow](01_flow.md)? The `SearchWeb` Node needs to pass the search results to the `AnswerQuestion` Node. Without a Shared Store, the `AnswerQuestion` Node wouldn't know what the web search found!\n\n**What *is* the Shared Store?**\n\nIn PocketFlow, the Shared Store is essentially a **dictionary**.  If you're new to programming, think of a dictionary as a container that holds key-value pairs. You can store data using a specific \"key\" and later retrieve it using that same key.\n\nFor example, we might store the search results with the key \"search_results\":\n\n```python\nshared_store[\"search_results\"] = \"The capital of France is Paris.\"\n```\n\nLater, another Node can access this information:\n\n```python\ncapital = shared_store[\"search_results\"]\nprint(capital) # Output: The capital of France is Paris.\n```\n\n**Key Concepts of the Shared Store**\n\n1.  **Centralized Data:** It's a single place where all Nodes can access and modify data.\n\n2.  **Key-Value Pairs:** Data is stored as key-value pairs, like a Python dictionary.\n\n3.  **Accessibility:** Any Node in the [Flow](01_flow.md) can access the Shared Store.\n\n4.  **Dynamic:** Nodes can add, modify, and delete data in the Shared Store during the [Flow](01_flow.md)'s execution.\n\n**How to Use the Shared Store**\n\nLet's revisit our `AddNumbers` Node from [Chapter 2: Node](02_node.md) and see how it uses the Shared Store:\n\n```python\nfrom pocketflow import Node\n\nclass AddNumbers(Node):\n    def prep(self, shared):\n        \"\"\"Prepares the input numbers from the shared store.\"\"\"\n        num1 = shared[\"num1\"]\n        num2 = shared[\"num2\"]\n        return num1, num2\n\n    def exec(self, inputs):\n        \"\"\"Adds the two numbers together.\"\"\"\n        num1, num2 = inputs\n        result = num1 + num2\n        return result\n\n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Stores the result in the shared store.\"\"\"\n        shared[\"result\"] = exec_res\n        return None  # No next node, the Flow can end\n```\n\n*   In the `prep` method, the Node retrieves the values associated with the keys \"num1\" and \"num2\" from the `shared` store.\n\n*   In the `post` method, the Node stores the calculated `result` in the `shared` store using the key \"result\".\n\n**Example Usage**\n\nHere's how the Shared Store is used when we run the [Flow](01_flow.md):\n\n```python\nfrom pocketflow import Flow\nfrom nodes import AddNumbers # Assuming the AddNumbers node is in nodes.py\n\n# Create an instance of the AddNumbers node\nadd_node = AddNumbers()\n\n# Create a Flow starting with the AddNumbers node\nflow = Flow(start=add_node)\n\n# Create a shared store with the input numbers\nshared_store = {\"num1\": 5, \"num2\": 3}\n\n# Run the flow\nflow.run(shared_store)\n\n# Print the result\nprint(shared_store[\"result\"])  # Output: 8\n```\n\n*   We create a `shared_store` (a dictionary) and initialize it with the input values for \"num1\" and \"num2\". This is how we *seed* the Shared Store with initial data.\n\n*   After the `flow.run(shared_store)` call, the `shared_store` now contains the key \"result\" with the value 8, because the `AddNumbers` Node stored it there.\n\n**Another Example: Passing the Question to the Answer Node**\nLet's look back at the nodes of our research agent in `cookbook/pocketflow-agent/nodes.py`. The `DecideAction` Node checks whether to search or answer the question and it has access to a \"question\" value in the shared store.\n\n```python\nclass DecideAction(Node):\n    def prep(self, shared):\n        # Prepare the context and question\n        question = shared[\"question\"] #Access question value from shared store\n        context = shared.get(\"context\", \"No previous search\")\n        return question, context\n    #Skipping the other functions for brevity.\n```\n\nIn this case, the `prep` function of `DecideAction` node accesses the question value from the shared store using the key \"question\".\n\n**Important Note:**\n\n*   **Key Existence:** It's good practice to check if a key exists in the Shared Store before accessing it, especially if the Node isn't the one who put the value there. You can use `shared.get(\"key\", default_value)` which returns `default_value` if the key doesn't exist. Or `if \"key\" in shared:` to check for a key's existence.\n\n**Under the Hood: How it Works**\n\nWhen you run a [Flow](01_flow.md), the `Flow` object creates and passes a Shared Store object to each Node's `prep`, `exec`, and `post` methods.\n\nHere's a simplified sequence diagram:\n\n```mermaid\nsequenceDiagram\n    participant Flow\n    participant Node1\n    participant Node2\n    participant SharedStore\n\n    Flow->>SharedStore: Create SharedStore\n    Flow->>Node1: run(SharedStore)\n    Node1->>SharedStore: Access/Modify data\n    Node1-->>Flow: (Next Action)\n    Flow->>Node2: run(SharedStore)\n    Node2->>SharedStore: Access/Modify data\n    Node2-->>Flow: (End)\n```\n\n*   The `Flow` creates a single `SharedStore` instance at the beginning.\n\n*   This same `SharedStore` instance is passed to each `Node` in the [Flow](01_flow.md).\n\n*   Each `Node` can then read from and write to the `SharedStore`.\n\n**Code Snippet: Flow Passing the Shared Store (From `pocketflow/__init__.py`)**\nThis is how the [Flow](01_flow.md) passes the Shared Store to each node:\n\n```python\nclass Flow(BaseNode):\n    def __init__(self,start): super().__init__();self.start=start\n    def get_next_node(self,curr,action):\n        nxt=curr.successors.get(action or \"default\")\n        if not nxt and curr.successors: warnings.warn(f\"Flow ends: '{action}' not found in {list(curr.successors)}\")\n        return nxt\n    def _orch(self,shared,params=None):\n        curr,p=copy.copy(self.start),(params or {**self.params})\n        while curr: curr.set_params(p);c=curr._run(shared);curr=copy.copy(self.get_next_node(curr,c))\n    def _run(self,shared): pr=self.prep(shared);self._orch(shared);return self.post(shared,pr,None)\n    def exec(self,prep_res): raise RuntimeError(\"Flow can't exec.\")\n```\nIn the `_orch` function, the shared store is maintained and passed into each subsequent node. Note the line `c=curr._run(shared)` indicating the shared store is passed into the node's run function.\n\n**Conclusion**\n\nIn this chapter, we learned about the Shared Store, a crucial component of PocketFlow that allows Nodes to communicate and share data. We saw how the Shared Store acts as a central memory, enabling seamless data transfer between processing steps. We learned how Nodes access and modify data in the Shared Store, and how the Flow manages the Shared Store behind the scenes.\n\nNow that we understand the Shared Store, we can appreciate how PocketFlow Nodes interact within a Flow to accomplish complex tasks. In the next chapter, [Prep, Exec, Post (Node Lifecycle)](04_prep__exec__post__node_lifecycle_.md), we'll dive deeper into the `prep`, `exec`, and `post` methods and understand the lifecycle of a Node.\n\n---\n# Chapter 4: Prep, Exec, Post (Node Lifecycle)\n\nIn [Chapter 2: Node](02_node.md), we learned about Nodes and how they encapsulate individual actions. We briefly touched upon the `prep`, `exec`, and `post` methods. Now, let's dive deeper into what these methods do and how they define the lifecycle of a Node.\n\nImagine you're a chef preparing a dish.\n\n*   **Prep:** You gather your ingredients, chop vegetables, and prepare your cooking station.\n*   **Exec:** You actually cook the dish \u2013 the main action.\n*   **Post:** You plate the dish, add garnishes, and serve it.\n\nThe `prep`, `exec`, and `post` methods in a Node are similar! They provide a structured way to define the different stages of a Node's operation.\n\n**The Problem: Organizing Node Logic**\n\nWithout a clear structure, the code inside a Node can become disorganized and hard to understand.  The `prep`, `exec`, and `post` pattern provides a consistent way to organize Node logic, making it easier to read, maintain, and debug.\n\n**Our Research Agent Example (One More Time!)**\n\nLet's revisit our research agent and focus on the `SearchWeb` Node. It needs to:\n\n1.  **Prep:** Get the search query from the [Shared Store](03_shared_store.md).\n2.  **Exec:** Actually perform the web search using the query.\n3.  **Post:** Save the search results back into the [Shared Store](03_shared_store.md) and decide what's next.\n\n**Key Concepts: Prep, Exec, Post**\n\n*   **Prep (Preparation):**  This method prepares the input data needed for the main task (`exec`). It typically retrieves data from the [Shared Store](03_shared_store.md) or performs any necessary data transformations.\n    *   **Input:** The [Shared Store](03_shared_store.md).\n    *   **Output:**  Data that the `exec` method needs (can be any Python object).\n\n*   **Exec (Execution):** This is the heart of the Node. It performs the core logic of the Node, using the data prepared by the `prep` method.\n    *   **Input:** The output of the `prep` method.\n    *   **Output:** The result of the Node's main operation (can be any Python object).\n\n*   **Post (Post-processing):** This method processes the output of the `exec` method. It typically saves the results back into the [Shared Store](03_shared_store.md), performs any cleanup, and determines the next step in the [Flow](01_flow.md). It specifies which Node to run next in the flow.\n    *   **Input:** The [Shared Store](03_shared_store.md), the output of `prep`, and the output of `exec`.\n    *   **Output:** A value that determines which Node to execute next in the [Flow](01_flow.md). If it returns `None`, the flow can end.\n\n**Example: Back to the `AddNumbers` Node**\n\nLet's examine the `AddNumbers` Node again:\n\n```python\nfrom pocketflow import Node\n\nclass AddNumbers(Node):\n    def prep(self, shared):\n        \"\"\"Prepares the input numbers from the shared store.\"\"\"\n        num1 = shared[\"num1\"]\n        num2 = shared[\"num2\"]\n        return num1, num2\n\n    def exec(self, inputs):\n        \"\"\"Adds the two numbers together.\"\"\"\n        num1, num2 = inputs\n        result = num1 + num2\n        return result\n\n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Stores the result in the shared store.\"\"\"\n        shared[\"result\"] = exec_res\n        return None  # No next node, the Flow can end\n```\n\n*   **`prep`**: Retrieves `num1` and `num2` from the [Shared Store](03_shared_store.md) and returns them as a tuple.\n*   **`exec`**: Takes the tuple from `prep`, adds the numbers, and returns the `result`.\n*   **`post`**: Stores the `result` in the [Shared Store](03_shared_store.md) and returns `None`, indicating the end of the [Flow](01_flow.md).\n\n**Another Example: `SearchWeb` Node from the Research Agent**\n\n```python\nfrom pocketflow import Node\nfrom utils import search_web # Assuming you have a search_web utility\n\nclass SearchWeb(Node):\n    def prep(self, shared):\n        \"\"\"Get the search query from the shared store.\"\"\"\n        return shared[\"search_query\"]\n        \n    def exec(self, search_query):\n        \"\"\"Search the web for the given query.\"\"\"\n        print(f\"\ud83c\udf10 Searching the web for: {search_query}\")\n        results = search_web(search_query)\n        return results\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Save the search results and go back to the decision node.\"\"\"\n        shared[\"context\"] = \"SEARCH RESULTS: \" + exec_res\n        return \"decide\" # Back to DecideAction node\n```\n\n*   **`prep`**:  Retrieves the `search_query` from the [Shared Store](03_shared_store.md).\n*   **`exec`**:  Performs the web search using `search_web` and returns the `results`.\n*   **`post`**: Saves the `results` into the [Shared Store](03_shared_store.md) under the key \"context\", and returns `\"decide\"` to tell the [Flow](01_flow.md) to go back to `DecideAction` ([Chapter 1: Flow](01_flow.md)).\n\n**How It Works: Example Execution**\n\nLet's trace the execution of the `SearchWeb` Node:\n\n1.  The [Flow](01_flow.md) calls the `run` method of the `SearchWeb` Node, passing the [Shared Store](03_shared_store.md).\n2.  The `prep` method retrieves the `search_query` (e.g., \"capital of France\") from the [Shared Store](03_shared_store.md).\n3.  The `exec` method uses the `search_query` to search the web. Let's say it gets the result \"Paris is the capital of France.\"\n4.  The `post` method saves the search result \"Paris is the capital of France.\" into the [Shared Store](03_shared_store.md). Also the return value is \"decide\".\n5.  The [Flow](01_flow.md) then uses the `\"decide\"` return value to determine the next Node to run.\n\n**Under the Hood: The `_run` Method**\n\nThe `Node` class in `pocketflow/__init__.py` has a `_run` method that orchestrates the `prep`, `exec`, and `post` methods:\n\n```python\nclass Node(BaseNode): #Simplified\n    def prep(self,shared): pass\n    def exec(self,prep_res): pass\n    def post(self,shared,prep_res,exec_res): pass\n    def _exec(self,prep_res): return self.exec(prep_res)\n    def _run(self,shared): p=self.prep(shared);e=self._exec(p);return self.post(shared,p,e)\n```\n\nAs you can see, `_run` calls `prep`, then `exec`, and finally `post` in sequence.  It passes the output of one method as input to the next.\n\nLet's visualize this with a sequence diagram:\n\n```mermaid\nsequenceDiagram\n    participant Node\n    participant SharedStore\n\n    Node->>Node: prep(shared_store)\n    Node-->>Node: prep_result\n    Node->>Node: exec(prep_result)\n    Node-->>Node: exec_result\n    Node->>Node: post(shared_store, prep_result, exec_result)\n    Node-->>Node: next_action\n```\n\n1.  `prep(shared_store)`: The `prep` method is called with the [Shared Store](03_shared_store.md).\n2.  `exec(prep_result)`: The `exec` method is called with the result of `prep`.\n3.  `post(shared_store, prep_result, exec_result)`: The `post` method is called with the [Shared Store](03_shared_store.md), the result of `prep`, and the result of `exec`. The `post` function's return value (next_action) controls what node to run next.\n\n**Why This Structure Matters**\n\n*   **Organization:** Clearly separates data preparation, core logic, and post-processing.\n*   **Testability:**  Each method can be tested independently.\n*   **Reusability:** Nodes become more modular and easier to reuse in different [Flows](01_flow.md).\n*   **Readability:**  The `prep`, `exec`, `post` structure makes the code easier to understand.\n\n**Conclusion**\n\nIn this chapter, we learned about the `prep`, `exec`, and `post` methods and how they define the lifecycle of a Node.  We saw how this structure helps organize Node logic, making it more readable, testable, and reusable. We saw how each function affects the [Shared Store](03_shared_store.md) and how the `post` function returns the next node to execute.\n\nNow that we understand the Node lifecycle, let's move on to [Chapter 5: BatchNode](05_batchnode.md) to see how we can process multiple items in a single Node using `BatchNode`.\n\n---\n# Chapter 5: BatchNode\n\nIn [Chapter 4: Prep, Exec, Post (Node Lifecycle)](04_prep__exec__post__node_lifecycle_.md), we learned about the `prep`, `exec`, and `post` methods and how they structure a Node's lifecycle. Now, let's explore how to efficiently process *multiple* data items within a single Node using the `BatchNode`.\n\nImagine you're running a bakery. Instead of baking one cookie at a time, you bake dozens on a single tray! The `BatchNode` is like that tray \u2013 it lets you process a \"batch\" of items together.\n\n**The Problem: Processing Many Items Efficiently**\n\nSometimes, you need to perform the same operation on a large number of independent data items. Doing this with a regular [Node](02_node.md) would involve iterating through each item individually, which can be slow and inefficient. The `BatchNode` solves this by allowing you to process these items in a batch, making the process much faster.\n\n**Our Use Case: Translating a Document into Multiple Languages**\n\nLet's say you have a document and you want to translate it into several languages (like Chinese, Spanish, French, etc.). Instead of creating a separate [Node](02_node.md) for each language, we can use a `BatchNode` to translate the document into all the languages at once!\n\n**What is a BatchNode?**\n\nA `BatchNode` is a special type of [Node](02_node.md) that's designed to process multiple data items in a single execution. Think of it like an assembly line:\n\n1.  **Prep:** The `prep` method prepares the input data and splits it into chunks. These chunks are individual items or mini-batches within the larger batch.\n2.  **Exec:** The `exec` method is applied to each chunk of data. It performs the same operation on each item in the chunk.\n3.  **Post:** The `post` method combines the results from processing all the chunks and prepares the final output.\n\n**Key Concepts of a BatchNode**\n\n1.  **Batch Processing:** The ability to process multiple independent data items together.\n2.  **Chunking:** Splitting the input data into smaller, manageable chunks.\n3.  **Parallelism (Optional):** The `exec` method can potentially process chunks in parallel (we'll explore this in later chapters). For now, we'll assume it's processed sequentially.\n4.  **Result Aggregation:** Combining the results from processing all the chunks into a final output.\n\n**Creating a BatchNode for Translation**\n\nLet's create a `BatchNode` that translates a given text into multiple languages:\n\n```python\nfrom pocketflow import BatchNode\n\nclass TranslateTextNode(BatchNode):\n    def prep(self, shared):\n        text = shared.get(\"text\", \"(No text provided)\")\n        languages = shared.get(\"languages\", [\"Chinese\", \"Spanish\"]) # Reduced languages for brevity\n        \n        # Create batches for each language translation\n        return [(text, lang) for lang in languages]\n\n    def exec(self, data_tuple):\n        text, language = data_tuple\n        #Simulate Translation for brevity\n        translation=f\"Translated to {language}: {text}\"\n        return {\"language\": language, \"translation\": translation}\n\n    def post(self, shared, prep_res, exec_res_list):\n        # Print each translation result\n        for result in exec_res_list:\n            language, translation = result[\"language\"], result[\"translation\"]\n            print(f\"Translation to {language}: {translation}\")\n        return None\n```\n\nHere's a breakdown:\n\n*   `TranslateTextNode` inherits from `BatchNode`.\n*   `prep(self, shared)`: This method retrieves the text to translate and the list of languages from the [Shared Store](03_shared_store.md). It then creates a list of tuples, where each tuple contains the text and a language. This list becomes the \"batch\" of items to be processed.\n*   `exec(self, data_tuple)`: This method takes a single tuple (text, language) as input. It simulates translation for demonstration purposes by returning a translated version of the text.\n*   `post(self, shared, prep_res, exec_res_list)`: This method receives a list of results, where each result is a dictionary containing the translated text and the language. It prints each translation to the console.\n\n**Using the BatchNode in a Flow**\n\nNow, let's see how to use this `BatchNode` in a [Flow](01_flow.md):\n\n```python\nfrom pocketflow import Flow\nfrom nodes import TranslateTextNode # Assuming the TranslateTextNode is in nodes.py\n\n# Create an instance of the TranslateTextNode\ntranslate_node = TranslateTextNode()\n\n# Create a Flow starting with the TranslateTextNode\nflow = Flow(start=translate_node)\n\n# Create a shared store with the input text and languages\nshared_store = {\n    \"text\": \"Hello, world!\",\n    \"languages\": [\"Chinese\", \"Spanish\"]  # Reduced languages for brevity\n}\n\n# Run the flow\nflow.run(shared_store)\n\n# Expected Output:\n# Translation to Chinese: Translated to Chinese: Hello, world!\n# Translation to Spanish: Translated to Spanish: Hello, world!\n```\n\nHere's what's happening:\n\n1.  We create an instance of the `TranslateTextNode`.\n2.  We create a [Flow](01_flow.md) that starts with the `TranslateTextNode`.\n3.  We create a `shared_store` ([Chapter 3: Shared Store](03_shared_store.md)) and populate it with the text and languages.\n4.  We run the [Flow](01_flow.md). The `TranslateTextNode` will translate the text into each language and print the results.\n\n**Let's Trace the Execution**\n\n1.  The [Flow](01_flow.md) starts at the `TranslateTextNode`.\n2.  The `TranslateTextNode`'s `prep` method is called. It retrieves the text and languages from the `shared_store` and creates a list of tuples: `[(\"Hello, world!\", \"Chinese\"), (\"Hello, world!\", \"Spanish\")]`.\n3.  The `TranslateTextNode`'s `exec` method is called *twice*:\n    *   First, with the tuple `(\"Hello, world!\", \"Chinese\")`. It returns `{\"language\": \"Chinese\", \"translation\": \"Translated to Chinese: Hello, world!\"}`.\n    *   Second, with the tuple `(\"Hello, world!\", \"Spanish\")`. It returns `{\"language\": \"Spanish\", \"translation\": \"Translated to Spanish: Hello, world!\"}`.\n4.  The `TranslateTextNode`'s `post` method is called with the list of results: `[{\"language\": \"Chinese\", \"translation\": \"Translated to Chinese: Hello, world!\"}, {\"language\": \"Spanish\", \"translation\": \"Translated to Spanish: Hello, world!\"}]`. It prints each translation to the console.\n5.  The [Flow](01_flow.md) finishes because the `TranslateTextNode` returned `None` in the `post` method.\n\n**Under the Hood: How BatchNode Works**\n\nWhen a [Flow](01_flow.md) runs a `BatchNode`, the `_exec` method is overridden to process items as a batch.\n\nHere's a simplified sequence diagram:\n\n```mermaid\nsequenceDiagram\n    participant Flow\n    participant BatchNode\n    participant SharedStore\n\n    Flow->>BatchNode: run(shared_store)\n    BatchNode->>SharedStore: access text, languages\n    BatchNode-->>Flow: [(text, lang1), (text, lang2), ...]\n    loop For each item in batch\n        Flow->>BatchNode: exec((text, lang_i))\n        BatchNode-->>Flow: {language: lang_i, translation: ...}\n    end\n    Flow->>BatchNode: post(shared_store, prep_res, exec_res_list)\n    BatchNode->>SharedStore: Store results (optional)\n    BatchNode-->>Flow: None\n```\n\n1.  The [Flow](01_flow.md) calls the `run` method of the `BatchNode`, passing the [Shared Store](03_shared_store.md).\n2.  The `prep` method retrieves the necessary data from the [Shared Store](03_shared_store.md) and prepares the batch of items.\n3.  The `exec` method is called for each item in the batch.\n4.  The `post` method receives a list of results from all the `exec` calls.\n\nLet's look at the relevant code from `pocketflow/__init__.py`:\n\n```python\nclass BatchNode(Node):\n    def _exec(self,items): return [super(BatchNode,self)._exec(i) for i in (items or [])]\n```\n\nThis code snippet shows that the `_exec` method of the `BatchNode` iterates through the `items` (which are the output from the `prep` function) and calls the `exec` method for each item. The results are then collected into a list and returned.\n\n**Benefits of Using BatchNode**\n\n*   **Improved Efficiency:** Processes multiple items in a single Node execution.\n*   **Simplified Code:** Reduces the need for manual iteration within the [Flow](01_flow.md).\n*   **Potential for Parallelism:** Makes it easier to parallelize processing (we'll see this later!).\n\n**Real World example - CSV Processing**\n\nAnother use case is processing CSV files.\n```python\nimport pandas as pd\nfrom pocketflow import BatchNode\n\nclass CSVProcessor(BatchNode):\n    def __init__(self, chunk_size=1000):\n        super().__init__()\n        self.chunk_size = chunk_size\n    \n    def prep(self, shared):\n        chunks = pd.read_csv(\n            shared[\"input_file\"],\n            chunksize=self.chunk_size\n        )\n        return chunks\n    \n    def exec(self, chunk):\n        return {\n            \"total_sales\": chunk[\"amount\"].sum(),\n            \"num_transactions\": len(chunk),\n            \"total_amount\": chunk[\"amount\"].sum()\n        }\n    \n    def post(self, shared, prep_res, exec_res_list):\n        total_sales = sum(res[\"total_sales\"] for res in exec_res_list)\n        shared[\"total_sales\"] = total_sales\n        return None\n```\n\nHere, the `prep` function reads the CSV file in chunks (batches) of `chunk_size`. The `exec` function processes each chunk and calculates sales stats, and the `post` function adds up all of the sales.\n\n**Conclusion**\n\nIn this chapter, we learned about the `BatchNode` and how it allows us to process multiple data items efficiently within a single Node. We saw how to create a `BatchNode` for translating text into multiple languages and how it works under the hood.\n\nNow that we understand `BatchNode`, let's move on to [Chapter 6: AsyncNode](06_asyncnode.md) to see how we can handle asynchronous operations within PocketFlow.\n\n---\n# Chapter 6: AsyncNode\n\nIn [Chapter 5: BatchNode](05_batchnode.md), we learned how to process multiple items efficiently using `BatchNode`. But what if some of the operations inside our Nodes take a *long* time, like waiting for a response from a website or a database? This is where `AsyncNode` comes in!\n\nImagine you're ordering food online. You don't want your whole computer to freeze while waiting for the restaurant to confirm your order, right? You want to be able to do other things while your food is being prepared. `AsyncNode` allows your PocketFlow to do just that - perform non-blocking, asynchronous operations.\n\n**The Problem: Dealing with Time-Consuming Operations**\n\nSome tasks take a while. Network requests, database queries, and complex computations can all block the execution of our [Flow](01_flow.md), making it slow and unresponsive. We want PocketFlow to continue processing other tasks while these long-running operations are in progress.\n\n**Our Use Case: Recipe Generation with an LLM**\n\nLet's say we want to build a simple recipe app. Our [Flow](01_flow.md) will:\n\n1.  Ask the user for an ingredient.\n2.  Fetch recipes containing that ingredient from a website (takes time!).\n3.  Use an LLM (Large Language Model) to suggest the best recipe from the fetched recipes (also takes time!).\n4.  Ask the user if they approve of the recipe.\n\nFetching recipes from a website and interacting with an LLM can take several seconds. We don't want our app to freeze during this time! `AsyncNode` will allow us to handle these operations asynchronously, making our app more responsive.\n\n**What is an AsyncNode?**\n\nAn `AsyncNode` is a special type of [Node](02_node.md) that allows us to perform asynchronous operations using the `async` and `await` keywords in Python.\n\n*   **`async`**:  This keyword declares a function as a *coroutine*. A coroutine is a special type of function that can be paused and resumed.\n*   **`await`**:  This keyword is used inside an `async` function to wait for another coroutine to complete *without blocking* the execution of the rest of the [Flow](01_flow.md).\n\nThink of `await` as telling your program, \"Hey, I'm going to wait for this to finish, but don't just sit there doing nothing! Go do something else in the meantime.\"\n\n**Key Concepts of an AsyncNode**\n\n1.  **Asynchronous Operations:** Performing tasks without blocking the main thread of execution.\n2.  **`async` and `await` Keywords:** Used to define and control asynchronous operations in Python.\n3.  **Non-Blocking:** Allows the [Flow](01_flow.md) to continue processing other tasks while waiting for asynchronous operations to complete.\n\n**Creating an AsyncNode for Fetching Recipes**\n\nLet's create an `AsyncNode` that fetches recipes from a website. We'll simplify things by using a dummy function that simulates a network request:\n\n```python\nimport asyncio\nfrom pocketflow import AsyncNode\n\n# Dummy function to simulate fetching recipes (takes time!)\nasync def fetch_recipes(ingredient):\n    print(f\"\ud83d\ude34 Fetching recipes for {ingredient}... This will take a moment.\")\n    await asyncio.sleep(2)  # Simulate a 2-second delay\n    return [\"Pasta with \" + ingredient, \"Pizza with \" + ingredient]\n\nclass FetchRecipesNode(AsyncNode):\n    async def prep_async(self, shared):\n        # Prep data (can be synchronous or asynchronous)\n        ingredient = shared.get(\"ingredient\", \"default_ingredient\")\n        return ingredient\n\n    async def exec_async(self, ingredient):\n        # Asynchronously fetch recipes\n        recipes = await fetch_recipes(ingredient)\n        return recipes\n\n    async def post_async(self, shared, prep_res, exec_res):\n        # Store the fetched recipes and return a value\n        shared[\"recipes\"] = exec_res\n        return \"suggest\"\n```\n\nHere's what's happening:\n\n*   `FetchRecipesNode` inherits from `AsyncNode`.\n*   `fetch_recipes(ingredient)` is an `async` function that simulates fetching recipes from a website. It uses `await asyncio.sleep(2)` to pause execution for 2 seconds, simulating the delay of a network request.\n*   `prep_async(self, shared)`: This asynchronous `prep` function retrieves the ingredient from the [Shared Store](03_shared_store.md).\n*   `exec_async(self, ingredient)`: This asynchronous `exec` function calls `fetch_recipes(ingredient)` using `await`. This means the `exec` function will pause execution until `fetch_recipes` completes, but *without blocking* the rest of the [Flow](01_flow.md).\n*   `post_async(self, shared, prep_res, exec_res)`: This asynchronous `post` function stores the fetched recipes in the [Shared Store](03_shared_store.md) and returns `\"suggest\"`.\n\n**Important:** Notice that we use `prep_async`, `exec_async`, and `post_async` instead of the regular `prep`, `exec`, and `post` methods. `AsyncNode` requires these `async` versions.\n\n**Using the AsyncNode in a Flow**\n\nNow, let's see how to use this `AsyncNode` in a [Flow](01_flow.md):\n\n```python\nimport asyncio\nfrom pocketflow import Flow\nfrom nodes import FetchRecipesNode # Assuming the FetchRecipesNode is in nodes.py\n\n# Create an instance of the FetchRecipesNode\nfetch_recipes_node = FetchRecipesNode()\n\n# Create a Flow starting with the FetchRecipesNode\nflow = Flow(start=fetch_recipes_node)\n\n# Create a shared store\nshared_store = {\"ingredient\": \"chicken\"}\n\n# Run the flow (using asyncio.run for async flows)\nasync def run_flow():\n    flow.run(shared_store)\n\nasyncio.run(run_flow())\n\n# Expected output:\n# \ud83d\ude34 Fetching recipes for chicken... This will take a moment.\n```\n\nHere's what's happening:\n\n1.  We create an instance of the `FetchRecipesNode`.\n2.  We create a [Flow](01_flow.md) that starts with the `FetchRecipesNode`.\n3.  We create a `shared_store` ([Chapter 3: Shared Store](03_shared_store.md)).\n4.  We define an `async` function `run_flow` that calls `flow.run(shared_store)`.\n5.  We use `asyncio.run(run_flow())` to run the asynchronous [Flow](01_flow.md). *This is crucial!* You need to use `asyncio.run` to properly execute an asynchronous [Flow](01_flow.md).\n\n**Let's Trace the Execution**\n\n1.  The [Flow](01_flow.md) starts at the `FetchRecipesNode`.\n2.  The `FetchRecipesNode`'s `prep_async` method is called. It retrieves the ingredient \"chicken\" from the `shared_store`.\n3.  The `FetchRecipesNode`'s `exec_async` method is called with the ingredient \"chicken\".\n4.  The `exec_async` method calls `await fetch_recipes(\"chicken\")`. The `fetch_recipes` function simulates a network request and pauses execution for 2 seconds.\n5.  *During those 2 seconds, the PocketFlow can theoretically do other things (though in this simple example, there's nothing else to do).*\n6.  After 2 seconds, `fetch_recipes` returns a list of recipes.\n7.  The `exec_async` method returns the list of recipes.\n8.  The `FetchRecipesNode`'s `post_async` method is called. It stores the recipes in the `shared_store` and returns `\"suggest\"`.\n9.  The [Flow](01_flow.md) finishes (or continues to the next node, if there is one).\n\n**Under the Hood: How AsyncNode Works**\n\nThe `AsyncNode` class overrides the regular `prep`, `exec`, and `post` methods to prevent accidental use. It also provides the `_run_async` method, which orchestrates the execution of the `prep_async`, `exec_async`, and `post_async` methods.\n\nHere's a simplified sequence diagram:\n\n```mermaid\nsequenceDiagram\n    participant Flow\n    participant AsyncNode\n    participant SharedStore\n    participant EventLoop\n\n    Flow->>AsyncNode: run_async(shared_store)\n    AsyncNode->>AsyncNode: prep_async(shared_store)\n    AsyncNode-->>Flow: prep_result\n    Flow->>AsyncNode: exec_async(prep_result)\n    AsyncNode->>EventLoop: await fetch_recipes(prep_result)\n    EventLoop-->>AsyncNode: recipes\n    AsyncNode-->>Flow: recipes\n    Flow->>AsyncNode: post_async(shared_store, prep_result, recipes)\n    AsyncNode->>SharedStore: store recipes\n    AsyncNode-->>Flow: next_node\n```\n\n1.  The [Flow](01_flow.md) calls the `run_async` method of the `AsyncNode`, passing the [Shared Store](03_shared_store.md).\n2.  The `prep_async` method retrieves the necessary data from the [Shared Store](03_shared_store.md).\n3.  The `exec_async` method performs the asynchronous operation using `await`. This allows the event loop to execute other tasks while waiting for the operation to complete.\n4.  The `post_async` method processes the result and updates the [Shared Store](03_shared_store.md).\n\nLet's look at some of the relevant code from `pocketflow/__init__.py`:\n\n```python\nclass AsyncNode(Node):\n    def prep(self,shared): raise RuntimeError(\"Use prep_async.\")\n    def exec(self,prep_res): raise RuntimeError(\"Use exec_async.\")\n    def post(self,shared,prep_res,exec_res): raise RuntimeError(\"Use post_async.\")\n    def exec_fallback(self,prep_res,exc): raise RuntimeError(\"Use exec_fallback_async.\")\n    def _run(self,shared): raise RuntimeError(\"Use run_async.\")\n    async def prep_async(self,shared): pass\n    async def exec_async(self,prep_res): pass\n    async def exec_fallback_async(self,prep_res,exc): raise exc\n    async def post_async(self,shared,prep_res,exec_res): pass\n    async def _exec(self,prep_res): \n        for i in range(self.max_retries):\n            try: return await self.exec_async(prep_res)\n            except Exception as e:\n                if i==self.max_retries-1: return await self.exec_fallback_async(prep_res,e)\n                if self.wait>0: await asyncio.sleep(self.wait)\n    async def run_async(self,shared): \n        if self.successors: warnings.warn(\"Node won't run successors. Use AsyncFlow.\")  \n        return await self._run_async(shared)\n    async def _run_async(self,shared): p=await self.prep_async(shared);e=await self._exec(p);return await self.post_async(shared,p,e)\n```\n\nKey takeaways from this code:\n\n*   The regular `prep`, `exec`, `post`, and `_run` functions are overridden to throw `RuntimeError`s if called. This enforces the use of the `async` versions.\n*   The `_exec` function wraps the execution of `exec_async` and takes care of retries. The key line is `return await self.exec_async(prep_res)`.\n*   `run_async` calls the `_run_async` function.\n\n**Conclusion**\n\nIn this chapter, we learned about the `AsyncNode` and how it allows us to perform asynchronous operations within PocketFlow. We saw how to create an `AsyncNode` for fetching recipes from a website and how it works under the hood. Using `AsyncNode` makes the workflow more responsive and enables interaction with time-consuming external services.\n\nNow that we understand `AsyncNode`, let's move on to [Chapter 7: AsyncFlow](07_asyncflow.md) to see how we can build entire asynchronous flows using `AsyncFlow`.\n\n\nRelevant Code Snippets:\n--- File: cookbook/pocketflow-async-basic/flow.py ---\n\"\"\"AsyncFlow implementation for recipe finder.\"\"\"\n\nfrom pocketflow import AsyncFlow, Node\nfrom nodes import FetchRecipes, SuggestRecipe, GetApproval\n\nclass NoOp(Node):\n    \"\"\"Node that does nothing, used to properly end the flow.\"\"\"\n    pass\n\ndef create_flow():\n    \"\"\"Create and connect nodes into a flow.\"\"\"\n    \n    # Create nodes\n    fetch = FetchRecipes()\n    suggest = SuggestRecipe()\n    approve = GetApproval()\n    end = NoOp()\n    \n    # Connect nodes\n    fetch - \"suggest\" >> suggest\n    suggest - \"approve\" >> approve\n    approve - \"retry\" >> suggest  # Loop back for another suggestion\n    approve - \"accept\" >> end     # Properly end the flow\n    \n    # Create flow starting with fetch\n    flow = AsyncFlow(start=fetch)\n    return flow \n\n--- File: pocketflow/__init__.py ---\nimport asyncio, warnings, copy, time\n\nclass BaseNode:\n    def __init__(self): self.params,self.successors={},{}\n    def set_params(self,params): self.params=params\n    def add_successor(self,node,action=\"default\"):\n        if action in self.successors: warnings.warn(f\"Overwriting successor for action '{action}'\")\n        self.successors[action]=node;return node\n    def prep(self,shared): pass\n    def exec(self,prep_res): pass\n    def post(self,shared,prep_res,exec_res): pass\n    def _exec(self,prep_res): return self.exec(prep_res)\n    def _run(self,shared): p=self.prep(shared);e=self._exec(p);return self.post(shared,p,e)\n    def run(self,shared): \n        if self.successors: warnings.warn(\"Node won't run successors. Use Flow.\")  \n        return self._run(shared)\n    def __rshift__(self,other): return self.add_successor(other)\n    def __sub__(self,action):\n        if isinstance(action,str): return _ConditionalTransition(self,action)\n        raise TypeError(\"Action must be a string\")\n\nclass _ConditionalTransition:\n    def __init__(self,src,action): self.src,self.action=src,action\n    def __rshift__(self,tgt): return self.src.add_successor(tgt,self.action)\n\nclass Node(BaseNode):\n    def __init__(self,max_retries=1,wait=0): super().__init__();self.max_retries,self.wait=max_retries,wait\n    def exec_fallback(self,prep_res,exc): raise exc\n    def _exec(self,prep_res):\n        for self.cur_retry in range(self.max_retries):\n            try: return self.exec(prep_res)\n            except Exception as e:\n                if self.cur_retry==self.max_retries-1: return self.exec_fallback(prep_res,e)\n                if self.wait>0: time.sleep(self.wait)\n\nclass BatchNode(Node):\n    def _exec(self,items): return [super(BatchNode,self)._exec(i) for i in (items or [])]\n\nclass Flow(BaseNode):\n    def __init__(self,start): super().__init__();self.start=start\n    def get_next_node(self,curr,action):\n        nxt=curr.successors.get(action or \"default\")\n        if not nxt and curr.successors: warnings.warn(f\"Flow ends: '{action}' not found in {list(curr.successors)}\")\n        return nxt\n    def _orch(self,shared,params=None):\n        curr,p=copy.copy(self.start),(params or {**self.params})\n        while curr: curr.set_params(p);c=curr._run(shared);curr=copy.copy(self.get_next_node(curr,c))\n    def _run(self,shared): pr=self.prep(shared);self._orch(shared);return self.post(shared,pr,None)\n    def exec(self,prep_res): raise RuntimeError(\"Flow can't exec.\")\n\nclass BatchFlow(Flow):\n    def _run(self,shared):\n        pr=self.prep(shared) or []\n        for bp in pr: self._orch(shared,{**self.params,**bp})\n        return self.post(shared,pr,None)\n\nclass AsyncNode(Node):\n    def prep(self,shared): raise RuntimeError(\"Use prep_async.\")\n    def exec(self,prep_res): raise RuntimeError(\"Use exec_async.\")\n    def post(self,shared,prep_res,exec_res): raise RuntimeError(\"Use post_async.\")\n    def exec_fallback(self,prep_res,exc): raise RuntimeError(\"Use exec_fallback_async.\")\n    def _run(self,shared): raise RuntimeError(\"Use run_async.\")\n    async def prep_async(self,shared): pass\n    async def exec_async(self,prep_res): pass\n    async def exec_fallback_async(self,prep_res,exc): raise exc\n    async def post_async(self,shared,prep_res,exec_res): pass\n    async def _exec(self,prep_res): \n        for i in range(self.max_retries):\n            try: return await self.exec_async(prep_res)\n            except Exception as e:\n                if i==self.max_retries-1: return await self.exec_fallback_async(prep_res,e)\n                if self.wait>0: await asyncio.sleep(self.wait)\n    async def run_async(self,shared): \n        if self.successors: warnings.warn(\"Node won't run successors. Use AsyncFlow.\")  \n        return await self._run_async(shared)\n    async def _run_async(self,shared): p=await self.prep_async(shared);e=await self._exec(p);return await self.post_async(shared,p,e)\n\nclass AsyncBatchNode(AsyncNode,BatchNode):\n    async def _exec(self,items): return [await super(AsyncBatchNode,self)._exec(i) for i in items]\n\nclass AsyncParallelBatchNode(AsyncNode,BatchNode):\n    async def _exec(self,items): return await asyncio.gather(*(super(AsyncParallelBatchNode,self)._exec(i) for i in items))\n\nclass AsyncFlow(Flow,AsyncNode):\n    async def _orch_async(self,shared,params=None):\n        curr,p=copy.copy(self.start),(params or {**self.params})\n        while curr:curr.set_params(p);c=await curr._run_async(shared) if isinstance(curr,AsyncNode) else curr._run(shared);curr=copy.copy(self.get_next_node(curr,c))\n    async def _run_async(self,shared): p=await self.prep_async(shared);await self._orch_async(shared);return await self.post_async(shared,p,None)\n\nclass AsyncBatchFlow(AsyncFlow,BatchFlow):\n    async def _run_async(self,shared):\n        pr=await self.prep_async(shared) or []\n        for bp in pr: await self._orch_async(shared,{**self.params,**bp})\n        return await self.post_async(shared,pr,None)\n\nclass AsyncParallelBatchFlow(AsyncFlow,BatchFlow):\n    async def _run_async(self,shared):\n        pr=await self.prep_async(shared) or []\n        await asyncio.gather(*(self._orch_async(shared,{**self.params,**bp}) for bp in pr))\n        return await self.post_async(shared,pr,None)\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 7: AsyncFlow`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "# Chapter 7: AsyncFlow\n\nIn [Chapter 6: AsyncNode](06_asyncnode.md), we learned how to handle asynchronous operations within individual Nodes. Now, let's see how to create an *entire* asynchronous workflow \u2013 a [Flow](01_flow.md) where *all* the Nodes can run concurrently! This is where `AsyncFlow` comes in.\n\nImagine you're a chef juggling multiple dishes simultaneously. You start the rice, then you start chopping vegetables for the stir-fry, and then you check on the baking chicken. You're not waiting for each dish to finish completely before starting the next one! `AsyncFlow` lets you do the same with your PocketFlow \u2013 run multiple Nodes concurrently, maximizing resource utilization.\n\n**The Problem: Running Async Nodes in Sequence**\n\nEven with `AsyncNode`, if you connect them in a regular [Flow](01_flow.md), they will still run sequentially. The `Flow` will wait for one `AsyncNode` to complete before starting the next. We want true concurrency, where multiple `AsyncNode`s can run *at the same time*.\n\n**Our Use Case: Recipe Finder (Expanded)**\n\nLet's expand our recipe finder app from [Chapter 6: AsyncNode](06_asyncnode.md). Now, after fetching recipes, we also want to:\n\n1.  **Suggest a Recipe:** Use an LLM to suggest the best recipe from the fetched recipes (this is an `AsyncNode`).\n2.  **Get Approval:** Ask the user if they approve of the suggested recipe (this can be a regular, synchronous Node).\n\nWe want to fetch recipes *and* suggest a recipe concurrently. If fetching recipes takes 5 seconds and suggesting a recipe takes 3 seconds, the total time should be closer to 5 seconds (the longer task) rather than 8 seconds (the sum of both).\n\n**What is an AsyncFlow?**\n\n`AsyncFlow` is a special type of [Flow](01_flow.md) designed to execute asynchronous Nodes concurrently. It uses the `asyncio` library to manage the execution of these Nodes in a non-blocking manner.\n\n**Key Concepts of AsyncFlow**\n\n1.  **Concurrency:** Running multiple Nodes at the same time.\n2.  **Asyncio Integration:** Leveraging Python's `asyncio` library for managing asynchronous tasks.\n3.  **Non-Blocking Execution:** Tasks can run independently without blocking the main thread.\n\n**Creating an AsyncFlow for Recipe Finding**\n\nFirst, let's define some simple nodes:\n\n```python\nimport asyncio\nfrom pocketflow import AsyncNode, Node\n\nasync def fetch_recipes(ingredient): #Same function as before\n    print(f\"\ud83d\ude34 Fetching recipes for {ingredient}... This will take a moment.\")\n    await asyncio.sleep(2)  # Simulate a 2-second delay\n    return [\"Pasta with \" + ingredient, \"Pizza with \" + ingredient]\n\nclass FetchRecipes(AsyncNode):\n    async def prep_async(self, shared): return shared.get(\"ingredient\", \"default_ingredient\")\n    async def exec_async(self, ingredient): return await fetch_recipes(ingredient)\n    async def post_async(self, shared, prep_res, exec_res): shared[\"recipes\"] = exec_res; return \"suggest\"\n\nclass SuggestRecipe(AsyncNode):\n    async def prep_async(self, shared):\n        recipes = shared.get(\"recipes\", [])\n        #Simulate that the prompt is generated based on the recipes\n        prompt = f\"\"\"Which of these is the best: {recipes}\"\"\"\n        return prompt\n    async def exec_async(self, prompt): #Simulate LLM, takes time\n        print(f\"\ud83e\udd14 Asking LLM to suggest the best recipe...This will take a moment.\")\n        await asyncio.sleep(1) #Simulate a 1-second delay\n        return \"Pasta\" if \"Pasta\" in prompt else \"Pizza\"\n    async def post_async(self, shared, prep_res, exec_res): shared[\"suggestion\"] = exec_res; return \"approve\"\n\nclass GetApproval(Node):\n    def prep(self, shared): return shared.get(\"suggestion\", \"unknown\")\n    def exec(self, suggestion):\n        approval = input(f\"Do you approve of {suggestion}? (yes/no): \")\n        return approval.lower() == \"yes\"\n    def post(self, shared, prep_res, exec_res): return \"accept\" if exec_res else \"retry\"\n\nclass NoOp(Node):\n    \"\"\"Node that does nothing, used to properly end the flow.\"\"\"\n    pass\n```\n\nKey highlights:\n* `FetchRecipes` fetches recipes asyncronously from a website. We saw this example in the last chapter.\n* `SuggestRecipe` uses an LLM to suggest the best recipe. This process takes a bit of time, so this operation also happens asyncronously.\n* `GetApproval` asks the user for input.\n\nNow, let's create the `AsyncFlow`:\n\n```python\nfrom pocketflow import AsyncFlow\nfrom nodes import FetchRecipes, SuggestRecipe, GetApproval, NoOp\n\ndef create_flow():\n    \"\"\"Create and connect nodes into a flow.\"\"\"\n    \n    # Create nodes\n    fetch = FetchRecipes()\n    suggest = SuggestRecipe()\n    approve = GetApproval()\n    end = NoOp()\n    \n    # Connect nodes\n    fetch - \"suggest\" >> suggest\n    suggest - \"approve\" >> approve\n    approve - \"retry\" >> suggest  # Loop back for another suggestion\n    approve - \"accept\" >> end     # Properly end the flow\n    \n    # Create flow starting with fetch\n    flow = AsyncFlow(start=fetch)\n    return flow\n```\n\n*   We create instances of `FetchRecipes`, `SuggestRecipe`, `GetApproval`, and `NoOp` Nodes.\n*   We connect the Nodes using the `>>` and `-` operators to define the [Flow](01_flow.md)'s sequence.\n*   **Crucially:** We create an `AsyncFlow` instance, passing in the starting Node (`fetch`).\n\n**Running the AsyncFlow**\nHere's the code to run this:\n\n```python\nimport asyncio\nfrom nodes import create_flow # Assuming the create_flow function is in nodes.py\n\nasync def main():\n    flow = create_flow()\n    shared_store = {\"ingredient\": \"chicken\"}\n    await flow._run_async(shared_store) #Manually invoke the async flow. The .run function won't work as it's overridden.\n    print(\"Flow completed.\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\nWhen we execute this code, there are some key points to note:\n\n1.  We need to wrap the execution of `AsyncFlow` within an `async` function (`main` in this case).\n2.  We use `asyncio.run(main())` to properly run the asynchronous code.\n3.  **It's VERY IMPORTANT that the AsyncFlow is invoked with `await flow._run_async(shared_store)`. If you run it with `flow.run(shared_store)` the asynchronous operations won't work.**\n\n**Let's Trace the Execution (Simplified)**\n\nBecause `AsyncFlow` allows concurrency, the exact sequence of events can vary. Here's a *possible* scenario:\n\n1.  The [Flow](01_flow.md) starts at the `FetchRecipes` Node.\n2.  `FetchRecipes` starts fetching recipes asynchronously (takes 2 seconds).\n3.  The [Flow](01_flow.md) *immediately* moves to the `SuggestRecipe` Node (without waiting for `FetchRecipes` to complete!).\n4.  `SuggestRecipe` tries to access the recipes, but they're not available yet. It waits.\n5.  After 2 seconds, `FetchRecipes` completes and saves the recipes to the [Shared Store](03_shared_store.md).\n6.  `SuggestRecipe` can now access the recipes and starts suggesting a recipe asynchronously (takes 1 second).\n7.  The [Flow](01_flow.md) moves to the `GetApproval` node, because at the flow level, it doesn't have to wait for `SuggestRecipe` to finish.\n8.  `GetApproval` gets the suggestion and prompts the user for approval.\n\n*In reality, `FetchRecipes` and `SuggestRecipe` can be running concurrently, and the waiting in step 4 is handled more efficiently by `asyncio`.*\n\n**Under the Hood: How AsyncFlow Works**\n\nThe key to `AsyncFlow` is the `_orch_async` method. Let's look at some relevant code from `pocketflow/__init__.py`:\n\n```python\nclass AsyncFlow(Flow,AsyncNode):\n    async def _orch_async(self,shared,params=None):\n        curr,p=copy.copy(self.start),(params or {**self.params})\n        while curr:curr.set_params(p);c=await curr._run_async(shared) if isinstance(curr,AsyncNode) else curr._run(shared);curr=copy.copy(self.get_next_node(curr,c))\n    async def _run_async(self,shared): p=await self.prep_async(shared);await self._orch_async(shared);return await self.post_async(shared,p,None)\n```\n\n*   `_orch_async` is the asynchronous orchestration method. The key part is `c=await curr._run_async(shared)`. This line `await`s the execution of the current Node, *allowing other asynchronous tasks to run while waiting*.\n* `curr=copy.copy(self.get_next_node(curr,c))` gets the next node based on return value.\n\nLet's illustrate the relationship with a sequence diagram.\n\n```mermaid\nsequenceDiagram\n    participant AsyncFlow\n    participant FetchRecipes\n    participant SuggestRecipe\n    participant SharedStore\n    participant EventLoop\n\n    AsyncFlow->>FetchRecipes: _run_async(shared_store)\n    FetchRecipes->>FetchRecipes: prep_async(shared_store)\n    FetchRecipes-->>AsyncFlow: prep_result\n    AsyncFlow->>FetchRecipes: exec_async(prep_result)\n    FetchRecipes->>EventLoop: await fetch_recipes(prep_result)\n    AsyncFlow->>SuggestRecipe: _run_async(shared_store)\n    SuggestRecipe->>SuggestRecipe: prep_async(shared_store)\n    SuggestRecipe->>SharedStore: access recipes\n    EventLoop-->>FetchRecipes: recipes\n    FetchRecipes-->>AsyncFlow: recipes\n    AsyncFlow->>FetchRecipes: post_async(shared_store, prep_result, recipes)\n```\n\nBecause it's an async flow, the `AsyncFlow` can move forward and execute the `SuggestRecipe` Node, even though `FetchRecipes` may still be running in the background, waiting for the simulated web call.\n\n**Benefits of Using AsyncFlow**\n\n*   **Improved Performance:** By running Nodes concurrently, you can significantly reduce the overall execution time of your [Flow](01_flow.md), especially when dealing with I/O-bound operations (like network requests or database queries).\n*   **Increased Responsiveness:** Your application remains responsive even when performing long-running tasks.\n\n**Important Notes:**\n\n*   **`await` Correctly:** Make sure to `await` all asynchronous operations within your `AsyncNode`s and `AsyncFlow`. For the top-level `AsyncFlow`, you must invoke it with `await flow._run_async(shared_store)`.\n*   **Error Handling:** Implement proper error handling within your `AsyncNode`s to gracefully handle exceptions that may occur during asynchronous operations.\n\n**Conclusion**\n\nIn this chapter, we learned about `AsyncFlow` and how it allows us to build entire asynchronous workflows in PocketFlow. We saw how to create an `AsyncFlow` for our recipe finder app and how it enables concurrency. Using `AsyncFlow` allows the PocketFlow to execute complex flows more effectively and efficiently.\n\nNow that we understand `AsyncFlow`, let's move on to [Chapter 8: AsyncParallelBatchNode](08_asyncparallelbatchnode.md) to see how we can combine asynchronous operations with batch processing for even greater performance!\n", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `PocketFlow` about the concept: \"AsyncParallelBatchNode\". This is Chapter 8.\n\nConcept Details:\n- Description:\nFacilitates highly parallel processing of batch items when used alongside AsyncFlow. It combines the benefits of AsyncNode and BatchNode to execute batch jobs concurrently rather than sequentially. This parallelism dramatically increases the performance of I/O-bound operations.\n\n\nComplete Tutorial Structure:\n1. [Flow](01_flow.md)\n2. [Node](02_node.md)\n3. [Shared Store](03_shared_store.md)\n4. [Prep, Exec, Post (Node Lifecycle)](04_prep__exec__post__node_lifecycle_.md)\n5. [BatchNode](05_batchnode.md)\n6. [AsyncNode](06_asyncnode.md)\n7. [AsyncFlow](07_asyncflow.md)\n8. [AsyncParallelBatchNode](08_asyncparallelbatchnode.md)\n\nContext from previous chapters (summary):\n# Chapter 1: Flow\n\nImagine you're directing a play. You have actors, each with specific roles (like answering a question or searching the web). But how do you tell them *when* to act, and *what* to do next?  That's where a **Flow** comes in!\n\nIn PocketFlow, a Flow is like the conductor of an orchestra. It's the orchestration layer that connects individual actions (we call them **Nodes**) into a functioning workflow. It defines the sequence in which these actions are executed and how data is passed between them.\n\n**Our Use Case: A Simple Research Agent**\n\nLet's say we're building a simple research agent that can either search the web or answer a question, depending on what's needed.  The agent will first *decide* what to do. If it needs to search, it will *search the web*. If it needs to answer, it will *answer the question*. After searching, it goes back to the deciding step.\n\nWithout a Flow, we'd have to manually call each action in the right order, which can get messy quickly. A Flow lets us define this process in a clear and organized way.\n\n**Key Concepts of a Flow**\n\n1.  **Nodes:** These are the individual steps or actions in our workflow. Think of them as the actors in our play. We'll dive deep into Nodes in [Chapter 2: Node](02_node.md). For now, just think of them as individual units of work.\n\n2.  **Sequence:** The Flow defines the order in which Nodes are executed.  It's the script of our play.\n\n3.  **Data Passing:** The Flow manages how data is passed between Nodes. This is how the actors communicate and share information.\n\n4.  **Starting Node:** Every flow must have a start node to kick off the process.\n\n5.  **Transitions:**  These define how the Flow moves from one Node to the next. We'll use special operators like `>>` (rshift) and `-` (subtraction) to define these transitions.\n\n**Creating Our Research Agent Flow**\n\nLet's look at how we can create a Flow for our research agent. The code might look like this (from `cookbook/pocketflow-agent/flow.py`):\n\n```python\nfrom pocketflow import Flow\nfrom nodes import DecideAction, SearchWeb, AnswerQuestion\n\ndef create_agent_flow():\n    # Create instances of each node\n    decide = DecideAction()\n    search = SearchWeb()\n    answer = AnswerQuestion()\n\n    # Connect the nodes\n    # If DecideAction returns \"search\", go to SearchWeb\n    decide - \"search\" >> search\n\n    # If DecideAction returns \"answer\", go to AnswerQuestion\n    decide - \"answer\" >> answer\n\n    # After SearchWeb completes and returns \"decide\", go back to DecideAction\n    search - \"decide\" >> decide\n\n    # Create and return the flow, starting with the DecideAction node\n    return Flow(start=decide)\n```\n\nHere's a breakdown:\n\n*   We import `Flow` from the `pocketflow` library.\n*   We import the `Node` types from our nodes file (`nodes.py`). Don't worry about the implementations of `DecideAction`, `SearchWeb`, and `AnswerQuestion` yet.  Just think of them as steps in our process.\n*   We create instances of each Node.\n*   We use the `>>` and `-` operators to define the transitions between Nodes. For example, `decide - \"search\" >> search` means \"If the `decide` Node returns 'search', then go to the `search` Node.\"\n*   We create a `Flow` object, specifying the `decide` Node as the starting point.\n\n**How it Works: Example**\n\nImagine the agent receives the question: \"What is the capital of France?\".\n\n1.  The `Flow` starts at the `DecideAction` Node.\n2.  The `DecideAction` Node determines that we need to search for the answer. It returns the string \"search\".\n3.  The `Flow` sees the transition `decide - \"search\" >> search` and moves to the `SearchWeb` Node.\n4.  The `SearchWeb` Node searches the web and finds the answer: \"Paris\". It stores the answer in memory and returns the string \"decide\".\n5.  The `Flow` sees the transition `search - \"decide\" >> decide` and moves back to the `DecideAction` Node.\n6.  Now `DecideAction` could determine we need to answer the original question. It returns the string \"answer\".\n7.  The `Flow` sees the transition `decide - \"answer\" >> answer` and moves to the `AnswerQuestion` Node.\n8.  The `AnswerQuestion` node crafts the answer using the information in memory and then returns its final response.\n\n**Under the Hood: A Peek Inside**\n\nLet's see what happens inside the `Flow` when we run it.  Here's a simplified sequence diagram:\n\n```mermaid\nsequenceDiagram\n    participant Flow\n    participant DecideAction\n    participant SearchWeb\n    participant AnswerQuestion\n    participant SharedStore\n\n    Flow->>DecideAction: run(shared_store)\n    DecideAction->>SharedStore: access/modify data\n    DecideAction-->>Flow: \"search\"\n    Flow->>SearchWeb: run(shared_store)\n    SearchWeb->>SharedStore: access/modify data\n    SearchWeb-->>Flow: \"decide\"\n    Flow->>DecideAction: run(shared_store)\n    DecideAction->>SharedStore: access/modify data\n    DecideAction-->>Flow: \"answer\"\n    Flow->>AnswerQuestion: run(shared_store)\n    AnswerQuestion->>SharedStore: access/modify data\n    AnswerQuestion-->>Flow: (End)\n```\n\n*   The `Flow` starts by calling the `run` method of the starting Node (`DecideAction` in our example).\n*   Each Node executes its logic and potentially modifies a `SharedStore` ([Chapter 3: Shared Store](03_shared_store.md)) which is used for passing data between the nodes.\n*   The Node returns a value (like \"search\" or \"answer\") that determines the next Node to execute, based on the transitions defined in the Flow.\n*   This process continues until there are no more transitions, or we hit a dead end (no successor node defined for a particular return value).\n\n**Code Dive: `Flow` Class**\n\nLet's look at some of the key parts of the `Flow` class (from `pocketflow/__init__.py`):\n\n```python\nclass Flow(BaseNode):\n    def __init__(self,start): super().__init__();self.start=start\n    def get_next_node(self,curr,action):\n        nxt=curr.successors.get(action or \"default\")\n        if not nxt and curr.successors: warnings.warn(f\"Flow ends: '{action}' not found in {list(curr.successors)}\")\n        return nxt\n    def _orch(self,shared,params=None):\n        curr,p=copy.copy(self.start),(params or {**self.params})\n        while curr: curr.set_params(p);c=curr._run(shared);curr=copy.copy(self.get_next_node(curr,c))\n    def _run(self,shared): pr=self.prep(shared);self._orch(shared);return self.post(shared,pr,None)\n    def exec(self,prep_res): raise RuntimeError(\"Flow can't exec.\")\n```\n\n*   `__init__(self, start)`: The constructor takes the starting Node as an argument.\n*   `get_next_node(self, curr, action)`:  This method determines the next Node to execute based on the return value (`action`) of the current Node (`curr`). It looks up the successor node in the current node's `successors` dictionary.\n*   `_orch(self, shared, params=None)`:  This is the \"orchestration\" method.  It iterates through the Nodes, executing each one in sequence based on the transitions.\n*   `_run(self, shared)`: This method kicks off the orchestration by calling `_orch(self, shared)`.\n\n**Conclusion**\n\nIn this chapter, we learned about the concept of a Flow in PocketFlow. A Flow is an orchestration layer that connects individual Nodes into a functioning workflow. It defines the sequence in which Nodes are executed and how data is passed between them. We saw how to create a Flow for a simple research agent and how it works under the hood.\n\nNow that we understand Flows, let's dive deeper into the building blocks of Flows: **Nodes**. In [Chapter 2: Node](02_node.md), we'll explore what Nodes are, how to create them, and how they interact with Flows.\n\n---\n# Chapter 2: Node\n\nIn [Chapter 1: Flow](01_flow.md), we learned about Flows and how they orchestrate different actions in a specific sequence. Now, let's zoom in on the individual *actions* themselves. These actions are called **Nodes**.\n\nImagine you're building a robot that can make a sandwich. The Flow would be the robot's overall plan: \"Get bread -> Get fillings -> Assemble sandwich -> Serve sandwich\".  Each of those steps \u2013 \"Get bread\", \"Get fillings\", etc. \u2013 would be a Node.\n\n**What Problem Do Nodes Solve?**\n\nNodes solve the problem of breaking down a complex task into smaller, manageable pieces. Instead of having one giant, complicated function, we can have a series of simpler Nodes, each responsible for a specific job. This makes our code easier to understand, test, and maintain.\n\n**Our Sandwich-Making Robot Example**\n\nLet's focus on the \"Get fillings\" step. This could be further broken down: \"Open refrigerator -> Check for desired fillings -> Take out fillings\".  Each of *those* could even be a Node!  For our purposes, let's say \"Get fillings\" is a single Node that figures out which fillings to get and retrieves them.\n\n**Key Concepts of a Node**\n\nA Node is like a mini-program that does one thing well. It has three main parts:\n\n1.  **Input(s):** What the Node needs to get its job done. In our \"Get fillings\" Node, the input might be a list of preferred fillings.\n\n2.  **Processing:** What the Node *does* with the input. This is the core logic of the Node.  In our example, it's the code that checks what fillings are available and retrieves them.\n\n3.  **Output(s):** What the Node produces after processing the input.  In our example, the output would be the actual fillings.\n\nIn PocketFlow, these three parts are defined using three special methods: `prep`, `exec`, and `post`. Don't worry too much about these names for now. Just remember the input, processing, and output.  We will discuss [Prep, Exec, Post (Node Lifecycle)](04_prep__exec__post__node_lifecycle_.md) in detail later.\n\n**Creating a Simple Node**\n\nLet's create a simple Node that adds two numbers.\n\n```python\nfrom pocketflow import Node\n\nclass AddNumbers(Node):\n    def prep(self, shared):\n        \"\"\"Prepares the input numbers from the shared store.\"\"\"\n        num1 = shared[\"num1\"]\n        num2 = shared[\"num2\"]\n        return num1, num2\n\n    def exec(self, inputs):\n        \"\"\"Adds the two numbers together.\"\"\"\n        num1, num2 = inputs\n        result = num1 + num2\n        return result\n\n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Stores the result in the shared store.\"\"\"\n        shared[\"result\"] = exec_res\n        return None  # No next node, the Flow can end\n```\n\nHere's what's happening:\n\n*   We import the `Node` class from `pocketflow`.\n*   We create a class called `AddNumbers` that *inherits* from `Node`. This means our `AddNumbers` class *is* a Node and has all the properties of a Node.\n*   `prep(self, shared)`: This method *prepares* the input for the `exec` method. It retrieves the two numbers (`num1` and `num2`) from the `shared` store (we'll talk about the [Shared Store](03_shared_store.md) in the next chapter!). It returns these two numbers as a tuple.\n*   `exec(self, inputs)`: This method *executes* the core logic of the Node. It takes the output of `prep` as `inputs`, adds the two numbers together, and returns the `result`.\n*   `post(self, shared, prep_res, exec_res)`: This method *posts* the result to the `shared` store. It takes the output of `exec` as `exec_res` and stores it in the `shared` store under the key \"result\".  It returns `None` in this case because there's no further action required in the flow after adding the numbers.\n\n**Using the Node in a Flow**\n\nNow, let's see how we can use this Node in a Flow:\n\n```python\nfrom pocketflow import Flow\nfrom nodes import AddNumbers # Assuming the AddNumbers node is in nodes.py\n\n# Create an instance of the AddNumbers node\nadd_node = AddNumbers()\n\n# Create a Flow starting with the AddNumbers node\nflow = Flow(start=add_node)\n\n# Create a shared store with the input numbers\nshared_store = {\"num1\": 5, \"num2\": 3}\n\n# Run the flow\nflow.run(shared_store)\n\n# Print the result\nprint(shared_store[\"result\"])  # Output: 8\n```\n\nHere's what's happening:\n\n*   We import `Flow` from `pocketflow`.\n*   We create an instance of our `AddNumbers` Node.\n*   We create a `Flow` that starts with our `AddNumbers` Node.\n*   We create a `shared_store` (a dictionary) and put the input numbers (`num1` and `num2`) into it.\n*   We run the `Flow` with the `shared_store`.\n*   Finally, we print the result, which is stored in the `shared_store` under the key \"result\".\n\n**Let's Trace the Execution**\n\n1.  The `Flow` starts at the `AddNumbers` Node.\n2.  The `AddNumbers` Node's `prep` method is called. It retrieves `num1` (5) and `num2` (3) from the `shared_store`.\n3.  The `AddNumbers` Node's `exec` method is called with the input (5, 3). It adds them together and returns 8.\n4.  The `AddNumbers` Node's `post` method is called with the result (8). It stores the result in the `shared_store` under the key \"result\".\n5.  The `Flow` finishes because the `AddNumbers` Node returned `None` in the `post` method, indicating there are no more Nodes to execute.\n\n**Under the Hood: A Closer Look**\n\nLet's see a simplified sequence diagram of what happens when we run the `Flow`:\n\n```mermaid\nsequenceDiagram\n    participant Flow\n    participant AddNumbers\n    participant SharedStore\n\n    Flow->>AddNumbers: run(shared_store)\n    AddNumbers->>SharedStore: access num1, num2\n    AddNumbers-->>Flow: num1, num2\n    Flow->>AddNumbers: exec(num1, num2)\n    AddNumbers-->>Flow: result\n    Flow->>AddNumbers: post(shared_store, prep_res, result)\n    AddNumbers->>SharedStore: store result\n    AddNumbers-->>Flow: None\n```\n\n*   The `Flow` calls the `run` method of the `AddNumbers` Node, passing the `shared_store`.\n*   The `AddNumbers` Node accesses the `shared_store` to get the input numbers.\n*   The `AddNumbers` Node executes its logic and returns the result to the `Flow`.\n*   The `AddNumbers` Node then updates the `shared_store` with the result.\n\nNow, let's peek at some of the relevant code from the `pocketflow/__init__.py` file:\n\n```python\nclass Node(BaseNode): #Simplified\n    def prep(self,shared): pass\n    def exec(self,prep_res): pass\n    def post(self,shared,prep_res,exec_res): pass\n    def _exec(self,prep_res): return self.exec(prep_res)\n    def _run(self,shared): p=self.prep(shared);e=self._exec(p);return self.post(shared,p,e)\n```\n\n*   `prep`, `exec`, and `post` are methods that you can override in your own Node classes to define the behavior of the Node.\n*   The `_run` method is responsible for calling `prep`, `exec`, and `post` in sequence.\n\n**Example: Back to the Research Agent**\nLet's look at our Research Agent code from [Chapter 1: Flow](01_flow.md).\n\n```python\nfrom pocketflow import Node\nfrom utils import call_llm, search_web\nimport yaml\n\nclass DecideAction(Node):\n    def prep(self, shared):\n        # Prepare the context and question\n        question = shared[\"question\"]\n        context = shared.get(\"context\", \"No previous search\")\n        return question, context\n\n    def exec(self, inputs):\n        # Call LLM to decide search or answer\n        question, context = inputs\n        prompt = f\"\"\"... (LLM prompt) ...\"\"\" #Skipping prompt for brevity\n        response = call_llm(prompt)\n        yaml_str = response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n        decision = yaml.safe_load(yaml_str)\n        return decision\n\n    def post(self, shared, prep_res, exec_res):\n        #Save search query and determine next step\n        if exec_res[\"action\"] == \"search\":\n            shared[\"search_query\"] = exec_res[\"search_query\"]\n        else:\n            shared[\"context\"] = exec_res[\"answer\"]\n        return exec_res[\"action\"]\n```\n\nIn this example, `DecideAction` Node decides whether to search or answer based on the provided question and context. The `prep` function grabs the values from the `shared` store, the `exec` function decides on the next action, and the `post` function save the search query (if applicable) and specifies the name of the next node to proceed to.\n\n**Different Types of Nodes**\n\nWhile we've focused on a basic Node here, PocketFlow offers different types of Nodes for different purposes, such as [BatchNode](05_batchnode.md) for processing multiple items at once, and [AsyncNode](06_asyncnode.md) for handling asynchronous operations.\n\n**Conclusion**\n\nIn this chapter, we learned about Nodes, the fundamental building blocks of PocketFlow. We saw how Nodes encapsulate individual actions, and how they are connected together in a Flow. We learned about the `prep`, `exec`, and `post` methods that define the behavior of a Node. Now that we understand Nodes, let's move on to [Chapter 3: Shared Store](03_shared_store.md) to see how Nodes can share data with each other.\n\n---\n# Chapter 3: Shared Store\n\nIn [Chapter 2: Node](02_node.md), we learned about Nodes and how they perform individual tasks. But how do these Nodes *talk* to each other? How does one Node pass its results to the next Node in the [Flow](01_flow.md)? That's where the **Shared Store** comes in!\n\nImagine a relay race. Each runner (Node) has a specific leg to run. The baton (data) needs to be passed from one runner to the next. The Shared Store is like the designated exchange zone where the baton is safely passed.\n\n**The Problem: Passing Information Between Nodes**\n\nWithout a shared space, each Node would operate in isolation.  They wouldn't know what other Nodes have done, and they couldn't build on each other's work.  The Shared Store provides a central location for Nodes to store and retrieve information, enabling collaboration and data flow within the PocketFlow ecosystem.\n\n**Our Research Agent Example (Again!)**\n\nRemember our research agent from [Chapter 1: Flow](01_flow.md)? The `SearchWeb` Node needs to pass the search results to the `AnswerQuestion` Node. Without a Shared Store, the `AnswerQuestion` Node wouldn't know what the web search found!\n\n**What *is* the Shared Store?**\n\nIn PocketFlow, the Shared Store is essentially a **dictionary**.  If you're new to programming, think of a dictionary as a container that holds key-value pairs. You can store data using a specific \"key\" and later retrieve it using that same key.\n\nFor example, we might store the search results with the key \"search_results\":\n\n```python\nshared_store[\"search_results\"] = \"The capital of France is Paris.\"\n```\n\nLater, another Node can access this information:\n\n```python\ncapital = shared_store[\"search_results\"]\nprint(capital) # Output: The capital of France is Paris.\n```\n\n**Key Concepts of the Shared Store**\n\n1.  **Centralized Data:** It's a single place where all Nodes can access and modify data.\n\n2.  **Key-Value Pairs:** Data is stored as key-value pairs, like a Python dictionary.\n\n3.  **Accessibility:** Any Node in the [Flow](01_flow.md) can access the Shared Store.\n\n4.  **Dynamic:** Nodes can add, modify, and delete data in the Shared Store during the [Flow](01_flow.md)'s execution.\n\n**How to Use the Shared Store**\n\nLet's revisit our `AddNumbers` Node from [Chapter 2: Node](02_node.md) and see how it uses the Shared Store:\n\n```python\nfrom pocketflow import Node\n\nclass AddNumbers(Node):\n    def prep(self, shared):\n        \"\"\"Prepares the input numbers from the shared store.\"\"\"\n        num1 = shared[\"num1\"]\n        num2 = shared[\"num2\"]\n        return num1, num2\n\n    def exec(self, inputs):\n        \"\"\"Adds the two numbers together.\"\"\"\n        num1, num2 = inputs\n        result = num1 + num2\n        return result\n\n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Stores the result in the shared store.\"\"\"\n        shared[\"result\"] = exec_res\n        return None  # No next node, the Flow can end\n```\n\n*   In the `prep` method, the Node retrieves the values associated with the keys \"num1\" and \"num2\" from the `shared` store.\n\n*   In the `post` method, the Node stores the calculated `result` in the `shared` store using the key \"result\".\n\n**Example Usage**\n\nHere's how the Shared Store is used when we run the [Flow](01_flow.md):\n\n```python\nfrom pocketflow import Flow\nfrom nodes import AddNumbers # Assuming the AddNumbers node is in nodes.py\n\n# Create an instance of the AddNumbers node\nadd_node = AddNumbers()\n\n# Create a Flow starting with the AddNumbers node\nflow = Flow(start=add_node)\n\n# Create a shared store with the input numbers\nshared_store = {\"num1\": 5, \"num2\": 3}\n\n# Run the flow\nflow.run(shared_store)\n\n# Print the result\nprint(shared_store[\"result\"])  # Output: 8\n```\n\n*   We create a `shared_store` (a dictionary) and initialize it with the input values for \"num1\" and \"num2\". This is how we *seed* the Shared Store with initial data.\n\n*   After the `flow.run(shared_store)` call, the `shared_store` now contains the key \"result\" with the value 8, because the `AddNumbers` Node stored it there.\n\n**Another Example: Passing the Question to the Answer Node**\nLet's look back at the nodes of our research agent in `cookbook/pocketflow-agent/nodes.py`. The `DecideAction` Node checks whether to search or answer the question and it has access to a \"question\" value in the shared store.\n\n```python\nclass DecideAction(Node):\n    def prep(self, shared):\n        # Prepare the context and question\n        question = shared[\"question\"] #Access question value from shared store\n        context = shared.get(\"context\", \"No previous search\")\n        return question, context\n    #Skipping the other functions for brevity.\n```\n\nIn this case, the `prep` function of `DecideAction` node accesses the question value from the shared store using the key \"question\".\n\n**Important Note:**\n\n*   **Key Existence:** It's good practice to check if a key exists in the Shared Store before accessing it, especially if the Node isn't the one who put the value there. You can use `shared.get(\"key\", default_value)` which returns `default_value` if the key doesn't exist. Or `if \"key\" in shared:` to check for a key's existence.\n\n**Under the Hood: How it Works**\n\nWhen you run a [Flow](01_flow.md), the `Flow` object creates and passes a Shared Store object to each Node's `prep`, `exec`, and `post` methods.\n\nHere's a simplified sequence diagram:\n\n```mermaid\nsequenceDiagram\n    participant Flow\n    participant Node1\n    participant Node2\n    participant SharedStore\n\n    Flow->>SharedStore: Create SharedStore\n    Flow->>Node1: run(SharedStore)\n    Node1->>SharedStore: Access/Modify data\n    Node1-->>Flow: (Next Action)\n    Flow->>Node2: run(SharedStore)\n    Node2->>SharedStore: Access/Modify data\n    Node2-->>Flow: (End)\n```\n\n*   The `Flow` creates a single `SharedStore` instance at the beginning.\n\n*   This same `SharedStore` instance is passed to each `Node` in the [Flow](01_flow.md).\n\n*   Each `Node` can then read from and write to the `SharedStore`.\n\n**Code Snippet: Flow Passing the Shared Store (From `pocketflow/__init__.py`)**\nThis is how the [Flow](01_flow.md) passes the Shared Store to each node:\n\n```python\nclass Flow(BaseNode):\n    def __init__(self,start): super().__init__();self.start=start\n    def get_next_node(self,curr,action):\n        nxt=curr.successors.get(action or \"default\")\n        if not nxt and curr.successors: warnings.warn(f\"Flow ends: '{action}' not found in {list(curr.successors)}\")\n        return nxt\n    def _orch(self,shared,params=None):\n        curr,p=copy.copy(self.start),(params or {**self.params})\n        while curr: curr.set_params(p);c=curr._run(shared);curr=copy.copy(self.get_next_node(curr,c))\n    def _run(self,shared): pr=self.prep(shared);self._orch(shared);return self.post(shared,pr,None)\n    def exec(self,prep_res): raise RuntimeError(\"Flow can't exec.\")\n```\nIn the `_orch` function, the shared store is maintained and passed into each subsequent node. Note the line `c=curr._run(shared)` indicating the shared store is passed into the node's run function.\n\n**Conclusion**\n\nIn this chapter, we learned about the Shared Store, a crucial component of PocketFlow that allows Nodes to communicate and share data. We saw how the Shared Store acts as a central memory, enabling seamless data transfer between processing steps. We learned how Nodes access and modify data in the Shared Store, and how the Flow manages the Shared Store behind the scenes.\n\nNow that we understand the Shared Store, we can appreciate how PocketFlow Nodes interact within a Flow to accomplish complex tasks. In the next chapter, [Prep, Exec, Post (Node Lifecycle)](04_prep__exec__post__node_lifecycle_.md), we'll dive deeper into the `prep`, `exec`, and `post` methods and understand the lifecycle of a Node.\n\n---\n# Chapter 4: Prep, Exec, Post (Node Lifecycle)\n\nIn [Chapter 2: Node](02_node.md), we learned about Nodes and how they encapsulate individual actions. We briefly touched upon the `prep`, `exec`, and `post` methods. Now, let's dive deeper into what these methods do and how they define the lifecycle of a Node.\n\nImagine you're a chef preparing a dish.\n\n*   **Prep:** You gather your ingredients, chop vegetables, and prepare your cooking station.\n*   **Exec:** You actually cook the dish \u2013 the main action.\n*   **Post:** You plate the dish, add garnishes, and serve it.\n\nThe `prep`, `exec`, and `post` methods in a Node are similar! They provide a structured way to define the different stages of a Node's operation.\n\n**The Problem: Organizing Node Logic**\n\nWithout a clear structure, the code inside a Node can become disorganized and hard to understand.  The `prep`, `exec`, and `post` pattern provides a consistent way to organize Node logic, making it easier to read, maintain, and debug.\n\n**Our Research Agent Example (One More Time!)**\n\nLet's revisit our research agent and focus on the `SearchWeb` Node. It needs to:\n\n1.  **Prep:** Get the search query from the [Shared Store](03_shared_store.md).\n2.  **Exec:** Actually perform the web search using the query.\n3.  **Post:** Save the search results back into the [Shared Store](03_shared_store.md) and decide what's next.\n\n**Key Concepts: Prep, Exec, Post**\n\n*   **Prep (Preparation):**  This method prepares the input data needed for the main task (`exec`). It typically retrieves data from the [Shared Store](03_shared_store.md) or performs any necessary data transformations.\n    *   **Input:** The [Shared Store](03_shared_store.md).\n    *   **Output:**  Data that the `exec` method needs (can be any Python object).\n\n*   **Exec (Execution):** This is the heart of the Node. It performs the core logic of the Node, using the data prepared by the `prep` method.\n    *   **Input:** The output of the `prep` method.\n    *   **Output:** The result of the Node's main operation (can be any Python object).\n\n*   **Post (Post-processing):** This method processes the output of the `exec` method. It typically saves the results back into the [Shared Store](03_shared_store.md), performs any cleanup, and determines the next step in the [Flow](01_flow.md). It specifies which Node to run next in the flow.\n    *   **Input:** The [Shared Store](03_shared_store.md), the output of `prep`, and the output of `exec`.\n    *   **Output:** A value that determines which Node to execute next in the [Flow](01_flow.md). If it returns `None`, the flow can end.\n\n**Example: Back to the `AddNumbers` Node**\n\nLet's examine the `AddNumbers` Node again:\n\n```python\nfrom pocketflow import Node\n\nclass AddNumbers(Node):\n    def prep(self, shared):\n        \"\"\"Prepares the input numbers from the shared store.\"\"\"\n        num1 = shared[\"num1\"]\n        num2 = shared[\"num2\"]\n        return num1, num2\n\n    def exec(self, inputs):\n        \"\"\"Adds the two numbers together.\"\"\"\n        num1, num2 = inputs\n        result = num1 + num2\n        return result\n\n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Stores the result in the shared store.\"\"\"\n        shared[\"result\"] = exec_res\n        return None  # No next node, the Flow can end\n```\n\n*   **`prep`**: Retrieves `num1` and `num2` from the [Shared Store](03_shared_store.md) and returns them as a tuple.\n*   **`exec`**: Takes the tuple from `prep`, adds the numbers, and returns the `result`.\n*   **`post`**: Stores the `result` in the [Shared Store](03_shared_store.md) and returns `None`, indicating the end of the [Flow](01_flow.md).\n\n**Another Example: `SearchWeb` Node from the Research Agent**\n\n```python\nfrom pocketflow import Node\nfrom utils import search_web # Assuming you have a search_web utility\n\nclass SearchWeb(Node):\n    def prep(self, shared):\n        \"\"\"Get the search query from the shared store.\"\"\"\n        return shared[\"search_query\"]\n        \n    def exec(self, search_query):\n        \"\"\"Search the web for the given query.\"\"\"\n        print(f\"\ud83c\udf10 Searching the web for: {search_query}\")\n        results = search_web(search_query)\n        return results\n    \n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Save the search results and go back to the decision node.\"\"\"\n        shared[\"context\"] = \"SEARCH RESULTS: \" + exec_res\n        return \"decide\" # Back to DecideAction node\n```\n\n*   **`prep`**:  Retrieves the `search_query` from the [Shared Store](03_shared_store.md).\n*   **`exec`**:  Performs the web search using `search_web` and returns the `results`.\n*   **`post`**: Saves the `results` into the [Shared Store](03_shared_store.md) under the key \"context\", and returns `\"decide\"` to tell the [Flow](01_flow.md) to go back to `DecideAction` ([Chapter 1: Flow](01_flow.md)).\n\n**How It Works: Example Execution**\n\nLet's trace the execution of the `SearchWeb` Node:\n\n1.  The [Flow](01_flow.md) calls the `run` method of the `SearchWeb` Node, passing the [Shared Store](03_shared_store.md).\n2.  The `prep` method retrieves the `search_query` (e.g., \"capital of France\") from the [Shared Store](03_shared_store.md).\n3.  The `exec` method uses the `search_query` to search the web. Let's say it gets the result \"Paris is the capital of France.\"\n4.  The `post` method saves the search result \"Paris is the capital of France.\" into the [Shared Store](03_shared_store.md). Also the return value is \"decide\".\n5.  The [Flow](01_flow.md) then uses the `\"decide\"` return value to determine the next Node to run.\n\n**Under the Hood: The `_run` Method**\n\nThe `Node` class in `pocketflow/__init__.py` has a `_run` method that orchestrates the `prep`, `exec`, and `post` methods:\n\n```python\nclass Node(BaseNode): #Simplified\n    def prep(self,shared): pass\n    def exec(self,prep_res): pass\n    def post(self,shared,prep_res,exec_res): pass\n    def _exec(self,prep_res): return self.exec(prep_res)\n    def _run(self,shared): p=self.prep(shared);e=self._exec(p);return self.post(shared,p,e)\n```\n\nAs you can see, `_run` calls `prep`, then `exec`, and finally `post` in sequence.  It passes the output of one method as input to the next.\n\nLet's visualize this with a sequence diagram:\n\n```mermaid\nsequenceDiagram\n    participant Node\n    participant SharedStore\n\n    Node->>Node: prep(shared_store)\n    Node-->>Node: prep_result\n    Node->>Node: exec(prep_result)\n    Node-->>Node: exec_result\n    Node->>Node: post(shared_store, prep_result, exec_result)\n    Node-->>Node: next_action\n```\n\n1.  `prep(shared_store)`: The `prep` method is called with the [Shared Store](03_shared_store.md).\n2.  `exec(prep_result)`: The `exec` method is called with the result of `prep`.\n3.  `post(shared_store, prep_result, exec_result)`: The `post` method is called with the [Shared Store](03_shared_store.md), the result of `prep`, and the result of `exec`. The `post` function's return value (next_action) controls what node to run next.\n\n**Why This Structure Matters**\n\n*   **Organization:** Clearly separates data preparation, core logic, and post-processing.\n*   **Testability:**  Each method can be tested independently.\n*   **Reusability:** Nodes become more modular and easier to reuse in different [Flows](01_flow.md).\n*   **Readability:**  The `prep`, `exec`, `post` structure makes the code easier to understand.\n\n**Conclusion**\n\nIn this chapter, we learned about the `prep`, `exec`, and `post` methods and how they define the lifecycle of a Node.  We saw how this structure helps organize Node logic, making it more readable, testable, and reusable. We saw how each function affects the [Shared Store](03_shared_store.md) and how the `post` function returns the next node to execute.\n\nNow that we understand the Node lifecycle, let's move on to [Chapter 5: BatchNode](05_batchnode.md) to see how we can process multiple items in a single Node using `BatchNode`.\n\n---\n# Chapter 5: BatchNode\n\nIn [Chapter 4: Prep, Exec, Post (Node Lifecycle)](04_prep__exec__post__node_lifecycle_.md), we learned about the `prep`, `exec`, and `post` methods and how they structure a Node's lifecycle. Now, let's explore how to efficiently process *multiple* data items within a single Node using the `BatchNode`.\n\nImagine you're running a bakery. Instead of baking one cookie at a time, you bake dozens on a single tray! The `BatchNode` is like that tray \u2013 it lets you process a \"batch\" of items together.\n\n**The Problem: Processing Many Items Efficiently**\n\nSometimes, you need to perform the same operation on a large number of independent data items. Doing this with a regular [Node](02_node.md) would involve iterating through each item individually, which can be slow and inefficient. The `BatchNode` solves this by allowing you to process these items in a batch, making the process much faster.\n\n**Our Use Case: Translating a Document into Multiple Languages**\n\nLet's say you have a document and you want to translate it into several languages (like Chinese, Spanish, French, etc.). Instead of creating a separate [Node](02_node.md) for each language, we can use a `BatchNode` to translate the document into all the languages at once!\n\n**What is a BatchNode?**\n\nA `BatchNode` is a special type of [Node](02_node.md) that's designed to process multiple data items in a single execution. Think of it like an assembly line:\n\n1.  **Prep:** The `prep` method prepares the input data and splits it into chunks. These chunks are individual items or mini-batches within the larger batch.\n2.  **Exec:** The `exec` method is applied to each chunk of data. It performs the same operation on each item in the chunk.\n3.  **Post:** The `post` method combines the results from processing all the chunks and prepares the final output.\n\n**Key Concepts of a BatchNode**\n\n1.  **Batch Processing:** The ability to process multiple independent data items together.\n2.  **Chunking:** Splitting the input data into smaller, manageable chunks.\n3.  **Parallelism (Optional):** The `exec` method can potentially process chunks in parallel (we'll explore this in later chapters). For now, we'll assume it's processed sequentially.\n4.  **Result Aggregation:** Combining the results from processing all the chunks into a final output.\n\n**Creating a BatchNode for Translation**\n\nLet's create a `BatchNode` that translates a given text into multiple languages:\n\n```python\nfrom pocketflow import BatchNode\n\nclass TranslateTextNode(BatchNode):\n    def prep(self, shared):\n        text = shared.get(\"text\", \"(No text provided)\")\n        languages = shared.get(\"languages\", [\"Chinese\", \"Spanish\"]) # Reduced languages for brevity\n        \n        # Create batches for each language translation\n        return [(text, lang) for lang in languages]\n\n    def exec(self, data_tuple):\n        text, language = data_tuple\n        #Simulate Translation for brevity\n        translation=f\"Translated to {language}: {text}\"\n        return {\"language\": language, \"translation\": translation}\n\n    def post(self, shared, prep_res, exec_res_list):\n        # Print each translation result\n        for result in exec_res_list:\n            language, translation = result[\"language\"], result[\"translation\"]\n            print(f\"Translation to {language}: {translation}\")\n        return None\n```\n\nHere's a breakdown:\n\n*   `TranslateTextNode` inherits from `BatchNode`.\n*   `prep(self, shared)`: This method retrieves the text to translate and the list of languages from the [Shared Store](03_shared_store.md). It then creates a list of tuples, where each tuple contains the text and a language. This list becomes the \"batch\" of items to be processed.\n*   `exec(self, data_tuple)`: This method takes a single tuple (text, language) as input. It simulates translation for demonstration purposes by returning a translated version of the text.\n*   `post(self, shared, prep_res, exec_res_list)`: This method receives a list of results, where each result is a dictionary containing the translated text and the language. It prints each translation to the console.\n\n**Using the BatchNode in a Flow**\n\nNow, let's see how to use this `BatchNode` in a [Flow](01_flow.md):\n\n```python\nfrom pocketflow import Flow\nfrom nodes import TranslateTextNode # Assuming the TranslateTextNode is in nodes.py\n\n# Create an instance of the TranslateTextNode\ntranslate_node = TranslateTextNode()\n\n# Create a Flow starting with the TranslateTextNode\nflow = Flow(start=translate_node)\n\n# Create a shared store with the input text and languages\nshared_store = {\n    \"text\": \"Hello, world!\",\n    \"languages\": [\"Chinese\", \"Spanish\"]  # Reduced languages for brevity\n}\n\n# Run the flow\nflow.run(shared_store)\n\n# Expected Output:\n# Translation to Chinese: Translated to Chinese: Hello, world!\n# Translation to Spanish: Translated to Spanish: Hello, world!\n```\n\nHere's what's happening:\n\n1.  We create an instance of the `TranslateTextNode`.\n2.  We create a [Flow](01_flow.md) that starts with the `TranslateTextNode`.\n3.  We create a `shared_store` ([Chapter 3: Shared Store](03_shared_store.md)) and populate it with the text and languages.\n4.  We run the [Flow](01_flow.md). The `TranslateTextNode` will translate the text into each language and print the results.\n\n**Let's Trace the Execution**\n\n1.  The [Flow](01_flow.md) starts at the `TranslateTextNode`.\n2.  The `TranslateTextNode`'s `prep` method is called. It retrieves the text and languages from the `shared_store` and creates a list of tuples: `[(\"Hello, world!\", \"Chinese\"), (\"Hello, world!\", \"Spanish\")]`.\n3.  The `TranslateTextNode`'s `exec` method is called *twice*:\n    *   First, with the tuple `(\"Hello, world!\", \"Chinese\")`. It returns `{\"language\": \"Chinese\", \"translation\": \"Translated to Chinese: Hello, world!\"}`.\n    *   Second, with the tuple `(\"Hello, world!\", \"Spanish\")`. It returns `{\"language\": \"Spanish\", \"translation\": \"Translated to Spanish: Hello, world!\"}`.\n4.  The `TranslateTextNode`'s `post` method is called with the list of results: `[{\"language\": \"Chinese\", \"translation\": \"Translated to Chinese: Hello, world!\"}, {\"language\": \"Spanish\", \"translation\": \"Translated to Spanish: Hello, world!\"}]`. It prints each translation to the console.\n5.  The [Flow](01_flow.md) finishes because the `TranslateTextNode` returned `None` in the `post` method.\n\n**Under the Hood: How BatchNode Works**\n\nWhen a [Flow](01_flow.md) runs a `BatchNode`, the `_exec` method is overridden to process items as a batch.\n\nHere's a simplified sequence diagram:\n\n```mermaid\nsequenceDiagram\n    participant Flow\n    participant BatchNode\n    participant SharedStore\n\n    Flow->>BatchNode: run(shared_store)\n    BatchNode->>SharedStore: access text, languages\n    BatchNode-->>Flow: [(text, lang1), (text, lang2), ...]\n    loop For each item in batch\n        Flow->>BatchNode: exec((text, lang_i))\n        BatchNode-->>Flow: {language: lang_i, translation: ...}\n    end\n    Flow->>BatchNode: post(shared_store, prep_res, exec_res_list)\n    BatchNode->>SharedStore: Store results (optional)\n    BatchNode-->>Flow: None\n```\n\n1.  The [Flow](01_flow.md) calls the `run` method of the `BatchNode`, passing the [Shared Store](03_shared_store.md).\n2.  The `prep` method retrieves the necessary data from the [Shared Store](03_shared_store.md) and prepares the batch of items.\n3.  The `exec` method is called for each item in the batch.\n4.  The `post` method receives a list of results from all the `exec` calls.\n\nLet's look at the relevant code from `pocketflow/__init__.py`:\n\n```python\nclass BatchNode(Node):\n    def _exec(self,items): return [super(BatchNode,self)._exec(i) for i in (items or [])]\n```\n\nThis code snippet shows that the `_exec` method of the `BatchNode` iterates through the `items` (which are the output from the `prep` function) and calls the `exec` method for each item. The results are then collected into a list and returned.\n\n**Benefits of Using BatchNode**\n\n*   **Improved Efficiency:** Processes multiple items in a single Node execution.\n*   **Simplified Code:** Reduces the need for manual iteration within the [Flow](01_flow.md).\n*   **Potential for Parallelism:** Makes it easier to parallelize processing (we'll see this later!).\n\n**Real World example - CSV Processing**\n\nAnother use case is processing CSV files.\n```python\nimport pandas as pd\nfrom pocketflow import BatchNode\n\nclass CSVProcessor(BatchNode):\n    def __init__(self, chunk_size=1000):\n        super().__init__()\n        self.chunk_size = chunk_size\n    \n    def prep(self, shared):\n        chunks = pd.read_csv(\n            shared[\"input_file\"],\n            chunksize=self.chunk_size\n        )\n        return chunks\n    \n    def exec(self, chunk):\n        return {\n            \"total_sales\": chunk[\"amount\"].sum(),\n            \"num_transactions\": len(chunk),\n            \"total_amount\": chunk[\"amount\"].sum()\n        }\n    \n    def post(self, shared, prep_res, exec_res_list):\n        total_sales = sum(res[\"total_sales\"] for res in exec_res_list)\n        shared[\"total_sales\"] = total_sales\n        return None\n```\n\nHere, the `prep` function reads the CSV file in chunks (batches) of `chunk_size`. The `exec` function processes each chunk and calculates sales stats, and the `post` function adds up all of the sales.\n\n**Conclusion**\n\nIn this chapter, we learned about the `BatchNode` and how it allows us to process multiple data items efficiently within a single Node. We saw how to create a `BatchNode` for translating text into multiple languages and how it works under the hood.\n\nNow that we understand `BatchNode`, let's move on to [Chapter 6: AsyncNode](06_asyncnode.md) to see how we can handle asynchronous operations within PocketFlow.\n\n---\n# Chapter 6: AsyncNode\n\nIn [Chapter 5: BatchNode](05_batchnode.md), we learned how to process multiple items efficiently using `BatchNode`. But what if some of the operations inside our Nodes take a *long* time, like waiting for a response from a website or a database? This is where `AsyncNode` comes in!\n\nImagine you're ordering food online. You don't want your whole computer to freeze while waiting for the restaurant to confirm your order, right? You want to be able to do other things while your food is being prepared. `AsyncNode` allows your PocketFlow to do just that - perform non-blocking, asynchronous operations.\n\n**The Problem: Dealing with Time-Consuming Operations**\n\nSome tasks take a while. Network requests, database queries, and complex computations can all block the execution of our [Flow](01_flow.md), making it slow and unresponsive. We want PocketFlow to continue processing other tasks while these long-running operations are in progress.\n\n**Our Use Case: Recipe Generation with an LLM**\n\nLet's say we want to build a simple recipe app. Our [Flow](01_flow.md) will:\n\n1.  Ask the user for an ingredient.\n2.  Fetch recipes containing that ingredient from a website (takes time!).\n3.  Use an LLM (Large Language Model) to suggest the best recipe from the fetched recipes (also takes time!).\n4.  Ask the user if they approve of the recipe.\n\nFetching recipes from a website and interacting with an LLM can take several seconds. We don't want our app to freeze during this time! `AsyncNode` will allow us to handle these operations asynchronously, making our app more responsive.\n\n**What is an AsyncNode?**\n\nAn `AsyncNode` is a special type of [Node](02_node.md) that allows us to perform asynchronous operations using the `async` and `await` keywords in Python.\n\n*   **`async`**:  This keyword declares a function as a *coroutine*. A coroutine is a special type of function that can be paused and resumed.\n*   **`await`**:  This keyword is used inside an `async` function to wait for another coroutine to complete *without blocking* the execution of the rest of the [Flow](01_flow.md).\n\nThink of `await` as telling your program, \"Hey, I'm going to wait for this to finish, but don't just sit there doing nothing! Go do something else in the meantime.\"\n\n**Key Concepts of an AsyncNode**\n\n1.  **Asynchronous Operations:** Performing tasks without blocking the main thread of execution.\n2.  **`async` and `await` Keywords:** Used to define and control asynchronous operations in Python.\n3.  **Non-Blocking:** Allows the [Flow](01_flow.md) to continue processing other tasks while waiting for asynchronous operations to complete.\n\n**Creating an AsyncNode for Fetching Recipes**\n\nLet's create an `AsyncNode` that fetches recipes from a website. We'll simplify things by using a dummy function that simulates a network request:\n\n```python\nimport asyncio\nfrom pocketflow import AsyncNode\n\n# Dummy function to simulate fetching recipes (takes time!)\nasync def fetch_recipes(ingredient):\n    print(f\"\ud83d\ude34 Fetching recipes for {ingredient}... This will take a moment.\")\n    await asyncio.sleep(2)  # Simulate a 2-second delay\n    return [\"Pasta with \" + ingredient, \"Pizza with \" + ingredient]\n\nclass FetchRecipesNode(AsyncNode):\n    async def prep_async(self, shared):\n        # Prep data (can be synchronous or asynchronous)\n        ingredient = shared.get(\"ingredient\", \"default_ingredient\")\n        return ingredient\n\n    async def exec_async(self, ingredient):\n        # Asynchronously fetch recipes\n        recipes = await fetch_recipes(ingredient)\n        return recipes\n\n    async def post_async(self, shared, prep_res, exec_res):\n        # Store the fetched recipes and return a value\n        shared[\"recipes\"] = exec_res\n        return \"suggest\"\n```\n\nHere's what's happening:\n\n*   `FetchRecipesNode` inherits from `AsyncNode`.\n*   `fetch_recipes(ingredient)` is an `async` function that simulates fetching recipes from a website. It uses `await asyncio.sleep(2)` to pause execution for 2 seconds, simulating the delay of a network request.\n*   `prep_async(self, shared)`: This asynchronous `prep` function retrieves the ingredient from the [Shared Store](03_shared_store.md).\n*   `exec_async(self, ingredient)`: This asynchronous `exec` function calls `fetch_recipes(ingredient)` using `await`. This means the `exec` function will pause execution until `fetch_recipes` completes, but *without blocking* the rest of the [Flow](01_flow.md).\n*   `post_async(self, shared, prep_res, exec_res)`: This asynchronous `post` function stores the fetched recipes in the [Shared Store](03_shared_store.md) and returns `\"suggest\"`.\n\n**Important:** Notice that we use `prep_async`, `exec_async`, and `post_async` instead of the regular `prep`, `exec`, and `post` methods. `AsyncNode` requires these `async` versions.\n\n**Using the AsyncNode in a Flow**\n\nNow, let's see how to use this `AsyncNode` in a [Flow](01_flow.md):\n\n```python\nimport asyncio\nfrom pocketflow import Flow\nfrom nodes import FetchRecipesNode # Assuming the FetchRecipesNode is in nodes.py\n\n# Create an instance of the FetchRecipesNode\nfetch_recipes_node = FetchRecipesNode()\n\n# Create a Flow starting with the FetchRecipesNode\nflow = Flow(start=fetch_recipes_node)\n\n# Create a shared store\nshared_store = {\"ingredient\": \"chicken\"}\n\n# Run the flow (using asyncio.run for async flows)\nasync def run_flow():\n    flow.run(shared_store)\n\nasyncio.run(run_flow())\n\n# Expected output:\n# \ud83d\ude34 Fetching recipes for chicken... This will take a moment.\n```\n\nHere's what's happening:\n\n1.  We create an instance of the `FetchRecipesNode`.\n2.  We create a [Flow](01_flow.md) that starts with the `FetchRecipesNode`.\n3.  We create a `shared_store` ([Chapter 3: Shared Store](03_shared_store.md)).\n4.  We define an `async` function `run_flow` that calls `flow.run(shared_store)`.\n5.  We use `asyncio.run(run_flow())` to run the asynchronous [Flow](01_flow.md). *This is crucial!* You need to use `asyncio.run` to properly execute an asynchronous [Flow](01_flow.md).\n\n**Let's Trace the Execution**\n\n1.  The [Flow](01_flow.md) starts at the `FetchRecipesNode`.\n2.  The `FetchRecipesNode`'s `prep_async` method is called. It retrieves the ingredient \"chicken\" from the `shared_store`.\n3.  The `FetchRecipesNode`'s `exec_async` method is called with the ingredient \"chicken\".\n4.  The `exec_async` method calls `await fetch_recipes(\"chicken\")`. The `fetch_recipes` function simulates a network request and pauses execution for 2 seconds.\n5.  *During those 2 seconds, the PocketFlow can theoretically do other things (though in this simple example, there's nothing else to do).*\n6.  After 2 seconds, `fetch_recipes` returns a list of recipes.\n7.  The `exec_async` method returns the list of recipes.\n8.  The `FetchRecipesNode`'s `post_async` method is called. It stores the recipes in the `shared_store` and returns `\"suggest\"`.\n9.  The [Flow](01_flow.md) finishes (or continues to the next node, if there is one).\n\n**Under the Hood: How AsyncNode Works**\n\nThe `AsyncNode` class overrides the regular `prep`, `exec`, and `post` methods to prevent accidental use. It also provides the `_run_async` method, which orchestrates the execution of the `prep_async`, `exec_async`, and `post_async` methods.\n\nHere's a simplified sequence diagram:\n\n```mermaid\nsequenceDiagram\n    participant Flow\n    participant AsyncNode\n    participant SharedStore\n    participant EventLoop\n\n    Flow->>AsyncNode: run_async(shared_store)\n    AsyncNode->>AsyncNode: prep_async(shared_store)\n    AsyncNode-->>Flow: prep_result\n    Flow->>AsyncNode: exec_async(prep_result)\n    AsyncNode->>EventLoop: await fetch_recipes(prep_result)\n    EventLoop-->>AsyncNode: recipes\n    AsyncNode-->>Flow: recipes\n    Flow->>AsyncNode: post_async(shared_store, prep_result, recipes)\n    AsyncNode->>SharedStore: store recipes\n    AsyncNode-->>Flow: next_node\n```\n\n1.  The [Flow](01_flow.md) calls the `run_async` method of the `AsyncNode`, passing the [Shared Store](03_shared_store.md).\n2.  The `prep_async` method retrieves the necessary data from the [Shared Store](03_shared_store.md).\n3.  The `exec_async` method performs the asynchronous operation using `await`. This allows the event loop to execute other tasks while waiting for the operation to complete.\n4.  The `post_async` method processes the result and updates the [Shared Store](03_shared_store.md).\n\nLet's look at some of the relevant code from `pocketflow/__init__.py`:\n\n```python\nclass AsyncNode(Node):\n    def prep(self,shared): raise RuntimeError(\"Use prep_async.\")\n    def exec(self,prep_res): raise RuntimeError(\"Use exec_async.\")\n    def post(self,shared,prep_res,exec_res): raise RuntimeError(\"Use post_async.\")\n    def exec_fallback(self,prep_res,exc): raise RuntimeError(\"Use exec_fallback_async.\")\n    def _run(self,shared): raise RuntimeError(\"Use run_async.\")\n    async def prep_async(self,shared): pass\n    async def exec_async(self,prep_res): pass\n    async def exec_fallback_async(self,prep_res,exc): raise exc\n    async def post_async(self,shared,prep_res,exec_res): pass\n    async def _exec(self,prep_res): \n        for i in range(self.max_retries):\n            try: return await self.exec_async(prep_res)\n            except Exception as e:\n                if i==self.max_retries-1: return await self.exec_fallback_async(prep_res,e)\n                if self.wait>0: await asyncio.sleep(self.wait)\n    async def run_async(self,shared): \n        if self.successors: warnings.warn(\"Node won't run successors. Use AsyncFlow.\")  \n        return await self._run_async(shared)\n    async def _run_async(self,shared): p=await self.prep_async(shared);e=await self._exec(p);return await self.post_async(shared,p,e)\n```\n\nKey takeaways from this code:\n\n*   The regular `prep`, `exec`, `post`, and `_run` functions are overridden to throw `RuntimeError`s if called. This enforces the use of the `async` versions.\n*   The `_exec` function wraps the execution of `exec_async` and takes care of retries. The key line is `return await self.exec_async(prep_res)`.\n*   `run_async` calls the `_run_async` function.\n\n**Conclusion**\n\nIn this chapter, we learned about the `AsyncNode` and how it allows us to perform asynchronous operations within PocketFlow. We saw how to create an `AsyncNode` for fetching recipes from a website and how it works under the hood. Using `AsyncNode` makes the workflow more responsive and enables interaction with time-consuming external services.\n\nNow that we understand `AsyncNode`, let's move on to [Chapter 7: AsyncFlow](07_asyncflow.md) to see how we can build entire asynchronous flows using `AsyncFlow`.\n\n---\n# Chapter 7: AsyncFlow\n\nIn [Chapter 6: AsyncNode](06_asyncnode.md), we learned how to handle asynchronous operations within individual Nodes. Now, let's see how to create an *entire* asynchronous workflow \u2013 a [Flow](01_flow.md) where *all* the Nodes can run concurrently! This is where `AsyncFlow` comes in.\n\nImagine you're a chef juggling multiple dishes simultaneously. You start the rice, then you start chopping vegetables for the stir-fry, and then you check on the baking chicken. You're not waiting for each dish to finish completely before starting the next one! `AsyncFlow` lets you do the same with your PocketFlow \u2013 run multiple Nodes concurrently, maximizing resource utilization.\n\n**The Problem: Running Async Nodes in Sequence**\n\nEven with `AsyncNode`, if you connect them in a regular [Flow](01_flow.md), they will still run sequentially. The `Flow` will wait for one `AsyncNode` to complete before starting the next. We want true concurrency, where multiple `AsyncNode`s can run *at the same time*.\n\n**Our Use Case: Recipe Finder (Expanded)**\n\nLet's expand our recipe finder app from [Chapter 6: AsyncNode](06_asyncnode.md). Now, after fetching recipes, we also want to:\n\n1.  **Suggest a Recipe:** Use an LLM to suggest the best recipe from the fetched recipes (this is an `AsyncNode`).\n2.  **Get Approval:** Ask the user if they approve of the suggested recipe (this can be a regular, synchronous Node).\n\nWe want to fetch recipes *and* suggest a recipe concurrently. If fetching recipes takes 5 seconds and suggesting a recipe takes 3 seconds, the total time should be closer to 5 seconds (the longer task) rather than 8 seconds (the sum of both).\n\n**What is an AsyncFlow?**\n\n`AsyncFlow` is a special type of [Flow](01_flow.md) designed to execute asynchronous Nodes concurrently. It uses the `asyncio` library to manage the execution of these Nodes in a non-blocking manner.\n\n**Key Concepts of AsyncFlow**\n\n1.  **Concurrency:** Running multiple Nodes at the same time.\n2.  **Asyncio Integration:** Leveraging Python's `asyncio` library for managing asynchronous tasks.\n3.  **Non-Blocking Execution:** Tasks can run independently without blocking the main thread.\n\n**Creating an AsyncFlow for Recipe Finding**\n\nFirst, let's define some simple nodes:\n\n```python\nimport asyncio\nfrom pocketflow import AsyncNode, Node\n\nasync def fetch_recipes(ingredient): #Same function as before\n    print(f\"\ud83d\ude34 Fetching recipes for {ingredient}... This will take a moment.\")\n    await asyncio.sleep(2)  # Simulate a 2-second delay\n    return [\"Pasta with \" + ingredient, \"Pizza with \" + ingredient]\n\nclass FetchRecipes(AsyncNode):\n    async def prep_async(self, shared): return shared.get(\"ingredient\", \"default_ingredient\")\n    async def exec_async(self, ingredient): return await fetch_recipes(ingredient)\n    async def post_async(self, shared, prep_res, exec_res): shared[\"recipes\"] = exec_res; return \"suggest\"\n\nclass SuggestRecipe(AsyncNode):\n    async def prep_async(self, shared):\n        recipes = shared.get(\"recipes\", [])\n        #Simulate that the prompt is generated based on the recipes\n        prompt = f\"\"\"Which of these is the best: {recipes}\"\"\"\n        return prompt\n    async def exec_async(self, prompt): #Simulate LLM, takes time\n        print(f\"\ud83e\udd14 Asking LLM to suggest the best recipe...This will take a moment.\")\n        await asyncio.sleep(1) #Simulate a 1-second delay\n        return \"Pasta\" if \"Pasta\" in prompt else \"Pizza\"\n    async def post_async(self, shared, prep_res, exec_res): shared[\"suggestion\"] = exec_res; return \"approve\"\n\nclass GetApproval(Node):\n    def prep(self, shared): return shared.get(\"suggestion\", \"unknown\")\n    def exec(self, suggestion):\n        approval = input(f\"Do you approve of {suggestion}? (yes/no): \")\n        return approval.lower() == \"yes\"\n    def post(self, shared, prep_res, exec_res): return \"accept\" if exec_res else \"retry\"\n\nclass NoOp(Node):\n    \"\"\"Node that does nothing, used to properly end the flow.\"\"\"\n    pass\n```\n\nKey highlights:\n* `FetchRecipes` fetches recipes asyncronously from a website. We saw this example in the last chapter.\n* `SuggestRecipe` uses an LLM to suggest the best recipe. This process takes a bit of time, so this operation also happens asyncronously.\n* `GetApproval` asks the user for input.\n\nNow, let's create the `AsyncFlow`:\n\n```python\nfrom pocketflow import AsyncFlow\nfrom nodes import FetchRecipes, SuggestRecipe, GetApproval, NoOp\n\ndef create_flow():\n    \"\"\"Create and connect nodes into a flow.\"\"\"\n    \n    # Create nodes\n    fetch = FetchRecipes()\n    suggest = SuggestRecipe()\n    approve = GetApproval()\n    end = NoOp()\n    \n    # Connect nodes\n    fetch - \"suggest\" >> suggest\n    suggest - \"approve\" >> approve\n    approve - \"retry\" >> suggest  # Loop back for another suggestion\n    approve - \"accept\" >> end     # Properly end the flow\n    \n    # Create flow starting with fetch\n    flow = AsyncFlow(start=fetch)\n    return flow\n```\n\n*   We create instances of `FetchRecipes`, `SuggestRecipe`, `GetApproval`, and `NoOp` Nodes.\n*   We connect the Nodes using the `>>` and `-` operators to define the [Flow](01_flow.md)'s sequence.\n*   **Crucially:** We create an `AsyncFlow` instance, passing in the starting Node (`fetch`).\n\n**Running the AsyncFlow**\nHere's the code to run this:\n\n```python\nimport asyncio\nfrom nodes import create_flow # Assuming the create_flow function is in nodes.py\n\nasync def main():\n    flow = create_flow()\n    shared_store = {\"ingredient\": \"chicken\"}\n    await flow._run_async(shared_store) #Manually invoke the async flow. The .run function won't work as it's overridden.\n    print(\"Flow completed.\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\nWhen we execute this code, there are some key points to note:\n\n1.  We need to wrap the execution of `AsyncFlow` within an `async` function (`main` in this case).\n2.  We use `asyncio.run(main())` to properly run the asynchronous code.\n3.  **It's VERY IMPORTANT that the AsyncFlow is invoked with `await flow._run_async(shared_store)`. If you run it with `flow.run(shared_store)` the asynchronous operations won't work.**\n\n**Let's Trace the Execution (Simplified)**\n\nBecause `AsyncFlow` allows concurrency, the exact sequence of events can vary. Here's a *possible* scenario:\n\n1.  The [Flow](01_flow.md) starts at the `FetchRecipes` Node.\n2.  `FetchRecipes` starts fetching recipes asynchronously (takes 2 seconds).\n3.  The [Flow](01_flow.md) *immediately* moves to the `SuggestRecipe` Node (without waiting for `FetchRecipes` to complete!).\n4.  `SuggestRecipe` tries to access the recipes, but they're not available yet. It waits.\n5.  After 2 seconds, `FetchRecipes` completes and saves the recipes to the [Shared Store](03_shared_store.md).\n6.  `SuggestRecipe` can now access the recipes and starts suggesting a recipe asynchronously (takes 1 second).\n7.  The [Flow](01_flow.md) moves to the `GetApproval` node, because at the flow level, it doesn't have to wait for `SuggestRecipe` to finish.\n8.  `GetApproval` gets the suggestion and prompts the user for approval.\n\n*In reality, `FetchRecipes` and `SuggestRecipe` can be running concurrently, and the waiting in step 4 is handled more efficiently by `asyncio`.*\n\n**Under the Hood: How AsyncFlow Works**\n\nThe key to `AsyncFlow` is the `_orch_async` method. Let's look at some relevant code from `pocketflow/__init__.py`:\n\n```python\nclass AsyncFlow(Flow,AsyncNode):\n    async def _orch_async(self,shared,params=None):\n        curr,p=copy.copy(self.start),(params or {**self.params})\n        while curr:curr.set_params(p);c=await curr._run_async(shared) if isinstance(curr,AsyncNode) else curr._run(shared);curr=copy.copy(self.get_next_node(curr,c))\n    async def _run_async(self,shared): p=await self.prep_async(shared);await self._orch_async(shared);return await self.post_async(shared,p,None)\n```\n\n*   `_orch_async` is the asynchronous orchestration method. The key part is `c=await curr._run_async(shared)`. This line `await`s the execution of the current Node, *allowing other asynchronous tasks to run while waiting*.\n* `curr=copy.copy(self.get_next_node(curr,c))` gets the next node based on return value.\n\nLet's illustrate the relationship with a sequence diagram.\n\n```mermaid\nsequenceDiagram\n    participant AsyncFlow\n    participant FetchRecipes\n    participant SuggestRecipe\n    participant SharedStore\n    participant EventLoop\n\n    AsyncFlow->>FetchRecipes: _run_async(shared_store)\n    FetchRecipes->>FetchRecipes: prep_async(shared_store)\n    FetchRecipes-->>AsyncFlow: prep_result\n    AsyncFlow->>FetchRecipes: exec_async(prep_result)\n    FetchRecipes->>EventLoop: await fetch_recipes(prep_result)\n    AsyncFlow->>SuggestRecipe: _run_async(shared_store)\n    SuggestRecipe->>SuggestRecipe: prep_async(shared_store)\n    SuggestRecipe->>SharedStore: access recipes\n    EventLoop-->>FetchRecipes: recipes\n    FetchRecipes-->>AsyncFlow: recipes\n    AsyncFlow->>FetchRecipes: post_async(shared_store, prep_result, recipes)\n```\n\nBecause it's an async flow, the `AsyncFlow` can move forward and execute the `SuggestRecipe` Node, even though `FetchRecipes` may still be running in the background, waiting for the simulated web call.\n\n**Benefits of Using AsyncFlow**\n\n*   **Improved Performance:** By running Nodes concurrently, you can significantly reduce the overall execution time of your [Flow](01_flow.md), especially when dealing with I/O-bound operations (like network requests or database queries).\n*   **Increased Responsiveness:** Your application remains responsive even when performing long-running tasks.\n\n**Important Notes:**\n\n*   **`await` Correctly:** Make sure to `await` all asynchronous operations within your `AsyncNode`s and `AsyncFlow`. For the top-level `AsyncFlow`, you must invoke it with `await flow._run_async(shared_store)`.\n*   **Error Handling:** Implement proper error handling within your `AsyncNode`s to gracefully handle exceptions that may occur during asynchronous operations.\n\n**Conclusion**\n\nIn this chapter, we learned about `AsyncFlow` and how it allows us to build entire asynchronous workflows in PocketFlow. We saw how to create an `AsyncFlow` for our recipe finder app and how it enables concurrency. Using `AsyncFlow` allows the PocketFlow to execute complex flows more effectively and efficiently.\n\nNow that we understand `AsyncFlow`, let's move on to [Chapter 8: AsyncParallelBatchNode](08_asyncparallelbatchnode.md) to see how we can combine asynchronous operations with batch processing for even greater performance!\n\n\nRelevant Code Snippets:\n--- File: cookbook/pocketflow-parallel-batch/main.py ---\nimport asyncio\nimport time\n\nfrom pocketflow import AsyncBatchNode, AsyncParallelBatchNode, AsyncFlow\n\n####################################\n# Dummy async function (1s delay)\n####################################\nasync def dummy_llm_summarize(text):\n    \"\"\"Simulates an async LLM call that takes 1 second.\"\"\"\n    await asyncio.sleep(1)\n    return f\"Summarized({len(text)} chars)\"\n\n###############################################\n# 1) AsyncBatchNode (sequential) version\n###############################################\n\nclass SummariesAsyncNode(AsyncBatchNode):\n    \"\"\"\n    Processes items sequentially in an async manner.\n    The next item won't start until the previous item has finished.\n    \"\"\"\n\n    async def prep_async(self, shared):\n        # Return a list of items to process.\n        # Each item is (filename, content).\n        return list(shared[\"data\"].items())\n\n    async def exec_async(self, item):\n        filename, content = item\n        print(f\"[Sequential] Summarizing {filename}...\")\n        summary = await dummy_llm_summarize(content)\n        return (filename, summary)\n\n    async def post_async(self, shared, prep_res, exec_res_list):\n        # exec_res_list is a list of (filename, summary)\n        shared[\"sequential_summaries\"] = dict(exec_res_list)\n        return \"done_sequential\"\n\n###############################################\n# 2) AsyncParallelBatchNode (concurrent) version\n###############################################\n\nclass SummariesAsyncParallelNode(AsyncParallelBatchNode):\n    \"\"\"\n    Processes items in parallel. Many LLM calls start at once.\n    \"\"\"\n\n    async def prep_async(self, shared):\n        return list(shared[\"data\"].items())\n\n    async def exec_async(self, item):\n        filename, content = item\n        print(f\"[Parallel] Summarizing {filename}...\")\n        summary = await dummy_llm_summarize(content)\n        return (filename, summary)\n\n    async def post_async(self, shared, prep_res, exec_res_list):\n        shared[\"parallel_summaries\"] = dict(exec_res_list)\n        return \"done_parallel\"\n\n###############################################\n# Demo comparing the two approaches\n###############################################\n\nasync def main():\n    # We'll use the same data for both flows\n    shared_data = {\n        \"data\": {\n            \"file1.txt\": \"Hello world 1\",\n            \"file2.txt\": \"Hello world 2\",\n            \"file3.txt\": \"Hello world 3\",\n        }\n    }\n\n    # 1) Run the sequential version\n    seq_node = SummariesAsyncNode()\n    seq_flow = AsyncFlow(start=seq_node)\n\n    print(\"\\n=== Running Sequential (AsyncBatchNode) ===\")\n    t0 = time.time()\n    await seq_flow.run_async(shared_data)\n    t1 = time.time()\n\n    # 2) Run the parallel version\n    par_node = SummariesAsyncParallelNode()\n    par_flow = AsyncFlow(start=par_node)\n\n    print(\"\\n=== Running Parallel (AsyncParallelBatchNode) ===\")\n    t2 = time.time()\n    await par_flow.run_async(shared_data)\n    t3 = time.time()\n\n    # Show times\n    print(\"\\n--- Results ---\")\n    print(f\"Sequential Summaries: {shared_data.get('sequential_summaries')}\")\n    print(f\"Parallel Summaries:   {shared_data.get('parallel_summaries')}\")\n\n    print(f\"Sequential took: {t1 - t0:.2f} seconds\")\n    print(f\"Parallel took:   {t3 - t2:.2f} seconds\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n--- File: pocketflow/__init__.py ---\nimport asyncio, warnings, copy, time\n\nclass BaseNode:\n    def __init__(self): self.params,self.successors={},{}\n    def set_params(self,params): self.params=params\n    def add_successor(self,node,action=\"default\"):\n        if action in self.successors: warnings.warn(f\"Overwriting successor for action '{action}'\")\n        self.successors[action]=node;return node\n    def prep(self,shared): pass\n    def exec(self,prep_res): pass\n    def post(self,shared,prep_res,exec_res): pass\n    def _exec(self,prep_res): return self.exec(prep_res)\n    def _run(self,shared): p=self.prep(shared);e=self._exec(p);return self.post(shared,p,e)\n    def run(self,shared): \n        if self.successors: warnings.warn(\"Node won't run successors. Use Flow.\")  \n        return self._run(shared)\n    def __rshift__(self,other): return self.add_successor(other)\n    def __sub__(self,action):\n        if isinstance(action,str): return _ConditionalTransition(self,action)\n        raise TypeError(\"Action must be a string\")\n\nclass _ConditionalTransition:\n    def __init__(self,src,action): self.src,self.action=src,action\n    def __rshift__(self,tgt): return self.src.add_successor(tgt,self.action)\n\nclass Node(BaseNode):\n    def __init__(self,max_retries=1,wait=0): super().__init__();self.max_retries,self.wait=max_retries,wait\n    def exec_fallback(self,prep_res,exc): raise exc\n    def _exec(self,prep_res):\n        for self.cur_retry in range(self.max_retries):\n            try: return self.exec(prep_res)\n            except Exception as e:\n                if self.cur_retry==self.max_retries-1: return self.exec_fallback(prep_res,e)\n                if self.wait>0: time.sleep(self.wait)\n\nclass BatchNode(Node):\n    def _exec(self,items): return [super(BatchNode,self)._exec(i) for i in (items or [])]\n\nclass Flow(BaseNode):\n    def __init__(self,start): super().__init__();self.start=start\n    def get_next_node(self,curr,action):\n        nxt=curr.successors.get(action or \"default\")\n        if not nxt and curr.successors: warnings.warn(f\"Flow ends: '{action}' not found in {list(curr.successors)}\")\n        return nxt\n    def _orch(self,shared,params=None):\n        curr,p=copy.copy(self.start),(params or {**self.params})\n        while curr: curr.set_params(p);c=curr._run(shared);curr=copy.copy(self.get_next_node(curr,c))\n    def _run(self,shared): pr=self.prep(shared);self._orch(shared);return self.post(shared,pr,None)\n    def exec(self,prep_res): raise RuntimeError(\"Flow can't exec.\")\n\nclass BatchFlow(Flow):\n    def _run(self,shared):\n        pr=self.prep(shared) or []\n        for bp in pr: self._orch(shared,{**self.params,**bp})\n        return self.post(shared,pr,None)\n\nclass AsyncNode(Node):\n    def prep(self,shared): raise RuntimeError(\"Use prep_async.\")\n    def exec(self,prep_res): raise RuntimeError(\"Use exec_async.\")\n    def post(self,shared,prep_res,exec_res): raise RuntimeError(\"Use post_async.\")\n    def exec_fallback(self,prep_res,exc): raise RuntimeError(\"Use exec_fallback_async.\")\n    def _run(self,shared): raise RuntimeError(\"Use run_async.\")\n    async def prep_async(self,shared): pass\n    async def exec_async(self,prep_res): pass\n    async def exec_fallback_async(self,prep_res,exc): raise exc\n    async def post_async(self,shared,prep_res,exec_res): pass\n    async def _exec(self,prep_res): \n        for i in range(self.max_retries):\n            try: return await self.exec_async(prep_res)\n            except Exception as e:\n                if i==self.max_retries-1: return await self.exec_fallback_async(prep_res,e)\n                if self.wait>0: await asyncio.sleep(self.wait)\n    async def run_async(self,shared): \n        if self.successors: warnings.warn(\"Node won't run successors. Use AsyncFlow.\")  \n        return await self._run_async(shared)\n    async def _run_async(self,shared): p=await self.prep_async(shared);e=await self._exec(p);return await self.post_async(shared,p,e)\n\nclass AsyncBatchNode(AsyncNode,BatchNode):\n    async def _exec(self,items): return [await super(AsyncBatchNode,self)._exec(i) for i in items]\n\nclass AsyncParallelBatchNode(AsyncNode,BatchNode):\n    async def _exec(self,items): return await asyncio.gather(*(super(AsyncParallelBatchNode,self)._exec(i) for i in items))\n\nclass AsyncFlow(Flow,AsyncNode):\n    async def _orch_async(self,shared,params=None):\n        curr,p=copy.copy(self.start),(params or {**self.params})\n        while curr:curr.set_params(p);c=await curr._run_async(shared) if isinstance(curr,AsyncNode) else curr._run(shared);curr=copy.copy(self.get_next_node(curr,c))\n    async def _run_async(self,shared): p=await self.prep_async(shared);await self._orch_async(shared);return await self.post_async(shared,p,None)\n\nclass AsyncBatchFlow(AsyncFlow,BatchFlow):\n    async def _run_async(self,shared):\n        pr=await self.prep_async(shared) or []\n        for bp in pr: await self._orch_async(shared,{**self.params,**bp})\n        return await self.post_async(shared,pr,None)\n\nclass AsyncParallelBatchFlow(AsyncFlow,BatchFlow):\n    async def _run_async(self,shared):\n        pr=await self.prep_async(shared) or []\n        await asyncio.gather(*(self._orch_async(shared,{**self.params,**bp}) for bp in pr))\n        return await self.post_async(shared,pr,None)\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 8: AsyncParallelBatchNode`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "# Chapter 8: AsyncParallelBatchNode\n\nIn [Chapter 7: AsyncFlow](07_asyncflow.md), we learned how to build entire asynchronous workflows using `AsyncFlow`. We saw how to make different parts of our processing *concurrent* \u2013 running at the same time to speed things up. Now, let's kick things into overdrive by combining the power of asynchronous processing with batch processing! This is where the `AsyncParallelBatchNode` comes in.\n\nImagine you're running a photo editing service. You need to apply a series of effects (filters, resizing, etc.) to *many* images. Each image's processing can happen independently, and each effect might involve waiting for some resource (like a database lookup or a complex calculation). `AsyncParallelBatchNode` helps you apply these effects to all your images *at the same time*, making the process incredibly efficient.\n\n**The Problem: Maximizing Throughput with Asynchronous Batch Operations**\n\nWe've learned how to process batches of items ([Chapter 5: BatchNode](05_batchnode.md)) and how to run asynchronous operations ([Chapter 6: AsyncNode](06_asyncnode.md)). But what if we want to do *both*? We want to process many items *concurrently*, using asynchronous operations to avoid blocking. A regular `AsyncBatchNode` would process the items sequentially, even if each item's processing involves asynchronous operations. This means it will await each item's async operation before starting the next item, meaning, operations are still blocking in nature.\n\n`AsyncParallelBatchNode` solves this by processing all items in the batch *in parallel*, maximizing throughput and minimizing overall processing time.\n\n**Our Use Case: Summarizing Multiple Documents with an LLM**\n\nLet's say we have a folder full of text documents, and we want to summarize each document using an LLM (Large Language Model). Sending requests to an LLM is a time-consuming, I/O-bound operation that benefits greatly from asynchronous processing. We want to summarize all documents at once, as quickly as possible.\n\n**Key Concepts: AsyncParallelBatchNode**\n\nThe `AsyncParallelBatchNode` combines several concepts:\n\n1.  **Asynchronous Operations ([Chapter 6: AsyncNode](06_asyncnode.md)):** Using `async` and `await` to perform non-blocking operations.\n2.  **Batch Processing ([Chapter 5: BatchNode](05_batchnode.md)):** Processing multiple items together in a single node.\n3.  **Parallel Execution:** Running multiple tasks *at the same time* using `asyncio.gather`.\n\n**How AsyncParallelBatchNode Works**\n\nHere's how it all comes together:\n\n1.  **Prep:** The `prep_async` method prepares the input data \u2013 in our case, a list of documents.\n2.  **Batching:** The `prep_async` method creates a list of `(filename, content)` pairs for each document. This list forms our batch.\n3.  **Parallel Exec:** The `exec_async` method is applied to *each* item in the batch *concurrently*. This is the magic sauce! The `AsyncParallelBatchNode` uses `asyncio.gather` to run all the `exec_async` calls in parallel.\n4.  **Post:** The `post_async` method collects the results from all the `exec_async` calls and processes them.\n\n**Creating an AsyncParallelBatchNode for Document Summarization**\n\nLet's create an `AsyncParallelBatchNode` that summarizes multiple documents:\n\n```python\nimport asyncio\nfrom pocketflow import AsyncParallelBatchNode\n\nasync def dummy_llm_summarize(text):\n    \"\"\"Simulates an async LLM call that takes 1 second.\"\"\"\n    await asyncio.sleep(1)\n    return f\"Summarized({len(text)} chars)\"\n\nclass SummariesAsyncParallelNode(AsyncParallelBatchNode):\n    \"\"\"Processes items in parallel. Many LLM calls start at once.\"\"\"\n\n    async def prep_async(self, shared):\n        # Return a list of items to process.\n        # Each item is (filename, content).\n        return list(shared[\"data\"].items())\n\n    async def exec_async(self, item):\n        filename, content = item\n        print(f\"[Parallel] Summarizing {filename}...\")\n        summary = await dummy_llm_summarize(content)\n        return (filename, summary)\n\n    async def post_async(self, shared, prep_res, exec_res_list):\n        shared[\"parallel_summaries\"] = dict(exec_res_list)\n        return \"done_parallel\"\n```\n\nHere's what's happening:\n\n*   `SummariesAsyncParallelNode` inherits from `AsyncParallelBatchNode`.\n*   `prep_async(self, shared)`: This method retrieves the document data from the [Shared Store](03_shared_store.md). It expects the `shared[\"data\"]` to be a dictionary where keys are filenames and values are the document contents. It returns a list of `(filename, content)` tuples.\n*   `exec_async(self, item)`: This method takes a single `(filename, content)` tuple as input. It calls `dummy_llm_summarize` (our simulated LLM) to summarize the document content.\n*   `post_async(self, shared, prep_res, exec_res_list)`: This method receives a list of results, where each result is a `(filename, summary)` tuple. It stores the summaries in the [Shared Store](03_shared_store.md).\n\n**Using the AsyncParallelBatchNode in a Flow**\n\nNow, let's see how to use this `AsyncParallelBatchNode` in an [AsyncFlow](07_asyncflow.md):\n\n```python\nimport asyncio\nfrom pocketflow import AsyncFlow\nfrom nodes import SummariesAsyncParallelNode # Assuming it's in nodes.py\n\nasync def main():\n    # We'll use the same data for both flows\n    shared_data = {\n        \"data\": {\n            \"file1.txt\": \"Hello world 1\",\n            \"file2.txt\": \"Hello world 2\",\n            \"file3.txt\": \"Hello world 3\",\n        }\n    }\n\n    par_node = SummariesAsyncParallelNode()\n    par_flow = AsyncFlow(start=par_node)\n\n    print(\"\\n=== Running Parallel (AsyncParallelBatchNode) ===\")\n    await par_flow.run_async(shared_data)\n    print(f\"Parallel Summaries:   {shared_data.get('parallel_summaries')}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\nHere's what's happening:\n\n1.  We create an instance of the `SummariesAsyncParallelNode`.\n2.  We create an `AsyncFlow` that starts with the `SummariesAsyncParallelNode`.\n3.  We create a `shared_data` dictionary and populate it with our document data.\n4.  We run the `AsyncFlow`.\n\n**Expected Output:**\n\nYou'll see output like this:\n\n```\n=== Running Parallel (AsyncParallelBatchNode) ===\n[Parallel] Summarizing file1.txt...\n[Parallel] Summarizing file2.txt...\n[Parallel] Summarizing file3.txt...\nParallel Summaries:   {'file1.txt': 'Summarized(13 chars)', 'file2.txt': 'Summarized(13 chars)', 'file3.txt': 'Summarized(13 chars)'}\n```\n\nNotice how all the \"Summarizing\" messages appear *almost* at the same time. This indicates that the documents are being processed in parallel!\n\n**Comparison with AsyncBatchNode (Sequential)**\n\nTo truly appreciate the power of `AsyncParallelBatchNode`, let's compare it to a regular `AsyncBatchNode` (which processes items *sequentially*). Here's the code for the `AsyncBatchNode` version:\n\n```python\nimport asyncio\nfrom pocketflow import AsyncBatchNode\n\nasync def dummy_llm_summarize(text):\n    \"\"\"Simulates an async LLM call that takes 1 second.\"\"\"\n    await asyncio.sleep(1)\n    return f\"Summarized({len(text)} chars)\"\n\nclass SummariesAsyncNode(AsyncBatchNode):\n    \"\"\"Processes items sequentially in an async manner.\"\"\"\n\n    async def prep_async(self, shared):\n        return list(shared[\"data\"].items())\n\n    async def exec_async(self, item):\n        filename, content = item\n        print(f\"[Sequential] Summarizing {filename}...\")\n        summary = await dummy_llm_summarize(content)\n        return (filename, summary)\n\n    async def post_async(self, shared, prep_res, exec_res_list):\n        shared[\"sequential_summaries\"] = dict(exec_res_list)\n        return \"done_sequential\"\n```\n\nThe only difference is that this class inherits from `AsyncBatchNode` instead of `AsyncParallelBatchNode`.\n\nIf you run *both* the `AsyncBatchNode` version and the `AsyncParallelBatchNode` version (see full code in `cookbook/pocketflow-parallel-batch/main.py`), you'll see a significant difference in execution time. The `AsyncBatchNode` version will take roughly 3 seconds (1 second per document, processed sequentially), while the `AsyncParallelBatchNode` version will take closer to 1 second (all documents processed concurrently).\n\n**Under the Hood: How AsyncParallelBatchNode Works**\n\nThe magic of `AsyncParallelBatchNode` happens in the `_exec` method. Let's look at the code from `pocketflow/__init__.py`:\n\n```python\nclass AsyncParallelBatchNode(AsyncNode,BatchNode):\n    async def _exec(self,items): return await asyncio.gather(*(super(AsyncParallelBatchNode,self)._exec(i) for i in items))\n```\n\nHere's what's happening:\n\n1.  `async def _exec(self, items)`: This defines the asynchronous `_exec` method.\n2.  `asyncio.gather(*(super(AsyncParallelBatchNode,self)._exec(i) for i in items))`: This is the key part!\n    *   `super(AsyncParallelBatchNode,self)._exec(i)`:  This calls the `_exec` method of the parent class (`BatchNode`), which in turn calls the `exec_async` method for each item `i` in the `items` list.\n    *   `*(...)`: This unpacks the generator expression into individual arguments for `asyncio.gather`.\n    *   `asyncio.gather(...)`: This function takes a variable number of awaitable objects (our `exec_async` calls) and runs them *concurrently*. It returns a list of the results in the order they were submitted.\n    *   `await`: The `await` keyword waits for *all* the tasks submitted to `asyncio.gather` to complete before returning the results.\n\nIn simpler terms, `asyncio.gather` takes a bunch of asynchronous functions and runs them all at the same time.\n\nLet's visualize this with a sequence diagram:\n\n```mermaid\nsequenceDiagram\n    participant AsyncParallelBatchNode\n    participant SharedStore\n    participant EventLoop\n    participant exec_async_1\n    participant exec_async_2\n    participant exec_async_3\n\n    AsyncParallelBatchNode->>SharedStore: access data\n    AsyncParallelBatchNode->>EventLoop: schedule exec_async_1\n    AsyncParallelBatchNode->>EventLoop: schedule exec_async_2\n    AsyncParallelBatchNode->>EventLoop: schedule exec_async_3\n    EventLoop->>exec_async_1: run\n    EventLoop->>exec_async_2: run\n    EventLoop->>exec_async_3: run\n    exec_async_1-->>AsyncParallelBatchNode: result_1\n    exec_async_2-->>AsyncParallelBatchNode: result_2\n    exec_async_3-->>AsyncParallelBatchNode: result_3\n    AsyncParallelBatchNode->>SharedStore: store results\n```\n\n1.  The `AsyncParallelBatchNode` retrieves the data from the [Shared Store](03_shared_store.md).\n2.  It schedules each `exec_async` method with the `EventLoop`. These executions are non-blocking.\n3.  The `EventLoop` orchestrates the execution of all the async functions at the same time.\n4.  Eventually, all the results are returned and stored in the [Shared Store](03_shared_store.md).\n\n**Benefits of Using AsyncParallelBatchNode**\n\n*   **Maximum Performance:** Combines the benefits of asynchronous processing and batch processing for optimal throughput.\n*   **Simplified Code:** Handles the complexities of parallel execution internally, making your code cleaner and easier to understand.\n\n**Conclusion**\n\nIn this chapter, we learned about the `AsyncParallelBatchNode` and how it allows us to combine asynchronous operations with batch processing for maximum performance. We saw how to create an `AsyncParallelBatchNode` for summarizing multiple documents and how it leverages `asyncio.gather` to achieve concurrency. Using `AsyncParallelBatchNode` can significantly improve the efficiency of your PocketFlows when dealing with I/O-bound operations and large datasets.\n\nCongratulations! You've now completed all the core chapters in this PocketFlow tutorial. You've learned about Flows, Nodes, Shared Stores, asynchronous operations, and batch processing. You're well-equipped to build your own complex and efficient workflows using PocketFlow!\n", "\nFor the project `Tutorial-Codebase-Knowledge`:\n\nCodebase Context:\n--- File Index 0: README.md ---\n<h1 align=\"center\">Turns Codebase into Easy Tutorial with AI</h1>\n\n![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)\n\n> *Ever stared at a new codebase written by others feeling completely lost? This tutorial shows you how to build an AI agent that analyzes GitHub repositories and creates beginner-friendly tutorials explaining exactly how the code works.*\n\n<p align=\"center\">\n  <img \n    src=\"./assets/banner.png\" width=\"800\"\n  />\n</p>\n\nThis is a tutorial project of [Pocket Flow](https://github.com/The-Pocket/PocketFlow), a 100-line LLM framework. It crawls GitHub repositories and build a knowledge base from the code. It analyzes entire codebases to identify core abstractions and how they interact, and transforms complex code into beginner-friendly tutorials with clear visualizations.\n\n- Check out the [YouTube Development Tutorial](https://youtu.be/AFY67zOpbSo) for more!\n\n- Check out the [Substack Post Tutorial](https://zacharyhuang.substack.com/p/ai-codebase-knowledge-builder-full) for more!\n\n## \u2b50 Example Results for Popular GitHub Repositories!\n\n<p align=\"center\">\n    <img \n      src=\"./assets/example.png\" width=\"600\"\n    />\n</p>\n\n\ud83e\udd2f All these tutorials are generated **entirely by AI** by crawling the GitHub repo!\n\n- [AutoGen Core](https://the-pocket.github.io/Tutorial-Codebase-Knowledge/AutoGen%20Core) - Build AI teams that talk, think, and solve problems together like coworkers!\n\n- [Browser Use](https://the-pocket.github.io/Tutorial-Codebase-Knowledge/Browser%20Use) - Let AI surf the web for you, clicking buttons and filling forms like a digital assistant!\n\n- [Celery](https://the-pocket.github.io/Tutorial-Codebase-Knowledge/Celery) - Supercharge your app with background tasks that run while you sleep!\n\n- [Click](https://the-pocket.github.io/Tutorial-Codebase-Knowledge/Click) - Turn Python functions into slick command-line tools with just a decorator!\n\n- [Crawl4AI](https://the-pocket.github.io/Tutorial-Codebase-Knowledge/Crawl4AI) - Train your AI to extract exactly what matters from any website!\n\n- [CrewAI](https://the-pocket.github.io/Tutorial-Codebase-Knowledge/CrewAI) - Assemble a dream team of AI specialists to tackle impossible problems!\n\n- [DSPy](https://the-pocket.github.io/Tutorial-Codebase-Knowledge/DSPy) - Build LLM apps like Lego blocks that optimize themselves!\n\n- [FastAPI](https://the-pocket.github.io/Tutorial-Codebase-Knowledge/FastAPI) - Create APIs at lightning speed with automatic docs that clients will love!\n\n- [Flask](https://the-pocket.github.io/Tutorial-Codebase-Knowledge/Flask) - Craft web apps with minimal code that scales from prototype to production!\n\n- [Google A2A](https://the-pocket.github.io/Tutorial-Codebase-Knowledge/Google%20A2A) - The universal language that lets AI agents collaborate across borders!\n\n- [LangGraph](https://the-pocket.github.io/Tutorial-Codebase-Knowledge/LangGraph) - Design AI agents as flowcharts where each step remembers what happened before!\n\n- [LevelDB](https://the-pocket.github.io/Tutorial-Codebase-Knowledge/LevelDB) - Store data at warp speed with Google's engine that powers blockchains!\n\n- [MCP Python SDK](https://the-pocket.github.io/Tutorial-Codebase-Knowledge/MCP%20Python%20SDK) - Build powerful apps that communicate through an elegant protocol without sweating the details!\n\n- [NumPy Core](https://the-pocket.github.io/Tutorial-Codebase-Knowledge/NumPy%20Core) - Master the engine behind data science that makes Python as fast as C!\n\n- [OpenManus](https://the-pocket.github.io/Tutorial-Codebase-Knowledge/OpenManus) - Build AI agents with digital brains that think, learn, and use tools just like humans do!\n\n- [Pydantic Core](https://the-pocket.github.io/Tutorial-Codebase-Knowledge/Pydantic%20Core) - Validate data at rocket speed with just Python type hints!\n\n- [Requests](https://the-pocket.github.io/Tutorial-Codebase-Knowledge/Requests) - Talk to the internet in Python with code so simple it feels like cheating!\n\n- [SmolaAgents](https://the-pocket.github.io/Tutorial-Codebase-Knowledge/SmolaAgents) - Build tiny AI agents that punch way above their weight class!\n\n\n## \ud83d\ude80 Getting Started\n\n1. Clone this repository\n\n2. Install dependencies: \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. Set up LLM in [`utils/call_llm.py`](./utils/call_llm.py) by providing credentials. By default, you can use the AI Studio key with this client for Gemini Pro 2.5:\n\n   ```python\n   client = genai.Client(\n     api_key=os.getenv(\"GEMINI_API_KEY\", \"your-api_key\"),\n   )\n   ```\n  \n   You can use your own models. We highly recommend the latest models with thinking capabilities (Claude 3.7 with thinking, O1). You can verify that it is correctly set up by running:\n   ```bash\n   python utils/call_llm.py\n   ```\n\n7. Generate a complete codebase tutorial by running the main script:\n    ```bash\n    # Analyze a GitHub repository\n    python main.py --repo https://github.com/username/repo --include \"*.py\" \"*.js\" --exclude \"tests/*\" --max-size 50000\n\n    # Or, analyze a local directory\n    python main.py --dir /path/to/your/codebase --include \"*.py\" --exclude \"*test*\"\n    ```\n    - `--repo` or `--dir` - Specify either a GitHub repo URL or a local directory path (required, mutually exclusive)\n    - `-n, --name` - Project name (optional, derived from URL/directory if omitted)\n    - `-t, --token` - GitHub token (or set GITHUB_TOKEN environment variable)\n    - `-o, --output` - Output directory (default: ./output)\n    - `-i, --include` - Files to include (e.g., \"*.py\" \"*.js\")\n    - `-e, --exclude` - Files to exclude (e.g., \"tests/*\" \"docs/*\")\n    - `-s, --max-size` - Maximum file size in bytes (default: 100KB)\n      \nThe application will crawl the repository, analyze the codebase structure, generate tutorial content, and save the output in the specified directory (default: ./output).\n\n\n## \ud83d\udca1 Development Tutorial\n\n- I built using [**Agentic Coding**](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to), the fastest development paradigm, where humans simply [design](docs/design.md) and agents [code](flow.py).\n\n- The secret weapon is [Pocket Flow](https://github.com/The-Pocket/PocketFlow), a 100-line LLM framework that lets Agents (e.g., Cursor AI) build for you\n  \n- Check out the Step-by-step YouTube development tutorial: \n\n<br>\n<div align=\"center\">\n  <a href=\"https://youtu.be/AFY67zOpbSo\" target=\"_blank\">\n    <img src=\"./assets/youtube_thumbnail.png\" width=\"500\" alt=\"IMAGE ALT TEXT\" style=\"cursor: pointer;\">\n  </a>\n</div>\n<br>\n\n\n--- File Index 1: api.py ---\n# api.py\n\nimport sys\nimport subprocess\nimport os\nimport logging\nimport re # Import re for parsing repo name\nfrom typing import Optional, List\nfrom urllib.parse import urlparse # To help parse repo name\n\nfrom fastapi import FastAPI, HTTPException, status\nfrom pydantic import BaseModel, Field\n\nfrom fastapi.middleware.cors import CORSMiddleware\n\n# --- Configuration ---\nMAIN_SCRIPT_PATH = \"main.py\"\nPYTHON_EXECUTABLE = sys.executable\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# --- Helper Function to Get Project Name ---\ndef get_project_name_from_url(repo_url: str) -> Optional[str]:\n    \"\"\"Extracts project name from GitHub URL.\"\"\"\n    try:\n        path = urlparse(repo_url).path\n        # Remove leading slash and .git suffix if present\n        name = path.lstrip('/').replace('.git', '')\n        # Get the last part of the path\n        project_name = name.split('/')[-1]\n        if project_name:\n            return project_name\n    except Exception:\n        pass # Ignore parsing errors\n    return None\n\n# --- Input Data Model (using Pydantic) ---\nclass GenerationRequest(BaseModel):\n    repo_url: str\n    name: Optional[str] = None\n    token: Optional[str] = None\n    output: str = Field(default=\"output\")\n    include: Optional[List[str]] = None\n    exclude: Optional[List[str]] = None\n    max_size: Optional[int] = None\n\n# --- FastAPI App ---\napp = FastAPI(\n    title=\"Codebase Tutorial Generator API\",\n    description=\"API to trigger the generation of tutorials from GitHub codebases.\",\n    version=\"1.0.0\",\n)\n\n# --- CORS Middleware ---\norigins = [\n    \"http://127.0.0.1:5500\", # VS Code Live Server default\n    \"http://localhost\",\n    \"http://localhost:8080\",\n    \"http://127.0.0.1\",\n    \"http://127.0.0.1:8080\",\n    \"null\", # file:// origin\n]\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=origins,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n# --- End CORS Middleware ---\n\n\n# --- API Endpoint ---\n@app.post(\n    \"/generate\",\n    summary=\"Trigger Tutorial Generation\",\n    description=\"Accepts repository details and options, then runs the tutorial generation script.\",\n    status_code=status.HTTP_200_OK\n)\nasync def generate_tutorial(request_data: GenerationRequest):\n    \"\"\"\n    API endpoint to trigger tutorial generation. Returns final output path on success.\n    \"\"\"\n    # --- Determine Project Name (needed for final path) ---\n    project_name = request_data.name\n    if not project_name:\n        project_name = get_project_name_from_url(request_data.repo_url)\n        if not project_name:\n             # If name wasn't provided and couldn't be derived, raise an error early\n             # Or alternatively, let main.py handle it, but we need it for the return path.\n             logger.error(f\"Could not derive project name from URL: {request_data.repo_url}\")\n             raise HTTPException(\n                 status_code=status.HTTP_400_BAD_REQUEST,\n                 detail=\"Project name not provided and could not be derived from repository URL.\"\n             )\n    \n    # --- Calculate Expected Final Output Path ---\n    # This mirrors the logic in CombineTutorial.prep\n    # Ensure forward slashes for URL compatibility later, though os.path.join is platform-aware\n    expected_final_path = os.path.join(request_data.output, project_name, 'html').replace('\\\\', '/')\n\n\n    # --- Build Command ---\n    command = [PYTHON_EXECUTABLE, MAIN_SCRIPT_PATH, request_data.repo_url]\n    # Use the derived or provided project name if specified (main.py will also derive if None)\n    if request_data.name:\n         command.extend([\"-n\", request_data.name])\n    if request_data.token:\n        command.extend([\"-t\", request_data.token])\n    command.extend([\"-o\", request_data.output]) # Pass base output dir\n    if request_data.max_size is not None:\n        command.extend([\"-s\", str(request_data.max_size)])\n    if request_data.include:\n        for pattern in request_data.include:\n            command.extend([\"-i\", pattern])\n    if request_data.exclude:\n        for pattern in request_data.exclude:\n            command.extend([\"-e\", pattern])\n\n    # --- Execute Script ---\n    logger.info(f\"Executing command: {' '.join(command)}\")\n\n    try:\n        result = subprocess.run(\n            command,\n            capture_output=True,\n            text=True,\n            check=True,\n            env=os.environ,\n            encoding='utf-8'\n        )\n\n        logger.info(f\"Script finished successfully.\")\n        logger.debug(f\"Script stdout:\\n{result.stdout}\")\n        if result.stderr:\n            logger.warning(f\"Script stderr:\\n{result.stderr}\")\n        \n        logger.info(f\"Final output directory: {expected_final_path}\")\n\n        # --- Success Response ---\n        # Return the *calculated* final path for redirection\n        return {\n            \"message\": \"Tutorial generation completed successfully.\",\n            # Key changed for clarity\n            \"final_output_directory\": expected_final_path, \n             # You could optionally still include stdout/stderr if needed for debugging\n            # \"script_stdout\": result.stdout,\n            # \"script_stderr\": result.stderr\n        }\n\n    except subprocess.CalledProcessError as e:\n        logger.error(f\"Script failed with exit code {e.returncode}\")\n        logger.error(f\"Script stdout:\\n{e.stdout}\")\n        logger.error(f\"Script stderr:\\n{e.stderr}\")\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail={\n                \"error\": \"Script execution failed.\",\n                \"details\": \"The tutorial generation script exited with an error.\",\n                \"return_code\": e.returncode,\n                \"script_stdout\": e.stdout,\n                \"script_stderr\": e.stderr\n            }\n        )\n    except FileNotFoundError:\n        error_msg = f\"Error: Script '{MAIN_SCRIPT_PATH}' or Python '{PYTHON_EXECUTABLE}' not found.\"\n        logger.error(error_msg)\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail={\"error\": \"Server configuration error.\", \"details\": error_msg}\n        )\n    except Exception as e:\n        logger.exception(\"An unexpected error occurred during script execution.\")\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail={\"error\": \"An unexpected internal server error occurred.\", \"details\": str(e)}\n        )\n\n# --- Root endpoint ---\n@app.get(\"/\", summary=\"Health Check\")\nasync def read_root():\n    return {\"message\": \"Tutorial Generator API is running.\"}\n\n# --- Uvicorn runner ---\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n\n--- File Index 2: converter/md_to_html.py ---\nfrom pathlib import Path\nimport markdown\nfrom md_mermaid import MermaidExtension\nimport sys\nfrom mermaid_extension import MermaidExtension as mermaid_ext\n\ndef convert_md_to_html(md_file_path, output_folder):\n    # Read the Markdown file\n    with open(md_file_path, \"r\", encoding=\"utf-8\") as md_file:\n        md_content = md_file.read()\n\n    # Initialize the Markdown object with the extensions\n    md = markdown.Markdown(extensions=[\n        # MermaidExtension(),  # Proper initialization of MermaidExtension\n        mermaid_ext(),\n        'fenced_code',\n        'codehilite',\n        'tables',\n        'toc'\n    ])\n\n    md_content = convert_hyperlink_to_html(md_content)\n    # Convert Markdown to HTML\n    html_content = md.convert(md_content)\n    \n    # wrap the HTML content in a basic HTML structure\n    html_content = f\"\"\"\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n<meta charset=\"UTF-8\">\n<title>{md_file_path}</title>\n        <script type=\"module\">\n            import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';\n            mermaid.initialize({{ startOnLoad: true }});\n        </script>\n       <style>\n            body {{\n                max-width: 800px;\n                margin: 2em auto;\n                font-family: system-ui, sans-serif;\n                line-height: 1.6;\n                color: #333;\n                padding: 0 1em;\n            }}\n            pre {{\n                background: #f6f8fa;\n                padding: 1em;\n                overflow-x: auto;\n            }}\n            code {{\n                background: #f0f0f0;\n                padding: 2px 4px;\n                border-radius: 4px;\n            }}\n            a {{\n                color: #0366d6;\n                text-decoration: none;\n            }}\n            a:hover {{\n                text-decoration: underline;\n            }}\n        </style>\n</head>\n<body>\n{html_content}\n</body>\n</html>\n\"\"\"   \n\n    # create the output file path\n    output_file_path = output_folder / md_file_path.relative_to(md_file_path.parent).with_suffix(\".html\")\n    \n    # write the HTML content to a file\n    with open(output_file_path, \"w\", encoding=\"utf-8\") as html_file:\n        html_file.write(html_content)\n        \n    print(f\"Converted {md_file_path} to {output_file_path}\")\n    \n\ndef convert_hyperlink_to_html(md_content):\n    # replace any hyperlinks with HTML links (.md to .html)\n    md_content = md_content.replace(\".md\", \".html\")\n    return md_content\n    \ndef convert_all_md_to_html(md_folder_path, output_folder=None):\n    md_folder_path = Path(md_folder_path)\n    output_folder = Path(output_folder) if output_folder else md_folder_path / \"html\"\n    output_folder.mkdir(parents=True, exist_ok=True)\n    \n    # iterate through all Markdown files in the folder\n    for md_file in md_folder_path.glob(\"**/*.md\"):\n        convert_md_to_html(md_file, output_folder)\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python md_to_html.py <md_folder_path> <output_folder>\")\n        sys.exit(1)\n    else:\n        md_folder_path = sys.argv[1]\n        output_folder = sys.argv[2] if len(sys.argv) > 2 else None\n        convert_all_md_to_html(md_folder_path, output_folder)\n\n--- File Index 3: flow.py ---\nfrom pocketflow import Flow\n# Import all node classes from nodes.py\nfrom nodes import (\n    FetchRepo,\n    IdentifyAbstractions,\n    AnalyzeRelationships,\n    OrderChapters,\n    WriteChapters,\n    CombineTutorial\n)\n\ndef create_tutorial_flow():\n    \"\"\"Creates and returns the codebase tutorial generation flow.\"\"\"\n\n    # Instantiate nodes\n    fetch_repo = FetchRepo()\n    identify_abstractions = IdentifyAbstractions(max_retries=3, wait=10)\n    analyze_relationships = AnalyzeRelationships(max_retries=3, wait=10)\n    order_chapters = OrderChapters(max_retries=3, wait=10)\n    write_chapters = WriteChapters(max_retries=3, wait=10) # This is a BatchNode\n    combine_tutorial = CombineTutorial()\n\n    # Connect nodes in sequence based on the design\n    fetch_repo >> identify_abstractions\n    identify_abstractions >> analyze_relationships\n    analyze_relationships >> order_chapters\n    order_chapters >> write_chapters\n    write_chapters >> combine_tutorial\n\n    # Create the flow starting with FetchRepo\n    tutorial_flow = Flow(start=fetch_repo)\n\n    return tutorial_flow\n\n--- File Index 4: main.py ---\nimport dotenv\nimport os\nimport argparse\n# Import the function that creates the flow\nfrom flow import create_tutorial_flow\n\ndotenv.load_dotenv()\n\n# Default file patterns\nDEFAULT_INCLUDE_PATTERNS = {\n    \"*.py\", \"*.js\", \"*.jsx\", \"*.ts\", \"*.tsx\", \"*.go\", \"*.java\", \"*.pyi\", \"*.pyx\", \n    \"*.c\", \"*.cc\", \"*.cpp\", \"*.h\", \"*.md\", \"*.rst\", \"Dockerfile\", \n    \"Makefile\", \"*.yaml\", \"*.yml\",\n}\n\nDEFAULT_EXCLUDE_PATTERNS = {\n    \"*test*\", \"tests/*\", \"docs/*\", \"examples/*\", \"v1/*\", \n    \"dist/*\", \"build/*\", \"experimental/*\", \"deprecated/*\", \n    \"legacy/*\", \".git/*\", \".github/*\", \".next/*\", \".vscode/*\", \"obj/*\", \"bin/*\", \"node_modules/*\", \"*.log\"\n}\n\n# --- Main Function ---\ndef main():\n    parser = argparse.ArgumentParser(description=\"Generate a tutorial for a GitHub codebase or local directory.\")\n    \n    # Create mutually exclusive group for source\n    source_group = parser.add_mutually_exclusive_group(required=True)\n    source_group.add_argument(\"--repo\", help=\"URL of the public GitHub repository.\")\n    source_group.add_argument(\"--dir\", help=\"Path to local directory.\")\n    \n    parser.add_argument(\"-n\", \"--name\", help=\"Project name (optional, derived from repo/directory if omitted).\")\n    parser.add_argument(\"-t\", \"--token\", help=\"GitHub personal access token (optional, reads from GITHUB_TOKEN env var if not provided).\")\n    parser.add_argument(\"-o\", \"--output\", default=\"output\", help=\"Base directory for output (default: ./output).\")\n    parser.add_argument(\"-i\", \"--include\", nargs=\"+\", help=\"Include file patterns (e.g. '*.py' '*.js'). Defaults to common code files if not specified.\")\n    parser.add_argument(\"-e\", \"--exclude\", nargs=\"+\", help=\"Exclude file patterns (e.g. 'tests/*' 'docs/*'). Defaults to test/build directories if not specified.\")\n    parser.add_argument(\"-s\", \"--max-size\", type=int, default=100000, help=\"Maximum file size in bytes (default: 100000, about 100KB).\")\n\n    args = parser.parse_args()\n\n    # Get GitHub token from argument or environment variable if using repo\n    github_token = None\n    if args.repo:\n        github_token = args.token or os.environ.get('GITHUB_TOKEN')\n        if not github_token:\n            print(\"Warning: No GitHub token provided. You might hit rate limits for public repositories.\")\n\n    # Initialize the shared dictionary with inputs\n    shared = {\n        \"repo_url\": args.repo,\n        \"local_dir\": args.dir,\n        \"project_name\": args.name, # Can be None, FetchRepo will derive it\n        \"github_token\": github_token,\n        \"output_dir\": args.output, # Base directory for CombineTutorial output\n\n        # Add include/exclude patterns and max file size\n        \"include_patterns\": set(args.include) if args.include else DEFAULT_INCLUDE_PATTERNS,\n        \"exclude_patterns\": set(args.exclude) if args.exclude else DEFAULT_EXCLUDE_PATTERNS,\n        \"max_file_size\": args.max_size,\n\n        # Outputs will be populated by the nodes\n        \"files\": [],\n        \"abstractions\": [],\n        \"relationships\": {},\n        \"chapter_order\": [],\n        \"chapters\": [],\n        \"final_output_dir\": None\n    }\n\n    print(f\"Starting tutorial generation for: {args.repo or args.dir}\")\n\n    # Create the flow instance\n    tutorial_flow = create_tutorial_flow()\n\n    # Run the flow\n    tutorial_flow.run(shared)\n    \nif __name__ == \"__main__\":\n    main()\n\n--- File Index 5: nodes.py ---\nimport os\nimport yaml\nfrom pocketflow import Node, BatchNode\nfrom utils.crawl_github_files import crawl_github_files\nfrom utils.call_llm import call_llm # Assuming you have this utility\nfrom utils.crawl_local_files import crawl_local_files\nfrom converter.md_to_html import convert_all_md_to_html\n\n\n# Helper to get content for specific file indices\ndef get_content_for_indices(files_data, indices):\n    content_map = {}\n    for i in indices:\n        if 0 <= i < len(files_data):\n            path, content = files_data[i]\n            content_map[f\"{i} # {path}\"] = content # Use index + path as key for context\n    return content_map\n\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"local_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        if not project_name:\n            # Basic name derivation from URL or directory\n            if repo_url:\n                project_name = repo_url.split('/')[-1].replace('.git', '')\n            else:\n                project_name = os.path.basename(os.path.abspath(local_dir))\n            shared[\"project_name\"] = project_name\n\n        # Get file patterns directly from shared\n        include_patterns = shared[\"include_patterns\"]\n        exclude_patterns = shared[\"exclude_patterns\"]\n        max_file_size = shared[\"max_file_size\"]\n\n        return {\n            \"repo_url\": repo_url,\n            \"local_dir\": local_dir,\n            \"token\": shared.get(\"github_token\"),\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns,\n            \"max_file_size\": max_file_size,\n            \"use_relative_paths\": True\n        }\n\n    def exec(self, prep_res):\n        if prep_res[\"repo_url\"]:\n            print(f\"Crawling repository: {prep_res['repo_url']}...\")\n            result = crawl_github_files(\n                repo_url=prep_res[\"repo_url\"],\n                token=prep_res[\"token\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        else:\n            print(f\"Crawling directory: {prep_res['local_dir']}...\")\n            result = crawl_local_files(\n                directory=prep_res[\"local_dir\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n            \n        # Convert dict to list of tuples: [(path, content), ...]\n        files_list = list(result.get(\"files\", {}).items())\n        print(f\"Fetched {len(files_list)} files.\")\n        return files_list\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"files\"] = exec_res # List of (path, content) tuples\n\nclass IdentifyAbstractions(Node):\n    def prep(self, shared):\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        \n        # Helper to create context from files, respecting limits (basic example)\n        def create_llm_context(files_data):\n            context = \"\"\n            file_info = [] # Store tuples of (index, path)\n            for i, (path, content) in enumerate(files_data):\n                entry = f\"--- File Index {i}: {path} ---\\n{content}\\n\\n\"\n                context += entry\n                file_info.append((i, path))\n\n            return context, file_info # file_info is list of (index, path)\n\n        context, file_info = create_llm_context(files_data)\n        # Format file info for the prompt (comment is just a hint for LLM)\n        file_listing_for_prompt = \"\\n\".join([f\"- {idx} # {path}\" for idx, path in file_info])\n        return context, file_listing_for_prompt, len(files_data), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, file_listing_for_prompt, file_count, project_name = prep_res  # Unpack project name\n        print(\"Identifying abstractions using LLM...\")\n        prompt = f\"\"\"\nFor the project `{project_name}`:\n\nCodebase Context:\n{context}\n\nAnalyze the codebase context.\nIdentify the top 5-10 core most important abstractions to help those new to the codebase.\n\nFor each abstraction, provide:\n1. A concise `name`.\n2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\nList of file indices and paths present in the context:\n{file_listing_for_prompt}\n\nFormat the output as a YAML list of dictionaries:\n\n```yaml\n- name: Query Processing\n  description: | \n    Explains what the abstraction does.\n    It's like a central dispatcher routing requests.\n  file_indices:\n    - 0 # path/to/file1.py\n    - 3 # path/to/related.py\n- name: Query Optimization\n  description: |\n    Another core concept, similar to a blueprint for objects.\n  file_indices:\n    - 5 # path/to/another.js\n# ... up to 10 abstractions\n```\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        abstractions = yaml.safe_load(yaml_str)\n\n        if not isinstance(abstractions, list):\n            raise ValueError(\"LLM Output is not a list\")\n\n        validated_abstractions = []\n        for item in abstractions:\n            if not isinstance(item, dict) or not all(k in item for k in [\"name\", \"description\", \"file_indices\"]):\n                raise ValueError(f\"Missing keys in abstraction item: {item}\")\n            if not isinstance(item[\"description\"], str):\n                 raise ValueError(f\"description is not a string in item: {item}\")\n            if not isinstance(item[\"file_indices\"], list):\n                 raise ValueError(f\"file_indices is not a list in item: {item}\")\n\n            # Validate indices\n            validated_indices = []\n            for idx_entry in item[\"file_indices\"]:\n                 try:\n                     if isinstance(idx_entry, int):\n                         idx = idx_entry\n                     elif isinstance(idx_entry, str) and '#' in idx_entry:\n                          idx = int(idx_entry.split('#')[0].strip())\n                     else:\n                          idx = int(str(idx_entry).strip())\n\n                     if not (0 <= idx < file_count):\n                         raise ValueError(f\"Invalid file index {idx} found in item {item['name']}. Max index is {file_count - 1}.\")\n                     validated_indices.append(idx)\n                 except (ValueError, TypeError):\n                      raise ValueError(f\"Could not parse index from entry: {idx_entry} in item {item['name']}\")\n\n            item[\"files\"] = sorted(list(set(validated_indices)))\n            # Store only the required fields\n            validated_abstractions.append({\n                \"name\": item[\"name\"],\n                \"description\": item[\"description\"],\n                \"files\": item[\"files\"]\n            })\n\n        print(f\"Identified {len(validated_abstractions)} abstractions.\")\n        return validated_abstractions\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"abstractions\"] = exec_res # List of {\"name\": str, \"description\": str, \"files\": [int]}\n\nclass AnalyzeRelationships(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"] # Now contains 'files' list of indices\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n\n        # Create context with abstraction names, indices, descriptions, and relevant file snippets\n        context = \"Identified Abstractions:\\n\"\n        all_relevant_indices = set()\n        abstraction_info_for_prompt = []\n        for i, abstr in enumerate(abstractions):\n            # Use 'files' which contains indices directly\n            file_indices_str = \", \".join(map(str, abstr['files']))\n            info_line = f\"- Index {i}: {abstr['name']} (Relevant file indices: [{file_indices_str}])\\n  Description: {abstr['description']}\"\n            context += info_line + \"\\n\"\n            abstraction_info_for_prompt.append(f\"{i} # {abstr['name']}\")\n            all_relevant_indices.update(abstr['files'])\n\n        context += \"\\nRelevant File Snippets (Referenced by Index and Path):\\n\"\n        # Get content for relevant files using helper\n        relevant_files_content_map = get_content_for_indices(\n            files_data,\n            sorted(list(all_relevant_indices))\n        )\n        # Format file content for context\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path} ---\\n{content}\"\n            for idx_path, content in relevant_files_content_map.items()\n        )\n        context += file_context_str\n\n        return context, \"\\n\".join(abstraction_info_for_prompt), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, abstraction_listing, project_name = prep_res  # Unpack project name\n        print(\"Analyzing relationships using LLM...\")\n        prompt = f\"\"\"\nBased on the following abstractions and relevant code snippets from the project `{project_name}`:\n\nList of Abstraction Indices and Names:\n{abstraction_listing}\n\nContext (Abstractions, Descriptions, Code):\n{context}\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        relationships_data = yaml.safe_load(yaml_str)\n\n        if not isinstance(relationships_data, dict) or not all(k in relationships_data for k in [\"summary\", \"relationships\"]):\n            raise ValueError(\"LLM output is not a dict or missing keys ('summary', 'relationships')\")\n        if not isinstance(relationships_data[\"summary\"], str):\n             raise ValueError(\"summary is not a string\")\n        if not isinstance(relationships_data[\"relationships\"], list):\n             raise ValueError(\"relationships is not a list\")\n\n        # Validate relationships structure\n        validated_relationships = []\n        num_abstractions = len(abstraction_listing.split('\\n'))\n        for rel in relationships_data[\"relationships\"]:\n             # Check for 'label' key\n             if not isinstance(rel, dict) or not all(k in rel for k in [\"from_abstraction\", \"to_abstraction\", \"label\"]):\n                  raise ValueError(f\"Missing keys (expected from_abstraction, to_abstraction, label) in relationship item: {rel}\")\n             # Validate 'label' is a string\n             if not isinstance(rel[\"label\"], str):\n                  raise ValueError(f\"Relationship label is not a string: {rel}\")\n\n             # Validate indices\n             try:\n                 from_idx = int(str(rel[\"from_abstraction\"]).split('#')[0].strip())\n                 to_idx = int(str(rel[\"to_abstraction\"]).split('#')[0].strip())\n                 if not (0 <= from_idx < num_abstractions and 0 <= to_idx < num_abstractions):\n                      raise ValueError(f\"Invalid index in relationship: from={from_idx}, to={to_idx}. Max index is {num_abstractions-1}.\")\n                 validated_relationships.append({\n                     \"from\": from_idx,\n                     \"to\": to_idx,\n                     \"label\": rel[\"label\"] \n                 })\n             except (ValueError, TypeError):\n                  raise ValueError(f\"Could not parse indices from relationship: {rel}\")\n\n        print(\"Generated project summary and relationship details.\")\n        return {\n            \"summary\": relationships_data[\"summary\"],\n            \"details\": validated_relationships # Store validated, index-based relationships\n        }\n\n\n    def post(self, shared, prep_res, exec_res):\n        # Structure is now {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        shared[\"relationships\"] = exec_res\n\nclass OrderChapters(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"]\n        relationships = shared[\"relationships\"]\n        project_name = shared[\"project_name\"]  # Get project name\n\n        # Prepare context for the LLM\n        abstraction_info_for_prompt = []\n        for i, a in enumerate(abstractions):\n            abstraction_info_for_prompt.append(f\"- {i} # {a['name']}\")\n        abstraction_listing = \"\\n\".join(abstraction_info_for_prompt)\n\n        context = f\"Project Summary:\\n{relationships['summary']}\\n\\n\"\n        context += \"Relationships (Indices refer to abstractions above):\\n\"\n        for rel in relationships['details']:\n             from_name = abstractions[rel['from']]['name']\n             to_name = abstractions[rel['to']]['name']\n             # Use 'label' instead of 'desc'\n             context += f\"- From {rel['from']} ({from_name}) to {rel['to']} ({to_name}): {rel['label']}\\n\"\n\n        return abstraction_listing, context, len(abstractions), project_name\n\n    def exec(self, prep_res):\n        abstraction_listing, context, num_abstractions, project_name = prep_res\n        print(\"Determining chapter order using LLM...\")\n        prompt = f\"\"\"\nGiven the following project abstractions and their relationships for the project ```` {project_name} ````:\n\nAbstractions (Index # Name):\n{abstraction_listing}\n\nContext about relationships and project summary:\n{context}\n\nIf you are going to make a tutorial for ```` {project_name} ````, what is the best order to explain these abstractions, from first to last?\nIdeally, first explain those that are the most important or foundational, perhaps user-facing concepts or entry points. Then move to more detailed, lower-level implementation details or supporting concepts.\n\nOutput the ordered list of abstraction indices, including the name in a comment for clarity. Use the format `idx # AbstractionName`.\n\n```yaml\n- 2 # FoundationalConcept\n- 0 # CoreClassA\n- 1 # CoreClassB (uses CoreClassA)\n- ...\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        # Rely on Node's built-in retry/fallback\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        ordered_indices_raw = yaml.safe_load(yaml_str)\n\n        if not isinstance(ordered_indices_raw, list):\n            raise ValueError(\"LLM output is not a list\")\n\n        ordered_indices = []\n        seen_indices = set()\n        for entry in ordered_indices_raw:\n            try:\n                 if isinstance(entry, int):\n                     idx = entry\n                 elif isinstance(entry, str) and '#' in entry:\n                      idx = int(entry.split('#')[0].strip())\n                 else:\n                      idx = int(str(entry).strip())\n\n                 if not (0 <= idx < num_abstractions):\n                      raise ValueError(f\"Invalid index {idx} in ordered list. Max index is {num_abstractions-1}.\")\n                 if idx in seen_indices:\n                     raise ValueError(f\"Duplicate index {idx} found in ordered list.\")\n                 ordered_indices.append(idx)\n                 seen_indices.add(idx)\n\n            except (ValueError, TypeError):\n                 raise ValueError(f\"Could not parse index from ordered list entry: {entry}\")\n\n        # Check if all abstractions are included\n        if len(ordered_indices) != num_abstractions:\n             raise ValueError(f\"Ordered list length ({len(ordered_indices)}) does not match number of abstractions ({num_abstractions}). Missing indices: {set(range(num_abstractions)) - seen_indices}\")\n\n        print(f\"Determined chapter order (indices): {ordered_indices}\")\n        return ordered_indices # Return the list of indices\n\n    def post(self, shared, prep_res, exec_res):\n        # exec_res is already the list of ordered indices\n        shared[\"chapter_order\"] = exec_res # List of indices\n\nclass WriteChapters(BatchNode):\n    def prep(self, shared):\n        chapter_order = shared[\"chapter_order\"] # List of indices\n        abstractions = shared[\"abstractions\"]   # List of dicts, now using 'files' with indices\n        files_data = shared[\"files\"]\n        # Get already written chapters to provide context\n        # We store them temporarily during the batch run, not in shared memory yet\n        # The 'previous_chapters_summary' will be built progressively in the exec context\n        self.chapters_written_so_far = [] # Use instance variable for temporary storage across exec calls\n\n        # Create a complete list of all chapters\n        all_chapters = []\n        chapter_filenames = {} # Store chapter filename mapping for linking\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                chapter_num = i + 1\n                chapter_name = abstractions[abstraction_index][\"name\"]\n                # Create safe filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in chapter_name).lower()\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                # Format with link\n                all_chapters.append(f\"{chapter_num}. [{chapter_name}]({filename})\")\n                # Store mapping of chapter index to filename for linking\n                chapter_filenames[abstraction_index] = {\"num\": chapter_num, \"name\": chapter_name, \"filename\": filename}\n        \n        # Create a formatted string with all chapters\n        full_chapter_listing = \"\\n\".join(all_chapters)\n\n        items_to_process = []\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                abstraction_details = abstractions[abstraction_index]\n                # Use 'files' (list of indices) directly\n                related_file_indices = abstraction_details.get(\"files\", [])\n                # Get content using helper, passing indices\n                related_files_content_map = get_content_for_indices(files_data, related_file_indices)\n                \n                # Get previous chapter info for transitions\n                prev_chapter = None\n                if i > 0:\n                    prev_idx = chapter_order[i-1]\n                    prev_chapter = chapter_filenames[prev_idx]\n                \n                # Get next chapter info for transitions\n                next_chapter = None\n                if i < len(chapter_order) - 1:\n                    next_idx = chapter_order[i+1]\n                    next_chapter = chapter_filenames[next_idx]\n\n                items_to_process.append({\n                    \"chapter_num\": i + 1,\n                    \"abstraction_index\": abstraction_index,\n                    \"abstraction_details\": abstraction_details,\n                    \"related_files_content_map\": related_files_content_map,\n                    \"project_name\": shared[\"project_name\"],  # Add project name\n                    \"full_chapter_listing\": full_chapter_listing,  # Add the full chapter listing\n                    \"chapter_filenames\": chapter_filenames,  # Add chapter filenames mapping\n                    \"prev_chapter\": prev_chapter,  # Add previous chapter info\n                    \"next_chapter\": next_chapter,  # Add next chapter info\n                    # previous_chapters_summary will be added dynamically in exec\n                })\n            else:\n                print(f\"Warning: Invalid abstraction index {abstraction_index} in chapter_order. Skipping.\")\n\n        print(f\"Preparing to write {len(items_to_process)} chapters...\")\n        return items_to_process # Iterable for BatchNode\n\n    def exec(self, item):\n        # This runs for each item prepared above\n        abstraction_name = item[\"abstraction_details\"][\"name\"]\n        chapter_num = item[\"chapter_num\"]\n        project_name = item.get(\"project_name\")  # Get from item\n        print(f\"Writing chapter {chapter_num} for: {abstraction_name} using LLM...\")\n\n        # Prepare file context string from the map\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path.split('# ')[1] if '# ' in idx_path else idx_path} ---\\n{content}\"\n            for idx_path, content in item[\"related_files_content_map\"].items()\n        )\n\n        # Get summary of chapters written *before* this one\n        # Use the temporary instance variable\n        previous_chapters_summary = \"\\n---\\n\".join(self.chapters_written_so_far)\n\n\n        prompt = f\"\"\"\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `{project_name}` about the concept: \"{abstraction_name}\". This is Chapter {chapter_num}.\n\nConcept Details:\n- Description:\n{item[\"abstraction_details\"][\"description\"]}\n\nComplete Tutorial Structure:\n{item[\"full_chapter_listing\"]}\n\nContext from previous chapters (summary):\n{previous_chapters_summary if previous_chapters_summary else \"This is the first chapter.\"}\n\nRelevant Code Snippets:\n{file_context_str if file_context_str else \"No specific code snippets provided for this abstraction.\"}\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter {chapter_num}: {abstraction_name}`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n\"\"\"\n        chapter_content = call_llm(prompt)\n        # Basic validation/cleanup\n        actual_heading = f\"# Chapter {chapter_num}: {abstraction_name}\"\n        if not chapter_content.strip().startswith(f\"# Chapter {chapter_num}\"):\n             # Add heading if missing or incorrect, trying to preserve content\n             lines = chapter_content.strip().split('\\n')\n             if lines and lines[0].strip().startswith(\"#\"): # If there's some heading, replace it\n                 lines[0] = actual_heading\n                 chapter_content = \"\\n\".join(lines)\n             else: # Otherwise, prepend it\n                 chapter_content = f\"{actual_heading}\\n\\n{chapter_content}\"\n\n        # Add the generated content to our temporary list for the next iteration's context\n        self.chapters_written_so_far.append(chapter_content)\n\n        return chapter_content # Return the Markdown string\n\n    def post(self, shared, prep_res, exec_res_list):\n        # exec_res_list contains the generated Markdown for each chapter, in order\n        shared[\"chapters\"] = exec_res_list\n        # Clean up the temporary instance variable\n        del self.chapters_written_so_far\n        print(f\"Finished writing {len(exec_res_list)} chapters.\")\n\nclass CombineTutorial(Node):\n    def prep(self, shared):\n        project_name = shared[\"project_name\"]\n        output_base_dir = shared.get(\"output_dir\", \"output\") # Default output dir\n        output_path = os.path.join(output_base_dir, project_name)\n        repo_url = shared[\"repo_url\"]  # Get the repository URL\n\n        # Use 'label' from relationships_data['details']\n        relationships_data = shared[\"relationships\"] # {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        chapter_order = shared[\"chapter_order\"] # indices\n        abstractions = shared[\"abstractions\"]   # list of dicts\n        chapters_content = shared[\"chapters\"]   # list of strings\n\n        # --- Generate Mermaid Diagram ---\n        mermaid_lines = [\"flowchart TD\"]\n        # Add nodes for each abstraction\n        for i, abstr in enumerate(abstractions):\n            # Sanitize name for Mermaid ID and label\n            node_id = f\"A{i}\"\n            sanitized_name = abstr['name'].replace('\"', '')\n            node_label = sanitized_name # Using sanitized name only, no index\n            mermaid_lines.append(f'    {node_id}[\"{node_label}\"]')\n        # Add edges for relationships using 'label'\n        for rel in relationships_data['details']:\n            from_node_id = f\"A{rel['from']}\"\n            to_node_id = f\"A{rel['to']}\"\n            # Sanitize 'label' for edge label\n            edge_label = rel['label'].replace('\"', '').replace('\\n', ' ') # Basic sanitization\n            # Limit edge label length for readability (optional, but good for diagrams)\n            max_label_len = 30 # Make it shorter for labels\n            if len(edge_label) > max_label_len:\n                edge_label = edge_label[:max_label_len-3] + \"...\"\n            mermaid_lines.append(f'    {from_node_id} -- \"{edge_label}\" --> {to_node_id}')\n\n        mermaid_diagram = \"\\n\".join(mermaid_lines)\n        # --- End Mermaid ---\n\n\n        # Prepare index.md content\n        index_content = f\"# Tutorial: {project_name}\\n\\n\"\n        index_content += f\"{relationships_data['summary']}\\n\\n\"\n        index_content += f\"**Source Repository:** [{repo_url}]({repo_url})\\n\\n\"\n\n        # Add Mermaid diagram for relationships\n        index_content += \"```mermaid\\n\"\n        index_content += mermaid_diagram + \"\\n\"\n        index_content += \"```\\n\\n\"\n\n        index_content += \"## Chapters\\n\\n\"\n\n        chapter_files = []\n        # Generate chapter links based on the determined order\n        for i, abstraction_index in enumerate(chapter_order):\n            # Ensure index is valid and we have content for it\n            if 0 <= abstraction_index < len(abstractions) and i < len(chapters_content):\n                abstraction_name = abstractions[abstraction_index][\"name\"]\n                # Sanitize name for filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in abstraction_name).lower()\n                # Use chapter number (i+1) for ordering filename\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                index_content += f\"{i+1}. [{abstraction_name}]({filename})\\n\"\n                \n                # Add attribution to chapter content\n                chapter_content = chapters_content[i]\n                if not chapter_content.endswith(\"\\n\\n\"):\n                    chapter_content += \"\\n\\n\"\n                chapter_content += \"---\\n\\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\"\n                \n                # Store filename and corresponding content\n                chapter_files.append({\"filename\": filename, \"content\": chapter_content})\n            else:\n                 print(f\"Warning: Mismatch between chapter order, abstractions, or content at index {i} (abstraction index {abstraction_index}). Skipping file generation for this entry.\")\n\n        # Add attribution to index content\n        index_content += \"\\n\\n---\\n\\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\"\n\n        return {\n            \"output_path\": output_path,\n            \"index_content\": index_content,\n            \"chapter_files\": chapter_files # List of {\"filename\": str, \"content\": str}\n        }\n\n    def exec(self, prep_res):\n        output_path = prep_res[\"output_path\"]\n        index_content = prep_res[\"index_content\"]\n        chapter_files = prep_res[\"chapter_files\"]\n\n        print(f\"Combining tutorial into directory: {output_path}\")\n        # Rely on Node's built-in retry/fallback\n        os.makedirs(output_path, exist_ok=True)\n\n        # Write index.md\n        index_filepath = os.path.join(output_path, \"index.md\")\n        with open(index_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(index_content)\n        print(f\"  - Wrote {index_filepath}\")\n\n        # Write chapter files\n        for chapter_info in chapter_files:\n            chapter_filepath = os.path.join(output_path, chapter_info[\"filename\"])\n            with open(chapter_filepath, \"w\", encoding=\"utf-8\") as f:\n                f.write(chapter_info[\"content\"])\n            print(f\"  - Wrote {chapter_filepath}\")\n\n        return output_path # Return the final path\n\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"final_output_dir\"] = exec_res # Store the output path\n        print(f\"\\nTutorial generation complete! Files are in: {exec_res}\")\n        convert_all_md_to_html(exec_res) # Convert all Markdown files to HTML\n        \n\n--- File Index 6: utils/__init__.py ---\n\n\n--- File Index 7: utils/call_llm.py ---\nfrom google import genai\nimport os\nimport logging\nimport json\nfrom datetime import datetime\n\n# Configure logging\nlog_directory = os.getenv(\"LOG_DIR\", \"logs\")\nos.makedirs(log_directory, exist_ok=True)\nlog_file = os.path.join(log_directory, f\"llm_calls_{datetime.now().strftime('%Y%m%d')}.log\")\n\n# Set up logger\nlogger = logging.getLogger(\"llm_logger\")\nlogger.setLevel(logging.INFO)\nlogger.propagate = False  # Prevent propagation to root logger\nfile_handler = logging.FileHandler(log_file)\nfile_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\nlogger.addHandler(file_handler)\n\n# Simple cache configuration\ncache_file = \"llm_cache.json\"\n\n# By default, we Google Gemini 2.5 pro, as it shows great performance for code understanding\ndef call_llm(prompt: str, use_cache: bool = True) -> str:\n    # Log the prompt\n    logger.info(f\"PROMPT: {prompt}\")\n    \n    # Check cache if enabled\n    if use_cache:\n        # Load cache from disk\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, 'r') as f:\n                    cache = json.load(f)\n            except:\n                logger.warning(f\"Failed to load cache, starting with empty cache\")\n        \n        # Return from cache if exists\n        if prompt in cache:\n            logger.info(f\"RESPONSE: {cache[prompt]}\")\n            return cache[prompt]\n    \n    # Call the LLM if not in cache or cache disabled\n    client = genai.Client(\n        vertexai=True, \n        # TODO: change to your own project id and location\n        project=os.getenv(\"GEMINI_PROJECT_ID\", \"your-project-id\"),\n        location=os.getenv(\"GEMINI_LOCATION\", \"us-central1\")\n    )\n    # You can comment the previous line and use the AI Studio key instead:\n    # client = genai.Client(\n    #     api_key=os.getenv(\"GEMINI_API_KEY\", \"your-api_key\"),\n    # )\n    model = os.getenv(\"GEMINI_MODEL\", \"gemini-2.5-pro-exp-03-25\")\n    response = client.models.generate_content(\n        model=model,\n        contents=[prompt]\n    )\n    response_text = response.text\n    \n    # Log the response\n    logger.info(f\"RESPONSE: {response_text}\")\n    \n    # Update cache if enabled\n    if use_cache:\n        # Load cache again to avoid overwrites\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, 'r') as f:\n                    cache = json.load(f)\n            except:\n                pass\n        \n        # Add to cache and save\n        cache[prompt] = response_text\n        try:\n            with open(cache_file, 'w') as f:\n                json.dump(cache, f)\n        except Exception as e:\n            logger.error(f\"Failed to save cache: {e}\")\n    \n    return response_text\n\n# # Use Anthropic Claude 3.7 Sonnet Extended Thinking\n# def call_llm(prompt, use_cache: bool = True):\n#     from anthropic import Anthropic\n#     client = Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\", \"your-api-key\"))\n#     response = client.messages.create(\n#         model=\"claude-3-7-sonnet-20250219\",\n#         max_tokens=21000,\n#         thinking={\n#             \"type\": \"enabled\",\n#             \"budget_tokens\": 20000\n#         },\n#         messages=[\n#             {\"role\": \"user\", \"content\": prompt}\n#         ]\n#     )\n#     return response.content[1].text\n\n# # Use OpenAI o1\n# def call_llm(prompt, use_cache: bool = True):    \n#     from openai import OpenAI\n#     client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n#     r = client.chat.completions.create(\n#         model=\"o1\",\n#         messages=[{\"role\": \"user\", \"content\": prompt}],\n#         response_format={\n#             \"type\": \"text\"\n#         },\n#         reasoning_effort=\"medium\",\n#         store=False\n#     )\n#     return r.choices[0].message.content\n\nif __name__ == \"__main__\":\n    test_prompt = \"Hello, how are you?\"\n    \n    # First call - should hit the API\n    print(\"Making call...\")\n    response1 = call_llm(test_prompt, use_cache=False)\n    print(f\"Response: {response1}\")\n    \n\n\n--- File Index 8: utils/crawl_github_files.py ---\nimport requests\nimport base64\nimport os\nimport tempfile\nimport git\nimport time\nimport fnmatch\nfrom typing import Union, Set, List, Dict, Tuple, Any\nfrom urllib.parse import urlparse\n\ndef crawl_github_files(\n    repo_url, \n    token=None, \n    max_file_size: int = 1 * 1024 * 1024,  # 1 MB\n    use_relative_paths: bool = False,\n    include_patterns: Union[str, Set[str]] = None,\n    exclude_patterns: Union[str, Set[str]] = None\n):\n    \"\"\"\n    Crawl files from a specific path in a GitHub repository at a specific commit.\n\n    Args:\n        repo_url (str): URL of the GitHub repository with specific path and commit\n                        (e.g., 'https://github.com/microsoft/autogen/tree/e45a15766746d95f8cfaaa705b0371267bec812e/python/packages/autogen-core/src/autogen_core')\n        token (str, optional): **GitHub personal access token.**\n            - **Required for private repositories.**\n            - **Recommended for public repos to avoid rate limits.**\n            - Can be passed explicitly or set via the `GITHUB_TOKEN` environment variable.\n        max_file_size (int, optional): Maximum file size in bytes to download (default: 1 MB)\n        use_relative_paths (bool, optional): If True, file paths will be relative to the specified subdirectory\n        include_patterns (str or set of str, optional): Pattern or set of patterns specifying which files to include (e.g., \"*.py\", {\"*.md\", \"*.txt\"}).\n                                                       If None, all files are included.\n        exclude_patterns (str or set of str, optional): Pattern or set of patterns specifying which files to exclude.\n                                                       If None, no files are excluded.\n\n    Returns:\n        dict: Dictionary with files and statistics\n    \"\"\"\n    # Convert single pattern to set\n    if include_patterns and isinstance(include_patterns, str):\n        include_patterns = {include_patterns}\n    if exclude_patterns and isinstance(exclude_patterns, str):\n        exclude_patterns = {exclude_patterns}\n\n    def should_include_file(file_path: str, file_name: str) -> bool:\n        \"\"\"Determine if a file should be included based on patterns\"\"\"\n        # If no include patterns are specified, include all files\n        if not include_patterns:\n            include_file = True\n        else:\n            # Check if file matches any include pattern\n            include_file = any(fnmatch.fnmatch(file_name, pattern) for pattern in include_patterns)\n\n        # If exclude patterns are specified, check if file should be excluded\n        if exclude_patterns and include_file:\n            # Exclude if file matches any exclude pattern\n            exclude_file = any(fnmatch.fnmatch(file_path, pattern) for pattern in exclude_patterns)\n            return not exclude_file\n\n        return include_file\n\n    # Detect SSH URL (git@ or .git suffix)\n    is_ssh_url = repo_url.startswith(\"git@\") or repo_url.endswith(\".git\")\n\n    if is_ssh_url:\n        # Clone repo via SSH to temp dir\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            print(f\"Cloning SSH repo {repo_url} to temp dir {tmpdirname} ...\")\n            try:\n                repo = git.Repo.clone_from(repo_url, tmpdirname)\n            except Exception as e:\n                print(f\"Error cloning repo: {e}\")\n                return {\"files\": {}, \"stats\": {\"error\": str(e)}}\n\n            # Attempt to checkout specific commit/branch if in URL\n            # Parse ref and subdir from SSH URL? SSH URLs don't have branch info embedded\n            # So rely on default branch, or user can checkout manually later\n            # Optionally, user can pass ref explicitly in future API\n\n            # Walk directory\n            files = {}\n            skipped_files = []\n\n            for root, dirs, filenames in os.walk(tmpdirname):\n                for filename in filenames:\n                    abs_path = os.path.join(root, filename)\n                    rel_path = os.path.relpath(abs_path, tmpdirname)\n\n                    # Check file size\n                    try:\n                        file_size = os.path.getsize(abs_path)\n                    except OSError:\n                        continue\n\n                    if file_size > max_file_size:\n                        skipped_files.append((rel_path, file_size))\n                        print(f\"Skipping {rel_path}: size {file_size} exceeds limit {max_file_size}\")\n                        continue\n\n                    # Check include/exclude patterns\n                    if not should_include_file(rel_path, filename):\n                        print(f\"Skipping {rel_path}: does not match include/exclude patterns\")\n                        continue\n\n                    # Read content\n                    try:\n                        with open(abs_path, \"r\", encoding=\"utf-8\") as f:\n                            content = f.read()\n                        files[rel_path] = content\n                        print(f\"Added {rel_path} ({file_size} bytes)\")\n                    except Exception as e:\n                        print(f\"Failed to read {rel_path}: {e}\")\n\n            return {\n                \"files\": files,\n                \"stats\": {\n                    \"downloaded_count\": len(files),\n                    \"skipped_count\": len(skipped_files),\n                    \"skipped_files\": skipped_files,\n                    \"base_path\": None,\n                    \"include_patterns\": include_patterns,\n                    \"exclude_patterns\": exclude_patterns,\n                    \"source\": \"ssh_clone\"\n                }\n            }\n\n    # Parse GitHub URL to extract owner, repo, commit/branch, and path\n    parsed_url = urlparse(repo_url)\n    path_parts = parsed_url.path.strip('/').split('/')\n    \n    if len(path_parts) < 2:\n        raise ValueError(f\"Invalid GitHub URL: {repo_url}\")\n    \n    # Extract the basic components\n    owner = path_parts[0]\n    repo = path_parts[1]\n    \n    # Check if URL contains a specific branch/commit\n    if 'tree' in path_parts:\n        tree_index = path_parts.index('tree')\n        ref = path_parts[tree_index + 1]\n        # Combine all parts after the ref as the path\n        path_start = tree_index + 2\n        specific_path = '/'.join(path_parts[path_start:]) if path_start < len(path_parts) else \"\"\n    else:\n        ref = \"main\"  # Default branch\n        specific_path = \"\"\n    \n    # Setup for GitHub API\n    headers = {\"Accept\": \"application/vnd.github.v3+json\"}\n    if token:\n        headers[\"Authorization\"] = f\"token {token}\"\n    \n    # Dictionary to store path -> content mapping\n    files = {}\n    skipped_files = []\n    \n    def should_include_file(file_path: str, file_name: str) -> bool:\n        \"\"\"Determine if a file should be included based on patterns\"\"\"\n        # If no include patterns are specified, include all files\n        if not include_patterns:\n            include_file = True\n        else:\n            # Check if file matches any include pattern\n            include_file = any(fnmatch.fnmatch(file_name, pattern) for pattern in include_patterns)\n        \n        # If exclude patterns are specified, check if file should be excluded\n        if exclude_patterns and include_file:\n            # Exclude if file matches any exclude pattern\n            exclude_file = any(fnmatch.fnmatch(file_path, pattern) for pattern in exclude_patterns)\n            return not exclude_file\n        \n        return include_file\n    \n    def fetch_contents(path):\n        \"\"\"Fetch contents of the repository at a specific path and commit\"\"\"\n        url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{path}\"\n        params = {\"ref\": ref}\n        \n        response = requests.get(url, headers=headers, params=params)\n        \n        if response.status_code == 403 and 'rate limit exceeded' in response.text.lower():\n            reset_time = int(response.headers.get('X-RateLimit-Reset', 0))\n            wait_time = max(reset_time - time.time(), 0) + 1\n            print(f\"Rate limit exceeded. Waiting for {wait_time:.0f} seconds...\")\n            time.sleep(wait_time)\n            return fetch_contents(path)\n            \n        if response.status_code == 404:\n            if not token:\n                print(f\"Error 404: Repository not found or is private.\\n\"\n                      f\"If this is a private repository, please provide a valid GitHub token via the 'token' argument or set the GITHUB_TOKEN environment variable.\")\n            else:\n                print(f\"Error 404: Path '{path}' not found in repository or insufficient permissions with the provided token.\\n\"\n                      f\"Please verify the token has access to this repository and the path exists.\")\n            return\n            \n        if response.status_code != 200:\n            print(f\"Error fetching {path}: {response.status_code} - {response.text}\")\n            return\n        \n        contents = response.json()\n        \n        # Handle both single file and directory responses\n        if not isinstance(contents, list):\n            contents = [contents]\n        \n        for item in contents:\n            item_path = item[\"path\"]\n            \n            # Calculate relative path if requested\n            if use_relative_paths and specific_path:\n                # Make sure the path is relative to the specified subdirectory\n                if item_path.startswith(specific_path):\n                    rel_path = item_path[len(specific_path):].lstrip('/')\n                else:\n                    rel_path = item_path\n            else:\n                rel_path = item_path\n            \n            if item[\"type\"] == \"file\":\n                # Check if file should be included based on patterns\n                if not should_include_file(rel_path, item[\"name\"]):\n                    print(f\"Skipping {rel_path}: Does not match include/exclude patterns\")\n                    continue\n                \n                # Check file size if available\n                file_size = item.get(\"size\", 0)\n                if file_size > max_file_size:\n                    skipped_files.append((item_path, file_size))\n                    print(f\"Skipping {rel_path}: File size ({file_size} bytes) exceeds limit ({max_file_size} bytes)\")\n                    continue\n                \n                # For files, get raw content\n                if \"download_url\" in item and item[\"download_url\"]:\n                    file_url = item[\"download_url\"]\n                    file_response = requests.get(file_url, headers=headers)\n                    \n                    # Final size check in case content-length header is available but differs from metadata\n                    content_length = int(file_response.headers.get('content-length', 0))\n                    if content_length > max_file_size:\n                        skipped_files.append((item_path, content_length))\n                        print(f\"Skipping {rel_path}: Content length ({content_length} bytes) exceeds limit ({max_file_size} bytes)\")\n                        continue\n                        \n                    if file_response.status_code == 200:\n                        files[rel_path] = file_response.text\n                        print(f\"Downloaded: {rel_path} ({file_size} bytes) \")\n                    else:\n                        print(f\"Failed to download {rel_path}: {file_response.status_code}\")\n                else:\n                    # Alternative method if download_url is not available\n                    content_response = requests.get(item[\"url\"], headers=headers)\n                    if content_response.status_code == 200:\n                        content_data = content_response.json()\n                        if content_data.get(\"encoding\") == \"base64\" and \"content\" in content_data:\n                            # Check size of base64 content before decoding\n                            if len(content_data[\"content\"]) * 0.75 > max_file_size:  # Approximate size calculation\n                                estimated_size = int(len(content_data[\"content\"]) * 0.75)\n                                skipped_files.append((item_path, estimated_size))\n                                print(f\"Skipping {rel_path}: Encoded content exceeds size limit\")\n                                continue\n                                \n                            file_content = base64.b64decode(content_data[\"content\"]).decode('utf-8')\n                            files[rel_path] = file_content\n                            print(f\"Downloaded: {rel_path} ({file_size} bytes)\")\n                        else:\n                            print(f\"Unexpected content format for {rel_path}\")\n                    else:\n                        print(f\"Failed to get content for {rel_path}: {content_response.status_code}\")\n            \n            elif item[\"type\"] == \"dir\":\n                # Recursively process subdirectories\n                fetch_contents(item_path)\n    \n    # Start crawling from the specified path\n    fetch_contents(specific_path)\n    \n    return {\n        \"files\": files,\n        \"stats\": {\n            \"downloaded_count\": len(files),\n            \"skipped_count\": len(skipped_files),\n            \"skipped_files\": skipped_files,\n            \"base_path\": specific_path if use_relative_paths else None,\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns\n        }\n    }\n\n# Example usage\nif __name__ == \"__main__\":\n    # Get token from environment variable (recommended for private repos)\n    github_token = os.environ.get(\"GITHUB_TOKEN\")\n    if not github_token:\n        print(\"Warning: No GitHub token found in environment variable 'GITHUB_TOKEN'.\\n\"\n              \"Private repositories will not be accessible without a token.\\n\"\n              \"To access private repos, set the environment variable or pass the token explicitly.\")\n    \n    repo_url = \"https://github.com/pydantic/pydantic/tree/6c38dc93f40a47f4d1350adca9ec0d72502e223f/pydantic\"\n    \n    # Example: Get Python and Markdown files, but exclude test files\n    result = crawl_github_files(\n        repo_url, \n        token=github_token,\n        max_file_size=1 * 1024 * 1024,  # 1 MB in bytes\n        use_relative_paths=True,  # Enable relative paths\n        include_patterns={\"*.py\", \"*.md\"},  # Include Python and Markdown files\n    )\n    \n    files = result[\"files\"]\n    stats = result[\"stats\"]\n    \n    print(f\"\\nDownloaded {stats['downloaded_count']} files.\")\n    print(f\"Skipped {stats['skipped_count']} files due to size limits or patterns.\")\n    print(f\"Base path for relative paths: {stats['base_path']}\")\n    print(f\"Include patterns: {stats['include_patterns']}\")\n    print(f\"Exclude patterns: {stats['exclude_patterns']}\")\n    \n    # Display all file paths in the dictionary\n    print(\"\\nFiles in dictionary:\")\n    for file_path in sorted(files.keys()):\n        print(f\"  {file_path}\")\n    \n    # Example: accessing content of a specific file\n    if files:\n        sample_file = next(iter(files))\n        print(f\"\\nSample file: {sample_file}\")\n        print(f\"Content preview: {files[sample_file][:200]}...\")\n\n--- File Index 9: utils/crawl_local_files.py ---\nimport os\nimport fnmatch\n\ndef crawl_local_files(directory, include_patterns=None, exclude_patterns=None, max_file_size=None, use_relative_paths=True):\n    \"\"\"\n    Crawl files in a local directory with similar interface as crawl_github_files.\n    \n    Args:\n        directory (str): Path to local directory\n        include_patterns (set): File patterns to include (e.g. {\"*.py\", \"*.js\"})\n        exclude_patterns (set): File patterns to exclude (e.g. {\"tests/*\"})\n        max_file_size (int): Maximum file size in bytes\n        use_relative_paths (bool): Whether to use paths relative to directory\n        \n    Returns:\n        dict: {\"files\": {filepath: content}}\n    \"\"\"\n    if not os.path.isdir(directory):\n        raise ValueError(f\"Directory does not exist: {directory}\")\n        \n    files_dict = {}\n    \n    for root, _, files in os.walk(directory):\n        for filename in files:\n            filepath = os.path.join(root, filename)\n            \n            # Get path relative to directory if requested\n            if use_relative_paths:\n                relpath = os.path.relpath(filepath, directory)\n            else:\n                relpath = filepath\n                \n            # Check if file matches any include pattern\n            included = False\n            if include_patterns:\n                for pattern in include_patterns:\n                    if fnmatch.fnmatch(relpath, pattern):\n                        included = True\n                        break\n            else:\n                included = True\n                \n            # Check if file matches any exclude pattern\n            excluded = False\n            if exclude_patterns:\n                for pattern in exclude_patterns:\n                    if fnmatch.fnmatch(relpath, pattern):\n                        excluded = True\n                        break\n                        \n            if not included or excluded:\n                continue\n                \n            # Check file size\n            if max_file_size and os.path.getsize(filepath) > max_file_size:\n                continue\n                \n            try:\n                with open(filepath, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                files_dict[relpath] = content\n            except Exception as e:\n                print(f\"Warning: Could not read file {filepath}: {e}\")\n                \n    return {\"files\": files_dict}\n\nif __name__ == \"__main__\":\n    print(\"--- Crawling parent directory ('..') ---\")\n    files_data = crawl_local_files(\"..\", exclude_patterns={\"*.pyc\", \"__pycache__/*\", \".git/*\", \"output/*\"})\n    print(f\"Found {len(files_data['files'])} files:\")\n    for path in files_data[\"files\"]:\n        print(f\"  {path}\")\n\n\n\nAnalyze the codebase context.\nIdentify the top 5-10 core most important abstractions to help those new to the codebase.\n\nFor each abstraction, provide:\n1. A concise `name`.\n2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\nList of file indices and paths present in the context:\n- 0 # README.md\n- 1 # api.py\n- 2 # converter/md_to_html.py\n- 3 # flow.py\n- 4 # main.py\n- 5 # nodes.py\n- 6 # utils/__init__.py\n- 7 # utils/call_llm.py\n- 8 # utils/crawl_github_files.py\n- 9 # utils/crawl_local_files.py\n\nFormat the output as a YAML list of dictionaries:\n\n```yaml\n- name: Query Processing\n  description: | \n    Explains what the abstraction does.\n    It's like a central dispatcher routing requests.\n  file_indices:\n    - 0 # path/to/file1.py\n    - 3 # path/to/related.py\n- name: Query Optimization\n  description: |\n    Another core concept, similar to a blueprint for objects.\n  file_indices:\n    - 5 # path/to/another.js\n# ... up to 10 abstractions\n```": "```yaml\n- name: Flow Orchestration\n  description: |\n    The Flow Orchestration system is the conductor of the codebase tutorial generation process.\n    It defines the sequence of steps, connecting various nodes like fetching the repository,\n    identifying key abstractions, analyzing relationships, writing chapters, and combining everything\n    into a final tutorial. It's like a manufacturing assembly line, where each station performs\n    a specific task, and the product moves from one stage to the next until completion.\n  file_indices:\n    - 3 # flow.py\n    - 4 # main.py\n    - 5 # nodes.py\n- name: Codebase Crawling\n  description: |\n    Codebase Crawling is the process of gathering all the necessary files from either a GitHub\n    repository or a local directory. This involves filtering files based on include and exclude\n    patterns, limiting the maximum file size, and handling potential errors. It's like a diligent\n    librarian collecting books from different shelves, ensuring they meet certain criteria before\n    adding them to the collection.\n  file_indices:\n    - 5 # nodes.py\n    - 8 # utils/crawl_github_files.py\n    - 9 # utils/crawl_local_files.py\n- name: Abstraction Identification\n  description: |\n    Abstraction Identification is the process of analyzing the codebase and identifying the most\n    important concepts or components to explain in the tutorial. This involves using an LLM to\n    understand the code and extract the core abstractions. It's like a detective piecing together\n    clues from a crime scene to identify the key suspects and their roles in the crime.\n  file_indices:\n    - 5 # nodes.py\n    - 7 # utils/call_llm.py\n- name: Relationship Analysis\n  description: |\n    Relationship Analysis is the process of understanding how the identified abstractions interact with\n    each other. This involves using an LLM to analyze the code and determine the key relationships\n    between the abstractions. It's like a network analyst mapping out the connections between\n    different computers in a network to understand how data flows between them.\n  file_indices:\n    - 5 # nodes.py\n    - 7 # utils/call_llm.py\n- name: Chapter Writing\n  description: |\n    Chapter Writing is the process of creating the actual tutorial content, explaining each abstraction\n    in a beginner-friendly way. This involves using an LLM to generate the chapter content and\n    incorporating relevant code snippets and diagrams. It's like a skilled storyteller weaving a\n    compelling narrative around different characters and events to educate and entertain the audience.\n  file_indices:\n    - 5 # nodes.py\n    - 7 # utils/call_llm.py\n- name: Tutorial Combination\n  description: |\n    Tutorial Combination is the final step in the tutorial generation process, where all the individual\n    chapters are combined into a single, cohesive tutorial. This involves generating an index page,\n    creating a Mermaid diagram to visualize the relationships between the abstractions, and converting\n    the Markdown files to HTML. It's like a bookbinder assembling all the chapters of a book and\n    adding a cover and table of contents to create a polished final product.\n  file_indices:\n    - 2 # converter/md_to_html.py\n    - 5 # nodes.py\n- name: LLM Interaction\n  description: |\n    The LLM Interaction module handles all calls to the Large Language Model (LLM). This includes\n    caching responses for efficiency, logging prompts and responses for debugging, and providing\n    a consistent interface for other modules to interact with the LLM. It's like a dedicated\n    interpreter translating human instructions into machine-understandable code.\n  file_indices:\n    - 7 # utils/call_llm.py\n    - 5 # nodes.py\n- name: API Endpoint\n  description: |\n    The API Endpoint provides a way to trigger the tutorial generation process remotely. It accepts\n    repository details and options, then runs the main script to generate the tutorial. It's like a\n    doorway that allows external programs to interact with the internal workings of the system.\n  file_indices:\n    - 1 # api.py\n- name: Configuration Management\n  description: |\n    The project relies on configuration through environment variables and argument parsing. This allows\n    for flexible customization of the tutorial generation process. It's like a control panel that\n    allows the user to adjust the settings of a machine to achieve the desired output.\n  file_indices:\n    - 4 # main.py\n    - 7 # utils/call_llm.py\n```", "\nBased on the following abstractions and relevant code snippets from the project `Tutorial-Codebase-Knowledge`:\n\nList of Abstraction Indices and Names:\n0 # Flow Orchestration\n1 # Codebase Crawling\n2 # Abstraction Identification\n3 # Relationship Analysis\n4 # Chapter Writing\n5 # Tutorial Combination\n6 # LLM Interaction\n7 # API Endpoint\n8 # Configuration Management\n\nContext (Abstractions, Descriptions, Code):\nIdentified Abstractions:\n- Index 0: Flow Orchestration (Relevant file indices: [3, 4, 5])\n  Description: The Flow Orchestration system is the conductor of the codebase tutorial generation process.\nIt defines the sequence of steps, connecting various nodes like fetching the repository,\nidentifying key abstractions, analyzing relationships, writing chapters, and combining everything\ninto a final tutorial. It's like a manufacturing assembly line, where each station performs\na specific task, and the product moves from one stage to the next until completion.\n\n- Index 1: Codebase Crawling (Relevant file indices: [5, 8, 9])\n  Description: Codebase Crawling is the process of gathering all the necessary files from either a GitHub\nrepository or a local directory. This involves filtering files based on include and exclude\npatterns, limiting the maximum file size, and handling potential errors. It's like a diligent\nlibrarian collecting books from different shelves, ensuring they meet certain criteria before\nadding them to the collection.\n\n- Index 2: Abstraction Identification (Relevant file indices: [5, 7])\n  Description: Abstraction Identification is the process of analyzing the codebase and identifying the most\nimportant concepts or components to explain in the tutorial. This involves using an LLM to\nunderstand the code and extract the core abstractions. It's like a detective piecing together\nclues from a crime scene to identify the key suspects and their roles in the crime.\n\n- Index 3: Relationship Analysis (Relevant file indices: [5, 7])\n  Description: Relationship Analysis is the process of understanding how the identified abstractions interact with\neach other. This involves using an LLM to analyze the code and determine the key relationships\nbetween the abstractions. It's like a network analyst mapping out the connections between\ndifferent computers in a network to understand how data flows between them.\n\n- Index 4: Chapter Writing (Relevant file indices: [5, 7])\n  Description: Chapter Writing is the process of creating the actual tutorial content, explaining each abstraction\nin a beginner-friendly way. This involves using an LLM to generate the chapter content and\nincorporating relevant code snippets and diagrams. It's like a skilled storyteller weaving a\ncompelling narrative around different characters and events to educate and entertain the audience.\n\n- Index 5: Tutorial Combination (Relevant file indices: [2, 5])\n  Description: Tutorial Combination is the final step in the tutorial generation process, where all the individual\nchapters are combined into a single, cohesive tutorial. This involves generating an index page,\ncreating a Mermaid diagram to visualize the relationships between the abstractions, and converting\nthe Markdown files to HTML. It's like a bookbinder assembling all the chapters of a book and\nadding a cover and table of contents to create a polished final product.\n\n- Index 6: LLM Interaction (Relevant file indices: [5, 7])\n  Description: The LLM Interaction module handles all calls to the Large Language Model (LLM). This includes\ncaching responses for efficiency, logging prompts and responses for debugging, and providing\na consistent interface for other modules to interact with the LLM. It's like a dedicated\ninterpreter translating human instructions into machine-understandable code.\n\n- Index 7: API Endpoint (Relevant file indices: [1])\n  Description: The API Endpoint provides a way to trigger the tutorial generation process remotely. It accepts\nrepository details and options, then runs the main script to generate the tutorial. It's like a\ndoorway that allows external programs to interact with the internal workings of the system.\n\n- Index 8: Configuration Management (Relevant file indices: [4, 7])\n  Description: The project relies on configuration through environment variables and argument parsing. This allows\nfor flexible customization of the tutorial generation process. It's like a control panel that\nallows the user to adjust the settings of a machine to achieve the desired output.\n\n\nRelevant File Snippets (Referenced by Index and Path):\n--- File: 1 # api.py ---\n# api.py\n\nimport sys\nimport subprocess\nimport os\nimport logging\nimport re # Import re for parsing repo name\nfrom typing import Optional, List\nfrom urllib.parse import urlparse # To help parse repo name\n\nfrom fastapi import FastAPI, HTTPException, status\nfrom pydantic import BaseModel, Field\n\nfrom fastapi.middleware.cors import CORSMiddleware\n\n# --- Configuration ---\nMAIN_SCRIPT_PATH = \"main.py\"\nPYTHON_EXECUTABLE = sys.executable\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# --- Helper Function to Get Project Name ---\ndef get_project_name_from_url(repo_url: str) -> Optional[str]:\n    \"\"\"Extracts project name from GitHub URL.\"\"\"\n    try:\n        path = urlparse(repo_url).path\n        # Remove leading slash and .git suffix if present\n        name = path.lstrip('/').replace('.git', '')\n        # Get the last part of the path\n        project_name = name.split('/')[-1]\n        if project_name:\n            return project_name\n    except Exception:\n        pass # Ignore parsing errors\n    return None\n\n# --- Input Data Model (using Pydantic) ---\nclass GenerationRequest(BaseModel):\n    repo_url: str\n    name: Optional[str] = None\n    token: Optional[str] = None\n    output: str = Field(default=\"output\")\n    include: Optional[List[str]] = None\n    exclude: Optional[List[str]] = None\n    max_size: Optional[int] = None\n\n# --- FastAPI App ---\napp = FastAPI(\n    title=\"Codebase Tutorial Generator API\",\n    description=\"API to trigger the generation of tutorials from GitHub codebases.\",\n    version=\"1.0.0\",\n)\n\n# --- CORS Middleware ---\norigins = [\n    \"http://127.0.0.1:5500\", # VS Code Live Server default\n    \"http://localhost\",\n    \"http://localhost:8080\",\n    \"http://127.0.0.1\",\n    \"http://127.0.0.1:8080\",\n    \"null\", # file:// origin\n]\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=origins,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n# --- End CORS Middleware ---\n\n\n# --- API Endpoint ---\n@app.post(\n    \"/generate\",\n    summary=\"Trigger Tutorial Generation\",\n    description=\"Accepts repository details and options, then runs the tutorial generation script.\",\n    status_code=status.HTTP_200_OK\n)\nasync def generate_tutorial(request_data: GenerationRequest):\n    \"\"\"\n    API endpoint to trigger tutorial generation. Returns final output path on success.\n    \"\"\"\n    # --- Determine Project Name (needed for final path) ---\n    project_name = request_data.name\n    if not project_name:\n        project_name = get_project_name_from_url(request_data.repo_url)\n        if not project_name:\n             # If name wasn't provided and couldn't be derived, raise an error early\n             # Or alternatively, let main.py handle it, but we need it for the return path.\n             logger.error(f\"Could not derive project name from URL: {request_data.repo_url}\")\n             raise HTTPException(\n                 status_code=status.HTTP_400_BAD_REQUEST,\n                 detail=\"Project name not provided and could not be derived from repository URL.\"\n             )\n    \n    # --- Calculate Expected Final Output Path ---\n    # This mirrors the logic in CombineTutorial.prep\n    # Ensure forward slashes for URL compatibility later, though os.path.join is platform-aware\n    expected_final_path = os.path.join(request_data.output, project_name, 'html').replace('\\\\', '/')\n\n\n    # --- Build Command ---\n    command = [PYTHON_EXECUTABLE, MAIN_SCRIPT_PATH, request_data.repo_url]\n    # Use the derived or provided project name if specified (main.py will also derive if None)\n    if request_data.name:\n         command.extend([\"-n\", request_data.name])\n    if request_data.token:\n        command.extend([\"-t\", request_data.token])\n    command.extend([\"-o\", request_data.output]) # Pass base output dir\n    if request_data.max_size is not None:\n        command.extend([\"-s\", str(request_data.max_size)])\n    if request_data.include:\n        for pattern in request_data.include:\n            command.extend([\"-i\", pattern])\n    if request_data.exclude:\n        for pattern in request_data.exclude:\n            command.extend([\"-e\", pattern])\n\n    # --- Execute Script ---\n    logger.info(f\"Executing command: {' '.join(command)}\")\n\n    try:\n        result = subprocess.run(\n            command,\n            capture_output=True,\n            text=True,\n            check=True,\n            env=os.environ,\n            encoding='utf-8'\n        )\n\n        logger.info(f\"Script finished successfully.\")\n        logger.debug(f\"Script stdout:\\n{result.stdout}\")\n        if result.stderr:\n            logger.warning(f\"Script stderr:\\n{result.stderr}\")\n        \n        logger.info(f\"Final output directory: {expected_final_path}\")\n\n        # --- Success Response ---\n        # Return the *calculated* final path for redirection\n        return {\n            \"message\": \"Tutorial generation completed successfully.\",\n            # Key changed for clarity\n            \"final_output_directory\": expected_final_path, \n             # You could optionally still include stdout/stderr if needed for debugging\n            # \"script_stdout\": result.stdout,\n            # \"script_stderr\": result.stderr\n        }\n\n    except subprocess.CalledProcessError as e:\n        logger.error(f\"Script failed with exit code {e.returncode}\")\n        logger.error(f\"Script stdout:\\n{e.stdout}\")\n        logger.error(f\"Script stderr:\\n{e.stderr}\")\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail={\n                \"error\": \"Script execution failed.\",\n                \"details\": \"The tutorial generation script exited with an error.\",\n                \"return_code\": e.returncode,\n                \"script_stdout\": e.stdout,\n                \"script_stderr\": e.stderr\n            }\n        )\n    except FileNotFoundError:\n        error_msg = f\"Error: Script '{MAIN_SCRIPT_PATH}' or Python '{PYTHON_EXECUTABLE}' not found.\"\n        logger.error(error_msg)\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail={\"error\": \"Server configuration error.\", \"details\": error_msg}\n        )\n    except Exception as e:\n        logger.exception(\"An unexpected error occurred during script execution.\")\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail={\"error\": \"An unexpected internal server error occurred.\", \"details\": str(e)}\n        )\n\n# --- Root endpoint ---\n@app.get(\"/\", summary=\"Health Check\")\nasync def read_root():\n    return {\"message\": \"Tutorial Generator API is running.\"}\n\n# --- Uvicorn runner ---\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n\n--- File: 2 # converter/md_to_html.py ---\nfrom pathlib import Path\nimport markdown\nfrom md_mermaid import MermaidExtension\nimport sys\nfrom mermaid_extension import MermaidExtension as mermaid_ext\n\ndef convert_md_to_html(md_file_path, output_folder):\n    # Read the Markdown file\n    with open(md_file_path, \"r\", encoding=\"utf-8\") as md_file:\n        md_content = md_file.read()\n\n    # Initialize the Markdown object with the extensions\n    md = markdown.Markdown(extensions=[\n        # MermaidExtension(),  # Proper initialization of MermaidExtension\n        mermaid_ext(),\n        'fenced_code',\n        'codehilite',\n        'tables',\n        'toc'\n    ])\n\n    md_content = convert_hyperlink_to_html(md_content)\n    # Convert Markdown to HTML\n    html_content = md.convert(md_content)\n    \n    # wrap the HTML content in a basic HTML structure\n    html_content = f\"\"\"\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n<meta charset=\"UTF-8\">\n<title>{md_file_path}</title>\n        <script type=\"module\">\n            import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';\n            mermaid.initialize({{ startOnLoad: true }});\n        </script>\n       <style>\n            body {{\n                max-width: 800px;\n                margin: 2em auto;\n                font-family: system-ui, sans-serif;\n                line-height: 1.6;\n                color: #333;\n                padding: 0 1em;\n            }}\n            pre {{\n                background: #f6f8fa;\n                padding: 1em;\n                overflow-x: auto;\n            }}\n            code {{\n                background: #f0f0f0;\n                padding: 2px 4px;\n                border-radius: 4px;\n            }}\n            a {{\n                color: #0366d6;\n                text-decoration: none;\n            }}\n            a:hover {{\n                text-decoration: underline;\n            }}\n        </style>\n</head>\n<body>\n{html_content}\n</body>\n</html>\n\"\"\"   \n\n    # create the output file path\n    output_file_path = output_folder / md_file_path.relative_to(md_file_path.parent).with_suffix(\".html\")\n    \n    # write the HTML content to a file\n    with open(output_file_path, \"w\", encoding=\"utf-8\") as html_file:\n        html_file.write(html_content)\n        \n    print(f\"Converted {md_file_path} to {output_file_path}\")\n    \n\ndef convert_hyperlink_to_html(md_content):\n    # replace any hyperlinks with HTML links (.md to .html)\n    md_content = md_content.replace(\".md\", \".html\")\n    return md_content\n    \ndef convert_all_md_to_html(md_folder_path, output_folder=None):\n    md_folder_path = Path(md_folder_path)\n    output_folder = Path(output_folder) if output_folder else md_folder_path / \"html\"\n    output_folder.mkdir(parents=True, exist_ok=True)\n    \n    # iterate through all Markdown files in the folder\n    for md_file in md_folder_path.glob(\"**/*.md\"):\n        convert_md_to_html(md_file, output_folder)\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python md_to_html.py <md_folder_path> <output_folder>\")\n        sys.exit(1)\n    else:\n        md_folder_path = sys.argv[1]\n        output_folder = sys.argv[2] if len(sys.argv) > 2 else None\n        convert_all_md_to_html(md_folder_path, output_folder)\n\n--- File: 3 # flow.py ---\nfrom pocketflow import Flow\n# Import all node classes from nodes.py\nfrom nodes import (\n    FetchRepo,\n    IdentifyAbstractions,\n    AnalyzeRelationships,\n    OrderChapters,\n    WriteChapters,\n    CombineTutorial\n)\n\ndef create_tutorial_flow():\n    \"\"\"Creates and returns the codebase tutorial generation flow.\"\"\"\n\n    # Instantiate nodes\n    fetch_repo = FetchRepo()\n    identify_abstractions = IdentifyAbstractions(max_retries=3, wait=10)\n    analyze_relationships = AnalyzeRelationships(max_retries=3, wait=10)\n    order_chapters = OrderChapters(max_retries=3, wait=10)\n    write_chapters = WriteChapters(max_retries=3, wait=10) # This is a BatchNode\n    combine_tutorial = CombineTutorial()\n\n    # Connect nodes in sequence based on the design\n    fetch_repo >> identify_abstractions\n    identify_abstractions >> analyze_relationships\n    analyze_relationships >> order_chapters\n    order_chapters >> write_chapters\n    write_chapters >> combine_tutorial\n\n    # Create the flow starting with FetchRepo\n    tutorial_flow = Flow(start=fetch_repo)\n\n    return tutorial_flow\n\n--- File: 4 # main.py ---\nimport dotenv\nimport os\nimport argparse\n# Import the function that creates the flow\nfrom flow import create_tutorial_flow\n\ndotenv.load_dotenv()\n\n# Default file patterns\nDEFAULT_INCLUDE_PATTERNS = {\n    \"*.py\", \"*.js\", \"*.jsx\", \"*.ts\", \"*.tsx\", \"*.go\", \"*.java\", \"*.pyi\", \"*.pyx\", \n    \"*.c\", \"*.cc\", \"*.cpp\", \"*.h\", \"*.md\", \"*.rst\", \"Dockerfile\", \n    \"Makefile\", \"*.yaml\", \"*.yml\",\n}\n\nDEFAULT_EXCLUDE_PATTERNS = {\n    \"*test*\", \"tests/*\", \"docs/*\", \"examples/*\", \"v1/*\", \n    \"dist/*\", \"build/*\", \"experimental/*\", \"deprecated/*\", \n    \"legacy/*\", \".git/*\", \".github/*\", \".next/*\", \".vscode/*\", \"obj/*\", \"bin/*\", \"node_modules/*\", \"*.log\"\n}\n\n# --- Main Function ---\ndef main():\n    parser = argparse.ArgumentParser(description=\"Generate a tutorial for a GitHub codebase or local directory.\")\n    \n    # Create mutually exclusive group for source\n    source_group = parser.add_mutually_exclusive_group(required=True)\n    source_group.add_argument(\"--repo\", help=\"URL of the public GitHub repository.\")\n    source_group.add_argument(\"--dir\", help=\"Path to local directory.\")\n    \n    parser.add_argument(\"-n\", \"--name\", help=\"Project name (optional, derived from repo/directory if omitted).\")\n    parser.add_argument(\"-t\", \"--token\", help=\"GitHub personal access token (optional, reads from GITHUB_TOKEN env var if not provided).\")\n    parser.add_argument(\"-o\", \"--output\", default=\"output\", help=\"Base directory for output (default: ./output).\")\n    parser.add_argument(\"-i\", \"--include\", nargs=\"+\", help=\"Include file patterns (e.g. '*.py' '*.js'). Defaults to common code files if not specified.\")\n    parser.add_argument(\"-e\", \"--exclude\", nargs=\"+\", help=\"Exclude file patterns (e.g. 'tests/*' 'docs/*'). Defaults to test/build directories if not specified.\")\n    parser.add_argument(\"-s\", \"--max-size\", type=int, default=100000, help=\"Maximum file size in bytes (default: 100000, about 100KB).\")\n\n    args = parser.parse_args()\n\n    # Get GitHub token from argument or environment variable if using repo\n    github_token = None\n    if args.repo:\n        github_token = args.token or os.environ.get('GITHUB_TOKEN')\n        if not github_token:\n            print(\"Warning: No GitHub token provided. You might hit rate limits for public repositories.\")\n\n    # Initialize the shared dictionary with inputs\n    shared = {\n        \"repo_url\": args.repo,\n        \"local_dir\": args.dir,\n        \"project_name\": args.name, # Can be None, FetchRepo will derive it\n        \"github_token\": github_token,\n        \"output_dir\": args.output, # Base directory for CombineTutorial output\n\n        # Add include/exclude patterns and max file size\n        \"include_patterns\": set(args.include) if args.include else DEFAULT_INCLUDE_PATTERNS,\n        \"exclude_patterns\": set(args.exclude) if args.exclude else DEFAULT_EXCLUDE_PATTERNS,\n        \"max_file_size\": args.max_size,\n\n        # Outputs will be populated by the nodes\n        \"files\": [],\n        \"abstractions\": [],\n        \"relationships\": {},\n        \"chapter_order\": [],\n        \"chapters\": [],\n        \"final_output_dir\": None\n    }\n\n    print(f\"Starting tutorial generation for: {args.repo or args.dir}\")\n\n    # Create the flow instance\n    tutorial_flow = create_tutorial_flow()\n\n    # Run the flow\n    tutorial_flow.run(shared)\n    \nif __name__ == \"__main__\":\n    main()\n\n--- File: 5 # nodes.py ---\nimport os\nimport yaml\nfrom pocketflow import Node, BatchNode\nfrom utils.crawl_github_files import crawl_github_files\nfrom utils.call_llm import call_llm # Assuming you have this utility\nfrom utils.crawl_local_files import crawl_local_files\nfrom converter.md_to_html import convert_all_md_to_html\n\n\n# Helper to get content for specific file indices\ndef get_content_for_indices(files_data, indices):\n    content_map = {}\n    for i in indices:\n        if 0 <= i < len(files_data):\n            path, content = files_data[i]\n            content_map[f\"{i} # {path}\"] = content # Use index + path as key for context\n    return content_map\n\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"local_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        if not project_name:\n            # Basic name derivation from URL or directory\n            if repo_url:\n                project_name = repo_url.split('/')[-1].replace('.git', '')\n            else:\n                project_name = os.path.basename(os.path.abspath(local_dir))\n            shared[\"project_name\"] = project_name\n\n        # Get file patterns directly from shared\n        include_patterns = shared[\"include_patterns\"]\n        exclude_patterns = shared[\"exclude_patterns\"]\n        max_file_size = shared[\"max_file_size\"]\n\n        return {\n            \"repo_url\": repo_url,\n            \"local_dir\": local_dir,\n            \"token\": shared.get(\"github_token\"),\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns,\n            \"max_file_size\": max_file_size,\n            \"use_relative_paths\": True\n        }\n\n    def exec(self, prep_res):\n        if prep_res[\"repo_url\"]:\n            print(f\"Crawling repository: {prep_res['repo_url']}...\")\n            result = crawl_github_files(\n                repo_url=prep_res[\"repo_url\"],\n                token=prep_res[\"token\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        else:\n            print(f\"Crawling directory: {prep_res['local_dir']}...\")\n            result = crawl_local_files(\n                directory=prep_res[\"local_dir\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n            \n        # Convert dict to list of tuples: [(path, content), ...]\n        files_list = list(result.get(\"files\", {}).items())\n        print(f\"Fetched {len(files_list)} files.\")\n        return files_list\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"files\"] = exec_res # List of (path, content) tuples\n\nclass IdentifyAbstractions(Node):\n    def prep(self, shared):\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        \n        # Helper to create context from files, respecting limits (basic example)\n        def create_llm_context(files_data):\n            context = \"\"\n            file_info = [] # Store tuples of (index, path)\n            for i, (path, content) in enumerate(files_data):\n                entry = f\"--- File Index {i}: {path} ---\\n{content}\\n\\n\"\n                context += entry\n                file_info.append((i, path))\n\n            return context, file_info # file_info is list of (index, path)\n\n        context, file_info = create_llm_context(files_data)\n        # Format file info for the prompt (comment is just a hint for LLM)\n        file_listing_for_prompt = \"\\n\".join([f\"- {idx} # {path}\" for idx, path in file_info])\n        return context, file_listing_for_prompt, len(files_data), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, file_listing_for_prompt, file_count, project_name = prep_res  # Unpack project name\n        print(\"Identifying abstractions using LLM...\")\n        prompt = f\"\"\"\nFor the project `{project_name}`:\n\nCodebase Context:\n{context}\n\nAnalyze the codebase context.\nIdentify the top 5-10 core most important abstractions to help those new to the codebase.\n\nFor each abstraction, provide:\n1. A concise `name`.\n2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\nList of file indices and paths present in the context:\n{file_listing_for_prompt}\n\nFormat the output as a YAML list of dictionaries:\n\n```yaml\n- name: Query Processing\n  description: | \n    Explains what the abstraction does.\n    It's like a central dispatcher routing requests.\n  file_indices:\n    - 0 # path/to/file1.py\n    - 3 # path/to/related.py\n- name: Query Optimization\n  description: |\n    Another core concept, similar to a blueprint for objects.\n  file_indices:\n    - 5 # path/to/another.js\n# ... up to 10 abstractions\n```\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        abstractions = yaml.safe_load(yaml_str)\n\n        if not isinstance(abstractions, list):\n            raise ValueError(\"LLM Output is not a list\")\n\n        validated_abstractions = []\n        for item in abstractions:\n            if not isinstance(item, dict) or not all(k in item for k in [\"name\", \"description\", \"file_indices\"]):\n                raise ValueError(f\"Missing keys in abstraction item: {item}\")\n            if not isinstance(item[\"description\"], str):\n                 raise ValueError(f\"description is not a string in item: {item}\")\n            if not isinstance(item[\"file_indices\"], list):\n                 raise ValueError(f\"file_indices is not a list in item: {item}\")\n\n            # Validate indices\n            validated_indices = []\n            for idx_entry in item[\"file_indices\"]:\n                 try:\n                     if isinstance(idx_entry, int):\n                         idx = idx_entry\n                     elif isinstance(idx_entry, str) and '#' in idx_entry:\n                          idx = int(idx_entry.split('#')[0].strip())\n                     else:\n                          idx = int(str(idx_entry).strip())\n\n                     if not (0 <= idx < file_count):\n                         raise ValueError(f\"Invalid file index {idx} found in item {item['name']}. Max index is {file_count - 1}.\")\n                     validated_indices.append(idx)\n                 except (ValueError, TypeError):\n                      raise ValueError(f\"Could not parse index from entry: {idx_entry} in item {item['name']}\")\n\n            item[\"files\"] = sorted(list(set(validated_indices)))\n            # Store only the required fields\n            validated_abstractions.append({\n                \"name\": item[\"name\"],\n                \"description\": item[\"description\"],\n                \"files\": item[\"files\"]\n            })\n\n        print(f\"Identified {len(validated_abstractions)} abstractions.\")\n        return validated_abstractions\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"abstractions\"] = exec_res # List of {\"name\": str, \"description\": str, \"files\": [int]}\n\nclass AnalyzeRelationships(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"] # Now contains 'files' list of indices\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n\n        # Create context with abstraction names, indices, descriptions, and relevant file snippets\n        context = \"Identified Abstractions:\\n\"\n        all_relevant_indices = set()\n        abstraction_info_for_prompt = []\n        for i, abstr in enumerate(abstractions):\n            # Use 'files' which contains indices directly\n            file_indices_str = \", \".join(map(str, abstr['files']))\n            info_line = f\"- Index {i}: {abstr['name']} (Relevant file indices: [{file_indices_str}])\\n  Description: {abstr['description']}\"\n            context += info_line + \"\\n\"\n            abstraction_info_for_prompt.append(f\"{i} # {abstr['name']}\")\n            all_relevant_indices.update(abstr['files'])\n\n        context += \"\\nRelevant File Snippets (Referenced by Index and Path):\\n\"\n        # Get content for relevant files using helper\n        relevant_files_content_map = get_content_for_indices(\n            files_data,\n            sorted(list(all_relevant_indices))\n        )\n        # Format file content for context\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path} ---\\n{content}\"\n            for idx_path, content in relevant_files_content_map.items()\n        )\n        context += file_context_str\n\n        return context, \"\\n\".join(abstraction_info_for_prompt), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, abstraction_listing, project_name = prep_res  # Unpack project name\n        print(\"Analyzing relationships using LLM...\")\n        prompt = f\"\"\"\nBased on the following abstractions and relevant code snippets from the project `{project_name}`:\n\nList of Abstraction Indices and Names:\n{abstraction_listing}\n\nContext (Abstractions, Descriptions, Code):\n{context}\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        relationships_data = yaml.safe_load(yaml_str)\n\n        if not isinstance(relationships_data, dict) or not all(k in relationships_data for k in [\"summary\", \"relationships\"]):\n            raise ValueError(\"LLM output is not a dict or missing keys ('summary', 'relationships')\")\n        if not isinstance(relationships_data[\"summary\"], str):\n             raise ValueError(\"summary is not a string\")\n        if not isinstance(relationships_data[\"relationships\"], list):\n             raise ValueError(\"relationships is not a list\")\n\n        # Validate relationships structure\n        validated_relationships = []\n        num_abstractions = len(abstraction_listing.split('\\n'))\n        for rel in relationships_data[\"relationships\"]:\n             # Check for 'label' key\n             if not isinstance(rel, dict) or not all(k in rel for k in [\"from_abstraction\", \"to_abstraction\", \"label\"]):\n                  raise ValueError(f\"Missing keys (expected from_abstraction, to_abstraction, label) in relationship item: {rel}\")\n             # Validate 'label' is a string\n             if not isinstance(rel[\"label\"], str):\n                  raise ValueError(f\"Relationship label is not a string: {rel}\")\n\n             # Validate indices\n             try:\n                 from_idx = int(str(rel[\"from_abstraction\"]).split('#')[0].strip())\n                 to_idx = int(str(rel[\"to_abstraction\"]).split('#')[0].strip())\n                 if not (0 <= from_idx < num_abstractions and 0 <= to_idx < num_abstractions):\n                      raise ValueError(f\"Invalid index in relationship: from={from_idx}, to={to_idx}. Max index is {num_abstractions-1}.\")\n                 validated_relationships.append({\n                     \"from\": from_idx,\n                     \"to\": to_idx,\n                     \"label\": rel[\"label\"] \n                 })\n             except (ValueError, TypeError):\n                  raise ValueError(f\"Could not parse indices from relationship: {rel}\")\n\n        print(\"Generated project summary and relationship details.\")\n        return {\n            \"summary\": relationships_data[\"summary\"],\n            \"details\": validated_relationships # Store validated, index-based relationships\n        }\n\n\n    def post(self, shared, prep_res, exec_res):\n        # Structure is now {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        shared[\"relationships\"] = exec_res\n\nclass OrderChapters(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"]\n        relationships = shared[\"relationships\"]\n        project_name = shared[\"project_name\"]  # Get project name\n\n        # Prepare context for the LLM\n        abstraction_info_for_prompt = []\n        for i, a in enumerate(abstractions):\n            abstraction_info_for_prompt.append(f\"- {i} # {a['name']}\")\n        abstraction_listing = \"\\n\".join(abstraction_info_for_prompt)\n\n        context = f\"Project Summary:\\n{relationships['summary']}\\n\\n\"\n        context += \"Relationships (Indices refer to abstractions above):\\n\"\n        for rel in relationships['details']:\n             from_name = abstractions[rel['from']]['name']\n             to_name = abstractions[rel['to']]['name']\n             # Use 'label' instead of 'desc'\n             context += f\"- From {rel['from']} ({from_name}) to {rel['to']} ({to_name}): {rel['label']}\\n\"\n\n        return abstraction_listing, context, len(abstractions), project_name\n\n    def exec(self, prep_res):\n        abstraction_listing, context, num_abstractions, project_name = prep_res\n        print(\"Determining chapter order using LLM...\")\n        prompt = f\"\"\"\nGiven the following project abstractions and their relationships for the project ```` {project_name} ````:\n\nAbstractions (Index # Name):\n{abstraction_listing}\n\nContext about relationships and project summary:\n{context}\n\nIf you are going to make a tutorial for ```` {project_name} ````, what is the best order to explain these abstractions, from first to last?\nIdeally, first explain those that are the most important or foundational, perhaps user-facing concepts or entry points. Then move to more detailed, lower-level implementation details or supporting concepts.\n\nOutput the ordered list of abstraction indices, including the name in a comment for clarity. Use the format `idx # AbstractionName`.\n\n```yaml\n- 2 # FoundationalConcept\n- 0 # CoreClassA\n- 1 # CoreClassB (uses CoreClassA)\n- ...\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        # Rely on Node's built-in retry/fallback\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        ordered_indices_raw = yaml.safe_load(yaml_str)\n\n        if not isinstance(ordered_indices_raw, list):\n            raise ValueError(\"LLM output is not a list\")\n\n        ordered_indices = []\n        seen_indices = set()\n        for entry in ordered_indices_raw:\n            try:\n                 if isinstance(entry, int):\n                     idx = entry\n                 elif isinstance(entry, str) and '#' in entry:\n                      idx = int(entry.split('#')[0].strip())\n                 else:\n                      idx = int(str(entry).strip())\n\n                 if not (0 <= idx < num_abstractions):\n                      raise ValueError(f\"Invalid index {idx} in ordered list. Max index is {num_abstractions-1}.\")\n                 if idx in seen_indices:\n                     raise ValueError(f\"Duplicate index {idx} found in ordered list.\")\n                 ordered_indices.append(idx)\n                 seen_indices.add(idx)\n\n            except (ValueError, TypeError):\n                 raise ValueError(f\"Could not parse index from ordered list entry: {entry}\")\n\n        # Check if all abstractions are included\n        if len(ordered_indices) != num_abstractions:\n             raise ValueError(f\"Ordered list length ({len(ordered_indices)}) does not match number of abstractions ({num_abstractions}). Missing indices: {set(range(num_abstractions)) - seen_indices}\")\n\n        print(f\"Determined chapter order (indices): {ordered_indices}\")\n        return ordered_indices # Return the list of indices\n\n    def post(self, shared, prep_res, exec_res):\n        # exec_res is already the list of ordered indices\n        shared[\"chapter_order\"] = exec_res # List of indices\n\nclass WriteChapters(BatchNode):\n    def prep(self, shared):\n        chapter_order = shared[\"chapter_order\"] # List of indices\n        abstractions = shared[\"abstractions\"]   # List of dicts, now using 'files' with indices\n        files_data = shared[\"files\"]\n        # Get already written chapters to provide context\n        # We store them temporarily during the batch run, not in shared memory yet\n        # The 'previous_chapters_summary' will be built progressively in the exec context\n        self.chapters_written_so_far = [] # Use instance variable for temporary storage across exec calls\n\n        # Create a complete list of all chapters\n        all_chapters = []\n        chapter_filenames = {} # Store chapter filename mapping for linking\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                chapter_num = i + 1\n                chapter_name = abstractions[abstraction_index][\"name\"]\n                # Create safe filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in chapter_name).lower()\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                # Format with link\n                all_chapters.append(f\"{chapter_num}. [{chapter_name}]({filename})\")\n                # Store mapping of chapter index to filename for linking\n                chapter_filenames[abstraction_index] = {\"num\": chapter_num, \"name\": chapter_name, \"filename\": filename}\n        \n        # Create a formatted string with all chapters\n        full_chapter_listing = \"\\n\".join(all_chapters)\n\n        items_to_process = []\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                abstraction_details = abstractions[abstraction_index]\n                # Use 'files' (list of indices) directly\n                related_file_indices = abstraction_details.get(\"files\", [])\n                # Get content using helper, passing indices\n                related_files_content_map = get_content_for_indices(files_data, related_file_indices)\n                \n                # Get previous chapter info for transitions\n                prev_chapter = None\n                if i > 0:\n                    prev_idx = chapter_order[i-1]\n                    prev_chapter = chapter_filenames[prev_idx]\n                \n                # Get next chapter info for transitions\n                next_chapter = None\n                if i < len(chapter_order) - 1:\n                    next_idx = chapter_order[i+1]\n                    next_chapter = chapter_filenames[next_idx]\n\n                items_to_process.append({\n                    \"chapter_num\": i + 1,\n                    \"abstraction_index\": abstraction_index,\n                    \"abstraction_details\": abstraction_details,\n                    \"related_files_content_map\": related_files_content_map,\n                    \"project_name\": shared[\"project_name\"],  # Add project name\n                    \"full_chapter_listing\": full_chapter_listing,  # Add the full chapter listing\n                    \"chapter_filenames\": chapter_filenames,  # Add chapter filenames mapping\n                    \"prev_chapter\": prev_chapter,  # Add previous chapter info\n                    \"next_chapter\": next_chapter,  # Add next chapter info\n                    # previous_chapters_summary will be added dynamically in exec\n                })\n            else:\n                print(f\"Warning: Invalid abstraction index {abstraction_index} in chapter_order. Skipping.\")\n\n        print(f\"Preparing to write {len(items_to_process)} chapters...\")\n        return items_to_process # Iterable for BatchNode\n\n    def exec(self, item):\n        # This runs for each item prepared above\n        abstraction_name = item[\"abstraction_details\"][\"name\"]\n        chapter_num = item[\"chapter_num\"]\n        project_name = item.get(\"project_name\")  # Get from item\n        print(f\"Writing chapter {chapter_num} for: {abstraction_name} using LLM...\")\n\n        # Prepare file context string from the map\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path.split('# ')[1] if '# ' in idx_path else idx_path} ---\\n{content}\"\n            for idx_path, content in item[\"related_files_content_map\"].items()\n        )\n\n        # Get summary of chapters written *before* this one\n        # Use the temporary instance variable\n        previous_chapters_summary = \"\\n---\\n\".join(self.chapters_written_so_far)\n\n\n        prompt = f\"\"\"\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `{project_name}` about the concept: \"{abstraction_name}\". This is Chapter {chapter_num}.\n\nConcept Details:\n- Description:\n{item[\"abstraction_details\"][\"description\"]}\n\nComplete Tutorial Structure:\n{item[\"full_chapter_listing\"]}\n\nContext from previous chapters (summary):\n{previous_chapters_summary if previous_chapters_summary else \"This is the first chapter.\"}\n\nRelevant Code Snippets:\n{file_context_str if file_context_str else \"No specific code snippets provided for this abstraction.\"}\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter {chapter_num}: {abstraction_name}`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n\"\"\"\n        chapter_content = call_llm(prompt)\n        # Basic validation/cleanup\n        actual_heading = f\"# Chapter {chapter_num}: {abstraction_name}\"\n        if not chapter_content.strip().startswith(f\"# Chapter {chapter_num}\"):\n             # Add heading if missing or incorrect, trying to preserve content\n             lines = chapter_content.strip().split('\\n')\n             if lines and lines[0].strip().startswith(\"#\"): # If there's some heading, replace it\n                 lines[0] = actual_heading\n                 chapter_content = \"\\n\".join(lines)\n             else: # Otherwise, prepend it\n                 chapter_content = f\"{actual_heading}\\n\\n{chapter_content}\"\n\n        # Add the generated content to our temporary list for the next iteration's context\n        self.chapters_written_so_far.append(chapter_content)\n\n        return chapter_content # Return the Markdown string\n\n    def post(self, shared, prep_res, exec_res_list):\n        # exec_res_list contains the generated Markdown for each chapter, in order\n        shared[\"chapters\"] = exec_res_list\n        # Clean up the temporary instance variable\n        del self.chapters_written_so_far\n        print(f\"Finished writing {len(exec_res_list)} chapters.\")\n\nclass CombineTutorial(Node):\n    def prep(self, shared):\n        project_name = shared[\"project_name\"]\n        output_base_dir = shared.get(\"output_dir\", \"output\") # Default output dir\n        output_path = os.path.join(output_base_dir, project_name)\n        repo_url = shared[\"repo_url\"]  # Get the repository URL\n\n        # Use 'label' from relationships_data['details']\n        relationships_data = shared[\"relationships\"] # {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        chapter_order = shared[\"chapter_order\"] # indices\n        abstractions = shared[\"abstractions\"]   # list of dicts\n        chapters_content = shared[\"chapters\"]   # list of strings\n\n        # --- Generate Mermaid Diagram ---\n        mermaid_lines = [\"flowchart TD\"]\n        # Add nodes for each abstraction\n        for i, abstr in enumerate(abstractions):\n            # Sanitize name for Mermaid ID and label\n            node_id = f\"A{i}\"\n            sanitized_name = abstr['name'].replace('\"', '')\n            node_label = sanitized_name # Using sanitized name only, no index\n            mermaid_lines.append(f'    {node_id}[\"{node_label}\"]')\n        # Add edges for relationships using 'label'\n        for rel in relationships_data['details']:\n            from_node_id = f\"A{rel['from']}\"\n            to_node_id = f\"A{rel['to']}\"\n            # Sanitize 'label' for edge label\n            edge_label = rel['label'].replace('\"', '').replace('\\n', ' ') # Basic sanitization\n            # Limit edge label length for readability (optional, but good for diagrams)\n            max_label_len = 30 # Make it shorter for labels\n            if len(edge_label) > max_label_len:\n                edge_label = edge_label[:max_label_len-3] + \"...\"\n            mermaid_lines.append(f'    {from_node_id} -- \"{edge_label}\" --> {to_node_id}')\n\n        mermaid_diagram = \"\\n\".join(mermaid_lines)\n        # --- End Mermaid ---\n\n\n        # Prepare index.md content\n        index_content = f\"# Tutorial: {project_name}\\n\\n\"\n        index_content += f\"{relationships_data['summary']}\\n\\n\"\n        index_content += f\"**Source Repository:** [{repo_url}]({repo_url})\\n\\n\"\n\n        # Add Mermaid diagram for relationships\n        index_content += \"```mermaid\\n\"\n        index_content += mermaid_diagram + \"\\n\"\n        index_content += \"```\\n\\n\"\n\n        index_content += \"## Chapters\\n\\n\"\n\n        chapter_files = []\n        # Generate chapter links based on the determined order\n        for i, abstraction_index in enumerate(chapter_order):\n            # Ensure index is valid and we have content for it\n            if 0 <= abstraction_index < len(abstractions) and i < len(chapters_content):\n                abstraction_name = abstractions[abstraction_index][\"name\"]\n                # Sanitize name for filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in abstraction_name).lower()\n                # Use chapter number (i+1) for ordering filename\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                index_content += f\"{i+1}. [{abstraction_name}]({filename})\\n\"\n                \n                # Add attribution to chapter content\n                chapter_content = chapters_content[i]\n                if not chapter_content.endswith(\"\\n\\n\"):\n                    chapter_content += \"\\n\\n\"\n                chapter_content += \"---\\n\\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\"\n                \n                # Store filename and corresponding content\n                chapter_files.append({\"filename\": filename, \"content\": chapter_content})\n            else:\n                 print(f\"Warning: Mismatch between chapter order, abstractions, or content at index {i} (abstraction index {abstraction_index}). Skipping file generation for this entry.\")\n\n        # Add attribution to index content\n        index_content += \"\\n\\n---\\n\\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\"\n\n        return {\n            \"output_path\": output_path,\n            \"index_content\": index_content,\n            \"chapter_files\": chapter_files # List of {\"filename\": str, \"content\": str}\n        }\n\n    def exec(self, prep_res):\n        output_path = prep_res[\"output_path\"]\n        index_content = prep_res[\"index_content\"]\n        chapter_files = prep_res[\"chapter_files\"]\n\n        print(f\"Combining tutorial into directory: {output_path}\")\n        # Rely on Node's built-in retry/fallback\n        os.makedirs(output_path, exist_ok=True)\n\n        # Write index.md\n        index_filepath = os.path.join(output_path, \"index.md\")\n        with open(index_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(index_content)\n        print(f\"  - Wrote {index_filepath}\")\n\n        # Write chapter files\n        for chapter_info in chapter_files:\n            chapter_filepath = os.path.join(output_path, chapter_info[\"filename\"])\n            with open(chapter_filepath, \"w\", encoding=\"utf-8\") as f:\n                f.write(chapter_info[\"content\"])\n            print(f\"  - Wrote {chapter_filepath}\")\n\n        return output_path # Return the final path\n\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"final_output_dir\"] = exec_res # Store the output path\n        print(f\"\\nTutorial generation complete! Files are in: {exec_res}\")\n        convert_all_md_to_html(exec_res) # Convert all Markdown files to HTML\n        \n\n--- File: 7 # utils/call_llm.py ---\nfrom google import genai\nimport os\nimport logging\nimport json\nfrom datetime import datetime\n\n# Configure logging\nlog_directory = os.getenv(\"LOG_DIR\", \"logs\")\nos.makedirs(log_directory, exist_ok=True)\nlog_file = os.path.join(log_directory, f\"llm_calls_{datetime.now().strftime('%Y%m%d')}.log\")\n\n# Set up logger\nlogger = logging.getLogger(\"llm_logger\")\nlogger.setLevel(logging.INFO)\nlogger.propagate = False  # Prevent propagation to root logger\nfile_handler = logging.FileHandler(log_file)\nfile_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\nlogger.addHandler(file_handler)\n\n# Simple cache configuration\ncache_file = \"llm_cache.json\"\n\n# By default, we Google Gemini 2.5 pro, as it shows great performance for code understanding\ndef call_llm(prompt: str, use_cache: bool = True) -> str:\n    # Log the prompt\n    logger.info(f\"PROMPT: {prompt}\")\n    \n    # Check cache if enabled\n    if use_cache:\n        # Load cache from disk\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, 'r') as f:\n                    cache = json.load(f)\n            except:\n                logger.warning(f\"Failed to load cache, starting with empty cache\")\n        \n        # Return from cache if exists\n        if prompt in cache:\n            logger.info(f\"RESPONSE: {cache[prompt]}\")\n            return cache[prompt]\n    \n    # Call the LLM if not in cache or cache disabled\n    client = genai.Client(\n        vertexai=True, \n        # TODO: change to your own project id and location\n        project=os.getenv(\"GEMINI_PROJECT_ID\", \"your-project-id\"),\n        location=os.getenv(\"GEMINI_LOCATION\", \"us-central1\")\n    )\n    # You can comment the previous line and use the AI Studio key instead:\n    # client = genai.Client(\n    #     api_key=os.getenv(\"GEMINI_API_KEY\", \"your-api_key\"),\n    # )\n    model = os.getenv(\"GEMINI_MODEL\", \"gemini-2.5-pro-exp-03-25\")\n    response = client.models.generate_content(\n        model=model,\n        contents=[prompt]\n    )\n    response_text = response.text\n    \n    # Log the response\n    logger.info(f\"RESPONSE: {response_text}\")\n    \n    # Update cache if enabled\n    if use_cache:\n        # Load cache again to avoid overwrites\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, 'r') as f:\n                    cache = json.load(f)\n            except:\n                pass\n        \n        # Add to cache and save\n        cache[prompt] = response_text\n        try:\n            with open(cache_file, 'w') as f:\n                json.dump(cache, f)\n        except Exception as e:\n            logger.error(f\"Failed to save cache: {e}\")\n    \n    return response_text\n\n# # Use Anthropic Claude 3.7 Sonnet Extended Thinking\n# def call_llm(prompt, use_cache: bool = True):\n#     from anthropic import Anthropic\n#     client = Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\", \"your-api-key\"))\n#     response = client.messages.create(\n#         model=\"claude-3-7-sonnet-20250219\",\n#         max_tokens=21000,\n#         thinking={\n#             \"type\": \"enabled\",\n#             \"budget_tokens\": 20000\n#         },\n#         messages=[\n#             {\"role\": \"user\", \"content\": prompt}\n#         ]\n#     )\n#     return response.content[1].text\n\n# # Use OpenAI o1\n# def call_llm(prompt, use_cache: bool = True):    \n#     from openai import OpenAI\n#     client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n#     r = client.chat.completions.create(\n#         model=\"o1\",\n#         messages=[{\"role\": \"user\", \"content\": prompt}],\n#         response_format={\n#             \"type\": \"text\"\n#         },\n#         reasoning_effort=\"medium\",\n#         store=False\n#     )\n#     return r.choices[0].message.content\n\nif __name__ == \"__main__\":\n    test_prompt = \"Hello, how are you?\"\n    \n    # First call - should hit the API\n    print(\"Making call...\")\n    response1 = call_llm(test_prompt, use_cache=False)\n    print(f\"Response: {response1}\")\n    \n\n\n--- File: 8 # utils/crawl_github_files.py ---\nimport requests\nimport base64\nimport os\nimport tempfile\nimport git\nimport time\nimport fnmatch\nfrom typing import Union, Set, List, Dict, Tuple, Any\nfrom urllib.parse import urlparse\n\ndef crawl_github_files(\n    repo_url, \n    token=None, \n    max_file_size: int = 1 * 1024 * 1024,  # 1 MB\n    use_relative_paths: bool = False,\n    include_patterns: Union[str, Set[str]] = None,\n    exclude_patterns: Union[str, Set[str]] = None\n):\n    \"\"\"\n    Crawl files from a specific path in a GitHub repository at a specific commit.\n\n    Args:\n        repo_url (str): URL of the GitHub repository with specific path and commit\n                        (e.g., 'https://github.com/microsoft/autogen/tree/e45a15766746d95f8cfaaa705b0371267bec812e/python/packages/autogen-core/src/autogen_core')\n        token (str, optional): **GitHub personal access token.**\n            - **Required for private repositories.**\n            - **Recommended for public repos to avoid rate limits.**\n            - Can be passed explicitly or set via the `GITHUB_TOKEN` environment variable.\n        max_file_size (int, optional): Maximum file size in bytes to download (default: 1 MB)\n        use_relative_paths (bool, optional): If True, file paths will be relative to the specified subdirectory\n        include_patterns (str or set of str, optional): Pattern or set of patterns specifying which files to include (e.g., \"*.py\", {\"*.md\", \"*.txt\"}).\n                                                       If None, all files are included.\n        exclude_patterns (str or set of str, optional): Pattern or set of patterns specifying which files to exclude.\n                                                       If None, no files are excluded.\n\n    Returns:\n        dict: Dictionary with files and statistics\n    \"\"\"\n    # Convert single pattern to set\n    if include_patterns and isinstance(include_patterns, str):\n        include_patterns = {include_patterns}\n    if exclude_patterns and isinstance(exclude_patterns, str):\n        exclude_patterns = {exclude_patterns}\n\n    def should_include_file(file_path: str, file_name: str) -> bool:\n        \"\"\"Determine if a file should be included based on patterns\"\"\"\n        # If no include patterns are specified, include all files\n        if not include_patterns:\n            include_file = True\n        else:\n            # Check if file matches any include pattern\n            include_file = any(fnmatch.fnmatch(file_name, pattern) for pattern in include_patterns)\n\n        # If exclude patterns are specified, check if file should be excluded\n        if exclude_patterns and include_file:\n            # Exclude if file matches any exclude pattern\n            exclude_file = any(fnmatch.fnmatch(file_path, pattern) for pattern in exclude_patterns)\n            return not exclude_file\n\n        return include_file\n\n    # Detect SSH URL (git@ or .git suffix)\n    is_ssh_url = repo_url.startswith(\"git@\") or repo_url.endswith(\".git\")\n\n    if is_ssh_url:\n        # Clone repo via SSH to temp dir\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            print(f\"Cloning SSH repo {repo_url} to temp dir {tmpdirname} ...\")\n            try:\n                repo = git.Repo.clone_from(repo_url, tmpdirname)\n            except Exception as e:\n                print(f\"Error cloning repo: {e}\")\n                return {\"files\": {}, \"stats\": {\"error\": str(e)}}\n\n            # Attempt to checkout specific commit/branch if in URL\n            # Parse ref and subdir from SSH URL? SSH URLs don't have branch info embedded\n            # So rely on default branch, or user can checkout manually later\n            # Optionally, user can pass ref explicitly in future API\n\n            # Walk directory\n            files = {}\n            skipped_files = []\n\n            for root, dirs, filenames in os.walk(tmpdirname):\n                for filename in filenames:\n                    abs_path = os.path.join(root, filename)\n                    rel_path = os.path.relpath(abs_path, tmpdirname)\n\n                    # Check file size\n                    try:\n                        file_size = os.path.getsize(abs_path)\n                    except OSError:\n                        continue\n\n                    if file_size > max_file_size:\n                        skipped_files.append((rel_path, file_size))\n                        print(f\"Skipping {rel_path}: size {file_size} exceeds limit {max_file_size}\")\n                        continue\n\n                    # Check include/exclude patterns\n                    if not should_include_file(rel_path, filename):\n                        print(f\"Skipping {rel_path}: does not match include/exclude patterns\")\n                        continue\n\n                    # Read content\n                    try:\n                        with open(abs_path, \"r\", encoding=\"utf-8\") as f:\n                            content = f.read()\n                        files[rel_path] = content\n                        print(f\"Added {rel_path} ({file_size} bytes)\")\n                    except Exception as e:\n                        print(f\"Failed to read {rel_path}: {e}\")\n\n            return {\n                \"files\": files,\n                \"stats\": {\n                    \"downloaded_count\": len(files),\n                    \"skipped_count\": len(skipped_files),\n                    \"skipped_files\": skipped_files,\n                    \"base_path\": None,\n                    \"include_patterns\": include_patterns,\n                    \"exclude_patterns\": exclude_patterns,\n                    \"source\": \"ssh_clone\"\n                }\n            }\n\n    # Parse GitHub URL to extract owner, repo, commit/branch, and path\n    parsed_url = urlparse(repo_url)\n    path_parts = parsed_url.path.strip('/').split('/')\n    \n    if len(path_parts) < 2:\n        raise ValueError(f\"Invalid GitHub URL: {repo_url}\")\n    \n    # Extract the basic components\n    owner = path_parts[0]\n    repo = path_parts[1]\n    \n    # Check if URL contains a specific branch/commit\n    if 'tree' in path_parts:\n        tree_index = path_parts.index('tree')\n        ref = path_parts[tree_index + 1]\n        # Combine all parts after the ref as the path\n        path_start = tree_index + 2\n        specific_path = '/'.join(path_parts[path_start:]) if path_start < len(path_parts) else \"\"\n    else:\n        ref = \"main\"  # Default branch\n        specific_path = \"\"\n    \n    # Setup for GitHub API\n    headers = {\"Accept\": \"application/vnd.github.v3+json\"}\n    if token:\n        headers[\"Authorization\"] = f\"token {token}\"\n    \n    # Dictionary to store path -> content mapping\n    files = {}\n    skipped_files = []\n    \n    def should_include_file(file_path: str, file_name: str) -> bool:\n        \"\"\"Determine if a file should be included based on patterns\"\"\"\n        # If no include patterns are specified, include all files\n        if not include_patterns:\n            include_file = True\n        else:\n            # Check if file matches any include pattern\n            include_file = any(fnmatch.fnmatch(file_name, pattern) for pattern in include_patterns)\n        \n        # If exclude patterns are specified, check if file should be excluded\n        if exclude_patterns and include_file:\n            # Exclude if file matches any exclude pattern\n            exclude_file = any(fnmatch.fnmatch(file_path, pattern) for pattern in exclude_patterns)\n            return not exclude_file\n        \n        return include_file\n    \n    def fetch_contents(path):\n        \"\"\"Fetch contents of the repository at a specific path and commit\"\"\"\n        url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{path}\"\n        params = {\"ref\": ref}\n        \n        response = requests.get(url, headers=headers, params=params)\n        \n        if response.status_code == 403 and 'rate limit exceeded' in response.text.lower():\n            reset_time = int(response.headers.get('X-RateLimit-Reset', 0))\n            wait_time = max(reset_time - time.time(), 0) + 1\n            print(f\"Rate limit exceeded. Waiting for {wait_time:.0f} seconds...\")\n            time.sleep(wait_time)\n            return fetch_contents(path)\n            \n        if response.status_code == 404:\n            if not token:\n                print(f\"Error 404: Repository not found or is private.\\n\"\n                      f\"If this is a private repository, please provide a valid GitHub token via the 'token' argument or set the GITHUB_TOKEN environment variable.\")\n            else:\n                print(f\"Error 404: Path '{path}' not found in repository or insufficient permissions with the provided token.\\n\"\n                      f\"Please verify the token has access to this repository and the path exists.\")\n            return\n            \n        if response.status_code != 200:\n            print(f\"Error fetching {path}: {response.status_code} - {response.text}\")\n            return\n        \n        contents = response.json()\n        \n        # Handle both single file and directory responses\n        if not isinstance(contents, list):\n            contents = [contents]\n        \n        for item in contents:\n            item_path = item[\"path\"]\n            \n            # Calculate relative path if requested\n            if use_relative_paths and specific_path:\n                # Make sure the path is relative to the specified subdirectory\n                if item_path.startswith(specific_path):\n                    rel_path = item_path[len(specific_path):].lstrip('/')\n                else:\n                    rel_path = item_path\n            else:\n                rel_path = item_path\n            \n            if item[\"type\"] == \"file\":\n                # Check if file should be included based on patterns\n                if not should_include_file(rel_path, item[\"name\"]):\n                    print(f\"Skipping {rel_path}: Does not match include/exclude patterns\")\n                    continue\n                \n                # Check file size if available\n                file_size = item.get(\"size\", 0)\n                if file_size > max_file_size:\n                    skipped_files.append((item_path, file_size))\n                    print(f\"Skipping {rel_path}: File size ({file_size} bytes) exceeds limit ({max_file_size} bytes)\")\n                    continue\n                \n                # For files, get raw content\n                if \"download_url\" in item and item[\"download_url\"]:\n                    file_url = item[\"download_url\"]\n                    file_response = requests.get(file_url, headers=headers)\n                    \n                    # Final size check in case content-length header is available but differs from metadata\n                    content_length = int(file_response.headers.get('content-length', 0))\n                    if content_length > max_file_size:\n                        skipped_files.append((item_path, content_length))\n                        print(f\"Skipping {rel_path}: Content length ({content_length} bytes) exceeds limit ({max_file_size} bytes)\")\n                        continue\n                        \n                    if file_response.status_code == 200:\n                        files[rel_path] = file_response.text\n                        print(f\"Downloaded: {rel_path} ({file_size} bytes) \")\n                    else:\n                        print(f\"Failed to download {rel_path}: {file_response.status_code}\")\n                else:\n                    # Alternative method if download_url is not available\n                    content_response = requests.get(item[\"url\"], headers=headers)\n                    if content_response.status_code == 200:\n                        content_data = content_response.json()\n                        if content_data.get(\"encoding\") == \"base64\" and \"content\" in content_data:\n                            # Check size of base64 content before decoding\n                            if len(content_data[\"content\"]) * 0.75 > max_file_size:  # Approximate size calculation\n                                estimated_size = int(len(content_data[\"content\"]) * 0.75)\n                                skipped_files.append((item_path, estimated_size))\n                                print(f\"Skipping {rel_path}: Encoded content exceeds size limit\")\n                                continue\n                                \n                            file_content = base64.b64decode(content_data[\"content\"]).decode('utf-8')\n                            files[rel_path] = file_content\n                            print(f\"Downloaded: {rel_path} ({file_size} bytes)\")\n                        else:\n                            print(f\"Unexpected content format for {rel_path}\")\n                    else:\n                        print(f\"Failed to get content for {rel_path}: {content_response.status_code}\")\n            \n            elif item[\"type\"] == \"dir\":\n                # Recursively process subdirectories\n                fetch_contents(item_path)\n    \n    # Start crawling from the specified path\n    fetch_contents(specific_path)\n    \n    return {\n        \"files\": files,\n        \"stats\": {\n            \"downloaded_count\": len(files),\n            \"skipped_count\": len(skipped_files),\n            \"skipped_files\": skipped_files,\n            \"base_path\": specific_path if use_relative_paths else None,\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns\n        }\n    }\n\n# Example usage\nif __name__ == \"__main__\":\n    # Get token from environment variable (recommended for private repos)\n    github_token = os.environ.get(\"GITHUB_TOKEN\")\n    if not github_token:\n        print(\"Warning: No GitHub token found in environment variable 'GITHUB_TOKEN'.\\n\"\n              \"Private repositories will not be accessible without a token.\\n\"\n              \"To access private repos, set the environment variable or pass the token explicitly.\")\n    \n    repo_url = \"https://github.com/pydantic/pydantic/tree/6c38dc93f40a47f4d1350adca9ec0d72502e223f/pydantic\"\n    \n    # Example: Get Python and Markdown files, but exclude test files\n    result = crawl_github_files(\n        repo_url, \n        token=github_token,\n        max_file_size=1 * 1024 * 1024,  # 1 MB in bytes\n        use_relative_paths=True,  # Enable relative paths\n        include_patterns={\"*.py\", \"*.md\"},  # Include Python and Markdown files\n    )\n    \n    files = result[\"files\"]\n    stats = result[\"stats\"]\n    \n    print(f\"\\nDownloaded {stats['downloaded_count']} files.\")\n    print(f\"Skipped {stats['skipped_count']} files due to size limits or patterns.\")\n    print(f\"Base path for relative paths: {stats['base_path']}\")\n    print(f\"Include patterns: {stats['include_patterns']}\")\n    print(f\"Exclude patterns: {stats['exclude_patterns']}\")\n    \n    # Display all file paths in the dictionary\n    print(\"\\nFiles in dictionary:\")\n    for file_path in sorted(files.keys()):\n        print(f\"  {file_path}\")\n    \n    # Example: accessing content of a specific file\n    if files:\n        sample_file = next(iter(files))\n        print(f\"\\nSample file: {sample_file}\")\n        print(f\"Content preview: {files[sample_file][:200]}...\")\n\n--- File: 9 # utils/crawl_local_files.py ---\nimport os\nimport fnmatch\n\ndef crawl_local_files(directory, include_patterns=None, exclude_patterns=None, max_file_size=None, use_relative_paths=True):\n    \"\"\"\n    Crawl files in a local directory with similar interface as crawl_github_files.\n    \n    Args:\n        directory (str): Path to local directory\n        include_patterns (set): File patterns to include (e.g. {\"*.py\", \"*.js\"})\n        exclude_patterns (set): File patterns to exclude (e.g. {\"tests/*\"})\n        max_file_size (int): Maximum file size in bytes\n        use_relative_paths (bool): Whether to use paths relative to directory\n        \n    Returns:\n        dict: {\"files\": {filepath: content}}\n    \"\"\"\n    if not os.path.isdir(directory):\n        raise ValueError(f\"Directory does not exist: {directory}\")\n        \n    files_dict = {}\n    \n    for root, _, files in os.walk(directory):\n        for filename in files:\n            filepath = os.path.join(root, filename)\n            \n            # Get path relative to directory if requested\n            if use_relative_paths:\n                relpath = os.path.relpath(filepath, directory)\n            else:\n                relpath = filepath\n                \n            # Check if file matches any include pattern\n            included = False\n            if include_patterns:\n                for pattern in include_patterns:\n                    if fnmatch.fnmatch(relpath, pattern):\n                        included = True\n                        break\n            else:\n                included = True\n                \n            # Check if file matches any exclude pattern\n            excluded = False\n            if exclude_patterns:\n                for pattern in exclude_patterns:\n                    if fnmatch.fnmatch(relpath, pattern):\n                        excluded = True\n                        break\n                        \n            if not included or excluded:\n                continue\n                \n            # Check file size\n            if max_file_size and os.path.getsize(filepath) > max_file_size:\n                continue\n                \n            try:\n                with open(filepath, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                files_dict[relpath] = content\n            except Exception as e:\n                print(f\"Warning: Could not read file {filepath}: {e}\")\n                \n    return {\"files\": files_dict}\n\nif __name__ == \"__main__\":\n    print(\"--- Crawling parent directory ('..') ---\")\n    files_data = crawl_local_files(\"..\", exclude_patterns={\"*.pyc\", \"__pycache__/*\", \".git/*\", \"output/*\"})\n    print(f\"Found {len(files_data['files'])} files:\")\n    for path in files_data[\"files\"]:\n        print(f\"  {path}\")\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n": "```yaml\nsummary: |\n  The Tutorial Codebase Knowledge project generates **beginner-friendly tutorials** from codebases. It *automatically identifies key abstractions*, analyzes their relationships, and creates chapters, ultimately combining them into a cohesive tutorial. This helps developers quickly understand and learn new codebases.\nrelationships:\n  - from_abstraction: 0 # Flow Orchestration\n    to_abstraction: 1 # Codebase Crawling\n    label: \"Orchestrates\"\n  - from_abstraction: 0 # Flow Orchestration\n    to_abstraction: 2 # Abstraction Identification\n    label: \"Orchestrates\"\n  - from_abstraction: 0 # Flow Orchestration\n    to_abstraction: 3 # Relationship Analysis\n    label: \"Orchestrates\"\n  - from_abstraction: 0 # Flow Orchestration\n    to_abstraction: 4 # Chapter Writing\n    label: \"Orchestrates\"\n  - from_abstraction: 0 # Flow Orchestration\n    to_abstraction: 5 # Tutorial Combination\n    label: \"Orchestrates\"\n  - from_abstraction: 2 # Abstraction Identification\n    to_abstraction: 6 # LLM Interaction\n    label: \"Uses\"\n  - from_abstraction: 3 # Relationship Analysis\n    to_abstraction: 6 # LLM Interaction\n    label: \"Uses\"\n  - from_abstraction: 4 # Chapter Writing\n    to_abstraction: 6 # LLM Interaction\n    label: \"Uses\"\n  - from_abstraction: 7 # API Endpoint\n    to_abstraction: 0 # Flow Orchestration\n    label: \"Triggers\"\n  - from_abstraction: 0 # Flow Orchestration\n    to_abstraction: 8 # Configuration Management\n    label: \"Configures\"\n  - from_abstraction: 1 # Codebase Crawling\n    to_abstraction: 8 # Configuration Management\n    label: \"Uses config\"\n```", "\nGiven the following project abstractions and their relationships for the project ```` Tutorial-Codebase-Knowledge ````:\n\nAbstractions (Index # Name):\n- 0 # Flow Orchestration\n- 1 # Codebase Crawling\n- 2 # Abstraction Identification\n- 3 # Relationship Analysis\n- 4 # Chapter Writing\n- 5 # Tutorial Combination\n- 6 # LLM Interaction\n- 7 # API Endpoint\n- 8 # Configuration Management\n\nContext about relationships and project summary:\nProject Summary:\nThe Tutorial Codebase Knowledge project generates **beginner-friendly tutorials** from codebases. It *automatically identifies key abstractions*, analyzes their relationships, and creates chapters, ultimately combining them into a cohesive tutorial. This helps developers quickly understand and learn new codebases.\n\n\nRelationships (Indices refer to abstractions above):\n- From 0 (Flow Orchestration) to 1 (Codebase Crawling): Orchestrates\n- From 0 (Flow Orchestration) to 2 (Abstraction Identification): Orchestrates\n- From 0 (Flow Orchestration) to 3 (Relationship Analysis): Orchestrates\n- From 0 (Flow Orchestration) to 4 (Chapter Writing): Orchestrates\n- From 0 (Flow Orchestration) to 5 (Tutorial Combination): Orchestrates\n- From 2 (Abstraction Identification) to 6 (LLM Interaction): Uses\n- From 3 (Relationship Analysis) to 6 (LLM Interaction): Uses\n- From 4 (Chapter Writing) to 6 (LLM Interaction): Uses\n- From 7 (API Endpoint) to 0 (Flow Orchestration): Triggers\n- From 0 (Flow Orchestration) to 8 (Configuration Management): Configures\n- From 1 (Codebase Crawling) to 8 (Configuration Management): Uses config\n\n\nIf you are going to make a tutorial for ```` Tutorial-Codebase-Knowledge ````, what is the best order to explain these abstractions, from first to last?\nIdeally, first explain those that are the most important or foundational, perhaps user-facing concepts or entry points. Then move to more detailed, lower-level implementation details or supporting concepts.\n\nOutput the ordered list of abstraction indices, including the name in a comment for clarity. Use the format `idx # AbstractionName`.\n\n```yaml\n- 2 # FoundationalConcept\n- 0 # CoreClassA\n- 1 # CoreClassB (uses CoreClassA)\n- ...\n```\n\nNow, provide the YAML output:\n": "```yaml\n- 7 # API Endpoint # User entry point; likely the first thing a user interacts with.\n- 0 # Flow Orchestration # Orchestrates the entire process, a high-level control flow.\n- 8 # Configuration Management # Important to understand how the project is set up.\n- 1 # Codebase Crawling # First step in the automated tutorial creation.\n- 2 # Abstraction Identification # Identifies the core elements of the codebase.\n- 3 # Relationship Analysis # Determines how the abstractions relate to each other.\n- 6 # LLM Interaction # Used by multiple components to generate text.\n- 4 # Chapter Writing # Creates the individual tutorial sections.\n- 5 # Tutorial Combination # Assembles the chapters into a final tutorial.\n```", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `Tutorial-Codebase-Knowledge` about the concept: \"API Endpoint\". This is Chapter 1.\n\nConcept Details:\n- Description:\nThe API Endpoint provides a way to trigger the tutorial generation process remotely. It accepts\nrepository details and options, then runs the main script to generate the tutorial. It's like a\ndoorway that allows external programs to interact with the internal workings of the system.\n\n\nComplete Tutorial Structure:\n1. [API Endpoint](01_api_endpoint.md)\n2. [Flow Orchestration](02_flow_orchestration.md)\n3. [Configuration Management](03_configuration_management.md)\n4. [Codebase Crawling](04_codebase_crawling.md)\n5. [Abstraction Identification](05_abstraction_identification.md)\n6. [Relationship Analysis](06_relationship_analysis.md)\n7. [LLM Interaction](07_llm_interaction.md)\n8. [Chapter Writing](08_chapter_writing.md)\n9. [Tutorial Combination](09_tutorial_combination.md)\n\nContext from previous chapters (summary):\nThis is the first chapter.\n\nRelevant Code Snippets:\n--- File: api.py ---\n# api.py\n\nimport sys\nimport subprocess\nimport os\nimport logging\nimport re # Import re for parsing repo name\nfrom typing import Optional, List\nfrom urllib.parse import urlparse # To help parse repo name\n\nfrom fastapi import FastAPI, HTTPException, status\nfrom pydantic import BaseModel, Field\n\nfrom fastapi.middleware.cors import CORSMiddleware\n\n# --- Configuration ---\nMAIN_SCRIPT_PATH = \"main.py\"\nPYTHON_EXECUTABLE = sys.executable\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# --- Helper Function to Get Project Name ---\ndef get_project_name_from_url(repo_url: str) -> Optional[str]:\n    \"\"\"Extracts project name from GitHub URL.\"\"\"\n    try:\n        path = urlparse(repo_url).path\n        # Remove leading slash and .git suffix if present\n        name = path.lstrip('/').replace('.git', '')\n        # Get the last part of the path\n        project_name = name.split('/')[-1]\n        if project_name:\n            return project_name\n    except Exception:\n        pass # Ignore parsing errors\n    return None\n\n# --- Input Data Model (using Pydantic) ---\nclass GenerationRequest(BaseModel):\n    repo_url: str\n    name: Optional[str] = None\n    token: Optional[str] = None\n    output: str = Field(default=\"output\")\n    include: Optional[List[str]] = None\n    exclude: Optional[List[str]] = None\n    max_size: Optional[int] = None\n\n# --- FastAPI App ---\napp = FastAPI(\n    title=\"Codebase Tutorial Generator API\",\n    description=\"API to trigger the generation of tutorials from GitHub codebases.\",\n    version=\"1.0.0\",\n)\n\n# --- CORS Middleware ---\norigins = [\n    \"http://127.0.0.1:5500\", # VS Code Live Server default\n    \"http://localhost\",\n    \"http://localhost:8080\",\n    \"http://127.0.0.1\",\n    \"http://127.0.0.1:8080\",\n    \"null\", # file:// origin\n]\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=origins,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n# --- End CORS Middleware ---\n\n\n# --- API Endpoint ---\n@app.post(\n    \"/generate\",\n    summary=\"Trigger Tutorial Generation\",\n    description=\"Accepts repository details and options, then runs the tutorial generation script.\",\n    status_code=status.HTTP_200_OK\n)\nasync def generate_tutorial(request_data: GenerationRequest):\n    \"\"\"\n    API endpoint to trigger tutorial generation. Returns final output path on success.\n    \"\"\"\n    # --- Determine Project Name (needed for final path) ---\n    project_name = request_data.name\n    if not project_name:\n        project_name = get_project_name_from_url(request_data.repo_url)\n        if not project_name:\n             # If name wasn't provided and couldn't be derived, raise an error early\n             # Or alternatively, let main.py handle it, but we need it for the return path.\n             logger.error(f\"Could not derive project name from URL: {request_data.repo_url}\")\n             raise HTTPException(\n                 status_code=status.HTTP_400_BAD_REQUEST,\n                 detail=\"Project name not provided and could not be derived from repository URL.\"\n             )\n    \n    # --- Calculate Expected Final Output Path ---\n    # This mirrors the logic in CombineTutorial.prep\n    # Ensure forward slashes for URL compatibility later, though os.path.join is platform-aware\n    expected_final_path = os.path.join(request_data.output, project_name, 'html').replace('\\\\', '/')\n\n\n    # --- Build Command ---\n    command = [PYTHON_EXECUTABLE, MAIN_SCRIPT_PATH, request_data.repo_url]\n    # Use the derived or provided project name if specified (main.py will also derive if None)\n    if request_data.name:\n         command.extend([\"-n\", request_data.name])\n    if request_data.token:\n        command.extend([\"-t\", request_data.token])\n    command.extend([\"-o\", request_data.output]) # Pass base output dir\n    if request_data.max_size is not None:\n        command.extend([\"-s\", str(request_data.max_size)])\n    if request_data.include:\n        for pattern in request_data.include:\n            command.extend([\"-i\", pattern])\n    if request_data.exclude:\n        for pattern in request_data.exclude:\n            command.extend([\"-e\", pattern])\n\n    # --- Execute Script ---\n    logger.info(f\"Executing command: {' '.join(command)}\")\n\n    try:\n        result = subprocess.run(\n            command,\n            capture_output=True,\n            text=True,\n            check=True,\n            env=os.environ,\n            encoding='utf-8'\n        )\n\n        logger.info(f\"Script finished successfully.\")\n        logger.debug(f\"Script stdout:\\n{result.stdout}\")\n        if result.stderr:\n            logger.warning(f\"Script stderr:\\n{result.stderr}\")\n        \n        logger.info(f\"Final output directory: {expected_final_path}\")\n\n        # --- Success Response ---\n        # Return the *calculated* final path for redirection\n        return {\n            \"message\": \"Tutorial generation completed successfully.\",\n            # Key changed for clarity\n            \"final_output_directory\": expected_final_path, \n             # You could optionally still include stdout/stderr if needed for debugging\n            # \"script_stdout\": result.stdout,\n            # \"script_stderr\": result.stderr\n        }\n\n    except subprocess.CalledProcessError as e:\n        logger.error(f\"Script failed with exit code {e.returncode}\")\n        logger.error(f\"Script stdout:\\n{e.stdout}\")\n        logger.error(f\"Script stderr:\\n{e.stderr}\")\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail={\n                \"error\": \"Script execution failed.\",\n                \"details\": \"The tutorial generation script exited with an error.\",\n                \"return_code\": e.returncode,\n                \"script_stdout\": e.stdout,\n                \"script_stderr\": e.stderr\n            }\n        )\n    except FileNotFoundError:\n        error_msg = f\"Error: Script '{MAIN_SCRIPT_PATH}' or Python '{PYTHON_EXECUTABLE}' not found.\"\n        logger.error(error_msg)\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail={\"error\": \"Server configuration error.\", \"details\": error_msg}\n        )\n    except Exception as e:\n        logger.exception(\"An unexpected error occurred during script execution.\")\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail={\"error\": \"An unexpected internal server error occurred.\", \"details\": str(e)}\n        )\n\n# --- Root endpoint ---\n@app.get(\"/\", summary=\"Health Check\")\nasync def read_root():\n    return {\"message\": \"Tutorial Generator API is running.\"}\n\n# --- Uvicorn runner ---\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 1: API Endpoint`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "# Chapter 1: API Endpoint\n\nWelcome to the first step in our journey to understanding how to generate tutorials from codebases! In this chapter, we'll explore the **API Endpoint**, which acts like the \"front door\" to our tutorial generation system.\n\nImagine you have a robot that can build LEGO sets. You wouldn't want to open up the robot and start fiddling with its internal wires every time you want it to build something, right? Instead, you'd want a simple button or a control panel to tell it what to do. The API Endpoint is like that button for our tutorial generator.\n\n**Why do we need an API Endpoint?**\n\nThink of it this way: you might want to trigger the tutorial generation process from:\n\n*   Your web browser\n*   Another program running on your computer\n*   Even from a different computer across the internet!\n\nAn API Endpoint allows all these different \"clients\" to communicate with our tutorial generation system in a standardized way. It's the central point of contact.\n\n**Key Concepts:**\n\n1.  **API (Application Programming Interface):** Think of it as a contract between two pieces of software. It defines how they can talk to each other. In our case, it defines how external programs can ask our system to generate a tutorial.\n\n2.  **Endpoint:** A specific URL (web address) that represents a particular function or resource in an API. In our case, it's the URL you send a request to in order to start the tutorial generation. It's like the specific button on our robot's control panel that says \"Build this LEGO set!\"\n\n3.  **Request:** The message you send to the API Endpoint to tell it what to do. This includes information like the repository URL, output folder and options for the tutorial generation.\n\n4.  **Response:** The message the API Endpoint sends back to you after it has processed your request. This might include a success message, an error message, or the path to the generated tutorial.\n\n**Our Use Case: Generating a tutorial for a GitHub repository**\n\nLet's say you want to generate a tutorial for a project on GitHub. The API Endpoint allows you to do this with a simple request. You'll need to provide the URL of the repository and any specific options for how you want the tutorial to be generated.\n\n**How to Use the API Endpoint:**\n\nOur API Endpoint is designed to receive a `POST` request to the `/generate` path. This request contains information about the repository for which you want to generate a tutorial.\n\nHere's an example of how you might send a request (this is just an example, the actual implementation will depend on how you're sending the request, e.g., using `curl`, `Postman`, or a Python script):\n\n```json\n{\n  \"repo_url\": \"https://github.com/fastapi/fastapi\",\n  \"name\": \"FastAPI\",\n  \"output\": \"tutorials\"\n}\n```\n\nThis request tells the system to:\n\n*   Generate a tutorial for the `https://github.com/fastapi/fastapi` repository.\n*   Use \"FastAPI\" as the project name.\n*   Save the tutorial in the `tutorials` output folder.\n\nAfter sending this request, the API Endpoint will process it and, if successful, return a response like this:\n\n```json\n{\n  \"message\": \"Tutorial generation completed successfully.\",\n  \"final_output_directory\": \"tutorials/FastAPI/html\"\n}\n```\n\nThis tells you that the tutorial was generated successfully and is located in the `tutorials/FastAPI/html` directory.\n\n**Internal Implementation: What happens under the hood?**\n\nWhen the API Endpoint receives a request, here's a simplified breakdown of what happens:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant API Endpoint\n    participant Main Script\n    participant Codebase\n    participant LLM\n\n    User->>API Endpoint: Sends tutorial generation request (repo URL, options)\n    API Endpoint->>Main Script: Executes main.py with provided arguments\n    Main Script->>Codebase: Crawls and analyzes the codebase\n    Main Script->>LLM: Interacts with LLM to generate content\n    Main Script->>API Endpoint: Returns success/failure and output path\n    API Endpoint->>User: Sends back response with result and output path\n```\n\nHere's a slightly more detailed explanation:\n\n1.  **Request Reception:** The API Endpoint receives the request with the repository URL and other options.\n2.  **Command Construction:**  It constructs a command line command to run the main script (`main.py`) with the provided parameters.\n3.  **Script Execution:** It executes the `main.py` script, which does the heavy lifting of cloning the repository, analyzing the code, and generating the tutorial content.\n4.  **Response Generation:** After the script finishes (either successfully or with an error), the API Endpoint creates a response with the appropriate message and, if successful, the path to the generated tutorial.\n5.  **Response Delivery:** Finally, it sends the response back to the user.\n\n**Code Snippets:**\n\nLet's look at some key parts of the code in `api.py`:\n\n```python\nfrom fastapi import FastAPI, HTTPException, status\nfrom pydantic import BaseModel, Field\n```\n\nThis imports the necessary libraries for creating the API using FastAPI (a Python framework for building APIs) and Pydantic (for data validation). We are importing `FastAPI` to create the API application, `HTTPException` to handle errors and send them to the user, and `BaseModel` and `Field` from `pydantic` to define the structure of our input data.\n\n```python\nclass GenerationRequest(BaseModel):\n    repo_url: str\n    name: Optional[str] = None\n    token: Optional[str] = None\n    output: str = Field(default=\"output\")\n    include: Optional[List[str]] = None\n    exclude: Optional[List[str]] = None\n    max_size: Optional[int] = None\n```\n\nThis defines the structure of the data that the API Endpoint expects to receive in the request. This is crucial for validation and ensuring that the input data is in the correct format. `repo_url` is required, while other fields like `name`, `token`, `output`, `include`, `exclude`, and `max_size` are optional.\n\n```python\n@app.post(\n    \"/generate\",\n    summary=\"Trigger Tutorial Generation\",\n    description=\"Accepts repository details and options, then runs the tutorial generation script.\",\n    status_code=status.HTTP_200_OK\n)\nasync def generate_tutorial(request_data: GenerationRequest):\n    # ... implementation details ...\n    return {\n        \"message\": \"Tutorial generation completed successfully.\",\n        \"final_output_directory\": expected_final_path,\n    }\n```\n\nThis code defines the `/generate` endpoint using the `@app.post` decorator. This tells FastAPI that this function should be called when a `POST` request is sent to the `/generate` URL. It receives the `request_data` (which is of type `GenerationRequest` that we defined above), does some processing, and returns a response containing a success message and the path to the generated tutorial.  The `status_code=status.HTTP_200_OK` ensures that a successful response returns a 200 OK status code.\n\n**CORS Middleware**\n\n```python\norigins = [\n    \"http://127.0.0.1:5500\", # VS Code Live Server default\n    \"http://localhost\",\n    \"http://localhost:8080\",\n    \"http://127.0.0.1\",\n    \"http://127.0.0.1:8080\",\n    \"null\", # file:// origin\n]\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=origins,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n```\n\nThis section is responsible for configuring Cross-Origin Resource Sharing (CORS). CORS is a security feature implemented by web browsers to restrict web pages from making requests to a different domain than the one which served the web page. Without CORS, you might not be able to send requests to your API from a webpage running on your local machine (e.g. during development). The middleware specifies the origins that are allowed to make requests to the API, ensuring that requests from these origins are not blocked by the browser.\n\n**Error Handling**\n\nThe `api.py` file includes comprehensive error handling to gracefully manage different scenarios that might arise during script execution.\n\n```python\n    except subprocess.CalledProcessError as e:\n        logger.error(f\"Script failed with exit code {e.returncode}\")\n        logger.error(f\"Script stdout:\\n{e.stdout}\")\n        logger.error(f\"Script stderr:\\n{e.stderr}\")\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail={\n                \"error\": \"Script execution failed.\",\n                \"details\": \"The tutorial generation script exited with an error.\",\n                \"return_code\": e.returncode,\n                \"script_stdout\": e.stdout,\n                \"script_stderr\": e.stderr\n            }\n        )\n    except FileNotFoundError:\n        error_msg = f\"Error: Script '{MAIN_SCRIPT_PATH}' or Python '{PYTHON_EXECUTABLE}' not found.\"\n        logger.error(error_msg)\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail={\"error\": \"Server configuration error.\", \"details\": error_msg}\n        )\n    except Exception as e:\n        logger.exception(\"An unexpected error occurred during script execution.\")\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail={\"error\": \"An unexpected internal server error occurred.\", \"details\": str(e)}\n        )\n```\n\n*   **`subprocess.CalledProcessError`**: Catches errors when the `main.py` script fails during execution. It logs the error details and raises an `HTTPException` with a 500 status code, providing details like the return code, stdout, and stderr of the script.\n*   **`FileNotFoundError`**: Handles cases where the `main.py` script or Python executable is not found. It raises an `HTTPException` with a 500 status code, indicating a server configuration issue.\n*   **`Exception`**: Serves as a catch-all for unexpected errors during script execution. It logs the exception and raises an `HTTPException` with a 500 status code, providing details about the error.\n\n**Conclusion:**\n\nIn this chapter, we've learned about the API Endpoint, which serves as the entry point to our tutorial generation system. We saw how it allows external programs to trigger the tutorial generation process with a simple request. We also explored the internal implementation and understood how it orchestrates the execution of the `main.py` script.\n\nNow that we know how to trigger the process, let's dive into [Flow Orchestration](02_flow_orchestration.md) in the next chapter to understand the overall sequence of steps involved in generating a tutorial.\n", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `Tutorial-Codebase-Knowledge` about the concept: \"Flow Orchestration\". This is Chapter 2.\n\nConcept Details:\n- Description:\nThe Flow Orchestration system is the conductor of the codebase tutorial generation process.\nIt defines the sequence of steps, connecting various nodes like fetching the repository,\nidentifying key abstractions, analyzing relationships, writing chapters, and combining everything\ninto a final tutorial. It's like a manufacturing assembly line, where each station performs\na specific task, and the product moves from one stage to the next until completion.\n\n\nComplete Tutorial Structure:\n1. [API Endpoint](01_api_endpoint.md)\n2. [Flow Orchestration](02_flow_orchestration.md)\n3. [Configuration Management](03_configuration_management.md)\n4. [Codebase Crawling](04_codebase_crawling.md)\n5. [Abstraction Identification](05_abstraction_identification.md)\n6. [Relationship Analysis](06_relationship_analysis.md)\n7. [LLM Interaction](07_llm_interaction.md)\n8. [Chapter Writing](08_chapter_writing.md)\n9. [Tutorial Combination](09_tutorial_combination.md)\n\nContext from previous chapters (summary):\n# Chapter 1: API Endpoint\n\nWelcome to the first step in our journey to understanding how to generate tutorials from codebases! In this chapter, we'll explore the **API Endpoint**, which acts like the \"front door\" to our tutorial generation system.\n\nImagine you have a robot that can build LEGO sets. You wouldn't want to open up the robot and start fiddling with its internal wires every time you want it to build something, right? Instead, you'd want a simple button or a control panel to tell it what to do. The API Endpoint is like that button for our tutorial generator.\n\n**Why do we need an API Endpoint?**\n\nThink of it this way: you might want to trigger the tutorial generation process from:\n\n*   Your web browser\n*   Another program running on your computer\n*   Even from a different computer across the internet!\n\nAn API Endpoint allows all these different \"clients\" to communicate with our tutorial generation system in a standardized way. It's the central point of contact.\n\n**Key Concepts:**\n\n1.  **API (Application Programming Interface):** Think of it as a contract between two pieces of software. It defines how they can talk to each other. In our case, it defines how external programs can ask our system to generate a tutorial.\n\n2.  **Endpoint:** A specific URL (web address) that represents a particular function or resource in an API. In our case, it's the URL you send a request to in order to start the tutorial generation. It's like the specific button on our robot's control panel that says \"Build this LEGO set!\"\n\n3.  **Request:** The message you send to the API Endpoint to tell it what to do. This includes information like the repository URL, output folder and options for the tutorial generation.\n\n4.  **Response:** The message the API Endpoint sends back to you after it has processed your request. This might include a success message, an error message, or the path to the generated tutorial.\n\n**Our Use Case: Generating a tutorial for a GitHub repository**\n\nLet's say you want to generate a tutorial for a project on GitHub. The API Endpoint allows you to do this with a simple request. You'll need to provide the URL of the repository and any specific options for how you want the tutorial to be generated.\n\n**How to Use the API Endpoint:**\n\nOur API Endpoint is designed to receive a `POST` request to the `/generate` path. This request contains information about the repository for which you want to generate a tutorial.\n\nHere's an example of how you might send a request (this is just an example, the actual implementation will depend on how you're sending the request, e.g., using `curl`, `Postman`, or a Python script):\n\n```json\n{\n  \"repo_url\": \"https://github.com/fastapi/fastapi\",\n  \"name\": \"FastAPI\",\n  \"output\": \"tutorials\"\n}\n```\n\nThis request tells the system to:\n\n*   Generate a tutorial for the `https://github.com/fastapi/fastapi` repository.\n*   Use \"FastAPI\" as the project name.\n*   Save the tutorial in the `tutorials` output folder.\n\nAfter sending this request, the API Endpoint will process it and, if successful, return a response like this:\n\n```json\n{\n  \"message\": \"Tutorial generation completed successfully.\",\n  \"final_output_directory\": \"tutorials/FastAPI/html\"\n}\n```\n\nThis tells you that the tutorial was generated successfully and is located in the `tutorials/FastAPI/html` directory.\n\n**Internal Implementation: What happens under the hood?**\n\nWhen the API Endpoint receives a request, here's a simplified breakdown of what happens:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant API Endpoint\n    participant Main Script\n    participant Codebase\n    participant LLM\n\n    User->>API Endpoint: Sends tutorial generation request (repo URL, options)\n    API Endpoint->>Main Script: Executes main.py with provided arguments\n    Main Script->>Codebase: Crawls and analyzes the codebase\n    Main Script->>LLM: Interacts with LLM to generate content\n    Main Script->>API Endpoint: Returns success/failure and output path\n    API Endpoint->>User: Sends back response with result and output path\n```\n\nHere's a slightly more detailed explanation:\n\n1.  **Request Reception:** The API Endpoint receives the request with the repository URL and other options.\n2.  **Command Construction:**  It constructs a command line command to run the main script (`main.py`) with the provided parameters.\n3.  **Script Execution:** It executes the `main.py` script, which does the heavy lifting of cloning the repository, analyzing the code, and generating the tutorial content.\n4.  **Response Generation:** After the script finishes (either successfully or with an error), the API Endpoint creates a response with the appropriate message and, if successful, the path to the generated tutorial.\n5.  **Response Delivery:** Finally, it sends the response back to the user.\n\n**Code Snippets:**\n\nLet's look at some key parts of the code in `api.py`:\n\n```python\nfrom fastapi import FastAPI, HTTPException, status\nfrom pydantic import BaseModel, Field\n```\n\nThis imports the necessary libraries for creating the API using FastAPI (a Python framework for building APIs) and Pydantic (for data validation). We are importing `FastAPI` to create the API application, `HTTPException` to handle errors and send them to the user, and `BaseModel` and `Field` from `pydantic` to define the structure of our input data.\n\n```python\nclass GenerationRequest(BaseModel):\n    repo_url: str\n    name: Optional[str] = None\n    token: Optional[str] = None\n    output: str = Field(default=\"output\")\n    include: Optional[List[str]] = None\n    exclude: Optional[List[str]] = None\n    max_size: Optional[int] = None\n```\n\nThis defines the structure of the data that the API Endpoint expects to receive in the request. This is crucial for validation and ensuring that the input data is in the correct format. `repo_url` is required, while other fields like `name`, `token`, `output`, `include`, `exclude`, and `max_size` are optional.\n\n```python\n@app.post(\n    \"/generate\",\n    summary=\"Trigger Tutorial Generation\",\n    description=\"Accepts repository details and options, then runs the tutorial generation script.\",\n    status_code=status.HTTP_200_OK\n)\nasync def generate_tutorial(request_data: GenerationRequest):\n    # ... implementation details ...\n    return {\n        \"message\": \"Tutorial generation completed successfully.\",\n        \"final_output_directory\": expected_final_path,\n    }\n```\n\nThis code defines the `/generate` endpoint using the `@app.post` decorator. This tells FastAPI that this function should be called when a `POST` request is sent to the `/generate` URL. It receives the `request_data` (which is of type `GenerationRequest` that we defined above), does some processing, and returns a response containing a success message and the path to the generated tutorial.  The `status_code=status.HTTP_200_OK` ensures that a successful response returns a 200 OK status code.\n\n**CORS Middleware**\n\n```python\norigins = [\n    \"http://127.0.0.1:5500\", # VS Code Live Server default\n    \"http://localhost\",\n    \"http://localhost:8080\",\n    \"http://127.0.0.1\",\n    \"http://127.0.0.1:8080\",\n    \"null\", # file:// origin\n]\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=origins,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n```\n\nThis section is responsible for configuring Cross-Origin Resource Sharing (CORS). CORS is a security feature implemented by web browsers to restrict web pages from making requests to a different domain than the one which served the web page. Without CORS, you might not be able to send requests to your API from a webpage running on your local machine (e.g. during development). The middleware specifies the origins that are allowed to make requests to the API, ensuring that requests from these origins are not blocked by the browser.\n\n**Error Handling**\n\nThe `api.py` file includes comprehensive error handling to gracefully manage different scenarios that might arise during script execution.\n\n```python\n    except subprocess.CalledProcessError as e:\n        logger.error(f\"Script failed with exit code {e.returncode}\")\n        logger.error(f\"Script stdout:\\n{e.stdout}\")\n        logger.error(f\"Script stderr:\\n{e.stderr}\")\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail={\n                \"error\": \"Script execution failed.\",\n                \"details\": \"The tutorial generation script exited with an error.\",\n                \"return_code\": e.returncode,\n                \"script_stdout\": e.stdout,\n                \"script_stderr\": e.stderr\n            }\n        )\n    except FileNotFoundError:\n        error_msg = f\"Error: Script '{MAIN_SCRIPT_PATH}' or Python '{PYTHON_EXECUTABLE}' not found.\"\n        logger.error(error_msg)\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail={\"error\": \"Server configuration error.\", \"details\": error_msg}\n        )\n    except Exception as e:\n        logger.exception(\"An unexpected error occurred during script execution.\")\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail={\"error\": \"An unexpected internal server error occurred.\", \"details\": str(e)}\n        )\n```\n\n*   **`subprocess.CalledProcessError`**: Catches errors when the `main.py` script fails during execution. It logs the error details and raises an `HTTPException` with a 500 status code, providing details like the return code, stdout, and stderr of the script.\n*   **`FileNotFoundError`**: Handles cases where the `main.py` script or Python executable is not found. It raises an `HTTPException` with a 500 status code, indicating a server configuration issue.\n*   **`Exception`**: Serves as a catch-all for unexpected errors during script execution. It logs the exception and raises an `HTTPException` with a 500 status code, providing details about the error.\n\n**Conclusion:**\n\nIn this chapter, we've learned about the API Endpoint, which serves as the entry point to our tutorial generation system. We saw how it allows external programs to trigger the tutorial generation process with a simple request. We also explored the internal implementation and understood how it orchestrates the execution of the `main.py` script.\n\nNow that we know how to trigger the process, let's dive into [Flow Orchestration](02_flow_orchestration.md) in the next chapter to understand the overall sequence of steps involved in generating a tutorial.\n\n\nRelevant Code Snippets:\n--- File: flow.py ---\nfrom pocketflow import Flow\n# Import all node classes from nodes.py\nfrom nodes import (\n    FetchRepo,\n    IdentifyAbstractions,\n    AnalyzeRelationships,\n    OrderChapters,\n    WriteChapters,\n    CombineTutorial\n)\n\ndef create_tutorial_flow():\n    \"\"\"Creates and returns the codebase tutorial generation flow.\"\"\"\n\n    # Instantiate nodes\n    fetch_repo = FetchRepo()\n    identify_abstractions = IdentifyAbstractions(max_retries=3, wait=10)\n    analyze_relationships = AnalyzeRelationships(max_retries=3, wait=10)\n    order_chapters = OrderChapters(max_retries=3, wait=10)\n    write_chapters = WriteChapters(max_retries=3, wait=10) # This is a BatchNode\n    combine_tutorial = CombineTutorial()\n\n    # Connect nodes in sequence based on the design\n    fetch_repo >> identify_abstractions\n    identify_abstractions >> analyze_relationships\n    analyze_relationships >> order_chapters\n    order_chapters >> write_chapters\n    write_chapters >> combine_tutorial\n\n    # Create the flow starting with FetchRepo\n    tutorial_flow = Flow(start=fetch_repo)\n\n    return tutorial_flow\n\n--- File: main.py ---\nimport dotenv\nimport os\nimport argparse\n# Import the function that creates the flow\nfrom flow import create_tutorial_flow\n\ndotenv.load_dotenv()\n\n# Default file patterns\nDEFAULT_INCLUDE_PATTERNS = {\n    \"*.py\", \"*.js\", \"*.jsx\", \"*.ts\", \"*.tsx\", \"*.go\", \"*.java\", \"*.pyi\", \"*.pyx\", \n    \"*.c\", \"*.cc\", \"*.cpp\", \"*.h\", \"*.md\", \"*.rst\", \"Dockerfile\", \n    \"Makefile\", \"*.yaml\", \"*.yml\",\n}\n\nDEFAULT_EXCLUDE_PATTERNS = {\n    \"*test*\", \"tests/*\", \"docs/*\", \"examples/*\", \"v1/*\", \n    \"dist/*\", \"build/*\", \"experimental/*\", \"deprecated/*\", \n    \"legacy/*\", \".git/*\", \".github/*\", \".next/*\", \".vscode/*\", \"obj/*\", \"bin/*\", \"node_modules/*\", \"*.log\"\n}\n\n# --- Main Function ---\ndef main():\n    parser = argparse.ArgumentParser(description=\"Generate a tutorial for a GitHub codebase or local directory.\")\n    \n    # Create mutually exclusive group for source\n    source_group = parser.add_mutually_exclusive_group(required=True)\n    source_group.add_argument(\"--repo\", help=\"URL of the public GitHub repository.\")\n    source_group.add_argument(\"--dir\", help=\"Path to local directory.\")\n    \n    parser.add_argument(\"-n\", \"--name\", help=\"Project name (optional, derived from repo/directory if omitted).\")\n    parser.add_argument(\"-t\", \"--token\", help=\"GitHub personal access token (optional, reads from GITHUB_TOKEN env var if not provided).\")\n    parser.add_argument(\"-o\", \"--output\", default=\"output\", help=\"Base directory for output (default: ./output).\")\n    parser.add_argument(\"-i\", \"--include\", nargs=\"+\", help=\"Include file patterns (e.g. '*.py' '*.js'). Defaults to common code files if not specified.\")\n    parser.add_argument(\"-e\", \"--exclude\", nargs=\"+\", help=\"Exclude file patterns (e.g. 'tests/*' 'docs/*'). Defaults to test/build directories if not specified.\")\n    parser.add_argument(\"-s\", \"--max-size\", type=int, default=100000, help=\"Maximum file size in bytes (default: 100000, about 100KB).\")\n\n    args = parser.parse_args()\n\n    # Get GitHub token from argument or environment variable if using repo\n    github_token = None\n    if args.repo:\n        github_token = args.token or os.environ.get('GITHUB_TOKEN')\n        if not github_token:\n            print(\"Warning: No GitHub token provided. You might hit rate limits for public repositories.\")\n\n    # Initialize the shared dictionary with inputs\n    shared = {\n        \"repo_url\": args.repo,\n        \"local_dir\": args.dir,\n        \"project_name\": args.name, # Can be None, FetchRepo will derive it\n        \"github_token\": github_token,\n        \"output_dir\": args.output, # Base directory for CombineTutorial output\n\n        # Add include/exclude patterns and max file size\n        \"include_patterns\": set(args.include) if args.include else DEFAULT_INCLUDE_PATTERNS,\n        \"exclude_patterns\": set(args.exclude) if args.exclude else DEFAULT_EXCLUDE_PATTERNS,\n        \"max_file_size\": args.max_size,\n\n        # Outputs will be populated by the nodes\n        \"files\": [],\n        \"abstractions\": [],\n        \"relationships\": {},\n        \"chapter_order\": [],\n        \"chapters\": [],\n        \"final_output_dir\": None\n    }\n\n    print(f\"Starting tutorial generation for: {args.repo or args.dir}\")\n\n    # Create the flow instance\n    tutorial_flow = create_tutorial_flow()\n\n    # Run the flow\n    tutorial_flow.run(shared)\n    \nif __name__ == \"__main__\":\n    main()\n\n--- File: nodes.py ---\nimport os\nimport yaml\nfrom pocketflow import Node, BatchNode\nfrom utils.crawl_github_files import crawl_github_files\nfrom utils.call_llm import call_llm # Assuming you have this utility\nfrom utils.crawl_local_files import crawl_local_files\nfrom converter.md_to_html import convert_all_md_to_html\n\n\n# Helper to get content for specific file indices\ndef get_content_for_indices(files_data, indices):\n    content_map = {}\n    for i in indices:\n        if 0 <= i < len(files_data):\n            path, content = files_data[i]\n            content_map[f\"{i} # {path}\"] = content # Use index + path as key for context\n    return content_map\n\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"local_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        if not project_name:\n            # Basic name derivation from URL or directory\n            if repo_url:\n                project_name = repo_url.split('/')[-1].replace('.git', '')\n            else:\n                project_name = os.path.basename(os.path.abspath(local_dir))\n            shared[\"project_name\"] = project_name\n\n        # Get file patterns directly from shared\n        include_patterns = shared[\"include_patterns\"]\n        exclude_patterns = shared[\"exclude_patterns\"]\n        max_file_size = shared[\"max_file_size\"]\n\n        return {\n            \"repo_url\": repo_url,\n            \"local_dir\": local_dir,\n            \"token\": shared.get(\"github_token\"),\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns,\n            \"max_file_size\": max_file_size,\n            \"use_relative_paths\": True\n        }\n\n    def exec(self, prep_res):\n        if prep_res[\"repo_url\"]:\n            print(f\"Crawling repository: {prep_res['repo_url']}...\")\n            result = crawl_github_files(\n                repo_url=prep_res[\"repo_url\"],\n                token=prep_res[\"token\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        else:\n            print(f\"Crawling directory: {prep_res['local_dir']}...\")\n            result = crawl_local_files(\n                directory=prep_res[\"local_dir\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n            \n        # Convert dict to list of tuples: [(path, content), ...]\n        files_list = list(result.get(\"files\", {}).items())\n        print(f\"Fetched {len(files_list)} files.\")\n        return files_list\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"files\"] = exec_res # List of (path, content) tuples\n\nclass IdentifyAbstractions(Node):\n    def prep(self, shared):\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        \n        # Helper to create context from files, respecting limits (basic example)\n        def create_llm_context(files_data):\n            context = \"\"\n            file_info = [] # Store tuples of (index, path)\n            for i, (path, content) in enumerate(files_data):\n                entry = f\"--- File Index {i}: {path} ---\\n{content}\\n\\n\"\n                context += entry\n                file_info.append((i, path))\n\n            return context, file_info # file_info is list of (index, path)\n\n        context, file_info = create_llm_context(files_data)\n        # Format file info for the prompt (comment is just a hint for LLM)\n        file_listing_for_prompt = \"\\n\".join([f\"- {idx} # {path}\" for idx, path in file_info])\n        return context, file_listing_for_prompt, len(files_data), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, file_listing_for_prompt, file_count, project_name = prep_res  # Unpack project name\n        print(\"Identifying abstractions using LLM...\")\n        prompt = f\"\"\"\nFor the project `{project_name}`:\n\nCodebase Context:\n{context}\n\nAnalyze the codebase context.\nIdentify the top 5-10 core most important abstractions to help those new to the codebase.\n\nFor each abstraction, provide:\n1. A concise `name`.\n2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\nList of file indices and paths present in the context:\n{file_listing_for_prompt}\n\nFormat the output as a YAML list of dictionaries:\n\n```yaml\n- name: Query Processing\n  description: | \n    Explains what the abstraction does.\n    It's like a central dispatcher routing requests.\n  file_indices:\n    - 0 # path/to/file1.py\n    - 3 # path/to/related.py\n- name: Query Optimization\n  description: |\n    Another core concept, similar to a blueprint for objects.\n  file_indices:\n    - 5 # path/to/another.js\n# ... up to 10 abstractions\n```\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        abstractions = yaml.safe_load(yaml_str)\n\n        if not isinstance(abstractions, list):\n            raise ValueError(\"LLM Output is not a list\")\n\n        validated_abstractions = []\n        for item in abstractions:\n            if not isinstance(item, dict) or not all(k in item for k in [\"name\", \"description\", \"file_indices\"]):\n                raise ValueError(f\"Missing keys in abstraction item: {item}\")\n            if not isinstance(item[\"description\"], str):\n                 raise ValueError(f\"description is not a string in item: {item}\")\n            if not isinstance(item[\"file_indices\"], list):\n                 raise ValueError(f\"file_indices is not a list in item: {item}\")\n\n            # Validate indices\n            validated_indices = []\n            for idx_entry in item[\"file_indices\"]:\n                 try:\n                     if isinstance(idx_entry, int):\n                         idx = idx_entry\n                     elif isinstance(idx_entry, str) and '#' in idx_entry:\n                          idx = int(idx_entry.split('#')[0].strip())\n                     else:\n                          idx = int(str(idx_entry).strip())\n\n                     if not (0 <= idx < file_count):\n                         raise ValueError(f\"Invalid file index {idx} found in item {item['name']}. Max index is {file_count - 1}.\")\n                     validated_indices.append(idx)\n                 except (ValueError, TypeError):\n                      raise ValueError(f\"Could not parse index from entry: {idx_entry} in item {item['name']}\")\n\n            item[\"files\"] = sorted(list(set(validated_indices)))\n            # Store only the required fields\n            validated_abstractions.append({\n                \"name\": item[\"name\"],\n                \"description\": item[\"description\"],\n                \"files\": item[\"files\"]\n            })\n\n        print(f\"Identified {len(validated_abstractions)} abstractions.\")\n        return validated_abstractions\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"abstractions\"] = exec_res # List of {\"name\": str, \"description\": str, \"files\": [int]}\n\nclass AnalyzeRelationships(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"] # Now contains 'files' list of indices\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n\n        # Create context with abstraction names, indices, descriptions, and relevant file snippets\n        context = \"Identified Abstractions:\\n\"\n        all_relevant_indices = set()\n        abstraction_info_for_prompt = []\n        for i, abstr in enumerate(abstractions):\n            # Use 'files' which contains indices directly\n            file_indices_str = \", \".join(map(str, abstr['files']))\n            info_line = f\"- Index {i}: {abstr['name']} (Relevant file indices: [{file_indices_str}])\\n  Description: {abstr['description']}\"\n            context += info_line + \"\\n\"\n            abstraction_info_for_prompt.append(f\"{i} # {abstr['name']}\")\n            all_relevant_indices.update(abstr['files'])\n\n        context += \"\\nRelevant File Snippets (Referenced by Index and Path):\\n\"\n        # Get content for relevant files using helper\n        relevant_files_content_map = get_content_for_indices(\n            files_data,\n            sorted(list(all_relevant_indices))\n        )\n        # Format file content for context\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path} ---\\n{content}\"\n            for idx_path, content in relevant_files_content_map.items()\n        )\n        context += file_context_str\n\n        return context, \"\\n\".join(abstraction_info_for_prompt), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, abstraction_listing, project_name = prep_res  # Unpack project name\n        print(\"Analyzing relationships using LLM...\")\n        prompt = f\"\"\"\nBased on the following abstractions and relevant code snippets from the project `{project_name}`:\n\nList of Abstraction Indices and Names:\n{abstraction_listing}\n\nContext (Abstractions, Descriptions, Code):\n{context}\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        relationships_data = yaml.safe_load(yaml_str)\n\n        if not isinstance(relationships_data, dict) or not all(k in relationships_data for k in [\"summary\", \"relationships\"]):\n            raise ValueError(\"LLM output is not a dict or missing keys ('summary', 'relationships')\")\n        if not isinstance(relationships_data[\"summary\"], str):\n             raise ValueError(\"summary is not a string\")\n        if not isinstance(relationships_data[\"relationships\"], list):\n             raise ValueError(\"relationships is not a list\")\n\n        # Validate relationships structure\n        validated_relationships = []\n        num_abstractions = len(abstraction_listing.split('\\n'))\n        for rel in relationships_data[\"relationships\"]:\n             # Check for 'label' key\n             if not isinstance(rel, dict) or not all(k in rel for k in [\"from_abstraction\", \"to_abstraction\", \"label\"]):\n                  raise ValueError(f\"Missing keys (expected from_abstraction, to_abstraction, label) in relationship item: {rel}\")\n             # Validate 'label' is a string\n             if not isinstance(rel[\"label\"], str):\n                  raise ValueError(f\"Relationship label is not a string: {rel}\")\n\n             # Validate indices\n             try:\n                 from_idx = int(str(rel[\"from_abstraction\"]).split('#')[0].strip())\n                 to_idx = int(str(rel[\"to_abstraction\"]).split('#')[0].strip())\n                 if not (0 <= from_idx < num_abstractions and 0 <= to_idx < num_abstractions):\n                      raise ValueError(f\"Invalid index in relationship: from={from_idx}, to={to_idx}. Max index is {num_abstractions-1}.\")\n                 validated_relationships.append({\n                     \"from\": from_idx,\n                     \"to\": to_idx,\n                     \"label\": rel[\"label\"] \n                 })\n             except (ValueError, TypeError):\n                  raise ValueError(f\"Could not parse indices from relationship: {rel}\")\n\n        print(\"Generated project summary and relationship details.\")\n        return {\n            \"summary\": relationships_data[\"summary\"],\n            \"details\": validated_relationships # Store validated, index-based relationships\n        }\n\n\n    def post(self, shared, prep_res, exec_res):\n        # Structure is now {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        shared[\"relationships\"] = exec_res\n\nclass OrderChapters(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"]\n        relationships = shared[\"relationships\"]\n        project_name = shared[\"project_name\"]  # Get project name\n\n        # Prepare context for the LLM\n        abstraction_info_for_prompt = []\n        for i, a in enumerate(abstractions):\n            abstraction_info_for_prompt.append(f\"- {i} # {a['name']}\")\n        abstraction_listing = \"\\n\".join(abstraction_info_for_prompt)\n\n        context = f\"Project Summary:\\n{relationships['summary']}\\n\\n\"\n        context += \"Relationships (Indices refer to abstractions above):\\n\"\n        for rel in relationships['details']:\n             from_name = abstractions[rel['from']]['name']\n             to_name = abstractions[rel['to']]['name']\n             # Use 'label' instead of 'desc'\n             context += f\"- From {rel['from']} ({from_name}) to {rel['to']} ({to_name}): {rel['label']}\\n\"\n\n        return abstraction_listing, context, len(abstractions), project_name\n\n    def exec(self, prep_res):\n        abstraction_listing, context, num_abstractions, project_name = prep_res\n        print(\"Determining chapter order using LLM...\")\n        prompt = f\"\"\"\nGiven the following project abstractions and their relationships for the project ```` {project_name} ````:\n\nAbstractions (Index # Name):\n{abstraction_listing}\n\nContext about relationships and project summary:\n{context}\n\nIf you are going to make a tutorial for ```` {project_name} ````, what is the best order to explain these abstractions, from first to last?\nIdeally, first explain those that are the most important or foundational, perhaps user-facing concepts or entry points. Then move to more detailed, lower-level implementation details or supporting concepts.\n\nOutput the ordered list of abstraction indices, including the name in a comment for clarity. Use the format `idx # AbstractionName`.\n\n```yaml\n- 2 # FoundationalConcept\n- 0 # CoreClassA\n- 1 # CoreClassB (uses CoreClassA)\n- ...\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        # Rely on Node's built-in retry/fallback\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        ordered_indices_raw = yaml.safe_load(yaml_str)\n\n        if not isinstance(ordered_indices_raw, list):\n            raise ValueError(\"LLM output is not a list\")\n\n        ordered_indices = []\n        seen_indices = set()\n        for entry in ordered_indices_raw:\n            try:\n                 if isinstance(entry, int):\n                     idx = entry\n                 elif isinstance(entry, str) and '#' in entry:\n                      idx = int(entry.split('#')[0].strip())\n                 else:\n                      idx = int(str(entry).strip())\n\n                 if not (0 <= idx < num_abstractions):\n                      raise ValueError(f\"Invalid index {idx} in ordered list. Max index is {num_abstractions-1}.\")\n                 if idx in seen_indices:\n                     raise ValueError(f\"Duplicate index {idx} found in ordered list.\")\n                 ordered_indices.append(idx)\n                 seen_indices.add(idx)\n\n            except (ValueError, TypeError):\n                 raise ValueError(f\"Could not parse index from ordered list entry: {entry}\")\n\n        # Check if all abstractions are included\n        if len(ordered_indices) != num_abstractions:\n             raise ValueError(f\"Ordered list length ({len(ordered_indices)}) does not match number of abstractions ({num_abstractions}). Missing indices: {set(range(num_abstractions)) - seen_indices}\")\n\n        print(f\"Determined chapter order (indices): {ordered_indices}\")\n        return ordered_indices # Return the list of indices\n\n    def post(self, shared, prep_res, exec_res):\n        # exec_res is already the list of ordered indices\n        shared[\"chapter_order\"] = exec_res # List of indices\n\nclass WriteChapters(BatchNode):\n    def prep(self, shared):\n        chapter_order = shared[\"chapter_order\"] # List of indices\n        abstractions = shared[\"abstractions\"]   # List of dicts, now using 'files' with indices\n        files_data = shared[\"files\"]\n        # Get already written chapters to provide context\n        # We store them temporarily during the batch run, not in shared memory yet\n        # The 'previous_chapters_summary' will be built progressively in the exec context\n        self.chapters_written_so_far = [] # Use instance variable for temporary storage across exec calls\n\n        # Create a complete list of all chapters\n        all_chapters = []\n        chapter_filenames = {} # Store chapter filename mapping for linking\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                chapter_num = i + 1\n                chapter_name = abstractions[abstraction_index][\"name\"]\n                # Create safe filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in chapter_name).lower()\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                # Format with link\n                all_chapters.append(f\"{chapter_num}. [{chapter_name}]({filename})\")\n                # Store mapping of chapter index to filename for linking\n                chapter_filenames[abstraction_index] = {\"num\": chapter_num, \"name\": chapter_name, \"filename\": filename}\n        \n        # Create a formatted string with all chapters\n        full_chapter_listing = \"\\n\".join(all_chapters)\n\n        items_to_process = []\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                abstraction_details = abstractions[abstraction_index]\n                # Use 'files' (list of indices) directly\n                related_file_indices = abstraction_details.get(\"files\", [])\n                # Get content using helper, passing indices\n                related_files_content_map = get_content_for_indices(files_data, related_file_indices)\n                \n                # Get previous chapter info for transitions\n                prev_chapter = None\n                if i > 0:\n                    prev_idx = chapter_order[i-1]\n                    prev_chapter = chapter_filenames[prev_idx]\n                \n                # Get next chapter info for transitions\n                next_chapter = None\n                if i < len(chapter_order) - 1:\n                    next_idx = chapter_order[i+1]\n                    next_chapter = chapter_filenames[next_idx]\n\n                items_to_process.append({\n                    \"chapter_num\": i + 1,\n                    \"abstraction_index\": abstraction_index,\n                    \"abstraction_details\": abstraction_details,\n                    \"related_files_content_map\": related_files_content_map,\n                    \"project_name\": shared[\"project_name\"],  # Add project name\n                    \"full_chapter_listing\": full_chapter_listing,  # Add the full chapter listing\n                    \"chapter_filenames\": chapter_filenames,  # Add chapter filenames mapping\n                    \"prev_chapter\": prev_chapter,  # Add previous chapter info\n                    \"next_chapter\": next_chapter,  # Add next chapter info\n                    # previous_chapters_summary will be added dynamically in exec\n                })\n            else:\n                print(f\"Warning: Invalid abstraction index {abstraction_index} in chapter_order. Skipping.\")\n\n        print(f\"Preparing to write {len(items_to_process)} chapters...\")\n        return items_to_process # Iterable for BatchNode\n\n    def exec(self, item):\n        # This runs for each item prepared above\n        abstraction_name = item[\"abstraction_details\"][\"name\"]\n        chapter_num = item[\"chapter_num\"]\n        project_name = item.get(\"project_name\")  # Get from item\n        print(f\"Writing chapter {chapter_num} for: {abstraction_name} using LLM...\")\n\n        # Prepare file context string from the map\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path.split('# ')[1] if '# ' in idx_path else idx_path} ---\\n{content}\"\n            for idx_path, content in item[\"related_files_content_map\"].items()\n        )\n\n        # Get summary of chapters written *before* this one\n        # Use the temporary instance variable\n        previous_chapters_summary = \"\\n---\\n\".join(self.chapters_written_so_far)\n\n\n        prompt = f\"\"\"\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `{project_name}` about the concept: \"{abstraction_name}\". This is Chapter {chapter_num}.\n\nConcept Details:\n- Description:\n{item[\"abstraction_details\"][\"description\"]}\n\nComplete Tutorial Structure:\n{item[\"full_chapter_listing\"]}\n\nContext from previous chapters (summary):\n{previous_chapters_summary if previous_chapters_summary else \"This is the first chapter.\"}\n\nRelevant Code Snippets:\n{file_context_str if file_context_str else \"No specific code snippets provided for this abstraction.\"}\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter {chapter_num}: {abstraction_name}`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n\"\"\"\n        chapter_content = call_llm(prompt)\n        # Basic validation/cleanup\n        actual_heading = f\"# Chapter {chapter_num}: {abstraction_name}\"\n        if not chapter_content.strip().startswith(f\"# Chapter {chapter_num}\"):\n             # Add heading if missing or incorrect, trying to preserve content\n             lines = chapter_content.strip().split('\\n')\n             if lines and lines[0].strip().startswith(\"#\"): # If there's some heading, replace it\n                 lines[0] = actual_heading\n                 chapter_content = \"\\n\".join(lines)\n             else: # Otherwise, prepend it\n                 chapter_content = f\"{actual_heading}\\n\\n{chapter_content}\"\n\n        # Add the generated content to our temporary list for the next iteration's context\n        self.chapters_written_so_far.append(chapter_content)\n\n        return chapter_content # Return the Markdown string\n\n    def post(self, shared, prep_res, exec_res_list):\n        # exec_res_list contains the generated Markdown for each chapter, in order\n        shared[\"chapters\"] = exec_res_list\n        # Clean up the temporary instance variable\n        del self.chapters_written_so_far\n        print(f\"Finished writing {len(exec_res_list)} chapters.\")\n\nclass CombineTutorial(Node):\n    def prep(self, shared):\n        project_name = shared[\"project_name\"]\n        output_base_dir = shared.get(\"output_dir\", \"output\") # Default output dir\n        output_path = os.path.join(output_base_dir, project_name)\n        repo_url = shared[\"repo_url\"]  # Get the repository URL\n\n        # Use 'label' from relationships_data['details']\n        relationships_data = shared[\"relationships\"] # {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        chapter_order = shared[\"chapter_order\"] # indices\n        abstractions = shared[\"abstractions\"]   # list of dicts\n        chapters_content = shared[\"chapters\"]   # list of strings\n\n        # --- Generate Mermaid Diagram ---\n        mermaid_lines = [\"flowchart TD\"]\n        # Add nodes for each abstraction\n        for i, abstr in enumerate(abstractions):\n            # Sanitize name for Mermaid ID and label\n            node_id = f\"A{i}\"\n            sanitized_name = abstr['name'].replace('\"', '')\n            node_label = sanitized_name # Using sanitized name only, no index\n            mermaid_lines.append(f'    {node_id}[\"{node_label}\"]')\n        # Add edges for relationships using 'label'\n        for rel in relationships_data['details']:\n            from_node_id = f\"A{rel['from']}\"\n            to_node_id = f\"A{rel['to']}\"\n            # Sanitize 'label' for edge label\n            edge_label = rel['label'].replace('\"', '').replace('\\n', ' ') # Basic sanitization\n            # Limit edge label length for readability (optional, but good for diagrams)\n            max_label_len = 30 # Make it shorter for labels\n            if len(edge_label) > max_label_len:\n                edge_label = edge_label[:max_label_len-3] + \"...\"\n            mermaid_lines.append(f'    {from_node_id} -- \"{edge_label}\" --> {to_node_id}')\n\n        mermaid_diagram = \"\\n\".join(mermaid_lines)\n        # --- End Mermaid ---\n\n\n        # Prepare index.md content\n        index_content = f\"# Tutorial: {project_name}\\n\\n\"\n        index_content += f\"{relationships_data['summary']}\\n\\n\"\n        index_content += f\"**Source Repository:** [{repo_url}]({repo_url})\\n\\n\"\n\n        # Add Mermaid diagram for relationships\n        index_content += \"```mermaid\\n\"\n        index_content += mermaid_diagram + \"\\n\"\n        index_content += \"```\\n\\n\"\n\n        index_content += \"## Chapters\\n\\n\"\n\n        chapter_files = []\n        # Generate chapter links based on the determined order\n        for i, abstraction_index in enumerate(chapter_order):\n            # Ensure index is valid and we have content for it\n            if 0 <= abstraction_index < len(abstractions) and i < len(chapters_content):\n                abstraction_name = abstractions[abstraction_index][\"name\"]\n                # Sanitize name for filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in abstraction_name).lower()\n                # Use chapter number (i+1) for ordering filename\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                index_content += f\"{i+1}. [{abstraction_name}]({filename})\\n\"\n                \n                # Add attribution to chapter content\n                chapter_content = chapters_content[i]\n                if not chapter_content.endswith(\"\\n\\n\"):\n                    chapter_content += \"\\n\\n\"\n                chapter_content += \"---\\n\\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\"\n                \n                # Store filename and corresponding content\n                chapter_files.append({\"filename\": filename, \"content\": chapter_content})\n            else:\n                 print(f\"Warning: Mismatch between chapter order, abstractions, or content at index {i} (abstraction index {abstraction_index}). Skipping file generation for this entry.\")\n\n        # Add attribution to index content\n        index_content += \"\\n\\n---\\n\\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\"\n\n        return {\n            \"output_path\": output_path,\n            \"index_content\": index_content,\n            \"chapter_files\": chapter_files # List of {\"filename\": str, \"content\": str}\n        }\n\n    def exec(self, prep_res):\n        output_path = prep_res[\"output_path\"]\n        index_content = prep_res[\"index_content\"]\n        chapter_files = prep_res[\"chapter_files\"]\n\n        print(f\"Combining tutorial into directory: {output_path}\")\n        # Rely on Node's built-in retry/fallback\n        os.makedirs(output_path, exist_ok=True)\n\n        # Write index.md\n        index_filepath = os.path.join(output_path, \"index.md\")\n        with open(index_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(index_content)\n        print(f\"  - Wrote {index_filepath}\")\n\n        # Write chapter files\n        for chapter_info in chapter_files:\n            chapter_filepath = os.path.join(output_path, chapter_info[\"filename\"])\n            with open(chapter_filepath, \"w\", encoding=\"utf-8\") as f:\n                f.write(chapter_info[\"content\"])\n            print(f\"  - Wrote {chapter_filepath}\")\n\n        return output_path # Return the final path\n\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"final_output_dir\"] = exec_res # Store the output path\n        print(f\"\\nTutorial generation complete! Files are in: {exec_res}\")\n        convert_all_md_to_html(exec_res) # Convert all Markdown files to HTML\n        \n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 2: Flow Orchestration`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "# Chapter 2: Flow Orchestration\n\nIn the previous chapter, [API Endpoint](01_api_endpoint.md), we learned how to trigger the tutorial generation process. Think of it like pressing the \"Start\" button on a machine. But what happens *after* you press that button? That's where Flow Orchestration comes in!\n\n## What is Flow Orchestration?\n\nImagine a factory that builds cars. You wouldn't just dump all the parts in a pile and hope a car magically appears, right? Instead, you'd have a carefully planned assembly line:\n\n1.  **Frame Assembly:** The car's frame is built.\n2.  **Engine Installation:** The engine is installed.\n3.  **Bodywork:** The body panels are attached.\n4.  **Painting:** The car gets its color.\n5.  **Interior Fitting:** Seats and other interior parts are added.\n6.  **Quality Control:** Final checks are performed.\n\nFlow Orchestration is like that assembly line for our tutorial generation system. It defines the *sequence of steps* needed to transform a codebase into a complete tutorial. It makes sure each task happens in the right order, passing the result of one task to the next.\n\nWithout flow orchestration, we'd have a chaotic mess. Chapters might be written before the codebase is even analyzed, or the tutorial might be combined before the chapters are finished! Flow Orchestration brings order to the process.\n\n## Key Concepts\n\n*   **Flow:** The overall process or pipeline of tasks. In our case, it's the entire tutorial generation process.\n*   **Node:** A single task or step in the flow. Each node performs a specific job, like fetching code, analyzing relationships, or writing a chapter.  Nodes can be thought of as functions in a programming language.\n*   **Sequence:** The order in which the nodes are executed.  This is crucial for ensuring the correct flow of data and dependencies.\n*   **Dependencies:**  A node might *depend* on the output of another node. For example, you can't write a chapter *about* an abstraction until you've identified *what* the abstractions *are*.\n\n## How Flow Orchestration Works in Our Project\n\nLet's look at the main steps in our tutorial generation flow:\n\n```mermaid\ngraph LR\n    A[Fetch Repo] --> B(Identify Abstractions)\n    B --> C{Analyze Relationships}\n    C --> D[Order Chapters]\n    D --> E((Write Chapters))\n    E --> F(Combine Tutorial)\n    style A fill:#f9f,stroke:#333,stroke-width:2px\n    style F fill:#f9f,stroke:#333,stroke-width:2px\n    style E fill:#ccf,stroke:#333,stroke-width:2px\n```\n\nHere's a breakdown of each node:\n\n1.  **Fetch Repo:**  Downloads the codebase from a remote repository (like GitHub) or reads it from a local directory. This node is responsible for getting the raw material needed to create the tutorial.  It outputs the files and their contents.\n2.  **Identify Abstractions:** Analyzes the codebase and identifies key concepts or abstractions that are important for understanding the project. This step essentially identifies the key topics to cover in the tutorial.  It outputs a list of abstractions, including their names, descriptions, and the files they're found in.\n3.  **Analyze Relationships:** Examines how the different abstractions interact with each other. This helps us understand the overall architecture of the codebase and how the different pieces fit together. The LLM creates a summary of the project and the relationships between the abstractions\n4.  **Order Chapters:** Determines the best order to present the abstractions in the tutorial.  Some abstractions might be foundational and need to be explained before others. The LLM figures out the order.\n5.  **Write Chapters:**  Generates the actual content for each chapter of the tutorial. It uses the abstraction details, relationships, and chapter order to create a beginner-friendly explanation of each concept. This node is marked with double parentheses `((...))` because it's a *BatchNode*. A `BatchNode` handles independent steps and can be run in parallel.\n6.  **Combine Tutorial:**  Assembles all the individual chapters into a complete tutorial, creating an index page and table of contents.\n\n## Code Example: Defining the Flow\n\nThe core of our flow orchestration is defined in `flow.py`:\n\n```python\nfrom pocketflow import Flow\n# Import all node classes from nodes.py\nfrom nodes import (\n    FetchRepo,\n    IdentifyAbstractions,\n    AnalyzeRelationships,\n    OrderChapters,\n    WriteChapters,\n    CombineTutorial\n)\n\ndef create_tutorial_flow():\n    \"\"\"Creates and returns the codebase tutorial generation flow.\"\"\"\n\n    # Instantiate nodes\n    fetch_repo = FetchRepo()\n    identify_abstractions = IdentifyAbstractions(max_retries=3, wait=10)\n    analyze_relationships = AnalyzeRelationships(max_retries=3, wait=10)\n    order_chapters = OrderChapters(max_retries=3, wait=10)\n    write_chapters = WriteChapters(max_retries=3, wait=10) # This is a BatchNode\n    combine_tutorial = CombineTutorial()\n\n    # Connect nodes in sequence based on the design\n    fetch_repo >> identify_abstractions\n    identify_abstractions >> analyze_relationships\n    analyze_relationships >> order_chapters\n    order_chapters >> write_chapters\n    write_chapters >> combine_tutorial\n\n    # Create the flow starting with FetchRepo\n    tutorial_flow = Flow(start=fetch_repo)\n\n    return tutorial_flow\n```\n\nLet's break down this code:\n\n*   **Importing Nodes:** We import all the node classes (like `FetchRepo`, `IdentifyAbstractions`, etc.) from the `nodes.py` file.  Think of these as importing different tools or functions that we need for our assembly line.\n*   **Instantiating Nodes:**  We create instances of each node class. For example, `fetch_repo = FetchRepo()` creates a specific instance of the `FetchRepo` node. This is where the maximum number of retries and wait time between retries is set for nodes that interact with the LLM. This helps handle cases where the LLM might be temporarily unavailable or returns an invalid response.\n*   **Connecting Nodes:** The `>>` operator defines the sequence of the flow. `fetch_repo >> identify_abstractions` means that the `IdentifyAbstractions` node will execute *after* the `FetchRepo` node completes.  The output of `FetchRepo` will be passed as input to `IdentifyAbstractions`.\n*   **Creating the Flow:** We use the `Flow` class from the `pocketflow` library to create the flow. We specify the `start` node, which is the first node to be executed.\n\nThis simple code defines the entire structure of our tutorial generation process!\n\n## Code Example: Running the Flow\n\nThe `main.py` file is responsible for setting up the flow with all the command line input parameters and calling the `Flow` to execute:\n\n```python\nimport dotenv\nimport os\nimport argparse\n# Import the function that creates the flow\nfrom flow import create_tutorial_flow\n\ndotenv.load_dotenv()\n\n# Default file patterns\nDEFAULT_INCLUDE_PATTERNS = {\n    \"*.py\", \"*.js\", \"*.jsx\", \"*.ts\", \"*.tsx\", \"*.go\", \"*.java\", \"*.pyi\", \"*.pyx\", \n    \"*.c\", \"*.cc\", \"*.cpp\", \"*.h\", \"*.md\", \"*.rst\", \"Dockerfile\", \n    \"Makefile\", \"*.yaml\", \"*.yml\",\n}\n\nDEFAULT_EXCLUDE_PATTERNS = {\n    \"*test*\", \"tests/*\", \"docs/*\", \"examples/*\", \"v1/*\", \n    \"dist/*\", \"build/*\", \"experimental/*\", \"deprecated/*\", \n    \"legacy/*\", \".git/*\", \".github/*\", \".next/*\", \".vscode/*\", \"obj/*\", \"bin/*\", \"node_modules/*\", \"*.log\"\n}\n\n# --- Main Function ---\ndef main():\n    parser = argparse.ArgumentParser(description=\"Generate a tutorial for a GitHub codebase or local directory.\")\n    \n    # Create mutually exclusive group for source\n    source_group = parser.add_mutually_exclusive_group(required=True)\n    source_group.add_argument(\"--repo\", help=\"URL of the public GitHub repository.\")\n    source_group.add_argument(\"--dir\", help=\"Path to local directory.\")\n    \n    parser.add_argument(\"-n\", \"--name\", help=\"Project name (optional, derived from repo/directory if omitted).\")\n    parser.add_argument(\"-t\", \"--token\", help=\"GitHub personal access token (optional, reads from GITHUB_TOKEN env var if not provided).\")\n    parser.add_argument(\"-o\", \"--output\", default=\"output\", help=\"Base directory for output (default: ./output).\")\n    parser.add_argument(\"-i\", \"--include\", nargs=\"+\", help=\"Include file patterns (e.g. '*.py' '*.js'). Defaults to common code files if not specified.\")\n    parser.add_argument(\"-e\", \"--exclude\", nargs=\"+\", help=\"Exclude file patterns (e.g. 'tests/*' 'docs/*'). Defaults to test/build directories if not specified.\")\n    parser.add_argument(\"-s\", \"--max-size\", type=int, default=100000, help=\"Maximum file size in bytes (default: 100000, about 100KB).\")\n\n    args = parser.parse_args()\n\n    # Get GitHub token from argument or environment variable if using repo\n    github_token = None\n    if args.repo:\n        github_token = args.token or os.environ.get('GITHUB_TOKEN')\n        if not github_token:\n            print(\"Warning: No GitHub token provided. You might hit rate limits for public repositories.\")\n\n    # Initialize the shared dictionary with inputs\n    shared = {\n        \"repo_url\": args.repo,\n        \"local_dir\": args.dir,\n        \"project_name\": args.name, # Can be None, FetchRepo will derive it\n        \"github_token\": github_token,\n        \"output_dir\": args.output, # Base directory for CombineTutorial output\n\n        # Add include/exclude patterns and max file size\n        \"include_patterns\": set(args.include) if args.include else DEFAULT_INCLUDE_PATTERNS,\n        \"exclude_patterns\": set(args.exclude) if args.exclude else DEFAULT_EXCLUDE_PATTERNS,\n        \"max_file_size\": args.max_size,\n\n        # Outputs will be populated by the nodes\n        \"files\": [],\n        \"abstractions\": [],\n        \"relationships\": {},\n        \"chapter_order\": [],\n        \"chapters\": [],\n        \"final_output_dir\": None\n    }\n\n    print(f\"Starting tutorial generation for: {args.repo or args.dir}\")\n\n    # Create the flow instance\n    tutorial_flow = create_tutorial_flow()\n\n    # Run the flow\n    tutorial_flow.run(shared)\n    \nif __name__ == \"__main__\":\n    main()\n```\n\nHere are the important parts:\n\n1.  **Parsing Arguments:** We use `argparse` to handle command-line arguments like the repository URL, project name, and output directory.\n2.  **Shared Dictionary:** The `shared` dictionary acts as a central store for data that is passed between the nodes. Each node can read data from this dictionary and write its results back to it.\n3.  **Creating the Flow:** We call `create_tutorial_flow()` to get an instance of the flow we defined in `flow.py`.\n4.  **Running the Flow:**  Finally, we call `tutorial_flow.run(shared)` to start the flow. The `shared` dictionary is passed to the `run` method so that the nodes can access and modify the data.\n\n## What Happens Inside a Node?\n\nEach node consists of three important methods. Looking at the example of `FetchRepo` in `nodes.py`:\n\n```python\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"local_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        if not project_name:\n            # Basic name derivation from URL or directory\n            if repo_url:\n                project_name = repo_url.split('/')[-1].replace('.git', '')\n            else:\n                project_name = os.path.basename(os.path.abspath(local_dir))\n            shared[\"project_name\"] = project_name\n\n        # Get file patterns directly from shared\n        include_patterns = shared[\"include_patterns\"]\n        exclude_patterns = shared[\"exclude_patterns\"]\n        max_file_size = shared[\"max_file_size\"]\n\n        return {\n            \"repo_url\": repo_url,\n            \"local_dir\": local_dir,\n            \"token\": shared.get(\"github_token\"),\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns,\n            \"max_file_size\": max_file_size,\n            \"use_relative_paths\": True\n        }\n\n    def exec(self, prep_res):\n        if prep_res[\"repo_url\"]:\n            print(f\"Crawling repository: {prep_res['repo_url']}...\")\n            result = crawl_github_files(\n                repo_url=prep_res[\"repo_url\"],\n                token=prep_res[\"token\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        else:\n            print(f\"Crawling directory: {prep_res['local_dir']}...\")\n            result = crawl_local_files(\n                directory=prep_res[\"local_dir\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n            \n        # Convert dict to list of tuples: [(path, content), ...]\n        files_list = list(result.get(\"files\", {}).items())\n        print(f\"Fetched {len(files_list)} files.\")\n        return files_list\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"files\"] = exec_res # List of (path, content) tuples\n```\n\n*   **`prep(shared)`:** This method prepares the input for the node's execution. It reads data from the `shared` dictionary and transforms it into a format that the `exec` method can use. It also performs any necessary setup or validation. Returns a `prep_res` object that contains everything necessary for the `exec` function to work.\n*   **`exec(prep_res)`:** This method performs the core logic of the node. It receives the prepared input from the `prep` method and executes the task. In the case of `FetchRepo`, it downloads the codebase from GitHub or reads it from the local directory. It returns the `exec_res` which is the output of the node.\n*   **`post(shared, prep_res, exec_res)`:** This method is executed *after* the `exec` method completes. It takes the output of the `exec` method (`exec_res`) and writes it back to the `shared` dictionary, making it available to the next node in the flow.  In the case of `FetchRepo`, it writes the list of files and their contents to the `shared[\"files\"]` variable.\n\n## Why Use Flow Orchestration?\n\n*   **Organization:** It provides a clear and structured way to define complex processes.\n*   **Maintainability:** It makes the code easier to understand and maintain by breaking it down into smaller, independent nodes.\n*   **Reusability:**  Nodes can be reused in different flows, making it easier to build new features or modify existing ones.\n*   **Testability:** Each node can be tested independently, making it easier to ensure the correctness of the entire system.\n*   **Error Handling:** `pocketflow` supports retries, fallbacks, error handling, and graceful termination, which are useful for nodes that interact with external resources.\n\n## Conclusion\n\nFlow Orchestration is a powerful technique for managing complex processes like our tutorial generation system. It provides a structured way to define the sequence of steps, making the code more organized, maintainable, and testable.\n\nIn the next chapter, [Configuration Management](03_configuration_management.md), we'll explore how to manage the different configuration settings that control the behavior of our system. This will allow us to customize the tutorial generation process for different codebases and use cases.\n", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `Tutorial-Codebase-Knowledge` about the concept: \"Configuration Management\". This is Chapter 3.\n\nConcept Details:\n- Description:\nThe project relies on configuration through environment variables and argument parsing. This allows\nfor flexible customization of the tutorial generation process. It's like a control panel that\nallows the user to adjust the settings of a machine to achieve the desired output.\n\n\nComplete Tutorial Structure:\n1. [API Endpoint](01_api_endpoint.md)\n2. [Flow Orchestration](02_flow_orchestration.md)\n3. [Configuration Management](03_configuration_management.md)\n4. [Codebase Crawling](04_codebase_crawling.md)\n5. [Abstraction Identification](05_abstraction_identification.md)\n6. [Relationship Analysis](06_relationship_analysis.md)\n7. [LLM Interaction](07_llm_interaction.md)\n8. [Chapter Writing](08_chapter_writing.md)\n9. [Tutorial Combination](09_tutorial_combination.md)\n\nContext from previous chapters (summary):\n# Chapter 1: API Endpoint\n\nWelcome to the first step in our journey to understanding how to generate tutorials from codebases! In this chapter, we'll explore the **API Endpoint**, which acts like the \"front door\" to our tutorial generation system.\n\nImagine you have a robot that can build LEGO sets. You wouldn't want to open up the robot and start fiddling with its internal wires every time you want it to build something, right? Instead, you'd want a simple button or a control panel to tell it what to do. The API Endpoint is like that button for our tutorial generator.\n\n**Why do we need an API Endpoint?**\n\nThink of it this way: you might want to trigger the tutorial generation process from:\n\n*   Your web browser\n*   Another program running on your computer\n*   Even from a different computer across the internet!\n\nAn API Endpoint allows all these different \"clients\" to communicate with our tutorial generation system in a standardized way. It's the central point of contact.\n\n**Key Concepts:**\n\n1.  **API (Application Programming Interface):** Think of it as a contract between two pieces of software. It defines how they can talk to each other. In our case, it defines how external programs can ask our system to generate a tutorial.\n\n2.  **Endpoint:** A specific URL (web address) that represents a particular function or resource in an API. In our case, it's the URL you send a request to in order to start the tutorial generation. It's like the specific button on our robot's control panel that says \"Build this LEGO set!\"\n\n3.  **Request:** The message you send to the API Endpoint to tell it what to do. This includes information like the repository URL, output folder and options for the tutorial generation.\n\n4.  **Response:** The message the API Endpoint sends back to you after it has processed your request. This might include a success message, an error message, or the path to the generated tutorial.\n\n**Our Use Case: Generating a tutorial for a GitHub repository**\n\nLet's say you want to generate a tutorial for a project on GitHub. The API Endpoint allows you to do this with a simple request. You'll need to provide the URL of the repository and any specific options for how you want the tutorial to be generated.\n\n**How to Use the API Endpoint:**\n\nOur API Endpoint is designed to receive a `POST` request to the `/generate` path. This request contains information about the repository for which you want to generate a tutorial.\n\nHere's an example of how you might send a request (this is just an example, the actual implementation will depend on how you're sending the request, e.g., using `curl`, `Postman`, or a Python script):\n\n```json\n{\n  \"repo_url\": \"https://github.com/fastapi/fastapi\",\n  \"name\": \"FastAPI\",\n  \"output\": \"tutorials\"\n}\n```\n\nThis request tells the system to:\n\n*   Generate a tutorial for the `https://github.com/fastapi/fastapi` repository.\n*   Use \"FastAPI\" as the project name.\n*   Save the tutorial in the `tutorials` output folder.\n\nAfter sending this request, the API Endpoint will process it and, if successful, return a response like this:\n\n```json\n{\n  \"message\": \"Tutorial generation completed successfully.\",\n  \"final_output_directory\": \"tutorials/FastAPI/html\"\n}\n```\n\nThis tells you that the tutorial was generated successfully and is located in the `tutorials/FastAPI/html` directory.\n\n**Internal Implementation: What happens under the hood?**\n\nWhen the API Endpoint receives a request, here's a simplified breakdown of what happens:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant API Endpoint\n    participant Main Script\n    participant Codebase\n    participant LLM\n\n    User->>API Endpoint: Sends tutorial generation request (repo URL, options)\n    API Endpoint->>Main Script: Executes main.py with provided arguments\n    Main Script->>Codebase: Crawls and analyzes the codebase\n    Main Script->>LLM: Interacts with LLM to generate content\n    Main Script->>API Endpoint: Returns success/failure and output path\n    API Endpoint->>User: Sends back response with result and output path\n```\n\nHere's a slightly more detailed explanation:\n\n1.  **Request Reception:** The API Endpoint receives the request with the repository URL and other options.\n2.  **Command Construction:**  It constructs a command line command to run the main script (`main.py`) with the provided parameters.\n3.  **Script Execution:** It executes the `main.py` script, which does the heavy lifting of cloning the repository, analyzing the code, and generating the tutorial content.\n4.  **Response Generation:** After the script finishes (either successfully or with an error), the API Endpoint creates a response with the appropriate message and, if successful, the path to the generated tutorial.\n5.  **Response Delivery:** Finally, it sends the response back to the user.\n\n**Code Snippets:**\n\nLet's look at some key parts of the code in `api.py`:\n\n```python\nfrom fastapi import FastAPI, HTTPException, status\nfrom pydantic import BaseModel, Field\n```\n\nThis imports the necessary libraries for creating the API using FastAPI (a Python framework for building APIs) and Pydantic (for data validation). We are importing `FastAPI` to create the API application, `HTTPException` to handle errors and send them to the user, and `BaseModel` and `Field` from `pydantic` to define the structure of our input data.\n\n```python\nclass GenerationRequest(BaseModel):\n    repo_url: str\n    name: Optional[str] = None\n    token: Optional[str] = None\n    output: str = Field(default=\"output\")\n    include: Optional[List[str]] = None\n    exclude: Optional[List[str]] = None\n    max_size: Optional[int] = None\n```\n\nThis defines the structure of the data that the API Endpoint expects to receive in the request. This is crucial for validation and ensuring that the input data is in the correct format. `repo_url` is required, while other fields like `name`, `token`, `output`, `include`, `exclude`, and `max_size` are optional.\n\n```python\n@app.post(\n    \"/generate\",\n    summary=\"Trigger Tutorial Generation\",\n    description=\"Accepts repository details and options, then runs the tutorial generation script.\",\n    status_code=status.HTTP_200_OK\n)\nasync def generate_tutorial(request_data: GenerationRequest):\n    # ... implementation details ...\n    return {\n        \"message\": \"Tutorial generation completed successfully.\",\n        \"final_output_directory\": expected_final_path,\n    }\n```\n\nThis code defines the `/generate` endpoint using the `@app.post` decorator. This tells FastAPI that this function should be called when a `POST` request is sent to the `/generate` URL. It receives the `request_data` (which is of type `GenerationRequest` that we defined above), does some processing, and returns a response containing a success message and the path to the generated tutorial.  The `status_code=status.HTTP_200_OK` ensures that a successful response returns a 200 OK status code.\n\n**CORS Middleware**\n\n```python\norigins = [\n    \"http://127.0.0.1:5500\", # VS Code Live Server default\n    \"http://localhost\",\n    \"http://localhost:8080\",\n    \"http://127.0.0.1\",\n    \"http://127.0.0.1:8080\",\n    \"null\", # file:// origin\n]\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=origins,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n```\n\nThis section is responsible for configuring Cross-Origin Resource Sharing (CORS). CORS is a security feature implemented by web browsers to restrict web pages from making requests to a different domain than the one which served the web page. Without CORS, you might not be able to send requests to your API from a webpage running on your local machine (e.g. during development). The middleware specifies the origins that are allowed to make requests to the API, ensuring that requests from these origins are not blocked by the browser.\n\n**Error Handling**\n\nThe `api.py` file includes comprehensive error handling to gracefully manage different scenarios that might arise during script execution.\n\n```python\n    except subprocess.CalledProcessError as e:\n        logger.error(f\"Script failed with exit code {e.returncode}\")\n        logger.error(f\"Script stdout:\\n{e.stdout}\")\n        logger.error(f\"Script stderr:\\n{e.stderr}\")\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail={\n                \"error\": \"Script execution failed.\",\n                \"details\": \"The tutorial generation script exited with an error.\",\n                \"return_code\": e.returncode,\n                \"script_stdout\": e.stdout,\n                \"script_stderr\": e.stderr\n            }\n        )\n    except FileNotFoundError:\n        error_msg = f\"Error: Script '{MAIN_SCRIPT_PATH}' or Python '{PYTHON_EXECUTABLE}' not found.\"\n        logger.error(error_msg)\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail={\"error\": \"Server configuration error.\", \"details\": error_msg}\n        )\n    except Exception as e:\n        logger.exception(\"An unexpected error occurred during script execution.\")\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail={\"error\": \"An unexpected internal server error occurred.\", \"details\": str(e)}\n        )\n```\n\n*   **`subprocess.CalledProcessError`**: Catches errors when the `main.py` script fails during execution. It logs the error details and raises an `HTTPException` with a 500 status code, providing details like the return code, stdout, and stderr of the script.\n*   **`FileNotFoundError`**: Handles cases where the `main.py` script or Python executable is not found. It raises an `HTTPException` with a 500 status code, indicating a server configuration issue.\n*   **`Exception`**: Serves as a catch-all for unexpected errors during script execution. It logs the exception and raises an `HTTPException` with a 500 status code, providing details about the error.\n\n**Conclusion:**\n\nIn this chapter, we've learned about the API Endpoint, which serves as the entry point to our tutorial generation system. We saw how it allows external programs to trigger the tutorial generation process with a simple request. We also explored the internal implementation and understood how it orchestrates the execution of the `main.py` script.\n\nNow that we know how to trigger the process, let's dive into [Flow Orchestration](02_flow_orchestration.md) in the next chapter to understand the overall sequence of steps involved in generating a tutorial.\n\n---\n# Chapter 2: Flow Orchestration\n\nIn the previous chapter, [API Endpoint](01_api_endpoint.md), we learned how to trigger the tutorial generation process. Think of it like pressing the \"Start\" button on a machine. But what happens *after* you press that button? That's where Flow Orchestration comes in!\n\n## What is Flow Orchestration?\n\nImagine a factory that builds cars. You wouldn't just dump all the parts in a pile and hope a car magically appears, right? Instead, you'd have a carefully planned assembly line:\n\n1.  **Frame Assembly:** The car's frame is built.\n2.  **Engine Installation:** The engine is installed.\n3.  **Bodywork:** The body panels are attached.\n4.  **Painting:** The car gets its color.\n5.  **Interior Fitting:** Seats and other interior parts are added.\n6.  **Quality Control:** Final checks are performed.\n\nFlow Orchestration is like that assembly line for our tutorial generation system. It defines the *sequence of steps* needed to transform a codebase into a complete tutorial. It makes sure each task happens in the right order, passing the result of one task to the next.\n\nWithout flow orchestration, we'd have a chaotic mess. Chapters might be written before the codebase is even analyzed, or the tutorial might be combined before the chapters are finished! Flow Orchestration brings order to the process.\n\n## Key Concepts\n\n*   **Flow:** The overall process or pipeline of tasks. In our case, it's the entire tutorial generation process.\n*   **Node:** A single task or step in the flow. Each node performs a specific job, like fetching code, analyzing relationships, or writing a chapter.  Nodes can be thought of as functions in a programming language.\n*   **Sequence:** The order in which the nodes are executed.  This is crucial for ensuring the correct flow of data and dependencies.\n*   **Dependencies:**  A node might *depend* on the output of another node. For example, you can't write a chapter *about* an abstraction until you've identified *what* the abstractions *are*.\n\n## How Flow Orchestration Works in Our Project\n\nLet's look at the main steps in our tutorial generation flow:\n\n```mermaid\ngraph LR\n    A[Fetch Repo] --> B(Identify Abstractions)\n    B --> C{Analyze Relationships}\n    C --> D[Order Chapters]\n    D --> E((Write Chapters))\n    E --> F(Combine Tutorial)\n    style A fill:#f9f,stroke:#333,stroke-width:2px\n    style F fill:#f9f,stroke:#333,stroke-width:2px\n    style E fill:#ccf,stroke:#333,stroke-width:2px\n```\n\nHere's a breakdown of each node:\n\n1.  **Fetch Repo:**  Downloads the codebase from a remote repository (like GitHub) or reads it from a local directory. This node is responsible for getting the raw material needed to create the tutorial.  It outputs the files and their contents.\n2.  **Identify Abstractions:** Analyzes the codebase and identifies key concepts or abstractions that are important for understanding the project. This step essentially identifies the key topics to cover in the tutorial.  It outputs a list of abstractions, including their names, descriptions, and the files they're found in.\n3.  **Analyze Relationships:** Examines how the different abstractions interact with each other. This helps us understand the overall architecture of the codebase and how the different pieces fit together. The LLM creates a summary of the project and the relationships between the abstractions\n4.  **Order Chapters:** Determines the best order to present the abstractions in the tutorial.  Some abstractions might be foundational and need to be explained before others. The LLM figures out the order.\n5.  **Write Chapters:**  Generates the actual content for each chapter of the tutorial. It uses the abstraction details, relationships, and chapter order to create a beginner-friendly explanation of each concept. This node is marked with double parentheses `((...))` because it's a *BatchNode*. A `BatchNode` handles independent steps and can be run in parallel.\n6.  **Combine Tutorial:**  Assembles all the individual chapters into a complete tutorial, creating an index page and table of contents.\n\n## Code Example: Defining the Flow\n\nThe core of our flow orchestration is defined in `flow.py`:\n\n```python\nfrom pocketflow import Flow\n# Import all node classes from nodes.py\nfrom nodes import (\n    FetchRepo,\n    IdentifyAbstractions,\n    AnalyzeRelationships,\n    OrderChapters,\n    WriteChapters,\n    CombineTutorial\n)\n\ndef create_tutorial_flow():\n    \"\"\"Creates and returns the codebase tutorial generation flow.\"\"\"\n\n    # Instantiate nodes\n    fetch_repo = FetchRepo()\n    identify_abstractions = IdentifyAbstractions(max_retries=3, wait=10)\n    analyze_relationships = AnalyzeRelationships(max_retries=3, wait=10)\n    order_chapters = OrderChapters(max_retries=3, wait=10)\n    write_chapters = WriteChapters(max_retries=3, wait=10) # This is a BatchNode\n    combine_tutorial = CombineTutorial()\n\n    # Connect nodes in sequence based on the design\n    fetch_repo >> identify_abstractions\n    identify_abstractions >> analyze_relationships\n    analyze_relationships >> order_chapters\n    order_chapters >> write_chapters\n    write_chapters >> combine_tutorial\n\n    # Create the flow starting with FetchRepo\n    tutorial_flow = Flow(start=fetch_repo)\n\n    return tutorial_flow\n```\n\nLet's break down this code:\n\n*   **Importing Nodes:** We import all the node classes (like `FetchRepo`, `IdentifyAbstractions`, etc.) from the `nodes.py` file.  Think of these as importing different tools or functions that we need for our assembly line.\n*   **Instantiating Nodes:**  We create instances of each node class. For example, `fetch_repo = FetchRepo()` creates a specific instance of the `FetchRepo` node. This is where the maximum number of retries and wait time between retries is set for nodes that interact with the LLM. This helps handle cases where the LLM might be temporarily unavailable or returns an invalid response.\n*   **Connecting Nodes:** The `>>` operator defines the sequence of the flow. `fetch_repo >> identify_abstractions` means that the `IdentifyAbstractions` node will execute *after* the `FetchRepo` node completes.  The output of `FetchRepo` will be passed as input to `IdentifyAbstractions`.\n*   **Creating the Flow:** We use the `Flow` class from the `pocketflow` library to create the flow. We specify the `start` node, which is the first node to be executed.\n\nThis simple code defines the entire structure of our tutorial generation process!\n\n## Code Example: Running the Flow\n\nThe `main.py` file is responsible for setting up the flow with all the command line input parameters and calling the `Flow` to execute:\n\n```python\nimport dotenv\nimport os\nimport argparse\n# Import the function that creates the flow\nfrom flow import create_tutorial_flow\n\ndotenv.load_dotenv()\n\n# Default file patterns\nDEFAULT_INCLUDE_PATTERNS = {\n    \"*.py\", \"*.js\", \"*.jsx\", \"*.ts\", \"*.tsx\", \"*.go\", \"*.java\", \"*.pyi\", \"*.pyx\", \n    \"*.c\", \"*.cc\", \"*.cpp\", \"*.h\", \"*.md\", \"*.rst\", \"Dockerfile\", \n    \"Makefile\", \"*.yaml\", \"*.yml\",\n}\n\nDEFAULT_EXCLUDE_PATTERNS = {\n    \"*test*\", \"tests/*\", \"docs/*\", \"examples/*\", \"v1/*\", \n    \"dist/*\", \"build/*\", \"experimental/*\", \"deprecated/*\", \n    \"legacy/*\", \".git/*\", \".github/*\", \".next/*\", \".vscode/*\", \"obj/*\", \"bin/*\", \"node_modules/*\", \"*.log\"\n}\n\n# --- Main Function ---\ndef main():\n    parser = argparse.ArgumentParser(description=\"Generate a tutorial for a GitHub codebase or local directory.\")\n    \n    # Create mutually exclusive group for source\n    source_group = parser.add_mutually_exclusive_group(required=True)\n    source_group.add_argument(\"--repo\", help=\"URL of the public GitHub repository.\")\n    source_group.add_argument(\"--dir\", help=\"Path to local directory.\")\n    \n    parser.add_argument(\"-n\", \"--name\", help=\"Project name (optional, derived from repo/directory if omitted).\")\n    parser.add_argument(\"-t\", \"--token\", help=\"GitHub personal access token (optional, reads from GITHUB_TOKEN env var if not provided).\")\n    parser.add_argument(\"-o\", \"--output\", default=\"output\", help=\"Base directory for output (default: ./output).\")\n    parser.add_argument(\"-i\", \"--include\", nargs=\"+\", help=\"Include file patterns (e.g. '*.py' '*.js'). Defaults to common code files if not specified.\")\n    parser.add_argument(\"-e\", \"--exclude\", nargs=\"+\", help=\"Exclude file patterns (e.g. 'tests/*' 'docs/*'). Defaults to test/build directories if not specified.\")\n    parser.add_argument(\"-s\", \"--max-size\", type=int, default=100000, help=\"Maximum file size in bytes (default: 100000, about 100KB).\")\n\n    args = parser.parse_args()\n\n    # Get GitHub token from argument or environment variable if using repo\n    github_token = None\n    if args.repo:\n        github_token = args.token or os.environ.get('GITHUB_TOKEN')\n        if not github_token:\n            print(\"Warning: No GitHub token provided. You might hit rate limits for public repositories.\")\n\n    # Initialize the shared dictionary with inputs\n    shared = {\n        \"repo_url\": args.repo,\n        \"local_dir\": args.dir,\n        \"project_name\": args.name, # Can be None, FetchRepo will derive it\n        \"github_token\": github_token,\n        \"output_dir\": args.output, # Base directory for CombineTutorial output\n\n        # Add include/exclude patterns and max file size\n        \"include_patterns\": set(args.include) if args.include else DEFAULT_INCLUDE_PATTERNS,\n        \"exclude_patterns\": set(args.exclude) if args.exclude else DEFAULT_EXCLUDE_PATTERNS,\n        \"max_file_size\": args.max_size,\n\n        # Outputs will be populated by the nodes\n        \"files\": [],\n        \"abstractions\": [],\n        \"relationships\": {},\n        \"chapter_order\": [],\n        \"chapters\": [],\n        \"final_output_dir\": None\n    }\n\n    print(f\"Starting tutorial generation for: {args.repo or args.dir}\")\n\n    # Create the flow instance\n    tutorial_flow = create_tutorial_flow()\n\n    # Run the flow\n    tutorial_flow.run(shared)\n    \nif __name__ == \"__main__\":\n    main()\n```\n\nHere are the important parts:\n\n1.  **Parsing Arguments:** We use `argparse` to handle command-line arguments like the repository URL, project name, and output directory.\n2.  **Shared Dictionary:** The `shared` dictionary acts as a central store for data that is passed between the nodes. Each node can read data from this dictionary and write its results back to it.\n3.  **Creating the Flow:** We call `create_tutorial_flow()` to get an instance of the flow we defined in `flow.py`.\n4.  **Running the Flow:**  Finally, we call `tutorial_flow.run(shared)` to start the flow. The `shared` dictionary is passed to the `run` method so that the nodes can access and modify the data.\n\n## What Happens Inside a Node?\n\nEach node consists of three important methods. Looking at the example of `FetchRepo` in `nodes.py`:\n\n```python\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"local_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        if not project_name:\n            # Basic name derivation from URL or directory\n            if repo_url:\n                project_name = repo_url.split('/')[-1].replace('.git', '')\n            else:\n                project_name = os.path.basename(os.path.abspath(local_dir))\n            shared[\"project_name\"] = project_name\n\n        # Get file patterns directly from shared\n        include_patterns = shared[\"include_patterns\"]\n        exclude_patterns = shared[\"exclude_patterns\"]\n        max_file_size = shared[\"max_file_size\"]\n\n        return {\n            \"repo_url\": repo_url,\n            \"local_dir\": local_dir,\n            \"token\": shared.get(\"github_token\"),\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns,\n            \"max_file_size\": max_file_size,\n            \"use_relative_paths\": True\n        }\n\n    def exec(self, prep_res):\n        if prep_res[\"repo_url\"]:\n            print(f\"Crawling repository: {prep_res['repo_url']}...\")\n            result = crawl_github_files(\n                repo_url=prep_res[\"repo_url\"],\n                token=prep_res[\"token\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        else:\n            print(f\"Crawling directory: {prep_res['local_dir']}...\")\n            result = crawl_local_files(\n                directory=prep_res[\"local_dir\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n            \n        # Convert dict to list of tuples: [(path, content), ...]\n        files_list = list(result.get(\"files\", {}).items())\n        print(f\"Fetched {len(files_list)} files.\")\n        return files_list\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"files\"] = exec_res # List of (path, content) tuples\n```\n\n*   **`prep(shared)`:** This method prepares the input for the node's execution. It reads data from the `shared` dictionary and transforms it into a format that the `exec` method can use. It also performs any necessary setup or validation. Returns a `prep_res` object that contains everything necessary for the `exec` function to work.\n*   **`exec(prep_res)`:** This method performs the core logic of the node. It receives the prepared input from the `prep` method and executes the task. In the case of `FetchRepo`, it downloads the codebase from GitHub or reads it from the local directory. It returns the `exec_res` which is the output of the node.\n*   **`post(shared, prep_res, exec_res)`:** This method is executed *after* the `exec` method completes. It takes the output of the `exec` method (`exec_res`) and writes it back to the `shared` dictionary, making it available to the next node in the flow.  In the case of `FetchRepo`, it writes the list of files and their contents to the `shared[\"files\"]` variable.\n\n## Why Use Flow Orchestration?\n\n*   **Organization:** It provides a clear and structured way to define complex processes.\n*   **Maintainability:** It makes the code easier to understand and maintain by breaking it down into smaller, independent nodes.\n*   **Reusability:**  Nodes can be reused in different flows, making it easier to build new features or modify existing ones.\n*   **Testability:** Each node can be tested independently, making it easier to ensure the correctness of the entire system.\n*   **Error Handling:** `pocketflow` supports retries, fallbacks, error handling, and graceful termination, which are useful for nodes that interact with external resources.\n\n## Conclusion\n\nFlow Orchestration is a powerful technique for managing complex processes like our tutorial generation system. It provides a structured way to define the sequence of steps, making the code more organized, maintainable, and testable.\n\nIn the next chapter, [Configuration Management](03_configuration_management.md), we'll explore how to manage the different configuration settings that control the behavior of our system. This will allow us to customize the tutorial generation process for different codebases and use cases.\n\n\nRelevant Code Snippets:\n--- File: main.py ---\nimport dotenv\nimport os\nimport argparse\n# Import the function that creates the flow\nfrom flow import create_tutorial_flow\n\ndotenv.load_dotenv()\n\n# Default file patterns\nDEFAULT_INCLUDE_PATTERNS = {\n    \"*.py\", \"*.js\", \"*.jsx\", \"*.ts\", \"*.tsx\", \"*.go\", \"*.java\", \"*.pyi\", \"*.pyx\", \n    \"*.c\", \"*.cc\", \"*.cpp\", \"*.h\", \"*.md\", \"*.rst\", \"Dockerfile\", \n    \"Makefile\", \"*.yaml\", \"*.yml\",\n}\n\nDEFAULT_EXCLUDE_PATTERNS = {\n    \"*test*\", \"tests/*\", \"docs/*\", \"examples/*\", \"v1/*\", \n    \"dist/*\", \"build/*\", \"experimental/*\", \"deprecated/*\", \n    \"legacy/*\", \".git/*\", \".github/*\", \".next/*\", \".vscode/*\", \"obj/*\", \"bin/*\", \"node_modules/*\", \"*.log\"\n}\n\n# --- Main Function ---\ndef main():\n    parser = argparse.ArgumentParser(description=\"Generate a tutorial for a GitHub codebase or local directory.\")\n    \n    # Create mutually exclusive group for source\n    source_group = parser.add_mutually_exclusive_group(required=True)\n    source_group.add_argument(\"--repo\", help=\"URL of the public GitHub repository.\")\n    source_group.add_argument(\"--dir\", help=\"Path to local directory.\")\n    \n    parser.add_argument(\"-n\", \"--name\", help=\"Project name (optional, derived from repo/directory if omitted).\")\n    parser.add_argument(\"-t\", \"--token\", help=\"GitHub personal access token (optional, reads from GITHUB_TOKEN env var if not provided).\")\n    parser.add_argument(\"-o\", \"--output\", default=\"output\", help=\"Base directory for output (default: ./output).\")\n    parser.add_argument(\"-i\", \"--include\", nargs=\"+\", help=\"Include file patterns (e.g. '*.py' '*.js'). Defaults to common code files if not specified.\")\n    parser.add_argument(\"-e\", \"--exclude\", nargs=\"+\", help=\"Exclude file patterns (e.g. 'tests/*' 'docs/*'). Defaults to test/build directories if not specified.\")\n    parser.add_argument(\"-s\", \"--max-size\", type=int, default=100000, help=\"Maximum file size in bytes (default: 100000, about 100KB).\")\n\n    args = parser.parse_args()\n\n    # Get GitHub token from argument or environment variable if using repo\n    github_token = None\n    if args.repo:\n        github_token = args.token or os.environ.get('GITHUB_TOKEN')\n        if not github_token:\n            print(\"Warning: No GitHub token provided. You might hit rate limits for public repositories.\")\n\n    # Initialize the shared dictionary with inputs\n    shared = {\n        \"repo_url\": args.repo,\n        \"local_dir\": args.dir,\n        \"project_name\": args.name, # Can be None, FetchRepo will derive it\n        \"github_token\": github_token,\n        \"output_dir\": args.output, # Base directory for CombineTutorial output\n\n        # Add include/exclude patterns and max file size\n        \"include_patterns\": set(args.include) if args.include else DEFAULT_INCLUDE_PATTERNS,\n        \"exclude_patterns\": set(args.exclude) if args.exclude else DEFAULT_EXCLUDE_PATTERNS,\n        \"max_file_size\": args.max_size,\n\n        # Outputs will be populated by the nodes\n        \"files\": [],\n        \"abstractions\": [],\n        \"relationships\": {},\n        \"chapter_order\": [],\n        \"chapters\": [],\n        \"final_output_dir\": None\n    }\n\n    print(f\"Starting tutorial generation for: {args.repo or args.dir}\")\n\n    # Create the flow instance\n    tutorial_flow = create_tutorial_flow()\n\n    # Run the flow\n    tutorial_flow.run(shared)\n    \nif __name__ == \"__main__\":\n    main()\n\n--- File: utils/call_llm.py ---\nfrom google import genai\nimport os\nimport logging\nimport json\nfrom datetime import datetime\n\n# Configure logging\nlog_directory = os.getenv(\"LOG_DIR\", \"logs\")\nos.makedirs(log_directory, exist_ok=True)\nlog_file = os.path.join(log_directory, f\"llm_calls_{datetime.now().strftime('%Y%m%d')}.log\")\n\n# Set up logger\nlogger = logging.getLogger(\"llm_logger\")\nlogger.setLevel(logging.INFO)\nlogger.propagate = False  # Prevent propagation to root logger\nfile_handler = logging.FileHandler(log_file)\nfile_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\nlogger.addHandler(file_handler)\n\n# Simple cache configuration\ncache_file = \"llm_cache.json\"\n\n# By default, we Google Gemini 2.5 pro, as it shows great performance for code understanding\ndef call_llm(prompt: str, use_cache: bool = True) -> str:\n    # Log the prompt\n    logger.info(f\"PROMPT: {prompt}\")\n    \n    # Check cache if enabled\n    if use_cache:\n        # Load cache from disk\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, 'r') as f:\n                    cache = json.load(f)\n            except:\n                logger.warning(f\"Failed to load cache, starting with empty cache\")\n        \n        # Return from cache if exists\n        if prompt in cache:\n            logger.info(f\"RESPONSE: {cache[prompt]}\")\n            return cache[prompt]\n    \n    # Call the LLM if not in cache or cache disabled\n    client = genai.Client(\n        vertexai=True, \n        # TODO: change to your own project id and location\n        project=os.getenv(\"GEMINI_PROJECT_ID\", \"your-project-id\"),\n        location=os.getenv(\"GEMINI_LOCATION\", \"us-central1\")\n    )\n    # You can comment the previous line and use the AI Studio key instead:\n    # client = genai.Client(\n    #     api_key=os.getenv(\"GEMINI_API_KEY\", \"your-api_key\"),\n    # )\n    model = os.getenv(\"GEMINI_MODEL\", \"gemini-2.5-pro-exp-03-25\")\n    response = client.models.generate_content(\n        model=model,\n        contents=[prompt]\n    )\n    response_text = response.text\n    \n    # Log the response\n    logger.info(f\"RESPONSE: {response_text}\")\n    \n    # Update cache if enabled\n    if use_cache:\n        # Load cache again to avoid overwrites\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, 'r') as f:\n                    cache = json.load(f)\n            except:\n                pass\n        \n        # Add to cache and save\n        cache[prompt] = response_text\n        try:\n            with open(cache_file, 'w') as f:\n                json.dump(cache, f)\n        except Exception as e:\n            logger.error(f\"Failed to save cache: {e}\")\n    \n    return response_text\n\n# # Use Anthropic Claude 3.7 Sonnet Extended Thinking\n# def call_llm(prompt, use_cache: bool = True):\n#     from anthropic import Anthropic\n#     client = Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\", \"your-api-key\"))\n#     response = client.messages.create(\n#         model=\"claude-3-7-sonnet-20250219\",\n#         max_tokens=21000,\n#         thinking={\n#             \"type\": \"enabled\",\n#             \"budget_tokens\": 20000\n#         },\n#         messages=[\n#             {\"role\": \"user\", \"content\": prompt}\n#         ]\n#     )\n#     return response.content[1].text\n\n# # Use OpenAI o1\n# def call_llm(prompt, use_cache: bool = True):    \n#     from openai import OpenAI\n#     client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n#     r = client.chat.completions.create(\n#         model=\"o1\",\n#         messages=[{\"role\": \"user\", \"content\": prompt}],\n#         response_format={\n#             \"type\": \"text\"\n#         },\n#         reasoning_effort=\"medium\",\n#         store=False\n#     )\n#     return r.choices[0].message.content\n\nif __name__ == \"__main__\":\n    test_prompt = \"Hello, how are you?\"\n    \n    # First call - should hit the API\n    print(\"Making call...\")\n    response1 = call_llm(test_prompt, use_cache=False)\n    print(f\"Response: {response1}\")\n    \n\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 3: Configuration Management`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "# Chapter 3: Configuration Management\n\nIn the previous chapter, [Flow Orchestration](02_flow_orchestration.md), we learned how to define the overall sequence of steps for generating a tutorial. Now, let's talk about how to *customize* that process. Think of it like this: you have a recipe for baking a cake, but you might want to adjust the recipe to make it chocolate or vanilla. That's where Configuration Management comes in!\n\n## What is Configuration Management?\n\nConfiguration Management is like having a control panel that allows you to adjust the settings of our tutorial generation \"machine\" to achieve the desired output. It lets you change things like:\n\n*   Which repository to generate a tutorial for.\n*   What name to give the project in the tutorial.\n*   Which files to include or exclude from analysis.\n*   Where to save the generated tutorial.\n\nWithout configuration management, we'd be stuck with a single, inflexible way of generating tutorials. It's like trying to bake every cake with the exact same recipe, even if you want different flavors!\n\n## Key Concepts\n\n1.  **Environment Variables:** These are like global settings that can be accessed by any program running on your computer. They're often used to store things like API keys or default values.\n2.  **Argument Parsing:** This allows you to pass settings directly to a program when you run it from the command line. It's like specifying the ingredients for your cake when you give the recipe to the baker.\n\n## Our Use Case: Generating a Tutorial for a Specific GitHub Repository with a Custom Name\n\nLet's say you want to generate a tutorial for the `requests` library on GitHub and name the project \"Python Requests Tutorial\". With Configuration Management, you can easily do this!\n\n## How to Use Configuration Management\n\nOur project uses both environment variables and argument parsing to manage configuration.\n\n### 1. Environment Variables\n\nEnvironment variables are useful for settings that you might want to reuse across multiple runs of the program, or that contain sensitive information you don't want to type directly into the command line.\n\nTo set an environment variable, you can use the following command in your terminal (replace `<your_token>` with your actual token):\n\n```bash\nexport GITHUB_TOKEN=<your_token>\n```\n\nOr, you can create a `.env` file in the project root directory:\n\n```\nGITHUB_TOKEN=<your_token>\n```\n\nThe `dotenv` library will automatically load these variables when the script runs.\n\nIn our `main.py` file, we use the `os.environ.get()` function to read the value of the `GITHUB_TOKEN` environment variable:\n\n```python\nimport os\ngithub_token = args.token or os.environ.get('GITHUB_TOKEN')\n```\n\nThis code first checks if the token was provided as a command-line argument (`args.token`). If not, it looks for the `GITHUB_TOKEN` environment variable.\n\n### 2. Argument Parsing\n\nArgument parsing allows you to specify settings directly when you run the `main.py` script from the command line.\n\nHere's how you would generate a tutorial for the `requests` library with a custom name:\n\n```bash\npython main.py --repo https://github.com/psf/requests -n \"Python Requests Tutorial\" -o tutorials\n```\n\nLet's break down this command:\n\n*   `python main.py`: This tells your computer to run the `main.py` script using Python.\n*   `--repo https://github.com/psf/requests`: This specifies the URL of the GitHub repository to use. `--repo` is the *argument* name, and `https://github.com/psf/requests` is the *value* of that argument.\n*   `-n \"Python Requests Tutorial\"`: This sets the project name to \"Python Requests Tutorial\". `-n` is a *short* argument name, and `\"Python Requests Tutorial\"` is the value.\n*   `-o tutorials`: This specifies that the tutorial output will be written to the `tutorials` directory.\n\nThe `argparse` module in Python handles the parsing of these command-line arguments. In `main.py`, we define the arguments that our script accepts:\n\n```python\nimport argparse\n\nparser = argparse.ArgumentParser(description=\"Generate a tutorial for a GitHub codebase or local directory.\")\nparser.add_argument(\"--repo\", help=\"URL of the public GitHub repository.\")\nparser.add_argument(\"-n\", \"--name\", help=\"Project name (optional, derived from repo/directory if omitted).\")\nparser.add_argument(\"-o\", \"--output\", default=\"output\", help=\"Base directory for output (default: ./output).\")\n# ... other arguments\nargs = parser.parse_args()\n```\n\nThis code defines three arguments: `--repo`, `--name` (with the short form `-n`), and `--output` (with the short form `-o`). The `help` parameter provides a description of each argument that is displayed when you run the script with the `--help` flag:\n\n```bash\npython main.py --help\n```\n\n### How Include and Exclude Patterns Work\n\nThe `-i` (or `--include`) and `-e` (or `--exclude`) options are used to specify which files to include or exclude from the tutorial generation process. This is helpful if you want to focus on specific parts of the codebase or ignore irrelevant files (like test files or documentation).\n\nFor example, to only include Python files and exclude test directories, you could use the following command:\n\n```bash\npython main.py --repo https://github.com/psf/requests -i \"*.py\" -e \"tests/*\"\n```\n\n## Internal Implementation\n\nHere's a simplified view of how Configuration Management works internally:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Command Line\n    participant Main Script\n    participant Args Namespace\n    User->>Command Line: Runs script with arguments: python main.py --repo ...\n    Command Line->>Main Script: Executes main.py\n    Main Script->>Args Namespace: Parses command-line arguments using argparse\n    Args Namespace->>Main Script: Returns arguments as an object (e.g., args.repo, args.name)\n    Main Script->>Main Script: Accesses argument values and uses them\n```\n\nHere's a breakdown:\n\n1.  **User Interaction:** The user runs the `main.py` script from the command line, providing arguments like the repository URL and project name.\n2.  **Argument Parsing:** The `argparse` module in `main.py` parses these arguments and stores them in an `Args Namespace` object.\n3.  **Accessing Arguments:** The `main.py` script accesses the argument values from the `Args Namespace` object (e.g., `args.repo`, `args.name`).\n4.  **Using Arguments:** The script uses these values to configure the tutorial generation process. For example, the `repo_url` value is passed to the [Codebase Crawling](04_codebase_crawling.md) node to download the codebase.\n\n## Code Example: Passing Configuration to Nodes\n\nThe parsed arguments are stored in the `shared` dictionary and passed to the nodes in the flow. For example, in `main.py`:\n\n```python\nshared = {\n    \"repo_url\": args.repo,\n    \"project_name\": args.name,\n    \"output_dir\": args.output,\n    # ... other arguments\n}\n\ntutorial_flow.run(shared)\n```\n\nEach node can then access the configuration values from the `shared` dictionary in its `prep` method. For example, in `nodes.py`:\n\n```python\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        project_name = shared.get(\"project_name\")\n        # ...\n        return {\n            \"repo_url\": repo_url,\n            # ...\n        }\n```\n\n## Why is Configuration Management Important?\n\n*   **Flexibility:** It allows you to customize the tutorial generation process for different codebases and use cases.\n*   **Reusability:** You can easily reuse the same script to generate tutorials for different projects by simply changing the configuration settings.\n*   **Automation:** You can automate the tutorial generation process by running the script with a predefined set of arguments.\n\n## Conclusion\n\nIn this chapter, we've learned about Configuration Management, which allows us to customize the tutorial generation process by using environment variables and argument parsing. This gives us the flexibility to generate tutorials for different codebases with different settings.\n\nIn the next chapter, [Codebase Crawling](04_codebase_crawling.md), we'll dive into the process of fetching and analyzing the codebase itself.\n", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `Tutorial-Codebase-Knowledge` about the concept: \"Codebase Crawling\". This is Chapter 4.\n\nConcept Details:\n- Description:\nCodebase Crawling is the process of gathering all the necessary files from either a GitHub\nrepository or a local directory. This involves filtering files based on include and exclude\npatterns, limiting the maximum file size, and handling potential errors. It's like a diligent\nlibrarian collecting books from different shelves, ensuring they meet certain criteria before\nadding them to the collection.\n\n\nComplete Tutorial Structure:\n1. [API Endpoint](01_api_endpoint.md)\n2. [Flow Orchestration](02_flow_orchestration.md)\n3. [Configuration Management](03_configuration_management.md)\n4. [Codebase Crawling](04_codebase_crawling.md)\n5. [Abstraction Identification](05_abstraction_identification.md)\n6. [Relationship Analysis](06_relationship_analysis.md)\n7. [LLM Interaction](07_llm_interaction.md)\n8. [Chapter Writing](08_chapter_writing.md)\n9. [Tutorial Combination](09_tutorial_combination.md)\n\nContext from previous chapters (summary):\n# Chapter 1: API Endpoint\n\nWelcome to the first step in our journey to understanding how to generate tutorials from codebases! In this chapter, we'll explore the **API Endpoint**, which acts like the \"front door\" to our tutorial generation system.\n\nImagine you have a robot that can build LEGO sets. You wouldn't want to open up the robot and start fiddling with its internal wires every time you want it to build something, right? Instead, you'd want a simple button or a control panel to tell it what to do. The API Endpoint is like that button for our tutorial generator.\n\n**Why do we need an API Endpoint?**\n\nThink of it this way: you might want to trigger the tutorial generation process from:\n\n*   Your web browser\n*   Another program running on your computer\n*   Even from a different computer across the internet!\n\nAn API Endpoint allows all these different \"clients\" to communicate with our tutorial generation system in a standardized way. It's the central point of contact.\n\n**Key Concepts:**\n\n1.  **API (Application Programming Interface):** Think of it as a contract between two pieces of software. It defines how they can talk to each other. In our case, it defines how external programs can ask our system to generate a tutorial.\n\n2.  **Endpoint:** A specific URL (web address) that represents a particular function or resource in an API. In our case, it's the URL you send a request to in order to start the tutorial generation. It's like the specific button on our robot's control panel that says \"Build this LEGO set!\"\n\n3.  **Request:** The message you send to the API Endpoint to tell it what to do. This includes information like the repository URL, output folder and options for the tutorial generation.\n\n4.  **Response:** The message the API Endpoint sends back to you after it has processed your request. This might include a success message, an error message, or the path to the generated tutorial.\n\n**Our Use Case: Generating a tutorial for a GitHub repository**\n\nLet's say you want to generate a tutorial for a project on GitHub. The API Endpoint allows you to do this with a simple request. You'll need to provide the URL of the repository and any specific options for how you want the tutorial to be generated.\n\n**How to Use the API Endpoint:**\n\nOur API Endpoint is designed to receive a `POST` request to the `/generate` path. This request contains information about the repository for which you want to generate a tutorial.\n\nHere's an example of how you might send a request (this is just an example, the actual implementation will depend on how you're sending the request, e.g., using `curl`, `Postman`, or a Python script):\n\n```json\n{\n  \"repo_url\": \"https://github.com/fastapi/fastapi\",\n  \"name\": \"FastAPI\",\n  \"output\": \"tutorials\"\n}\n```\n\nThis request tells the system to:\n\n*   Generate a tutorial for the `https://github.com/fastapi/fastapi` repository.\n*   Use \"FastAPI\" as the project name.\n*   Save the tutorial in the `tutorials` output folder.\n\nAfter sending this request, the API Endpoint will process it and, if successful, return a response like this:\n\n```json\n{\n  \"message\": \"Tutorial generation completed successfully.\",\n  \"final_output_directory\": \"tutorials/FastAPI/html\"\n}\n```\n\nThis tells you that the tutorial was generated successfully and is located in the `tutorials/FastAPI/html` directory.\n\n**Internal Implementation: What happens under the hood?**\n\nWhen the API Endpoint receives a request, here's a simplified breakdown of what happens:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant API Endpoint\n    participant Main Script\n    participant Codebase\n    participant LLM\n\n    User->>API Endpoint: Sends tutorial generation request (repo URL, options)\n    API Endpoint->>Main Script: Executes main.py with provided arguments\n    Main Script->>Codebase: Crawls and analyzes the codebase\n    Main Script->>LLM: Interacts with LLM to generate content\n    Main Script->>API Endpoint: Returns success/failure and output path\n    API Endpoint->>User: Sends back response with result and output path\n```\n\nHere's a slightly more detailed explanation:\n\n1.  **Request Reception:** The API Endpoint receives the request with the repository URL and other options.\n2.  **Command Construction:**  It constructs a command line command to run the main script (`main.py`) with the provided parameters.\n3.  **Script Execution:** It executes the `main.py` script, which does the heavy lifting of cloning the repository, analyzing the code, and generating the tutorial content.\n4.  **Response Generation:** After the script finishes (either successfully or with an error), the API Endpoint creates a response with the appropriate message and, if successful, the path to the generated tutorial.\n5.  **Response Delivery:** Finally, it sends the response back to the user.\n\n**Code Snippets:**\n\nLet's look at some key parts of the code in `api.py`:\n\n```python\nfrom fastapi import FastAPI, HTTPException, status\nfrom pydantic import BaseModel, Field\n```\n\nThis imports the necessary libraries for creating the API using FastAPI (a Python framework for building APIs) and Pydantic (for data validation). We are importing `FastAPI` to create the API application, `HTTPException` to handle errors and send them to the user, and `BaseModel` and `Field` from `pydantic` to define the structure of our input data.\n\n```python\nclass GenerationRequest(BaseModel):\n    repo_url: str\n    name: Optional[str] = None\n    token: Optional[str] = None\n    output: str = Field(default=\"output\")\n    include: Optional[List[str]] = None\n    exclude: Optional[List[str]] = None\n    max_size: Optional[int] = None\n```\n\nThis defines the structure of the data that the API Endpoint expects to receive in the request. This is crucial for validation and ensuring that the input data is in the correct format. `repo_url` is required, while other fields like `name`, `token`, `output`, `include`, `exclude`, and `max_size` are optional.\n\n```python\n@app.post(\n    \"/generate\",\n    summary=\"Trigger Tutorial Generation\",\n    description=\"Accepts repository details and options, then runs the tutorial generation script.\",\n    status_code=status.HTTP_200_OK\n)\nasync def generate_tutorial(request_data: GenerationRequest):\n    # ... implementation details ...\n    return {\n        \"message\": \"Tutorial generation completed successfully.\",\n        \"final_output_directory\": expected_final_path,\n    }\n```\n\nThis code defines the `/generate` endpoint using the `@app.post` decorator. This tells FastAPI that this function should be called when a `POST` request is sent to the `/generate` URL. It receives the `request_data` (which is of type `GenerationRequest` that we defined above), does some processing, and returns a response containing a success message and the path to the generated tutorial.  The `status_code=status.HTTP_200_OK` ensures that a successful response returns a 200 OK status code.\n\n**CORS Middleware**\n\n```python\norigins = [\n    \"http://127.0.0.1:5500\", # VS Code Live Server default\n    \"http://localhost\",\n    \"http://localhost:8080\",\n    \"http://127.0.0.1\",\n    \"http://127.0.0.1:8080\",\n    \"null\", # file:// origin\n]\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=origins,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n```\n\nThis section is responsible for configuring Cross-Origin Resource Sharing (CORS). CORS is a security feature implemented by web browsers to restrict web pages from making requests to a different domain than the one which served the web page. Without CORS, you might not be able to send requests to your API from a webpage running on your local machine (e.g. during development). The middleware specifies the origins that are allowed to make requests to the API, ensuring that requests from these origins are not blocked by the browser.\n\n**Error Handling**\n\nThe `api.py` file includes comprehensive error handling to gracefully manage different scenarios that might arise during script execution.\n\n```python\n    except subprocess.CalledProcessError as e:\n        logger.error(f\"Script failed with exit code {e.returncode}\")\n        logger.error(f\"Script stdout:\\n{e.stdout}\")\n        logger.error(f\"Script stderr:\\n{e.stderr}\")\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail={\n                \"error\": \"Script execution failed.\",\n                \"details\": \"The tutorial generation script exited with an error.\",\n                \"return_code\": e.returncode,\n                \"script_stdout\": e.stdout,\n                \"script_stderr\": e.stderr\n            }\n        )\n    except FileNotFoundError:\n        error_msg = f\"Error: Script '{MAIN_SCRIPT_PATH}' or Python '{PYTHON_EXECUTABLE}' not found.\"\n        logger.error(error_msg)\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail={\"error\": \"Server configuration error.\", \"details\": error_msg}\n        )\n    except Exception as e:\n        logger.exception(\"An unexpected error occurred during script execution.\")\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail={\"error\": \"An unexpected internal server error occurred.\", \"details\": str(e)}\n        )\n```\n\n*   **`subprocess.CalledProcessError`**: Catches errors when the `main.py` script fails during execution. It logs the error details and raises an `HTTPException` with a 500 status code, providing details like the return code, stdout, and stderr of the script.\n*   **`FileNotFoundError`**: Handles cases where the `main.py` script or Python executable is not found. It raises an `HTTPException` with a 500 status code, indicating a server configuration issue.\n*   **`Exception`**: Serves as a catch-all for unexpected errors during script execution. It logs the exception and raises an `HTTPException` with a 500 status code, providing details about the error.\n\n**Conclusion:**\n\nIn this chapter, we've learned about the API Endpoint, which serves as the entry point to our tutorial generation system. We saw how it allows external programs to trigger the tutorial generation process with a simple request. We also explored the internal implementation and understood how it orchestrates the execution of the `main.py` script.\n\nNow that we know how to trigger the process, let's dive into [Flow Orchestration](02_flow_orchestration.md) in the next chapter to understand the overall sequence of steps involved in generating a tutorial.\n\n---\n# Chapter 2: Flow Orchestration\n\nIn the previous chapter, [API Endpoint](01_api_endpoint.md), we learned how to trigger the tutorial generation process. Think of it like pressing the \"Start\" button on a machine. But what happens *after* you press that button? That's where Flow Orchestration comes in!\n\n## What is Flow Orchestration?\n\nImagine a factory that builds cars. You wouldn't just dump all the parts in a pile and hope a car magically appears, right? Instead, you'd have a carefully planned assembly line:\n\n1.  **Frame Assembly:** The car's frame is built.\n2.  **Engine Installation:** The engine is installed.\n3.  **Bodywork:** The body panels are attached.\n4.  **Painting:** The car gets its color.\n5.  **Interior Fitting:** Seats and other interior parts are added.\n6.  **Quality Control:** Final checks are performed.\n\nFlow Orchestration is like that assembly line for our tutorial generation system. It defines the *sequence of steps* needed to transform a codebase into a complete tutorial. It makes sure each task happens in the right order, passing the result of one task to the next.\n\nWithout flow orchestration, we'd have a chaotic mess. Chapters might be written before the codebase is even analyzed, or the tutorial might be combined before the chapters are finished! Flow Orchestration brings order to the process.\n\n## Key Concepts\n\n*   **Flow:** The overall process or pipeline of tasks. In our case, it's the entire tutorial generation process.\n*   **Node:** A single task or step in the flow. Each node performs a specific job, like fetching code, analyzing relationships, or writing a chapter.  Nodes can be thought of as functions in a programming language.\n*   **Sequence:** The order in which the nodes are executed.  This is crucial for ensuring the correct flow of data and dependencies.\n*   **Dependencies:**  A node might *depend* on the output of another node. For example, you can't write a chapter *about* an abstraction until you've identified *what* the abstractions *are*.\n\n## How Flow Orchestration Works in Our Project\n\nLet's look at the main steps in our tutorial generation flow:\n\n```mermaid\ngraph LR\n    A[Fetch Repo] --> B(Identify Abstractions)\n    B --> C{Analyze Relationships}\n    C --> D[Order Chapters]\n    D --> E((Write Chapters))\n    E --> F(Combine Tutorial)\n    style A fill:#f9f,stroke:#333,stroke-width:2px\n    style F fill:#f9f,stroke:#333,stroke-width:2px\n    style E fill:#ccf,stroke:#333,stroke-width:2px\n```\n\nHere's a breakdown of each node:\n\n1.  **Fetch Repo:**  Downloads the codebase from a remote repository (like GitHub) or reads it from a local directory. This node is responsible for getting the raw material needed to create the tutorial.  It outputs the files and their contents.\n2.  **Identify Abstractions:** Analyzes the codebase and identifies key concepts or abstractions that are important for understanding the project. This step essentially identifies the key topics to cover in the tutorial.  It outputs a list of abstractions, including their names, descriptions, and the files they're found in.\n3.  **Analyze Relationships:** Examines how the different abstractions interact with each other. This helps us understand the overall architecture of the codebase and how the different pieces fit together. The LLM creates a summary of the project and the relationships between the abstractions\n4.  **Order Chapters:** Determines the best order to present the abstractions in the tutorial.  Some abstractions might be foundational and need to be explained before others. The LLM figures out the order.\n5.  **Write Chapters:**  Generates the actual content for each chapter of the tutorial. It uses the abstraction details, relationships, and chapter order to create a beginner-friendly explanation of each concept. This node is marked with double parentheses `((...))` because it's a *BatchNode*. A `BatchNode` handles independent steps and can be run in parallel.\n6.  **Combine Tutorial:**  Assembles all the individual chapters into a complete tutorial, creating an index page and table of contents.\n\n## Code Example: Defining the Flow\n\nThe core of our flow orchestration is defined in `flow.py`:\n\n```python\nfrom pocketflow import Flow\n# Import all node classes from nodes.py\nfrom nodes import (\n    FetchRepo,\n    IdentifyAbstractions,\n    AnalyzeRelationships,\n    OrderChapters,\n    WriteChapters,\n    CombineTutorial\n)\n\ndef create_tutorial_flow():\n    \"\"\"Creates and returns the codebase tutorial generation flow.\"\"\"\n\n    # Instantiate nodes\n    fetch_repo = FetchRepo()\n    identify_abstractions = IdentifyAbstractions(max_retries=3, wait=10)\n    analyze_relationships = AnalyzeRelationships(max_retries=3, wait=10)\n    order_chapters = OrderChapters(max_retries=3, wait=10)\n    write_chapters = WriteChapters(max_retries=3, wait=10) # This is a BatchNode\n    combine_tutorial = CombineTutorial()\n\n    # Connect nodes in sequence based on the design\n    fetch_repo >> identify_abstractions\n    identify_abstractions >> analyze_relationships\n    analyze_relationships >> order_chapters\n    order_chapters >> write_chapters\n    write_chapters >> combine_tutorial\n\n    # Create the flow starting with FetchRepo\n    tutorial_flow = Flow(start=fetch_repo)\n\n    return tutorial_flow\n```\n\nLet's break down this code:\n\n*   **Importing Nodes:** We import all the node classes (like `FetchRepo`, `IdentifyAbstractions`, etc.) from the `nodes.py` file.  Think of these as importing different tools or functions that we need for our assembly line.\n*   **Instantiating Nodes:**  We create instances of each node class. For example, `fetch_repo = FetchRepo()` creates a specific instance of the `FetchRepo` node. This is where the maximum number of retries and wait time between retries is set for nodes that interact with the LLM. This helps handle cases where the LLM might be temporarily unavailable or returns an invalid response.\n*   **Connecting Nodes:** The `>>` operator defines the sequence of the flow. `fetch_repo >> identify_abstractions` means that the `IdentifyAbstractions` node will execute *after* the `FetchRepo` node completes.  The output of `FetchRepo` will be passed as input to `IdentifyAbstractions`.\n*   **Creating the Flow:** We use the `Flow` class from the `pocketflow` library to create the flow. We specify the `start` node, which is the first node to be executed.\n\nThis simple code defines the entire structure of our tutorial generation process!\n\n## Code Example: Running the Flow\n\nThe `main.py` file is responsible for setting up the flow with all the command line input parameters and calling the `Flow` to execute:\n\n```python\nimport dotenv\nimport os\nimport argparse\n# Import the function that creates the flow\nfrom flow import create_tutorial_flow\n\ndotenv.load_dotenv()\n\n# Default file patterns\nDEFAULT_INCLUDE_PATTERNS = {\n    \"*.py\", \"*.js\", \"*.jsx\", \"*.ts\", \"*.tsx\", \"*.go\", \"*.java\", \"*.pyi\", \"*.pyx\", \n    \"*.c\", \"*.cc\", \"*.cpp\", \"*.h\", \"*.md\", \"*.rst\", \"Dockerfile\", \n    \"Makefile\", \"*.yaml\", \"*.yml\",\n}\n\nDEFAULT_EXCLUDE_PATTERNS = {\n    \"*test*\", \"tests/*\", \"docs/*\", \"examples/*\", \"v1/*\", \n    \"dist/*\", \"build/*\", \"experimental/*\", \"deprecated/*\", \n    \"legacy/*\", \".git/*\", \".github/*\", \".next/*\", \".vscode/*\", \"obj/*\", \"bin/*\", \"node_modules/*\", \"*.log\"\n}\n\n# --- Main Function ---\ndef main():\n    parser = argparse.ArgumentParser(description=\"Generate a tutorial for a GitHub codebase or local directory.\")\n    \n    # Create mutually exclusive group for source\n    source_group = parser.add_mutually_exclusive_group(required=True)\n    source_group.add_argument(\"--repo\", help=\"URL of the public GitHub repository.\")\n    source_group.add_argument(\"--dir\", help=\"Path to local directory.\")\n    \n    parser.add_argument(\"-n\", \"--name\", help=\"Project name (optional, derived from repo/directory if omitted).\")\n    parser.add_argument(\"-t\", \"--token\", help=\"GitHub personal access token (optional, reads from GITHUB_TOKEN env var if not provided).\")\n    parser.add_argument(\"-o\", \"--output\", default=\"output\", help=\"Base directory for output (default: ./output).\")\n    parser.add_argument(\"-i\", \"--include\", nargs=\"+\", help=\"Include file patterns (e.g. '*.py' '*.js'). Defaults to common code files if not specified.\")\n    parser.add_argument(\"-e\", \"--exclude\", nargs=\"+\", help=\"Exclude file patterns (e.g. 'tests/*' 'docs/*'). Defaults to test/build directories if not specified.\")\n    parser.add_argument(\"-s\", \"--max-size\", type=int, default=100000, help=\"Maximum file size in bytes (default: 100000, about 100KB).\")\n\n    args = parser.parse_args()\n\n    # Get GitHub token from argument or environment variable if using repo\n    github_token = None\n    if args.repo:\n        github_token = args.token or os.environ.get('GITHUB_TOKEN')\n        if not github_token:\n            print(\"Warning: No GitHub token provided. You might hit rate limits for public repositories.\")\n\n    # Initialize the shared dictionary with inputs\n    shared = {\n        \"repo_url\": args.repo,\n        \"local_dir\": args.dir,\n        \"project_name\": args.name, # Can be None, FetchRepo will derive it\n        \"github_token\": github_token,\n        \"output_dir\": args.output, # Base directory for CombineTutorial output\n\n        # Add include/exclude patterns and max file size\n        \"include_patterns\": set(args.include) if args.include else DEFAULT_INCLUDE_PATTERNS,\n        \"exclude_patterns\": set(args.exclude) if args.exclude else DEFAULT_EXCLUDE_PATTERNS,\n        \"max_file_size\": args.max_size,\n\n        # Outputs will be populated by the nodes\n        \"files\": [],\n        \"abstractions\": [],\n        \"relationships\": {},\n        \"chapter_order\": [],\n        \"chapters\": [],\n        \"final_output_dir\": None\n    }\n\n    print(f\"Starting tutorial generation for: {args.repo or args.dir}\")\n\n    # Create the flow instance\n    tutorial_flow = create_tutorial_flow()\n\n    # Run the flow\n    tutorial_flow.run(shared)\n    \nif __name__ == \"__main__\":\n    main()\n```\n\nHere are the important parts:\n\n1.  **Parsing Arguments:** We use `argparse` to handle command-line arguments like the repository URL, project name, and output directory.\n2.  **Shared Dictionary:** The `shared` dictionary acts as a central store for data that is passed between the nodes. Each node can read data from this dictionary and write its results back to it.\n3.  **Creating the Flow:** We call `create_tutorial_flow()` to get an instance of the flow we defined in `flow.py`.\n4.  **Running the Flow:**  Finally, we call `tutorial_flow.run(shared)` to start the flow. The `shared` dictionary is passed to the `run` method so that the nodes can access and modify the data.\n\n## What Happens Inside a Node?\n\nEach node consists of three important methods. Looking at the example of `FetchRepo` in `nodes.py`:\n\n```python\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"local_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        if not project_name:\n            # Basic name derivation from URL or directory\n            if repo_url:\n                project_name = repo_url.split('/')[-1].replace('.git', '')\n            else:\n                project_name = os.path.basename(os.path.abspath(local_dir))\n            shared[\"project_name\"] = project_name\n\n        # Get file patterns directly from shared\n        include_patterns = shared[\"include_patterns\"]\n        exclude_patterns = shared[\"exclude_patterns\"]\n        max_file_size = shared[\"max_file_size\"]\n\n        return {\n            \"repo_url\": repo_url,\n            \"local_dir\": local_dir,\n            \"token\": shared.get(\"github_token\"),\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns,\n            \"max_file_size\": max_file_size,\n            \"use_relative_paths\": True\n        }\n\n    def exec(self, prep_res):\n        if prep_res[\"repo_url\"]:\n            print(f\"Crawling repository: {prep_res['repo_url']}...\")\n            result = crawl_github_files(\n                repo_url=prep_res[\"repo_url\"],\n                token=prep_res[\"token\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        else:\n            print(f\"Crawling directory: {prep_res['local_dir']}...\")\n            result = crawl_local_files(\n                directory=prep_res[\"local_dir\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n            \n        # Convert dict to list of tuples: [(path, content), ...]\n        files_list = list(result.get(\"files\", {}).items())\n        print(f\"Fetched {len(files_list)} files.\")\n        return files_list\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"files\"] = exec_res # List of (path, content) tuples\n```\n\n*   **`prep(shared)`:** This method prepares the input for the node's execution. It reads data from the `shared` dictionary and transforms it into a format that the `exec` method can use. It also performs any necessary setup or validation. Returns a `prep_res` object that contains everything necessary for the `exec` function to work.\n*   **`exec(prep_res)`:** This method performs the core logic of the node. It receives the prepared input from the `prep` method and executes the task. In the case of `FetchRepo`, it downloads the codebase from GitHub or reads it from the local directory. It returns the `exec_res` which is the output of the node.\n*   **`post(shared, prep_res, exec_res)`:** This method is executed *after* the `exec` method completes. It takes the output of the `exec` method (`exec_res`) and writes it back to the `shared` dictionary, making it available to the next node in the flow.  In the case of `FetchRepo`, it writes the list of files and their contents to the `shared[\"files\"]` variable.\n\n## Why Use Flow Orchestration?\n\n*   **Organization:** It provides a clear and structured way to define complex processes.\n*   **Maintainability:** It makes the code easier to understand and maintain by breaking it down into smaller, independent nodes.\n*   **Reusability:**  Nodes can be reused in different flows, making it easier to build new features or modify existing ones.\n*   **Testability:** Each node can be tested independently, making it easier to ensure the correctness of the entire system.\n*   **Error Handling:** `pocketflow` supports retries, fallbacks, error handling, and graceful termination, which are useful for nodes that interact with external resources.\n\n## Conclusion\n\nFlow Orchestration is a powerful technique for managing complex processes like our tutorial generation system. It provides a structured way to define the sequence of steps, making the code more organized, maintainable, and testable.\n\nIn the next chapter, [Configuration Management](03_configuration_management.md), we'll explore how to manage the different configuration settings that control the behavior of our system. This will allow us to customize the tutorial generation process for different codebases and use cases.\n\n---\n# Chapter 3: Configuration Management\n\nIn the previous chapter, [Flow Orchestration](02_flow_orchestration.md), we learned how to define the overall sequence of steps for generating a tutorial. Now, let's talk about how to *customize* that process. Think of it like this: you have a recipe for baking a cake, but you might want to adjust the recipe to make it chocolate or vanilla. That's where Configuration Management comes in!\n\n## What is Configuration Management?\n\nConfiguration Management is like having a control panel that allows you to adjust the settings of our tutorial generation \"machine\" to achieve the desired output. It lets you change things like:\n\n*   Which repository to generate a tutorial for.\n*   What name to give the project in the tutorial.\n*   Which files to include or exclude from analysis.\n*   Where to save the generated tutorial.\n\nWithout configuration management, we'd be stuck with a single, inflexible way of generating tutorials. It's like trying to bake every cake with the exact same recipe, even if you want different flavors!\n\n## Key Concepts\n\n1.  **Environment Variables:** These are like global settings that can be accessed by any program running on your computer. They're often used to store things like API keys or default values.\n2.  **Argument Parsing:** This allows you to pass settings directly to a program when you run it from the command line. It's like specifying the ingredients for your cake when you give the recipe to the baker.\n\n## Our Use Case: Generating a Tutorial for a Specific GitHub Repository with a Custom Name\n\nLet's say you want to generate a tutorial for the `requests` library on GitHub and name the project \"Python Requests Tutorial\". With Configuration Management, you can easily do this!\n\n## How to Use Configuration Management\n\nOur project uses both environment variables and argument parsing to manage configuration.\n\n### 1. Environment Variables\n\nEnvironment variables are useful for settings that you might want to reuse across multiple runs of the program, or that contain sensitive information you don't want to type directly into the command line.\n\nTo set an environment variable, you can use the following command in your terminal (replace `<your_token>` with your actual token):\n\n```bash\nexport GITHUB_TOKEN=<your_token>\n```\n\nOr, you can create a `.env` file in the project root directory:\n\n```\nGITHUB_TOKEN=<your_token>\n```\n\nThe `dotenv` library will automatically load these variables when the script runs.\n\nIn our `main.py` file, we use the `os.environ.get()` function to read the value of the `GITHUB_TOKEN` environment variable:\n\n```python\nimport os\ngithub_token = args.token or os.environ.get('GITHUB_TOKEN')\n```\n\nThis code first checks if the token was provided as a command-line argument (`args.token`). If not, it looks for the `GITHUB_TOKEN` environment variable.\n\n### 2. Argument Parsing\n\nArgument parsing allows you to specify settings directly when you run the `main.py` script from the command line.\n\nHere's how you would generate a tutorial for the `requests` library with a custom name:\n\n```bash\npython main.py --repo https://github.com/psf/requests -n \"Python Requests Tutorial\" -o tutorials\n```\n\nLet's break down this command:\n\n*   `python main.py`: This tells your computer to run the `main.py` script using Python.\n*   `--repo https://github.com/psf/requests`: This specifies the URL of the GitHub repository to use. `--repo` is the *argument* name, and `https://github.com/psf/requests` is the *value* of that argument.\n*   `-n \"Python Requests Tutorial\"`: This sets the project name to \"Python Requests Tutorial\". `-n` is a *short* argument name, and `\"Python Requests Tutorial\"` is the value.\n*   `-o tutorials`: This specifies that the tutorial output will be written to the `tutorials` directory.\n\nThe `argparse` module in Python handles the parsing of these command-line arguments. In `main.py`, we define the arguments that our script accepts:\n\n```python\nimport argparse\n\nparser = argparse.ArgumentParser(description=\"Generate a tutorial for a GitHub codebase or local directory.\")\nparser.add_argument(\"--repo\", help=\"URL of the public GitHub repository.\")\nparser.add_argument(\"-n\", \"--name\", help=\"Project name (optional, derived from repo/directory if omitted).\")\nparser.add_argument(\"-o\", \"--output\", default=\"output\", help=\"Base directory for output (default: ./output).\")\n# ... other arguments\nargs = parser.parse_args()\n```\n\nThis code defines three arguments: `--repo`, `--name` (with the short form `-n`), and `--output` (with the short form `-o`). The `help` parameter provides a description of each argument that is displayed when you run the script with the `--help` flag:\n\n```bash\npython main.py --help\n```\n\n### How Include and Exclude Patterns Work\n\nThe `-i` (or `--include`) and `-e` (or `--exclude`) options are used to specify which files to include or exclude from the tutorial generation process. This is helpful if you want to focus on specific parts of the codebase or ignore irrelevant files (like test files or documentation).\n\nFor example, to only include Python files and exclude test directories, you could use the following command:\n\n```bash\npython main.py --repo https://github.com/psf/requests -i \"*.py\" -e \"tests/*\"\n```\n\n## Internal Implementation\n\nHere's a simplified view of how Configuration Management works internally:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Command Line\n    participant Main Script\n    participant Args Namespace\n    User->>Command Line: Runs script with arguments: python main.py --repo ...\n    Command Line->>Main Script: Executes main.py\n    Main Script->>Args Namespace: Parses command-line arguments using argparse\n    Args Namespace->>Main Script: Returns arguments as an object (e.g., args.repo, args.name)\n    Main Script->>Main Script: Accesses argument values and uses them\n```\n\nHere's a breakdown:\n\n1.  **User Interaction:** The user runs the `main.py` script from the command line, providing arguments like the repository URL and project name.\n2.  **Argument Parsing:** The `argparse` module in `main.py` parses these arguments and stores them in an `Args Namespace` object.\n3.  **Accessing Arguments:** The `main.py` script accesses the argument values from the `Args Namespace` object (e.g., `args.repo`, `args.name`).\n4.  **Using Arguments:** The script uses these values to configure the tutorial generation process. For example, the `repo_url` value is passed to the [Codebase Crawling](04_codebase_crawling.md) node to download the codebase.\n\n## Code Example: Passing Configuration to Nodes\n\nThe parsed arguments are stored in the `shared` dictionary and passed to the nodes in the flow. For example, in `main.py`:\n\n```python\nshared = {\n    \"repo_url\": args.repo,\n    \"project_name\": args.name,\n    \"output_dir\": args.output,\n    # ... other arguments\n}\n\ntutorial_flow.run(shared)\n```\n\nEach node can then access the configuration values from the `shared` dictionary in its `prep` method. For example, in `nodes.py`:\n\n```python\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        project_name = shared.get(\"project_name\")\n        # ...\n        return {\n            \"repo_url\": repo_url,\n            # ...\n        }\n```\n\n## Why is Configuration Management Important?\n\n*   **Flexibility:** It allows you to customize the tutorial generation process for different codebases and use cases.\n*   **Reusability:** You can easily reuse the same script to generate tutorials for different projects by simply changing the configuration settings.\n*   **Automation:** You can automate the tutorial generation process by running the script with a predefined set of arguments.\n\n## Conclusion\n\nIn this chapter, we've learned about Configuration Management, which allows us to customize the tutorial generation process by using environment variables and argument parsing. This gives us the flexibility to generate tutorials for different codebases with different settings.\n\nIn the next chapter, [Codebase Crawling](04_codebase_crawling.md), we'll dive into the process of fetching and analyzing the codebase itself.\n\n\nRelevant Code Snippets:\n--- File: nodes.py ---\nimport os\nimport yaml\nfrom pocketflow import Node, BatchNode\nfrom utils.crawl_github_files import crawl_github_files\nfrom utils.call_llm import call_llm # Assuming you have this utility\nfrom utils.crawl_local_files import crawl_local_files\nfrom converter.md_to_html import convert_all_md_to_html\n\n\n# Helper to get content for specific file indices\ndef get_content_for_indices(files_data, indices):\n    content_map = {}\n    for i in indices:\n        if 0 <= i < len(files_data):\n            path, content = files_data[i]\n            content_map[f\"{i} # {path}\"] = content # Use index + path as key for context\n    return content_map\n\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"local_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        if not project_name:\n            # Basic name derivation from URL or directory\n            if repo_url:\n                project_name = repo_url.split('/')[-1].replace('.git', '')\n            else:\n                project_name = os.path.basename(os.path.abspath(local_dir))\n            shared[\"project_name\"] = project_name\n\n        # Get file patterns directly from shared\n        include_patterns = shared[\"include_patterns\"]\n        exclude_patterns = shared[\"exclude_patterns\"]\n        max_file_size = shared[\"max_file_size\"]\n\n        return {\n            \"repo_url\": repo_url,\n            \"local_dir\": local_dir,\n            \"token\": shared.get(\"github_token\"),\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns,\n            \"max_file_size\": max_file_size,\n            \"use_relative_paths\": True\n        }\n\n    def exec(self, prep_res):\n        if prep_res[\"repo_url\"]:\n            print(f\"Crawling repository: {prep_res['repo_url']}...\")\n            result = crawl_github_files(\n                repo_url=prep_res[\"repo_url\"],\n                token=prep_res[\"token\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        else:\n            print(f\"Crawling directory: {prep_res['local_dir']}...\")\n            result = crawl_local_files(\n                directory=prep_res[\"local_dir\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n            \n        # Convert dict to list of tuples: [(path, content), ...]\n        files_list = list(result.get(\"files\", {}).items())\n        print(f\"Fetched {len(files_list)} files.\")\n        return files_list\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"files\"] = exec_res # List of (path, content) tuples\n\nclass IdentifyAbstractions(Node):\n    def prep(self, shared):\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        \n        # Helper to create context from files, respecting limits (basic example)\n        def create_llm_context(files_data):\n            context = \"\"\n            file_info = [] # Store tuples of (index, path)\n            for i, (path, content) in enumerate(files_data):\n                entry = f\"--- File Index {i}: {path} ---\\n{content}\\n\\n\"\n                context += entry\n                file_info.append((i, path))\n\n            return context, file_info # file_info is list of (index, path)\n\n        context, file_info = create_llm_context(files_data)\n        # Format file info for the prompt (comment is just a hint for LLM)\n        file_listing_for_prompt = \"\\n\".join([f\"- {idx} # {path}\" for idx, path in file_info])\n        return context, file_listing_for_prompt, len(files_data), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, file_listing_for_prompt, file_count, project_name = prep_res  # Unpack project name\n        print(\"Identifying abstractions using LLM...\")\n        prompt = f\"\"\"\nFor the project `{project_name}`:\n\nCodebase Context:\n{context}\n\nAnalyze the codebase context.\nIdentify the top 5-10 core most important abstractions to help those new to the codebase.\n\nFor each abstraction, provide:\n1. A concise `name`.\n2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\nList of file indices and paths present in the context:\n{file_listing_for_prompt}\n\nFormat the output as a YAML list of dictionaries:\n\n```yaml\n- name: Query Processing\n  description: | \n    Explains what the abstraction does.\n    It's like a central dispatcher routing requests.\n  file_indices:\n    - 0 # path/to/file1.py\n    - 3 # path/to/related.py\n- name: Query Optimization\n  description: |\n    Another core concept, similar to a blueprint for objects.\n  file_indices:\n    - 5 # path/to/another.js\n# ... up to 10 abstractions\n```\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        abstractions = yaml.safe_load(yaml_str)\n\n        if not isinstance(abstractions, list):\n            raise ValueError(\"LLM Output is not a list\")\n\n        validated_abstractions = []\n        for item in abstractions:\n            if not isinstance(item, dict) or not all(k in item for k in [\"name\", \"description\", \"file_indices\"]):\n                raise ValueError(f\"Missing keys in abstraction item: {item}\")\n            if not isinstance(item[\"description\"], str):\n                 raise ValueError(f\"description is not a string in item: {item}\")\n            if not isinstance(item[\"file_indices\"], list):\n                 raise ValueError(f\"file_indices is not a list in item: {item}\")\n\n            # Validate indices\n            validated_indices = []\n            for idx_entry in item[\"file_indices\"]:\n                 try:\n                     if isinstance(idx_entry, int):\n                         idx = idx_entry\n                     elif isinstance(idx_entry, str) and '#' in idx_entry:\n                          idx = int(idx_entry.split('#')[0].strip())\n                     else:\n                          idx = int(str(idx_entry).strip())\n\n                     if not (0 <= idx < file_count):\n                         raise ValueError(f\"Invalid file index {idx} found in item {item['name']}. Max index is {file_count - 1}.\")\n                     validated_indices.append(idx)\n                 except (ValueError, TypeError):\n                      raise ValueError(f\"Could not parse index from entry: {idx_entry} in item {item['name']}\")\n\n            item[\"files\"] = sorted(list(set(validated_indices)))\n            # Store only the required fields\n            validated_abstractions.append({\n                \"name\": item[\"name\"],\n                \"description\": item[\"description\"],\n                \"files\": item[\"files\"]\n            })\n\n        print(f\"Identified {len(validated_abstractions)} abstractions.\")\n        return validated_abstractions\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"abstractions\"] = exec_res # List of {\"name\": str, \"description\": str, \"files\": [int]}\n\nclass AnalyzeRelationships(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"] # Now contains 'files' list of indices\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n\n        # Create context with abstraction names, indices, descriptions, and relevant file snippets\n        context = \"Identified Abstractions:\\n\"\n        all_relevant_indices = set()\n        abstraction_info_for_prompt = []\n        for i, abstr in enumerate(abstractions):\n            # Use 'files' which contains indices directly\n            file_indices_str = \", \".join(map(str, abstr['files']))\n            info_line = f\"- Index {i}: {abstr['name']} (Relevant file indices: [{file_indices_str}])\\n  Description: {abstr['description']}\"\n            context += info_line + \"\\n\"\n            abstraction_info_for_prompt.append(f\"{i} # {abstr['name']}\")\n            all_relevant_indices.update(abstr['files'])\n\n        context += \"\\nRelevant File Snippets (Referenced by Index and Path):\\n\"\n        # Get content for relevant files using helper\n        relevant_files_content_map = get_content_for_indices(\n            files_data,\n            sorted(list(all_relevant_indices))\n        )\n        # Format file content for context\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path} ---\\n{content}\"\n            for idx_path, content in relevant_files_content_map.items()\n        )\n        context += file_context_str\n\n        return context, \"\\n\".join(abstraction_info_for_prompt), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, abstraction_listing, project_name = prep_res  # Unpack project name\n        print(\"Analyzing relationships using LLM...\")\n        prompt = f\"\"\"\nBased on the following abstractions and relevant code snippets from the project `{project_name}`:\n\nList of Abstraction Indices and Names:\n{abstraction_listing}\n\nContext (Abstractions, Descriptions, Code):\n{context}\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        relationships_data = yaml.safe_load(yaml_str)\n\n        if not isinstance(relationships_data, dict) or not all(k in relationships_data for k in [\"summary\", \"relationships\"]):\n            raise ValueError(\"LLM output is not a dict or missing keys ('summary', 'relationships')\")\n        if not isinstance(relationships_data[\"summary\"], str):\n             raise ValueError(\"summary is not a string\")\n        if not isinstance(relationships_data[\"relationships\"], list):\n             raise ValueError(\"relationships is not a list\")\n\n        # Validate relationships structure\n        validated_relationships = []\n        num_abstractions = len(abstraction_listing.split('\\n'))\n        for rel in relationships_data[\"relationships\"]:\n             # Check for 'label' key\n             if not isinstance(rel, dict) or not all(k in rel for k in [\"from_abstraction\", \"to_abstraction\", \"label\"]):\n                  raise ValueError(f\"Missing keys (expected from_abstraction, to_abstraction, label) in relationship item: {rel}\")\n             # Validate 'label' is a string\n             if not isinstance(rel[\"label\"], str):\n                  raise ValueError(f\"Relationship label is not a string: {rel}\")\n\n             # Validate indices\n             try:\n                 from_idx = int(str(rel[\"from_abstraction\"]).split('#')[0].strip())\n                 to_idx = int(str(rel[\"to_abstraction\"]).split('#')[0].strip())\n                 if not (0 <= from_idx < num_abstractions and 0 <= to_idx < num_abstractions):\n                      raise ValueError(f\"Invalid index in relationship: from={from_idx}, to={to_idx}. Max index is {num_abstractions-1}.\")\n                 validated_relationships.append({\n                     \"from\": from_idx,\n                     \"to\": to_idx,\n                     \"label\": rel[\"label\"] \n                 })\n             except (ValueError, TypeError):\n                  raise ValueError(f\"Could not parse indices from relationship: {rel}\")\n\n        print(\"Generated project summary and relationship details.\")\n        return {\n            \"summary\": relationships_data[\"summary\"],\n            \"details\": validated_relationships # Store validated, index-based relationships\n        }\n\n\n    def post(self, shared, prep_res, exec_res):\n        # Structure is now {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        shared[\"relationships\"] = exec_res\n\nclass OrderChapters(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"]\n        relationships = shared[\"relationships\"]\n        project_name = shared[\"project_name\"]  # Get project name\n\n        # Prepare context for the LLM\n        abstraction_info_for_prompt = []\n        for i, a in enumerate(abstractions):\n            abstraction_info_for_prompt.append(f\"- {i} # {a['name']}\")\n        abstraction_listing = \"\\n\".join(abstraction_info_for_prompt)\n\n        context = f\"Project Summary:\\n{relationships['summary']}\\n\\n\"\n        context += \"Relationships (Indices refer to abstractions above):\\n\"\n        for rel in relationships['details']:\n             from_name = abstractions[rel['from']]['name']\n             to_name = abstractions[rel['to']]['name']\n             # Use 'label' instead of 'desc'\n             context += f\"- From {rel['from']} ({from_name}) to {rel['to']} ({to_name}): {rel['label']}\\n\"\n\n        return abstraction_listing, context, len(abstractions), project_name\n\n    def exec(self, prep_res):\n        abstraction_listing, context, num_abstractions, project_name = prep_res\n        print(\"Determining chapter order using LLM...\")\n        prompt = f\"\"\"\nGiven the following project abstractions and their relationships for the project ```` {project_name} ````:\n\nAbstractions (Index # Name):\n{abstraction_listing}\n\nContext about relationships and project summary:\n{context}\n\nIf you are going to make a tutorial for ```` {project_name} ````, what is the best order to explain these abstractions, from first to last?\nIdeally, first explain those that are the most important or foundational, perhaps user-facing concepts or entry points. Then move to more detailed, lower-level implementation details or supporting concepts.\n\nOutput the ordered list of abstraction indices, including the name in a comment for clarity. Use the format `idx # AbstractionName`.\n\n```yaml\n- 2 # FoundationalConcept\n- 0 # CoreClassA\n- 1 # CoreClassB (uses CoreClassA)\n- ...\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        # Rely on Node's built-in retry/fallback\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        ordered_indices_raw = yaml.safe_load(yaml_str)\n\n        if not isinstance(ordered_indices_raw, list):\n            raise ValueError(\"LLM output is not a list\")\n\n        ordered_indices = []\n        seen_indices = set()\n        for entry in ordered_indices_raw:\n            try:\n                 if isinstance(entry, int):\n                     idx = entry\n                 elif isinstance(entry, str) and '#' in entry:\n                      idx = int(entry.split('#')[0].strip())\n                 else:\n                      idx = int(str(entry).strip())\n\n                 if not (0 <= idx < num_abstractions):\n                      raise ValueError(f\"Invalid index {idx} in ordered list. Max index is {num_abstractions-1}.\")\n                 if idx in seen_indices:\n                     raise ValueError(f\"Duplicate index {idx} found in ordered list.\")\n                 ordered_indices.append(idx)\n                 seen_indices.add(idx)\n\n            except (ValueError, TypeError):\n                 raise ValueError(f\"Could not parse index from ordered list entry: {entry}\")\n\n        # Check if all abstractions are included\n        if len(ordered_indices) != num_abstractions:\n             raise ValueError(f\"Ordered list length ({len(ordered_indices)}) does not match number of abstractions ({num_abstractions}). Missing indices: {set(range(num_abstractions)) - seen_indices}\")\n\n        print(f\"Determined chapter order (indices): {ordered_indices}\")\n        return ordered_indices # Return the list of indices\n\n    def post(self, shared, prep_res, exec_res):\n        # exec_res is already the list of ordered indices\n        shared[\"chapter_order\"] = exec_res # List of indices\n\nclass WriteChapters(BatchNode):\n    def prep(self, shared):\n        chapter_order = shared[\"chapter_order\"] # List of indices\n        abstractions = shared[\"abstractions\"]   # List of dicts, now using 'files' with indices\n        files_data = shared[\"files\"]\n        # Get already written chapters to provide context\n        # We store them temporarily during the batch run, not in shared memory yet\n        # The 'previous_chapters_summary' will be built progressively in the exec context\n        self.chapters_written_so_far = [] # Use instance variable for temporary storage across exec calls\n\n        # Create a complete list of all chapters\n        all_chapters = []\n        chapter_filenames = {} # Store chapter filename mapping for linking\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                chapter_num = i + 1\n                chapter_name = abstractions[abstraction_index][\"name\"]\n                # Create safe filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in chapter_name).lower()\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                # Format with link\n                all_chapters.append(f\"{chapter_num}. [{chapter_name}]({filename})\")\n                # Store mapping of chapter index to filename for linking\n                chapter_filenames[abstraction_index] = {\"num\": chapter_num, \"name\": chapter_name, \"filename\": filename}\n        \n        # Create a formatted string with all chapters\n        full_chapter_listing = \"\\n\".join(all_chapters)\n\n        items_to_process = []\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                abstraction_details = abstractions[abstraction_index]\n                # Use 'files' (list of indices) directly\n                related_file_indices = abstraction_details.get(\"files\", [])\n                # Get content using helper, passing indices\n                related_files_content_map = get_content_for_indices(files_data, related_file_indices)\n                \n                # Get previous chapter info for transitions\n                prev_chapter = None\n                if i > 0:\n                    prev_idx = chapter_order[i-1]\n                    prev_chapter = chapter_filenames[prev_idx]\n                \n                # Get next chapter info for transitions\n                next_chapter = None\n                if i < len(chapter_order) - 1:\n                    next_idx = chapter_order[i+1]\n                    next_chapter = chapter_filenames[next_idx]\n\n                items_to_process.append({\n                    \"chapter_num\": i + 1,\n                    \"abstraction_index\": abstraction_index,\n                    \"abstraction_details\": abstraction_details,\n                    \"related_files_content_map\": related_files_content_map,\n                    \"project_name\": shared[\"project_name\"],  # Add project name\n                    \"full_chapter_listing\": full_chapter_listing,  # Add the full chapter listing\n                    \"chapter_filenames\": chapter_filenames,  # Add chapter filenames mapping\n                    \"prev_chapter\": prev_chapter,  # Add previous chapter info\n                    \"next_chapter\": next_chapter,  # Add next chapter info\n                    # previous_chapters_summary will be added dynamically in exec\n                })\n            else:\n                print(f\"Warning: Invalid abstraction index {abstraction_index} in chapter_order. Skipping.\")\n\n        print(f\"Preparing to write {len(items_to_process)} chapters...\")\n        return items_to_process # Iterable for BatchNode\n\n    def exec(self, item):\n        # This runs for each item prepared above\n        abstraction_name = item[\"abstraction_details\"][\"name\"]\n        chapter_num = item[\"chapter_num\"]\n        project_name = item.get(\"project_name\")  # Get from item\n        print(f\"Writing chapter {chapter_num} for: {abstraction_name} using LLM...\")\n\n        # Prepare file context string from the map\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path.split('# ')[1] if '# ' in idx_path else idx_path} ---\\n{content}\"\n            for idx_path, content in item[\"related_files_content_map\"].items()\n        )\n\n        # Get summary of chapters written *before* this one\n        # Use the temporary instance variable\n        previous_chapters_summary = \"\\n---\\n\".join(self.chapters_written_so_far)\n\n\n        prompt = f\"\"\"\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `{project_name}` about the concept: \"{abstraction_name}\". This is Chapter {chapter_num}.\n\nConcept Details:\n- Description:\n{item[\"abstraction_details\"][\"description\"]}\n\nComplete Tutorial Structure:\n{item[\"full_chapter_listing\"]}\n\nContext from previous chapters (summary):\n{previous_chapters_summary if previous_chapters_summary else \"This is the first chapter.\"}\n\nRelevant Code Snippets:\n{file_context_str if file_context_str else \"No specific code snippets provided for this abstraction.\"}\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter {chapter_num}: {abstraction_name}`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n\"\"\"\n        chapter_content = call_llm(prompt)\n        # Basic validation/cleanup\n        actual_heading = f\"# Chapter {chapter_num}: {abstraction_name}\"\n        if not chapter_content.strip().startswith(f\"# Chapter {chapter_num}\"):\n             # Add heading if missing or incorrect, trying to preserve content\n             lines = chapter_content.strip().split('\\n')\n             if lines and lines[0].strip().startswith(\"#\"): # If there's some heading, replace it\n                 lines[0] = actual_heading\n                 chapter_content = \"\\n\".join(lines)\n             else: # Otherwise, prepend it\n                 chapter_content = f\"{actual_heading}\\n\\n{chapter_content}\"\n\n        # Add the generated content to our temporary list for the next iteration's context\n        self.chapters_written_so_far.append(chapter_content)\n\n        return chapter_content # Return the Markdown string\n\n    def post(self, shared, prep_res, exec_res_list):\n        # exec_res_list contains the generated Markdown for each chapter, in order\n        shared[\"chapters\"] = exec_res_list\n        # Clean up the temporary instance variable\n        del self.chapters_written_so_far\n        print(f\"Finished writing {len(exec_res_list)} chapters.\")\n\nclass CombineTutorial(Node):\n    def prep(self, shared):\n        project_name = shared[\"project_name\"]\n        output_base_dir = shared.get(\"output_dir\", \"output\") # Default output dir\n        output_path = os.path.join(output_base_dir, project_name)\n        repo_url = shared[\"repo_url\"]  # Get the repository URL\n\n        # Use 'label' from relationships_data['details']\n        relationships_data = shared[\"relationships\"] # {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        chapter_order = shared[\"chapter_order\"] # indices\n        abstractions = shared[\"abstractions\"]   # list of dicts\n        chapters_content = shared[\"chapters\"]   # list of strings\n\n        # --- Generate Mermaid Diagram ---\n        mermaid_lines = [\"flowchart TD\"]\n        # Add nodes for each abstraction\n        for i, abstr in enumerate(abstractions):\n            # Sanitize name for Mermaid ID and label\n            node_id = f\"A{i}\"\n            sanitized_name = abstr['name'].replace('\"', '')\n            node_label = sanitized_name # Using sanitized name only, no index\n            mermaid_lines.append(f'    {node_id}[\"{node_label}\"]')\n        # Add edges for relationships using 'label'\n        for rel in relationships_data['details']:\n            from_node_id = f\"A{rel['from']}\"\n            to_node_id = f\"A{rel['to']}\"\n            # Sanitize 'label' for edge label\n            edge_label = rel['label'].replace('\"', '').replace('\\n', ' ') # Basic sanitization\n            # Limit edge label length for readability (optional, but good for diagrams)\n            max_label_len = 30 # Make it shorter for labels\n            if len(edge_label) > max_label_len:\n                edge_label = edge_label[:max_label_len-3] + \"...\"\n            mermaid_lines.append(f'    {from_node_id} -- \"{edge_label}\" --> {to_node_id}')\n\n        mermaid_diagram = \"\\n\".join(mermaid_lines)\n        # --- End Mermaid ---\n\n\n        # Prepare index.md content\n        index_content = f\"# Tutorial: {project_name}\\n\\n\"\n        index_content += f\"{relationships_data['summary']}\\n\\n\"\n        index_content += f\"**Source Repository:** [{repo_url}]({repo_url})\\n\\n\"\n\n        # Add Mermaid diagram for relationships\n        index_content += \"```mermaid\\n\"\n        index_content += mermaid_diagram + \"\\n\"\n        index_content += \"```\\n\\n\"\n\n        index_content += \"## Chapters\\n\\n\"\n\n        chapter_files = []\n        # Generate chapter links based on the determined order\n        for i, abstraction_index in enumerate(chapter_order):\n            # Ensure index is valid and we have content for it\n            if 0 <= abstraction_index < len(abstractions) and i < len(chapters_content):\n                abstraction_name = abstractions[abstraction_index][\"name\"]\n                # Sanitize name for filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in abstraction_name).lower()\n                # Use chapter number (i+1) for ordering filename\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                index_content += f\"{i+1}. [{abstraction_name}]({filename})\\n\"\n                \n                # Add attribution to chapter content\n                chapter_content = chapters_content[i]\n                if not chapter_content.endswith(\"\\n\\n\"):\n                    chapter_content += \"\\n\\n\"\n                chapter_content += \"---\\n\\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\"\n                \n                # Store filename and corresponding content\n                chapter_files.append({\"filename\": filename, \"content\": chapter_content})\n            else:\n                 print(f\"Warning: Mismatch between chapter order, abstractions, or content at index {i} (abstraction index {abstraction_index}). Skipping file generation for this entry.\")\n\n        # Add attribution to index content\n        index_content += \"\\n\\n---\\n\\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\"\n\n        return {\n            \"output_path\": output_path,\n            \"index_content\": index_content,\n            \"chapter_files\": chapter_files # List of {\"filename\": str, \"content\": str}\n        }\n\n    def exec(self, prep_res):\n        output_path = prep_res[\"output_path\"]\n        index_content = prep_res[\"index_content\"]\n        chapter_files = prep_res[\"chapter_files\"]\n\n        print(f\"Combining tutorial into directory: {output_path}\")\n        # Rely on Node's built-in retry/fallback\n        os.makedirs(output_path, exist_ok=True)\n\n        # Write index.md\n        index_filepath = os.path.join(output_path, \"index.md\")\n        with open(index_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(index_content)\n        print(f\"  - Wrote {index_filepath}\")\n\n        # Write chapter files\n        for chapter_info in chapter_files:\n            chapter_filepath = os.path.join(output_path, chapter_info[\"filename\"])\n            with open(chapter_filepath, \"w\", encoding=\"utf-8\") as f:\n                f.write(chapter_info[\"content\"])\n            print(f\"  - Wrote {chapter_filepath}\")\n\n        return output_path # Return the final path\n\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"final_output_dir\"] = exec_res # Store the output path\n        print(f\"\\nTutorial generation complete! Files are in: {exec_res}\")\n        convert_all_md_to_html(exec_res) # Convert all Markdown files to HTML\n        \n\n--- File: utils/crawl_github_files.py ---\nimport requests\nimport base64\nimport os\nimport tempfile\nimport git\nimport time\nimport fnmatch\nfrom typing import Union, Set, List, Dict, Tuple, Any\nfrom urllib.parse import urlparse\n\ndef crawl_github_files(\n    repo_url, \n    token=None, \n    max_file_size: int = 1 * 1024 * 1024,  # 1 MB\n    use_relative_paths: bool = False,\n    include_patterns: Union[str, Set[str]] = None,\n    exclude_patterns: Union[str, Set[str]] = None\n):\n    \"\"\"\n    Crawl files from a specific path in a GitHub repository at a specific commit.\n\n    Args:\n        repo_url (str): URL of the GitHub repository with specific path and commit\n                        (e.g., 'https://github.com/microsoft/autogen/tree/e45a15766746d95f8cfaaa705b0371267bec812e/python/packages/autogen-core/src/autogen_core')\n        token (str, optional): **GitHub personal access token.**\n            - **Required for private repositories.**\n            - **Recommended for public repos to avoid rate limits.**\n            - Can be passed explicitly or set via the `GITHUB_TOKEN` environment variable.\n        max_file_size (int, optional): Maximum file size in bytes to download (default: 1 MB)\n        use_relative_paths (bool, optional): If True, file paths will be relative to the specified subdirectory\n        include_patterns (str or set of str, optional): Pattern or set of patterns specifying which files to include (e.g., \"*.py\", {\"*.md\", \"*.txt\"}).\n                                                       If None, all files are included.\n        exclude_patterns (str or set of str, optional): Pattern or set of patterns specifying which files to exclude.\n                                                       If None, no files are excluded.\n\n    Returns:\n        dict: Dictionary with files and statistics\n    \"\"\"\n    # Convert single pattern to set\n    if include_patterns and isinstance(include_patterns, str):\n        include_patterns = {include_patterns}\n    if exclude_patterns and isinstance(exclude_patterns, str):\n        exclude_patterns = {exclude_patterns}\n\n    def should_include_file(file_path: str, file_name: str) -> bool:\n        \"\"\"Determine if a file should be included based on patterns\"\"\"\n        # If no include patterns are specified, include all files\n        if not include_patterns:\n            include_file = True\n        else:\n            # Check if file matches any include pattern\n            include_file = any(fnmatch.fnmatch(file_name, pattern) for pattern in include_patterns)\n\n        # If exclude patterns are specified, check if file should be excluded\n        if exclude_patterns and include_file:\n            # Exclude if file matches any exclude pattern\n            exclude_file = any(fnmatch.fnmatch(file_path, pattern) for pattern in exclude_patterns)\n            return not exclude_file\n\n        return include_file\n\n    # Detect SSH URL (git@ or .git suffix)\n    is_ssh_url = repo_url.startswith(\"git@\") or repo_url.endswith(\".git\")\n\n    if is_ssh_url:\n        # Clone repo via SSH to temp dir\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            print(f\"Cloning SSH repo {repo_url} to temp dir {tmpdirname} ...\")\n            try:\n                repo = git.Repo.clone_from(repo_url, tmpdirname)\n            except Exception as e:\n                print(f\"Error cloning repo: {e}\")\n                return {\"files\": {}, \"stats\": {\"error\": str(e)}}\n\n            # Attempt to checkout specific commit/branch if in URL\n            # Parse ref and subdir from SSH URL? SSH URLs don't have branch info embedded\n            # So rely on default branch, or user can checkout manually later\n            # Optionally, user can pass ref explicitly in future API\n\n            # Walk directory\n            files = {}\n            skipped_files = []\n\n            for root, dirs, filenames in os.walk(tmpdirname):\n                for filename in filenames:\n                    abs_path = os.path.join(root, filename)\n                    rel_path = os.path.relpath(abs_path, tmpdirname)\n\n                    # Check file size\n                    try:\n                        file_size = os.path.getsize(abs_path)\n                    except OSError:\n                        continue\n\n                    if file_size > max_file_size:\n                        skipped_files.append((rel_path, file_size))\n                        print(f\"Skipping {rel_path}: size {file_size} exceeds limit {max_file_size}\")\n                        continue\n\n                    # Check include/exclude patterns\n                    if not should_include_file(rel_path, filename):\n                        print(f\"Skipping {rel_path}: does not match include/exclude patterns\")\n                        continue\n\n                    # Read content\n                    try:\n                        with open(abs_path, \"r\", encoding=\"utf-8\") as f:\n                            content = f.read()\n                        files[rel_path] = content\n                        print(f\"Added {rel_path} ({file_size} bytes)\")\n                    except Exception as e:\n                        print(f\"Failed to read {rel_path}: {e}\")\n\n            return {\n                \"files\": files,\n                \"stats\": {\n                    \"downloaded_count\": len(files),\n                    \"skipped_count\": len(skipped_files),\n                    \"skipped_files\": skipped_files,\n                    \"base_path\": None,\n                    \"include_patterns\": include_patterns,\n                    \"exclude_patterns\": exclude_patterns,\n                    \"source\": \"ssh_clone\"\n                }\n            }\n\n    # Parse GitHub URL to extract owner, repo, commit/branch, and path\n    parsed_url = urlparse(repo_url)\n    path_parts = parsed_url.path.strip('/').split('/')\n    \n    if len(path_parts) < 2:\n        raise ValueError(f\"Invalid GitHub URL: {repo_url}\")\n    \n    # Extract the basic components\n    owner = path_parts[0]\n    repo = path_parts[1]\n    \n    # Check if URL contains a specific branch/commit\n    if 'tree' in path_parts:\n        tree_index = path_parts.index('tree')\n        ref = path_parts[tree_index + 1]\n        # Combine all parts after the ref as the path\n        path_start = tree_index + 2\n        specific_path = '/'.join(path_parts[path_start:]) if path_start < len(path_parts) else \"\"\n    else:\n        ref = \"main\"  # Default branch\n        specific_path = \"\"\n    \n    # Setup for GitHub API\n    headers = {\"Accept\": \"application/vnd.github.v3+json\"}\n    if token:\n        headers[\"Authorization\"] = f\"token {token}\"\n    \n    # Dictionary to store path -> content mapping\n    files = {}\n    skipped_files = []\n    \n    def should_include_file(file_path: str, file_name: str) -> bool:\n        \"\"\"Determine if a file should be included based on patterns\"\"\"\n        # If no include patterns are specified, include all files\n        if not include_patterns:\n            include_file = True\n        else:\n            # Check if file matches any include pattern\n            include_file = any(fnmatch.fnmatch(file_name, pattern) for pattern in include_patterns)\n        \n        # If exclude patterns are specified, check if file should be excluded\n        if exclude_patterns and include_file:\n            # Exclude if file matches any exclude pattern\n            exclude_file = any(fnmatch.fnmatch(file_path, pattern) for pattern in exclude_patterns)\n            return not exclude_file\n        \n        return include_file\n    \n    def fetch_contents(path):\n        \"\"\"Fetch contents of the repository at a specific path and commit\"\"\"\n        url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{path}\"\n        params = {\"ref\": ref}\n        \n        response = requests.get(url, headers=headers, params=params)\n        \n        if response.status_code == 403 and 'rate limit exceeded' in response.text.lower():\n            reset_time = int(response.headers.get('X-RateLimit-Reset', 0))\n            wait_time = max(reset_time - time.time(), 0) + 1\n            print(f\"Rate limit exceeded. Waiting for {wait_time:.0f} seconds...\")\n            time.sleep(wait_time)\n            return fetch_contents(path)\n            \n        if response.status_code == 404:\n            if not token:\n                print(f\"Error 404: Repository not found or is private.\\n\"\n                      f\"If this is a private repository, please provide a valid GitHub token via the 'token' argument or set the GITHUB_TOKEN environment variable.\")\n            else:\n                print(f\"Error 404: Path '{path}' not found in repository or insufficient permissions with the provided token.\\n\"\n                      f\"Please verify the token has access to this repository and the path exists.\")\n            return\n            \n        if response.status_code != 200:\n            print(f\"Error fetching {path}: {response.status_code} - {response.text}\")\n            return\n        \n        contents = response.json()\n        \n        # Handle both single file and directory responses\n        if not isinstance(contents, list):\n            contents = [contents]\n        \n        for item in contents:\n            item_path = item[\"path\"]\n            \n            # Calculate relative path if requested\n            if use_relative_paths and specific_path:\n                # Make sure the path is relative to the specified subdirectory\n                if item_path.startswith(specific_path):\n                    rel_path = item_path[len(specific_path):].lstrip('/')\n                else:\n                    rel_path = item_path\n            else:\n                rel_path = item_path\n            \n            if item[\"type\"] == \"file\":\n                # Check if file should be included based on patterns\n                if not should_include_file(rel_path, item[\"name\"]):\n                    print(f\"Skipping {rel_path}: Does not match include/exclude patterns\")\n                    continue\n                \n                # Check file size if available\n                file_size = item.get(\"size\", 0)\n                if file_size > max_file_size:\n                    skipped_files.append((item_path, file_size))\n                    print(f\"Skipping {rel_path}: File size ({file_size} bytes) exceeds limit ({max_file_size} bytes)\")\n                    continue\n                \n                # For files, get raw content\n                if \"download_url\" in item and item[\"download_url\"]:\n                    file_url = item[\"download_url\"]\n                    file_response = requests.get(file_url, headers=headers)\n                    \n                    # Final size check in case content-length header is available but differs from metadata\n                    content_length = int(file_response.headers.get('content-length', 0))\n                    if content_length > max_file_size:\n                        skipped_files.append((item_path, content_length))\n                        print(f\"Skipping {rel_path}: Content length ({content_length} bytes) exceeds limit ({max_file_size} bytes)\")\n                        continue\n                        \n                    if file_response.status_code == 200:\n                        files[rel_path] = file_response.text\n                        print(f\"Downloaded: {rel_path} ({file_size} bytes) \")\n                    else:\n                        print(f\"Failed to download {rel_path}: {file_response.status_code}\")\n                else:\n                    # Alternative method if download_url is not available\n                    content_response = requests.get(item[\"url\"], headers=headers)\n                    if content_response.status_code == 200:\n                        content_data = content_response.json()\n                        if content_data.get(\"encoding\") == \"base64\" and \"content\" in content_data:\n                            # Check size of base64 content before decoding\n                            if len(content_data[\"content\"]) * 0.75 > max_file_size:  # Approximate size calculation\n                                estimated_size = int(len(content_data[\"content\"]) * 0.75)\n                                skipped_files.append((item_path, estimated_size))\n                                print(f\"Skipping {rel_path}: Encoded content exceeds size limit\")\n                                continue\n                                \n                            file_content = base64.b64decode(content_data[\"content\"]).decode('utf-8')\n                            files[rel_path] = file_content\n                            print(f\"Downloaded: {rel_path} ({file_size} bytes)\")\n                        else:\n                            print(f\"Unexpected content format for {rel_path}\")\n                    else:\n                        print(f\"Failed to get content for {rel_path}: {content_response.status_code}\")\n            \n            elif item[\"type\"] == \"dir\":\n                # Recursively process subdirectories\n                fetch_contents(item_path)\n    \n    # Start crawling from the specified path\n    fetch_contents(specific_path)\n    \n    return {\n        \"files\": files,\n        \"stats\": {\n            \"downloaded_count\": len(files),\n            \"skipped_count\": len(skipped_files),\n            \"skipped_files\": skipped_files,\n            \"base_path\": specific_path if use_relative_paths else None,\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns\n        }\n    }\n\n# Example usage\nif __name__ == \"__main__\":\n    # Get token from environment variable (recommended for private repos)\n    github_token = os.environ.get(\"GITHUB_TOKEN\")\n    if not github_token:\n        print(\"Warning: No GitHub token found in environment variable 'GITHUB_TOKEN'.\\n\"\n              \"Private repositories will not be accessible without a token.\\n\"\n              \"To access private repos, set the environment variable or pass the token explicitly.\")\n    \n    repo_url = \"https://github.com/pydantic/pydantic/tree/6c38dc93f40a47f4d1350adca9ec0d72502e223f/pydantic\"\n    \n    # Example: Get Python and Markdown files, but exclude test files\n    result = crawl_github_files(\n        repo_url, \n        token=github_token,\n        max_file_size=1 * 1024 * 1024,  # 1 MB in bytes\n        use_relative_paths=True,  # Enable relative paths\n        include_patterns={\"*.py\", \"*.md\"},  # Include Python and Markdown files\n    )\n    \n    files = result[\"files\"]\n    stats = result[\"stats\"]\n    \n    print(f\"\\nDownloaded {stats['downloaded_count']} files.\")\n    print(f\"Skipped {stats['skipped_count']} files due to size limits or patterns.\")\n    print(f\"Base path for relative paths: {stats['base_path']}\")\n    print(f\"Include patterns: {stats['include_patterns']}\")\n    print(f\"Exclude patterns: {stats['exclude_patterns']}\")\n    \n    # Display all file paths in the dictionary\n    print(\"\\nFiles in dictionary:\")\n    for file_path in sorted(files.keys()):\n        print(f\"  {file_path}\")\n    \n    # Example: accessing content of a specific file\n    if files:\n        sample_file = next(iter(files))\n        print(f\"\\nSample file: {sample_file}\")\n        print(f\"Content preview: {files[sample_file][:200]}...\")\n\n--- File: utils/crawl_local_files.py ---\nimport os\nimport fnmatch\n\ndef crawl_local_files(directory, include_patterns=None, exclude_patterns=None, max_file_size=None, use_relative_paths=True):\n    \"\"\"\n    Crawl files in a local directory with similar interface as crawl_github_files.\n    \n    Args:\n        directory (str): Path to local directory\n        include_patterns (set): File patterns to include (e.g. {\"*.py\", \"*.js\"})\n        exclude_patterns (set): File patterns to exclude (e.g. {\"tests/*\"})\n        max_file_size (int): Maximum file size in bytes\n        use_relative_paths (bool): Whether to use paths relative to directory\n        \n    Returns:\n        dict: {\"files\": {filepath: content}}\n    \"\"\"\n    if not os.path.isdir(directory):\n        raise ValueError(f\"Directory does not exist: {directory}\")\n        \n    files_dict = {}\n    \n    for root, _, files in os.walk(directory):\n        for filename in files:\n            filepath = os.path.join(root, filename)\n            \n            # Get path relative to directory if requested\n            if use_relative_paths:\n                relpath = os.path.relpath(filepath, directory)\n            else:\n                relpath = filepath\n                \n            # Check if file matches any include pattern\n            included = False\n            if include_patterns:\n                for pattern in include_patterns:\n                    if fnmatch.fnmatch(relpath, pattern):\n                        included = True\n                        break\n            else:\n                included = True\n                \n            # Check if file matches any exclude pattern\n            excluded = False\n            if exclude_patterns:\n                for pattern in exclude_patterns:\n                    if fnmatch.fnmatch(relpath, pattern):\n                        excluded = True\n                        break\n                        \n            if not included or excluded:\n                continue\n                \n            # Check file size\n            if max_file_size and os.path.getsize(filepath) > max_file_size:\n                continue\n                \n            try:\n                with open(filepath, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                files_dict[relpath] = content\n            except Exception as e:\n                print(f\"Warning: Could not read file {filepath}: {e}\")\n                \n    return {\"files\": files_dict}\n\nif __name__ == \"__main__\":\n    print(\"--- Crawling parent directory ('..') ---\")\n    files_data = crawl_local_files(\"..\", exclude_patterns={\"*.pyc\", \"__pycache__/*\", \".git/*\", \"output/*\"})\n    print(f\"Found {len(files_data['files'])} files:\")\n    for path in files_data[\"files\"]:\n        print(f\"  {path}\")\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 4: Codebase Crawling`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "# Chapter 4: Codebase Crawling\n\nIn the previous chapter, [Configuration Management](03_configuration_management.md), we learned how to customize our tutorial generation process using command-line arguments and environment variables. Now, let's explore how we actually *get* the code that we'll be using to generate the tutorial! That's where Codebase Crawling comes in.\n\n## What is Codebase Crawling?\n\nImagine you're writing a report on a popular book. You can't write the report without actually *reading* the book first, right? Codebase Crawling is like reading the book (or, more accurately, the code) for our tutorial generation system.\n\nSpecifically, Codebase Crawling is the process of gathering all the necessary files from either:\n\n*   A remote GitHub repository, or\n*   A local directory on your computer.\n\nIt's like a diligent librarian collecting books from different shelves, ensuring they meet certain criteria before adding them to the collection. The criteria might include:\n\n*   **File Type:** Only including certain types of files (e.g., Python files, JavaScript files).\n*   **File Size:** Ignoring files that are too large.\n*   **Location:** Only including files in certain directories, or excluding files in others (like test directories).\n\nWithout Codebase Crawling, our system wouldn't have any code to analyze, and we wouldn't be able to generate a tutorial!\n\n## Key Concepts\n\n*   **Repository URL:** The address of the GitHub repository (e.g., `https://github.com/fastapi/fastapi`).\n*   **Local Directory:** The path to a directory on your computer (e.g., `/Users/myuser/myproject`).\n*   **Include Patterns:**  Patterns that specify which files to include (e.g., `*.py` for all Python files).  Think of these as \"keywords\" to search for in the library.\n*   **Exclude Patterns:** Patterns that specify which files to exclude (e.g., `tests/*` to exclude all files in the `tests` directory).  Think of these as \"forbidden words\" to avoid in the search.\n*   **Maximum File Size:** A limit on the size of the files to include. This helps prevent the system from trying to process very large files, which can be slow or cause errors.\n\n## How Codebase Crawling Works in Our Project\n\nThe Codebase Crawling process is handled by the `FetchRepo` node in our flow. Remember from [Flow Orchestration](02_flow_orchestration.md) that each node has three important steps: `prep`, `exec`, and `post`.\n\nLet's look at how Codebase Crawling is implemented in the `FetchRepo` node in `nodes.py`:\n\n```python\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"local_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        # ... (derive project name if not provided)\n\n        # Get file patterns directly from shared\n        include_patterns = shared[\"include_patterns\"]\n        exclude_patterns = shared[\"exclude_patterns\"]\n        max_file_size = shared[\"max_file_size\"]\n\n        return {\n            \"repo_url\": repo_url,\n            \"local_dir\": local_dir,\n            \"token\": shared.get(\"github_token\"),\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns,\n            \"max_file_size\": max_file_size,\n            \"use_relative_paths\": True\n        }\n\n    def exec(self, prep_res):\n        if prep_res[\"repo_url\"]:\n            print(f\"Crawling repository: {prep_res['repo_url']}...\")\n            result = crawl_github_files(\n                repo_url=prep_res[\"repo_url\"],\n                token=prep_res[\"token\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        else:\n            print(f\"Crawling directory: {prep_res['local_dir']}...\")\n            result = crawl_local_files(\n                directory=prep_res[\"local_dir\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n            \n        # Convert dict to list of tuples: [(path, content), ...]\n        files_list = list(result.get(\"files\", {}).items())\n        print(f\"Fetched {len(files_list)} files.\")\n        return files_list\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"files\"] = exec_res # List of (path, content) tuples\n```\n\nLet's break down what's happening:\n\n1.  **`prep(shared)`:**\n    *   This method retrieves the configuration values (repository URL, local directory, include patterns, exclude patterns, maximum file size) from the `shared` dictionary. This is how the node knows *what* to crawl and *how* to crawl it.\n    *   It prepares these values into a `prep_res` object which is a dictionary containing the configuration for the `exec` function to use.\n\n2.  **`exec(prep_res)`:**\n    *   This method checks if a `repo_url` is provided. If so, it calls the `crawl_github_files` function to crawl the GitHub repository.\n    *   If a `repo_url` is *not* provided (meaning we're crawling a local directory), it calls the `crawl_local_files` function to crawl the local directory.\n    *   The `crawl_github_files` and `crawl_local_files` functions do the actual work of fetching the files and their contents.\n    *   Finally, the result from the crawl is converted into a list of tuples where each tuple contains a file path and file content.\n\n3.  **`post(shared, prep_res, exec_res)`:**\n    *   This method takes the list of files (the `exec_res` output) and stores it in the `shared[\"files\"]` variable. This makes the files available to the next node in the flow, which is [Abstraction Identification](05_abstraction_identification.md).\n\n### Crawling GitHub Repositories: `crawl_github_files`\n\nThe `crawl_github_files` function in `utils/crawl_github_files.py` is responsible for fetching files from a GitHub repository.  Here's a simplified view of how it works:\n\n```python\ndef crawl_github_files(\n    repo_url, \n    token=None, \n    max_file_size: int = 1 * 1024 * 1024,  # 1 MB\n    use_relative_paths: bool = False,\n    include_patterns: Union[str, Set[str]] = None,\n    exclude_patterns: Union[str, Set[str]] = None\n):\n    # Parse the repo_url to extract owner, repo name, branch and path\n    # Uses the GitHub REST API to get the files and their content\n\n    files = {}\n    skipped_files = []\n\n    # Fetch contents of the repository at a specific path and commit\n    def fetch_contents(path):\n        # Use the GitHub REST API to get the contents of the given path\n        # If it's a file, download it and add it to the files dictionary\n        # If it's a directory, recursively call fetch_contents on the directory\n\n        # Check if file should be included based on patterns\n        def should_include_file(file_path: str, file_name: str) -> bool:\n            # If no include patterns are specified, include all files\n            # Otherwise, check if the file matches any of the include patterns\n            # If exclude patterns are specified, exclude files that match the exclude patterns\n            return True or False # Simplified logic\n\n    # Start crawling from the specified path\n    fetch_contents(specific_path)\n\n    return {\n        \"files\": files,\n        \"stats\": {\n            \"downloaded_count\": len(files),\n            \"skipped_count\": len(skipped_files),\n            \"skipped_files\": skipped_files,\n            \"base_path\": specific_path if use_relative_paths else None,\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns\n        }\n    }\n```\n\nHere's a breakdown:\n\n1.  **Parsing the Repository URL:** The function first parses the `repo_url` to extract the owner, repository name, branch/commit, and path within the repository. This information is used to construct the URLs for the GitHub API requests.\n2.  **Using the GitHub REST API:** It uses the GitHub REST API to fetch the contents of the repository. The API allows us to retrieve information about files and directories in the repository.\n3.  **Handling Different Content Types:** The API returns different data depending on whether the item is a file or a directory.\n    *   **Files:** For files, the function downloads the file content and stores it in the `files` dictionary.  It also checks the file size against the `max_file_size` limit.\n    *   **Directories:** For directories, the function recursively calls itself to process the contents of the subdirectory. This allows it to traverse the entire repository structure.\n4.  **Include/Exclude Patterns:** The function uses the `should_include_file` helper function to check if a file should be included based on the `include_patterns` and `exclude_patterns`. This allows you to filter the files that are included in the tutorial generation process.  If a file matches an exclude pattern, it will not be included.\n5.  **Error Handling:** The function includes error handling to deal with potential issues like rate limits (which can occur if you make too many requests to the GitHub API in a short period of time) and network errors.\n\n### Crawling Local Directories: `crawl_local_files`\n\nThe `crawl_local_files` function in `utils/crawl_local_files.py` is responsible for fetching files from a local directory. Here's a simplified view of how it works:\n\n```python\nimport os\nimport fnmatch\n\ndef crawl_local_files(directory, include_patterns=None, exclude_patterns=None, max_file_size=None, use_relative_paths=True):\n    \"\"\"\n    Crawl files in a local directory with similar interface as crawl_github_files.\n    \"\"\"\n    files_dict = {}\n\n    # Traverse through all files and folders in the directory\n    for root, _, files in os.walk(directory):\n        for filename in files:\n            filepath = os.path.join(root, filename) # Get the full path\n            relpath = os.path.relpath(filepath, directory) # Path relative to directory\n\n            # Check include and exclude patterns\n            included = True #Simplified\n            excluded = False #Simplified\n            if not included or excluded:\n                continue\n\n            # Check file size\n            if max_file_size and os.path.getsize(filepath) > max_file_size:\n                continue\n\n            try:\n                with open(filepath, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                files_dict[relpath] = content #Store content\n            except Exception as e:\n                print(f\"Warning: Could not read file {filepath}: {e}\")\n\n    return {\"files\": files_dict}\n```\n\nHere's a breakdown:\n\n1.  **Traversing the Directory:** The function uses `os.walk` to traverse the directory tree, visiting each directory and file within the specified `directory`.\n2.  **Constructing File Paths:** For each file, the function constructs the full path to the file using `os.path.join`. It also calculates the relative path to the file (relative to the specified `directory`) using `os.path.relpath`.\n3.  **Include/Exclude Patterns:** Similar to `crawl_github_files`, the function checks if a file should be included based on the `include_patterns` and `exclude_patterns`.\n4.  **File Size Check:** The function checks the file size against the `max_file_size` limit.\n5.  **Reading File Content:** If the file passes all the checks, the function reads the content of the file and stores it in the `files_dict` dictionary, using the file path as the key.\n6.  **Error Handling:** The function includes basic error handling to catch potential issues like file read errors.\n\n## How to Use Codebase Crawling\n\nYou don't directly call `crawl_github_files` or `crawl_local_files` yourself. Instead, you provide the necessary configuration (repository URL or local directory, include patterns, exclude patterns, maximum file size) when you run the `main.py` script.  The `FetchRepo` node then uses this configuration to crawl the codebase.\n\nFor example, to generate a tutorial for the `fastapi` repository, including only Python files and excluding the `tests` directory, you would use the following command:\n\n```bash\npython main.py --repo https://github.com/fastapi/fastapi -i \"*.py\" -e \"tests/*\"\n```\n\nTo generate a tutorial for a local directory, you would use the `--dir` option instead of `--repo`:\n\n```bash\npython main.py --dir /path/to/my/project -i \"*.py\" -e \"tests/*\"\n```\n\n## Why is Codebase Crawling Important?\n\n*   **Data Acquisition:** It's the first step in the tutorial generation process, providing the raw data (the codebase) that is needed to generate the tutorial.\n*   **Filtering:** It allows you to filter the files that are included in the tutorial, focusing on the most relevant parts of the codebase and ignoring irrelevant files (like test files or documentation).\n*   **Efficiency:** By limiting the maximum file size, it helps prevent the system from trying to process very large files, which can be slow or cause errors.\n\n## Conclusion\n\nIn this chapter, we've learned about Codebase Crawling, which is the process of gathering the necessary files from either a GitHub repository or a local directory. We've seen how it uses include and exclude patterns to filter files, and how it limits the maximum file size to prevent performance issues.\n\nIn the next chapter, [Abstraction Identification](05_abstraction_identification.md), we'll explore how we analyze the codebase to identify the key concepts that should be covered in the tutorial.\n", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `Tutorial-Codebase-Knowledge` about the concept: \"Abstraction Identification\". This is Chapter 5.\n\nConcept Details:\n- Description:\nAbstraction Identification is the process of analyzing the codebase and identifying the most\nimportant concepts or components to explain in the tutorial. This involves using an LLM to\nunderstand the code and extract the core abstractions. It's like a detective piecing together\nclues from a crime scene to identify the key suspects and their roles in the crime.\n\n\nComplete Tutorial Structure:\n1. [API Endpoint](01_api_endpoint.md)\n2. [Flow Orchestration](02_flow_orchestration.md)\n3. [Configuration Management](03_configuration_management.md)\n4. [Codebase Crawling](04_codebase_crawling.md)\n5. [Abstraction Identification](05_abstraction_identification.md)\n6. [Relationship Analysis](06_relationship_analysis.md)\n7. [LLM Interaction](07_llm_interaction.md)\n8. [Chapter Writing](08_chapter_writing.md)\n9. [Tutorial Combination](09_tutorial_combination.md)\n\nContext from previous chapters (summary):\n# Chapter 1: API Endpoint\n\nWelcome to the first step in our journey to understanding how to generate tutorials from codebases! In this chapter, we'll explore the **API Endpoint**, which acts like the \"front door\" to our tutorial generation system.\n\nImagine you have a robot that can build LEGO sets. You wouldn't want to open up the robot and start fiddling with its internal wires every time you want it to build something, right? Instead, you'd want a simple button or a control panel to tell it what to do. The API Endpoint is like that button for our tutorial generator.\n\n**Why do we need an API Endpoint?**\n\nThink of it this way: you might want to trigger the tutorial generation process from:\n\n*   Your web browser\n*   Another program running on your computer\n*   Even from a different computer across the internet!\n\nAn API Endpoint allows all these different \"clients\" to communicate with our tutorial generation system in a standardized way. It's the central point of contact.\n\n**Key Concepts:**\n\n1.  **API (Application Programming Interface):** Think of it as a contract between two pieces of software. It defines how they can talk to each other. In our case, it defines how external programs can ask our system to generate a tutorial.\n\n2.  **Endpoint:** A specific URL (web address) that represents a particular function or resource in an API. In our case, it's the URL you send a request to in order to start the tutorial generation. It's like the specific button on our robot's control panel that says \"Build this LEGO set!\"\n\n3.  **Request:** The message you send to the API Endpoint to tell it what to do. This includes information like the repository URL, output folder and options for the tutorial generation.\n\n4.  **Response:** The message the API Endpoint sends back to you after it has processed your request. This might include a success message, an error message, or the path to the generated tutorial.\n\n**Our Use Case: Generating a tutorial for a GitHub repository**\n\nLet's say you want to generate a tutorial for a project on GitHub. The API Endpoint allows you to do this with a simple request. You'll need to provide the URL of the repository and any specific options for how you want the tutorial to be generated.\n\n**How to Use the API Endpoint:**\n\nOur API Endpoint is designed to receive a `POST` request to the `/generate` path. This request contains information about the repository for which you want to generate a tutorial.\n\nHere's an example of how you might send a request (this is just an example, the actual implementation will depend on how you're sending the request, e.g., using `curl`, `Postman`, or a Python script):\n\n```json\n{\n  \"repo_url\": \"https://github.com/fastapi/fastapi\",\n  \"name\": \"FastAPI\",\n  \"output\": \"tutorials\"\n}\n```\n\nThis request tells the system to:\n\n*   Generate a tutorial for the `https://github.com/fastapi/fastapi` repository.\n*   Use \"FastAPI\" as the project name.\n*   Save the tutorial in the `tutorials` output folder.\n\nAfter sending this request, the API Endpoint will process it and, if successful, return a response like this:\n\n```json\n{\n  \"message\": \"Tutorial generation completed successfully.\",\n  \"final_output_directory\": \"tutorials/FastAPI/html\"\n}\n```\n\nThis tells you that the tutorial was generated successfully and is located in the `tutorials/FastAPI/html` directory.\n\n**Internal Implementation: What happens under the hood?**\n\nWhen the API Endpoint receives a request, here's a simplified breakdown of what happens:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant API Endpoint\n    participant Main Script\n    participant Codebase\n    participant LLM\n\n    User->>API Endpoint: Sends tutorial generation request (repo URL, options)\n    API Endpoint->>Main Script: Executes main.py with provided arguments\n    Main Script->>Codebase: Crawls and analyzes the codebase\n    Main Script->>LLM: Interacts with LLM to generate content\n    Main Script->>API Endpoint: Returns success/failure and output path\n    API Endpoint->>User: Sends back response with result and output path\n```\n\nHere's a slightly more detailed explanation:\n\n1.  **Request Reception:** The API Endpoint receives the request with the repository URL and other options.\n2.  **Command Construction:**  It constructs a command line command to run the main script (`main.py`) with the provided parameters.\n3.  **Script Execution:** It executes the `main.py` script, which does the heavy lifting of cloning the repository, analyzing the code, and generating the tutorial content.\n4.  **Response Generation:** After the script finishes (either successfully or with an error), the API Endpoint creates a response with the appropriate message and, if successful, the path to the generated tutorial.\n5.  **Response Delivery:** Finally, it sends the response back to the user.\n\n**Code Snippets:**\n\nLet's look at some key parts of the code in `api.py`:\n\n```python\nfrom fastapi import FastAPI, HTTPException, status\nfrom pydantic import BaseModel, Field\n```\n\nThis imports the necessary libraries for creating the API using FastAPI (a Python framework for building APIs) and Pydantic (for data validation). We are importing `FastAPI` to create the API application, `HTTPException` to handle errors and send them to the user, and `BaseModel` and `Field` from `pydantic` to define the structure of our input data.\n\n```python\nclass GenerationRequest(BaseModel):\n    repo_url: str\n    name: Optional[str] = None\n    token: Optional[str] = None\n    output: str = Field(default=\"output\")\n    include: Optional[List[str]] = None\n    exclude: Optional[List[str]] = None\n    max_size: Optional[int] = None\n```\n\nThis defines the structure of the data that the API Endpoint expects to receive in the request. This is crucial for validation and ensuring that the input data is in the correct format. `repo_url` is required, while other fields like `name`, `token`, `output`, `include`, `exclude`, and `max_size` are optional.\n\n```python\n@app.post(\n    \"/generate\",\n    summary=\"Trigger Tutorial Generation\",\n    description=\"Accepts repository details and options, then runs the tutorial generation script.\",\n    status_code=status.HTTP_200_OK\n)\nasync def generate_tutorial(request_data: GenerationRequest):\n    # ... implementation details ...\n    return {\n        \"message\": \"Tutorial generation completed successfully.\",\n        \"final_output_directory\": expected_final_path,\n    }\n```\n\nThis code defines the `/generate` endpoint using the `@app.post` decorator. This tells FastAPI that this function should be called when a `POST` request is sent to the `/generate` URL. It receives the `request_data` (which is of type `GenerationRequest` that we defined above), does some processing, and returns a response containing a success message and the path to the generated tutorial.  The `status_code=status.HTTP_200_OK` ensures that a successful response returns a 200 OK status code.\n\n**CORS Middleware**\n\n```python\norigins = [\n    \"http://127.0.0.1:5500\", # VS Code Live Server default\n    \"http://localhost\",\n    \"http://localhost:8080\",\n    \"http://127.0.0.1\",\n    \"http://127.0.0.1:8080\",\n    \"null\", # file:// origin\n]\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=origins,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n```\n\nThis section is responsible for configuring Cross-Origin Resource Sharing (CORS). CORS is a security feature implemented by web browsers to restrict web pages from making requests to a different domain than the one which served the web page. Without CORS, you might not be able to send requests to your API from a webpage running on your local machine (e.g. during development). The middleware specifies the origins that are allowed to make requests to the API, ensuring that requests from these origins are not blocked by the browser.\n\n**Error Handling**\n\nThe `api.py` file includes comprehensive error handling to gracefully manage different scenarios that might arise during script execution.\n\n```python\n    except subprocess.CalledProcessError as e:\n        logger.error(f\"Script failed with exit code {e.returncode}\")\n        logger.error(f\"Script stdout:\\n{e.stdout}\")\n        logger.error(f\"Script stderr:\\n{e.stderr}\")\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail={\n                \"error\": \"Script execution failed.\",\n                \"details\": \"The tutorial generation script exited with an error.\",\n                \"return_code\": e.returncode,\n                \"script_stdout\": e.stdout,\n                \"script_stderr\": e.stderr\n            }\n        )\n    except FileNotFoundError:\n        error_msg = f\"Error: Script '{MAIN_SCRIPT_PATH}' or Python '{PYTHON_EXECUTABLE}' not found.\"\n        logger.error(error_msg)\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail={\"error\": \"Server configuration error.\", \"details\": error_msg}\n        )\n    except Exception as e:\n        logger.exception(\"An unexpected error occurred during script execution.\")\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail={\"error\": \"An unexpected internal server error occurred.\", \"details\": str(e)}\n        )\n```\n\n*   **`subprocess.CalledProcessError`**: Catches errors when the `main.py` script fails during execution. It logs the error details and raises an `HTTPException` with a 500 status code, providing details like the return code, stdout, and stderr of the script.\n*   **`FileNotFoundError`**: Handles cases where the `main.py` script or Python executable is not found. It raises an `HTTPException` with a 500 status code, indicating a server configuration issue.\n*   **`Exception`**: Serves as a catch-all for unexpected errors during script execution. It logs the exception and raises an `HTTPException` with a 500 status code, providing details about the error.\n\n**Conclusion:**\n\nIn this chapter, we've learned about the API Endpoint, which serves as the entry point to our tutorial generation system. We saw how it allows external programs to trigger the tutorial generation process with a simple request. We also explored the internal implementation and understood how it orchestrates the execution of the `main.py` script.\n\nNow that we know how to trigger the process, let's dive into [Flow Orchestration](02_flow_orchestration.md) in the next chapter to understand the overall sequence of steps involved in generating a tutorial.\n\n---\n# Chapter 2: Flow Orchestration\n\nIn the previous chapter, [API Endpoint](01_api_endpoint.md), we learned how to trigger the tutorial generation process. Think of it like pressing the \"Start\" button on a machine. But what happens *after* you press that button? That's where Flow Orchestration comes in!\n\n## What is Flow Orchestration?\n\nImagine a factory that builds cars. You wouldn't just dump all the parts in a pile and hope a car magically appears, right? Instead, you'd have a carefully planned assembly line:\n\n1.  **Frame Assembly:** The car's frame is built.\n2.  **Engine Installation:** The engine is installed.\n3.  **Bodywork:** The body panels are attached.\n4.  **Painting:** The car gets its color.\n5.  **Interior Fitting:** Seats and other interior parts are added.\n6.  **Quality Control:** Final checks are performed.\n\nFlow Orchestration is like that assembly line for our tutorial generation system. It defines the *sequence of steps* needed to transform a codebase into a complete tutorial. It makes sure each task happens in the right order, passing the result of one task to the next.\n\nWithout flow orchestration, we'd have a chaotic mess. Chapters might be written before the codebase is even analyzed, or the tutorial might be combined before the chapters are finished! Flow Orchestration brings order to the process.\n\n## Key Concepts\n\n*   **Flow:** The overall process or pipeline of tasks. In our case, it's the entire tutorial generation process.\n*   **Node:** A single task or step in the flow. Each node performs a specific job, like fetching code, analyzing relationships, or writing a chapter.  Nodes can be thought of as functions in a programming language.\n*   **Sequence:** The order in which the nodes are executed.  This is crucial for ensuring the correct flow of data and dependencies.\n*   **Dependencies:**  A node might *depend* on the output of another node. For example, you can't write a chapter *about* an abstraction until you've identified *what* the abstractions *are*.\n\n## How Flow Orchestration Works in Our Project\n\nLet's look at the main steps in our tutorial generation flow:\n\n```mermaid\ngraph LR\n    A[Fetch Repo] --> B(Identify Abstractions)\n    B --> C{Analyze Relationships}\n    C --> D[Order Chapters]\n    D --> E((Write Chapters))\n    E --> F(Combine Tutorial)\n    style A fill:#f9f,stroke:#333,stroke-width:2px\n    style F fill:#f9f,stroke:#333,stroke-width:2px\n    style E fill:#ccf,stroke:#333,stroke-width:2px\n```\n\nHere's a breakdown of each node:\n\n1.  **Fetch Repo:**  Downloads the codebase from a remote repository (like GitHub) or reads it from a local directory. This node is responsible for getting the raw material needed to create the tutorial.  It outputs the files and their contents.\n2.  **Identify Abstractions:** Analyzes the codebase and identifies key concepts or abstractions that are important for understanding the project. This step essentially identifies the key topics to cover in the tutorial.  It outputs a list of abstractions, including their names, descriptions, and the files they're found in.\n3.  **Analyze Relationships:** Examines how the different abstractions interact with each other. This helps us understand the overall architecture of the codebase and how the different pieces fit together. The LLM creates a summary of the project and the relationships between the abstractions\n4.  **Order Chapters:** Determines the best order to present the abstractions in the tutorial.  Some abstractions might be foundational and need to be explained before others. The LLM figures out the order.\n5.  **Write Chapters:**  Generates the actual content for each chapter of the tutorial. It uses the abstraction details, relationships, and chapter order to create a beginner-friendly explanation of each concept. This node is marked with double parentheses `((...))` because it's a *BatchNode*. A `BatchNode` handles independent steps and can be run in parallel.\n6.  **Combine Tutorial:**  Assembles all the individual chapters into a complete tutorial, creating an index page and table of contents.\n\n## Code Example: Defining the Flow\n\nThe core of our flow orchestration is defined in `flow.py`:\n\n```python\nfrom pocketflow import Flow\n# Import all node classes from nodes.py\nfrom nodes import (\n    FetchRepo,\n    IdentifyAbstractions,\n    AnalyzeRelationships,\n    OrderChapters,\n    WriteChapters,\n    CombineTutorial\n)\n\ndef create_tutorial_flow():\n    \"\"\"Creates and returns the codebase tutorial generation flow.\"\"\"\n\n    # Instantiate nodes\n    fetch_repo = FetchRepo()\n    identify_abstractions = IdentifyAbstractions(max_retries=3, wait=10)\n    analyze_relationships = AnalyzeRelationships(max_retries=3, wait=10)\n    order_chapters = OrderChapters(max_retries=3, wait=10)\n    write_chapters = WriteChapters(max_retries=3, wait=10) # This is a BatchNode\n    combine_tutorial = CombineTutorial()\n\n    # Connect nodes in sequence based on the design\n    fetch_repo >> identify_abstractions\n    identify_abstractions >> analyze_relationships\n    analyze_relationships >> order_chapters\n    order_chapters >> write_chapters\n    write_chapters >> combine_tutorial\n\n    # Create the flow starting with FetchRepo\n    tutorial_flow = Flow(start=fetch_repo)\n\n    return tutorial_flow\n```\n\nLet's break down this code:\n\n*   **Importing Nodes:** We import all the node classes (like `FetchRepo`, `IdentifyAbstractions`, etc.) from the `nodes.py` file.  Think of these as importing different tools or functions that we need for our assembly line.\n*   **Instantiating Nodes:**  We create instances of each node class. For example, `fetch_repo = FetchRepo()` creates a specific instance of the `FetchRepo` node. This is where the maximum number of retries and wait time between retries is set for nodes that interact with the LLM. This helps handle cases where the LLM might be temporarily unavailable or returns an invalid response.\n*   **Connecting Nodes:** The `>>` operator defines the sequence of the flow. `fetch_repo >> identify_abstractions` means that the `IdentifyAbstractions` node will execute *after* the `FetchRepo` node completes.  The output of `FetchRepo` will be passed as input to `IdentifyAbstractions`.\n*   **Creating the Flow:** We use the `Flow` class from the `pocketflow` library to create the flow. We specify the `start` node, which is the first node to be executed.\n\nThis simple code defines the entire structure of our tutorial generation process!\n\n## Code Example: Running the Flow\n\nThe `main.py` file is responsible for setting up the flow with all the command line input parameters and calling the `Flow` to execute:\n\n```python\nimport dotenv\nimport os\nimport argparse\n# Import the function that creates the flow\nfrom flow import create_tutorial_flow\n\ndotenv.load_dotenv()\n\n# Default file patterns\nDEFAULT_INCLUDE_PATTERNS = {\n    \"*.py\", \"*.js\", \"*.jsx\", \"*.ts\", \"*.tsx\", \"*.go\", \"*.java\", \"*.pyi\", \"*.pyx\", \n    \"*.c\", \"*.cc\", \"*.cpp\", \"*.h\", \"*.md\", \"*.rst\", \"Dockerfile\", \n    \"Makefile\", \"*.yaml\", \"*.yml\",\n}\n\nDEFAULT_EXCLUDE_PATTERNS = {\n    \"*test*\", \"tests/*\", \"docs/*\", \"examples/*\", \"v1/*\", \n    \"dist/*\", \"build/*\", \"experimental/*\", \"deprecated/*\", \n    \"legacy/*\", \".git/*\", \".github/*\", \".next/*\", \".vscode/*\", \"obj/*\", \"bin/*\", \"node_modules/*\", \"*.log\"\n}\n\n# --- Main Function ---\ndef main():\n    parser = argparse.ArgumentParser(description=\"Generate a tutorial for a GitHub codebase or local directory.\")\n    \n    # Create mutually exclusive group for source\n    source_group = parser.add_mutually_exclusive_group(required=True)\n    source_group.add_argument(\"--repo\", help=\"URL of the public GitHub repository.\")\n    source_group.add_argument(\"--dir\", help=\"Path to local directory.\")\n    \n    parser.add_argument(\"-n\", \"--name\", help=\"Project name (optional, derived from repo/directory if omitted).\")\n    parser.add_argument(\"-t\", \"--token\", help=\"GitHub personal access token (optional, reads from GITHUB_TOKEN env var if not provided).\")\n    parser.add_argument(\"-o\", \"--output\", default=\"output\", help=\"Base directory for output (default: ./output).\")\n    parser.add_argument(\"-i\", \"--include\", nargs=\"+\", help=\"Include file patterns (e.g. '*.py' '*.js'). Defaults to common code files if not specified.\")\n    parser.add_argument(\"-e\", \"--exclude\", nargs=\"+\", help=\"Exclude file patterns (e.g. 'tests/*' 'docs/*'). Defaults to test/build directories if not specified.\")\n    parser.add_argument(\"-s\", \"--max-size\", type=int, default=100000, help=\"Maximum file size in bytes (default: 100000, about 100KB).\")\n\n    args = parser.parse_args()\n\n    # Get GitHub token from argument or environment variable if using repo\n    github_token = None\n    if args.repo:\n        github_token = args.token or os.environ.get('GITHUB_TOKEN')\n        if not github_token:\n            print(\"Warning: No GitHub token provided. You might hit rate limits for public repositories.\")\n\n    # Initialize the shared dictionary with inputs\n    shared = {\n        \"repo_url\": args.repo,\n        \"local_dir\": args.dir,\n        \"project_name\": args.name, # Can be None, FetchRepo will derive it\n        \"github_token\": github_token,\n        \"output_dir\": args.output, # Base directory for CombineTutorial output\n\n        # Add include/exclude patterns and max file size\n        \"include_patterns\": set(args.include) if args.include else DEFAULT_INCLUDE_PATTERNS,\n        \"exclude_patterns\": set(args.exclude) if args.exclude else DEFAULT_EXCLUDE_PATTERNS,\n        \"max_file_size\": args.max_size,\n\n        # Outputs will be populated by the nodes\n        \"files\": [],\n        \"abstractions\": [],\n        \"relationships\": {},\n        \"chapter_order\": [],\n        \"chapters\": [],\n        \"final_output_dir\": None\n    }\n\n    print(f\"Starting tutorial generation for: {args.repo or args.dir}\")\n\n    # Create the flow instance\n    tutorial_flow = create_tutorial_flow()\n\n    # Run the flow\n    tutorial_flow.run(shared)\n    \nif __name__ == \"__main__\":\n    main()\n```\n\nHere are the important parts:\n\n1.  **Parsing Arguments:** We use `argparse` to handle command-line arguments like the repository URL, project name, and output directory.\n2.  **Shared Dictionary:** The `shared` dictionary acts as a central store for data that is passed between the nodes. Each node can read data from this dictionary and write its results back to it.\n3.  **Creating the Flow:** We call `create_tutorial_flow()` to get an instance of the flow we defined in `flow.py`.\n4.  **Running the Flow:**  Finally, we call `tutorial_flow.run(shared)` to start the flow. The `shared` dictionary is passed to the `run` method so that the nodes can access and modify the data.\n\n## What Happens Inside a Node?\n\nEach node consists of three important methods. Looking at the example of `FetchRepo` in `nodes.py`:\n\n```python\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"local_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        if not project_name:\n            # Basic name derivation from URL or directory\n            if repo_url:\n                project_name = repo_url.split('/')[-1].replace('.git', '')\n            else:\n                project_name = os.path.basename(os.path.abspath(local_dir))\n            shared[\"project_name\"] = project_name\n\n        # Get file patterns directly from shared\n        include_patterns = shared[\"include_patterns\"]\n        exclude_patterns = shared[\"exclude_patterns\"]\n        max_file_size = shared[\"max_file_size\"]\n\n        return {\n            \"repo_url\": repo_url,\n            \"local_dir\": local_dir,\n            \"token\": shared.get(\"github_token\"),\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns,\n            \"max_file_size\": max_file_size,\n            \"use_relative_paths\": True\n        }\n\n    def exec(self, prep_res):\n        if prep_res[\"repo_url\"]:\n            print(f\"Crawling repository: {prep_res['repo_url']}...\")\n            result = crawl_github_files(\n                repo_url=prep_res[\"repo_url\"],\n                token=prep_res[\"token\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        else:\n            print(f\"Crawling directory: {prep_res['local_dir']}...\")\n            result = crawl_local_files(\n                directory=prep_res[\"local_dir\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n            \n        # Convert dict to list of tuples: [(path, content), ...]\n        files_list = list(result.get(\"files\", {}).items())\n        print(f\"Fetched {len(files_list)} files.\")\n        return files_list\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"files\"] = exec_res # List of (path, content) tuples\n```\n\n*   **`prep(shared)`:** This method prepares the input for the node's execution. It reads data from the `shared` dictionary and transforms it into a format that the `exec` method can use. It also performs any necessary setup or validation. Returns a `prep_res` object that contains everything necessary for the `exec` function to work.\n*   **`exec(prep_res)`:** This method performs the core logic of the node. It receives the prepared input from the `prep` method and executes the task. In the case of `FetchRepo`, it downloads the codebase from GitHub or reads it from the local directory. It returns the `exec_res` which is the output of the node.\n*   **`post(shared, prep_res, exec_res)`:** This method is executed *after* the `exec` method completes. It takes the output of the `exec` method (`exec_res`) and writes it back to the `shared` dictionary, making it available to the next node in the flow.  In the case of `FetchRepo`, it writes the list of files and their contents to the `shared[\"files\"]` variable.\n\n## Why Use Flow Orchestration?\n\n*   **Organization:** It provides a clear and structured way to define complex processes.\n*   **Maintainability:** It makes the code easier to understand and maintain by breaking it down into smaller, independent nodes.\n*   **Reusability:**  Nodes can be reused in different flows, making it easier to build new features or modify existing ones.\n*   **Testability:** Each node can be tested independently, making it easier to ensure the correctness of the entire system.\n*   **Error Handling:** `pocketflow` supports retries, fallbacks, error handling, and graceful termination, which are useful for nodes that interact with external resources.\n\n## Conclusion\n\nFlow Orchestration is a powerful technique for managing complex processes like our tutorial generation system. It provides a structured way to define the sequence of steps, making the code more organized, maintainable, and testable.\n\nIn the next chapter, [Configuration Management](03_configuration_management.md), we'll explore how to manage the different configuration settings that control the behavior of our system. This will allow us to customize the tutorial generation process for different codebases and use cases.\n\n---\n# Chapter 3: Configuration Management\n\nIn the previous chapter, [Flow Orchestration](02_flow_orchestration.md), we learned how to define the overall sequence of steps for generating a tutorial. Now, let's talk about how to *customize* that process. Think of it like this: you have a recipe for baking a cake, but you might want to adjust the recipe to make it chocolate or vanilla. That's where Configuration Management comes in!\n\n## What is Configuration Management?\n\nConfiguration Management is like having a control panel that allows you to adjust the settings of our tutorial generation \"machine\" to achieve the desired output. It lets you change things like:\n\n*   Which repository to generate a tutorial for.\n*   What name to give the project in the tutorial.\n*   Which files to include or exclude from analysis.\n*   Where to save the generated tutorial.\n\nWithout configuration management, we'd be stuck with a single, inflexible way of generating tutorials. It's like trying to bake every cake with the exact same recipe, even if you want different flavors!\n\n## Key Concepts\n\n1.  **Environment Variables:** These are like global settings that can be accessed by any program running on your computer. They're often used to store things like API keys or default values.\n2.  **Argument Parsing:** This allows you to pass settings directly to a program when you run it from the command line. It's like specifying the ingredients for your cake when you give the recipe to the baker.\n\n## Our Use Case: Generating a Tutorial for a Specific GitHub Repository with a Custom Name\n\nLet's say you want to generate a tutorial for the `requests` library on GitHub and name the project \"Python Requests Tutorial\". With Configuration Management, you can easily do this!\n\n## How to Use Configuration Management\n\nOur project uses both environment variables and argument parsing to manage configuration.\n\n### 1. Environment Variables\n\nEnvironment variables are useful for settings that you might want to reuse across multiple runs of the program, or that contain sensitive information you don't want to type directly into the command line.\n\nTo set an environment variable, you can use the following command in your terminal (replace `<your_token>` with your actual token):\n\n```bash\nexport GITHUB_TOKEN=<your_token>\n```\n\nOr, you can create a `.env` file in the project root directory:\n\n```\nGITHUB_TOKEN=<your_token>\n```\n\nThe `dotenv` library will automatically load these variables when the script runs.\n\nIn our `main.py` file, we use the `os.environ.get()` function to read the value of the `GITHUB_TOKEN` environment variable:\n\n```python\nimport os\ngithub_token = args.token or os.environ.get('GITHUB_TOKEN')\n```\n\nThis code first checks if the token was provided as a command-line argument (`args.token`). If not, it looks for the `GITHUB_TOKEN` environment variable.\n\n### 2. Argument Parsing\n\nArgument parsing allows you to specify settings directly when you run the `main.py` script from the command line.\n\nHere's how you would generate a tutorial for the `requests` library with a custom name:\n\n```bash\npython main.py --repo https://github.com/psf/requests -n \"Python Requests Tutorial\" -o tutorials\n```\n\nLet's break down this command:\n\n*   `python main.py`: This tells your computer to run the `main.py` script using Python.\n*   `--repo https://github.com/psf/requests`: This specifies the URL of the GitHub repository to use. `--repo` is the *argument* name, and `https://github.com/psf/requests` is the *value* of that argument.\n*   `-n \"Python Requests Tutorial\"`: This sets the project name to \"Python Requests Tutorial\". `-n` is a *short* argument name, and `\"Python Requests Tutorial\"` is the value.\n*   `-o tutorials`: This specifies that the tutorial output will be written to the `tutorials` directory.\n\nThe `argparse` module in Python handles the parsing of these command-line arguments. In `main.py`, we define the arguments that our script accepts:\n\n```python\nimport argparse\n\nparser = argparse.ArgumentParser(description=\"Generate a tutorial for a GitHub codebase or local directory.\")\nparser.add_argument(\"--repo\", help=\"URL of the public GitHub repository.\")\nparser.add_argument(\"-n\", \"--name\", help=\"Project name (optional, derived from repo/directory if omitted).\")\nparser.add_argument(\"-o\", \"--output\", default=\"output\", help=\"Base directory for output (default: ./output).\")\n# ... other arguments\nargs = parser.parse_args()\n```\n\nThis code defines three arguments: `--repo`, `--name` (with the short form `-n`), and `--output` (with the short form `-o`). The `help` parameter provides a description of each argument that is displayed when you run the script with the `--help` flag:\n\n```bash\npython main.py --help\n```\n\n### How Include and Exclude Patterns Work\n\nThe `-i` (or `--include`) and `-e` (or `--exclude`) options are used to specify which files to include or exclude from the tutorial generation process. This is helpful if you want to focus on specific parts of the codebase or ignore irrelevant files (like test files or documentation).\n\nFor example, to only include Python files and exclude test directories, you could use the following command:\n\n```bash\npython main.py --repo https://github.com/psf/requests -i \"*.py\" -e \"tests/*\"\n```\n\n## Internal Implementation\n\nHere's a simplified view of how Configuration Management works internally:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Command Line\n    participant Main Script\n    participant Args Namespace\n    User->>Command Line: Runs script with arguments: python main.py --repo ...\n    Command Line->>Main Script: Executes main.py\n    Main Script->>Args Namespace: Parses command-line arguments using argparse\n    Args Namespace->>Main Script: Returns arguments as an object (e.g., args.repo, args.name)\n    Main Script->>Main Script: Accesses argument values and uses them\n```\n\nHere's a breakdown:\n\n1.  **User Interaction:** The user runs the `main.py` script from the command line, providing arguments like the repository URL and project name.\n2.  **Argument Parsing:** The `argparse` module in `main.py` parses these arguments and stores them in an `Args Namespace` object.\n3.  **Accessing Arguments:** The `main.py` script accesses the argument values from the `Args Namespace` object (e.g., `args.repo`, `args.name`).\n4.  **Using Arguments:** The script uses these values to configure the tutorial generation process. For example, the `repo_url` value is passed to the [Codebase Crawling](04_codebase_crawling.md) node to download the codebase.\n\n## Code Example: Passing Configuration to Nodes\n\nThe parsed arguments are stored in the `shared` dictionary and passed to the nodes in the flow. For example, in `main.py`:\n\n```python\nshared = {\n    \"repo_url\": args.repo,\n    \"project_name\": args.name,\n    \"output_dir\": args.output,\n    # ... other arguments\n}\n\ntutorial_flow.run(shared)\n```\n\nEach node can then access the configuration values from the `shared` dictionary in its `prep` method. For example, in `nodes.py`:\n\n```python\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        project_name = shared.get(\"project_name\")\n        # ...\n        return {\n            \"repo_url\": repo_url,\n            # ...\n        }\n```\n\n## Why is Configuration Management Important?\n\n*   **Flexibility:** It allows you to customize the tutorial generation process for different codebases and use cases.\n*   **Reusability:** You can easily reuse the same script to generate tutorials for different projects by simply changing the configuration settings.\n*   **Automation:** You can automate the tutorial generation process by running the script with a predefined set of arguments.\n\n## Conclusion\n\nIn this chapter, we've learned about Configuration Management, which allows us to customize the tutorial generation process by using environment variables and argument parsing. This gives us the flexibility to generate tutorials for different codebases with different settings.\n\nIn the next chapter, [Codebase Crawling](04_codebase_crawling.md), we'll dive into the process of fetching and analyzing the codebase itself.\n\n---\n# Chapter 4: Codebase Crawling\n\nIn the previous chapter, [Configuration Management](03_configuration_management.md), we learned how to customize our tutorial generation process using command-line arguments and environment variables. Now, let's explore how we actually *get* the code that we'll be using to generate the tutorial! That's where Codebase Crawling comes in.\n\n## What is Codebase Crawling?\n\nImagine you're writing a report on a popular book. You can't write the report without actually *reading* the book first, right? Codebase Crawling is like reading the book (or, more accurately, the code) for our tutorial generation system.\n\nSpecifically, Codebase Crawling is the process of gathering all the necessary files from either:\n\n*   A remote GitHub repository, or\n*   A local directory on your computer.\n\nIt's like a diligent librarian collecting books from different shelves, ensuring they meet certain criteria before adding them to the collection. The criteria might include:\n\n*   **File Type:** Only including certain types of files (e.g., Python files, JavaScript files).\n*   **File Size:** Ignoring files that are too large.\n*   **Location:** Only including files in certain directories, or excluding files in others (like test directories).\n\nWithout Codebase Crawling, our system wouldn't have any code to analyze, and we wouldn't be able to generate a tutorial!\n\n## Key Concepts\n\n*   **Repository URL:** The address of the GitHub repository (e.g., `https://github.com/fastapi/fastapi`).\n*   **Local Directory:** The path to a directory on your computer (e.g., `/Users/myuser/myproject`).\n*   **Include Patterns:**  Patterns that specify which files to include (e.g., `*.py` for all Python files).  Think of these as \"keywords\" to search for in the library.\n*   **Exclude Patterns:** Patterns that specify which files to exclude (e.g., `tests/*` to exclude all files in the `tests` directory).  Think of these as \"forbidden words\" to avoid in the search.\n*   **Maximum File Size:** A limit on the size of the files to include. This helps prevent the system from trying to process very large files, which can be slow or cause errors.\n\n## How Codebase Crawling Works in Our Project\n\nThe Codebase Crawling process is handled by the `FetchRepo` node in our flow. Remember from [Flow Orchestration](02_flow_orchestration.md) that each node has three important steps: `prep`, `exec`, and `post`.\n\nLet's look at how Codebase Crawling is implemented in the `FetchRepo` node in `nodes.py`:\n\n```python\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"local_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        # ... (derive project name if not provided)\n\n        # Get file patterns directly from shared\n        include_patterns = shared[\"include_patterns\"]\n        exclude_patterns = shared[\"exclude_patterns\"]\n        max_file_size = shared[\"max_file_size\"]\n\n        return {\n            \"repo_url\": repo_url,\n            \"local_dir\": local_dir,\n            \"token\": shared.get(\"github_token\"),\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns,\n            \"max_file_size\": max_file_size,\n            \"use_relative_paths\": True\n        }\n\n    def exec(self, prep_res):\n        if prep_res[\"repo_url\"]:\n            print(f\"Crawling repository: {prep_res['repo_url']}...\")\n            result = crawl_github_files(\n                repo_url=prep_res[\"repo_url\"],\n                token=prep_res[\"token\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        else:\n            print(f\"Crawling directory: {prep_res['local_dir']}...\")\n            result = crawl_local_files(\n                directory=prep_res[\"local_dir\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n            \n        # Convert dict to list of tuples: [(path, content), ...]\n        files_list = list(result.get(\"files\", {}).items())\n        print(f\"Fetched {len(files_list)} files.\")\n        return files_list\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"files\"] = exec_res # List of (path, content) tuples\n```\n\nLet's break down what's happening:\n\n1.  **`prep(shared)`:**\n    *   This method retrieves the configuration values (repository URL, local directory, include patterns, exclude patterns, maximum file size) from the `shared` dictionary. This is how the node knows *what* to crawl and *how* to crawl it.\n    *   It prepares these values into a `prep_res` object which is a dictionary containing the configuration for the `exec` function to use.\n\n2.  **`exec(prep_res)`:**\n    *   This method checks if a `repo_url` is provided. If so, it calls the `crawl_github_files` function to crawl the GitHub repository.\n    *   If a `repo_url` is *not* provided (meaning we're crawling a local directory), it calls the `crawl_local_files` function to crawl the local directory.\n    *   The `crawl_github_files` and `crawl_local_files` functions do the actual work of fetching the files and their contents.\n    *   Finally, the result from the crawl is converted into a list of tuples where each tuple contains a file path and file content.\n\n3.  **`post(shared, prep_res, exec_res)`:**\n    *   This method takes the list of files (the `exec_res` output) and stores it in the `shared[\"files\"]` variable. This makes the files available to the next node in the flow, which is [Abstraction Identification](05_abstraction_identification.md).\n\n### Crawling GitHub Repositories: `crawl_github_files`\n\nThe `crawl_github_files` function in `utils/crawl_github_files.py` is responsible for fetching files from a GitHub repository.  Here's a simplified view of how it works:\n\n```python\ndef crawl_github_files(\n    repo_url, \n    token=None, \n    max_file_size: int = 1 * 1024 * 1024,  # 1 MB\n    use_relative_paths: bool = False,\n    include_patterns: Union[str, Set[str]] = None,\n    exclude_patterns: Union[str, Set[str]] = None\n):\n    # Parse the repo_url to extract owner, repo name, branch and path\n    # Uses the GitHub REST API to get the files and their content\n\n    files = {}\n    skipped_files = []\n\n    # Fetch contents of the repository at a specific path and commit\n    def fetch_contents(path):\n        # Use the GitHub REST API to get the contents of the given path\n        # If it's a file, download it and add it to the files dictionary\n        # If it's a directory, recursively call fetch_contents on the directory\n\n        # Check if file should be included based on patterns\n        def should_include_file(file_path: str, file_name: str) -> bool:\n            # If no include patterns are specified, include all files\n            # Otherwise, check if the file matches any of the include patterns\n            # If exclude patterns are specified, exclude files that match the exclude patterns\n            return True or False # Simplified logic\n\n    # Start crawling from the specified path\n    fetch_contents(specific_path)\n\n    return {\n        \"files\": files,\n        \"stats\": {\n            \"downloaded_count\": len(files),\n            \"skipped_count\": len(skipped_files),\n            \"skipped_files\": skipped_files,\n            \"base_path\": specific_path if use_relative_paths else None,\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns\n        }\n    }\n```\n\nHere's a breakdown:\n\n1.  **Parsing the Repository URL:** The function first parses the `repo_url` to extract the owner, repository name, branch/commit, and path within the repository. This information is used to construct the URLs for the GitHub API requests.\n2.  **Using the GitHub REST API:** It uses the GitHub REST API to fetch the contents of the repository. The API allows us to retrieve information about files and directories in the repository.\n3.  **Handling Different Content Types:** The API returns different data depending on whether the item is a file or a directory.\n    *   **Files:** For files, the function downloads the file content and stores it in the `files` dictionary.  It also checks the file size against the `max_file_size` limit.\n    *   **Directories:** For directories, the function recursively calls itself to process the contents of the subdirectory. This allows it to traverse the entire repository structure.\n4.  **Include/Exclude Patterns:** The function uses the `should_include_file` helper function to check if a file should be included based on the `include_patterns` and `exclude_patterns`. This allows you to filter the files that are included in the tutorial generation process.  If a file matches an exclude pattern, it will not be included.\n5.  **Error Handling:** The function includes error handling to deal with potential issues like rate limits (which can occur if you make too many requests to the GitHub API in a short period of time) and network errors.\n\n### Crawling Local Directories: `crawl_local_files`\n\nThe `crawl_local_files` function in `utils/crawl_local_files.py` is responsible for fetching files from a local directory. Here's a simplified view of how it works:\n\n```python\nimport os\nimport fnmatch\n\ndef crawl_local_files(directory, include_patterns=None, exclude_patterns=None, max_file_size=None, use_relative_paths=True):\n    \"\"\"\n    Crawl files in a local directory with similar interface as crawl_github_files.\n    \"\"\"\n    files_dict = {}\n\n    # Traverse through all files and folders in the directory\n    for root, _, files in os.walk(directory):\n        for filename in files:\n            filepath = os.path.join(root, filename) # Get the full path\n            relpath = os.path.relpath(filepath, directory) # Path relative to directory\n\n            # Check include and exclude patterns\n            included = True #Simplified\n            excluded = False #Simplified\n            if not included or excluded:\n                continue\n\n            # Check file size\n            if max_file_size and os.path.getsize(filepath) > max_file_size:\n                continue\n\n            try:\n                with open(filepath, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                files_dict[relpath] = content #Store content\n            except Exception as e:\n                print(f\"Warning: Could not read file {filepath}: {e}\")\n\n    return {\"files\": files_dict}\n```\n\nHere's a breakdown:\n\n1.  **Traversing the Directory:** The function uses `os.walk` to traverse the directory tree, visiting each directory and file within the specified `directory`.\n2.  **Constructing File Paths:** For each file, the function constructs the full path to the file using `os.path.join`. It also calculates the relative path to the file (relative to the specified `directory`) using `os.path.relpath`.\n3.  **Include/Exclude Patterns:** Similar to `crawl_github_files`, the function checks if a file should be included based on the `include_patterns` and `exclude_patterns`.\n4.  **File Size Check:** The function checks the file size against the `max_file_size` limit.\n5.  **Reading File Content:** If the file passes all the checks, the function reads the content of the file and stores it in the `files_dict` dictionary, using the file path as the key.\n6.  **Error Handling:** The function includes basic error handling to catch potential issues like file read errors.\n\n## How to Use Codebase Crawling\n\nYou don't directly call `crawl_github_files` or `crawl_local_files` yourself. Instead, you provide the necessary configuration (repository URL or local directory, include patterns, exclude patterns, maximum file size) when you run the `main.py` script.  The `FetchRepo` node then uses this configuration to crawl the codebase.\n\nFor example, to generate a tutorial for the `fastapi` repository, including only Python files and excluding the `tests` directory, you would use the following command:\n\n```bash\npython main.py --repo https://github.com/fastapi/fastapi -i \"*.py\" -e \"tests/*\"\n```\n\nTo generate a tutorial for a local directory, you would use the `--dir` option instead of `--repo`:\n\n```bash\npython main.py --dir /path/to/my/project -i \"*.py\" -e \"tests/*\"\n```\n\n## Why is Codebase Crawling Important?\n\n*   **Data Acquisition:** It's the first step in the tutorial generation process, providing the raw data (the codebase) that is needed to generate the tutorial.\n*   **Filtering:** It allows you to filter the files that are included in the tutorial, focusing on the most relevant parts of the codebase and ignoring irrelevant files (like test files or documentation).\n*   **Efficiency:** By limiting the maximum file size, it helps prevent the system from trying to process very large files, which can be slow or cause errors.\n\n## Conclusion\n\nIn this chapter, we've learned about Codebase Crawling, which is the process of gathering the necessary files from either a GitHub repository or a local directory. We've seen how it uses include and exclude patterns to filter files, and how it limits the maximum file size to prevent performance issues.\n\nIn the next chapter, [Abstraction Identification](05_abstraction_identification.md), we'll explore how we analyze the codebase to identify the key concepts that should be covered in the tutorial.\n\n\nRelevant Code Snippets:\n--- File: nodes.py ---\nimport os\nimport yaml\nfrom pocketflow import Node, BatchNode\nfrom utils.crawl_github_files import crawl_github_files\nfrom utils.call_llm import call_llm # Assuming you have this utility\nfrom utils.crawl_local_files import crawl_local_files\nfrom converter.md_to_html import convert_all_md_to_html\n\n\n# Helper to get content for specific file indices\ndef get_content_for_indices(files_data, indices):\n    content_map = {}\n    for i in indices:\n        if 0 <= i < len(files_data):\n            path, content = files_data[i]\n            content_map[f\"{i} # {path}\"] = content # Use index + path as key for context\n    return content_map\n\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"local_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        if not project_name:\n            # Basic name derivation from URL or directory\n            if repo_url:\n                project_name = repo_url.split('/')[-1].replace('.git', '')\n            else:\n                project_name = os.path.basename(os.path.abspath(local_dir))\n            shared[\"project_name\"] = project_name\n\n        # Get file patterns directly from shared\n        include_patterns = shared[\"include_patterns\"]\n        exclude_patterns = shared[\"exclude_patterns\"]\n        max_file_size = shared[\"max_file_size\"]\n\n        return {\n            \"repo_url\": repo_url,\n            \"local_dir\": local_dir,\n            \"token\": shared.get(\"github_token\"),\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns,\n            \"max_file_size\": max_file_size,\n            \"use_relative_paths\": True\n        }\n\n    def exec(self, prep_res):\n        if prep_res[\"repo_url\"]:\n            print(f\"Crawling repository: {prep_res['repo_url']}...\")\n            result = crawl_github_files(\n                repo_url=prep_res[\"repo_url\"],\n                token=prep_res[\"token\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        else:\n            print(f\"Crawling directory: {prep_res['local_dir']}...\")\n            result = crawl_local_files(\n                directory=prep_res[\"local_dir\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n            \n        # Convert dict to list of tuples: [(path, content), ...]\n        files_list = list(result.get(\"files\", {}).items())\n        print(f\"Fetched {len(files_list)} files.\")\n        return files_list\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"files\"] = exec_res # List of (path, content) tuples\n\nclass IdentifyAbstractions(Node):\n    def prep(self, shared):\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        \n        # Helper to create context from files, respecting limits (basic example)\n        def create_llm_context(files_data):\n            context = \"\"\n            file_info = [] # Store tuples of (index, path)\n            for i, (path, content) in enumerate(files_data):\n                entry = f\"--- File Index {i}: {path} ---\\n{content}\\n\\n\"\n                context += entry\n                file_info.append((i, path))\n\n            return context, file_info # file_info is list of (index, path)\n\n        context, file_info = create_llm_context(files_data)\n        # Format file info for the prompt (comment is just a hint for LLM)\n        file_listing_for_prompt = \"\\n\".join([f\"- {idx} # {path}\" for idx, path in file_info])\n        return context, file_listing_for_prompt, len(files_data), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, file_listing_for_prompt, file_count, project_name = prep_res  # Unpack project name\n        print(\"Identifying abstractions using LLM...\")\n        prompt = f\"\"\"\nFor the project `{project_name}`:\n\nCodebase Context:\n{context}\n\nAnalyze the codebase context.\nIdentify the top 5-10 core most important abstractions to help those new to the codebase.\n\nFor each abstraction, provide:\n1. A concise `name`.\n2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\nList of file indices and paths present in the context:\n{file_listing_for_prompt}\n\nFormat the output as a YAML list of dictionaries:\n\n```yaml\n- name: Query Processing\n  description: | \n    Explains what the abstraction does.\n    It's like a central dispatcher routing requests.\n  file_indices:\n    - 0 # path/to/file1.py\n    - 3 # path/to/related.py\n- name: Query Optimization\n  description: |\n    Another core concept, similar to a blueprint for objects.\n  file_indices:\n    - 5 # path/to/another.js\n# ... up to 10 abstractions\n```\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        abstractions = yaml.safe_load(yaml_str)\n\n        if not isinstance(abstractions, list):\n            raise ValueError(\"LLM Output is not a list\")\n\n        validated_abstractions = []\n        for item in abstractions:\n            if not isinstance(item, dict) or not all(k in item for k in [\"name\", \"description\", \"file_indices\"]):\n                raise ValueError(f\"Missing keys in abstraction item: {item}\")\n            if not isinstance(item[\"description\"], str):\n                 raise ValueError(f\"description is not a string in item: {item}\")\n            if not isinstance(item[\"file_indices\"], list):\n                 raise ValueError(f\"file_indices is not a list in item: {item}\")\n\n            # Validate indices\n            validated_indices = []\n            for idx_entry in item[\"file_indices\"]:\n                 try:\n                     if isinstance(idx_entry, int):\n                         idx = idx_entry\n                     elif isinstance(idx_entry, str) and '#' in idx_entry:\n                          idx = int(idx_entry.split('#')[0].strip())\n                     else:\n                          idx = int(str(idx_entry).strip())\n\n                     if not (0 <= idx < file_count):\n                         raise ValueError(f\"Invalid file index {idx} found in item {item['name']}. Max index is {file_count - 1}.\")\n                     validated_indices.append(idx)\n                 except (ValueError, TypeError):\n                      raise ValueError(f\"Could not parse index from entry: {idx_entry} in item {item['name']}\")\n\n            item[\"files\"] = sorted(list(set(validated_indices)))\n            # Store only the required fields\n            validated_abstractions.append({\n                \"name\": item[\"name\"],\n                \"description\": item[\"description\"],\n                \"files\": item[\"files\"]\n            })\n\n        print(f\"Identified {len(validated_abstractions)} abstractions.\")\n        return validated_abstractions\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"abstractions\"] = exec_res # List of {\"name\": str, \"description\": str, \"files\": [int]}\n\nclass AnalyzeRelationships(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"] # Now contains 'files' list of indices\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n\n        # Create context with abstraction names, indices, descriptions, and relevant file snippets\n        context = \"Identified Abstractions:\\n\"\n        all_relevant_indices = set()\n        abstraction_info_for_prompt = []\n        for i, abstr in enumerate(abstractions):\n            # Use 'files' which contains indices directly\n            file_indices_str = \", \".join(map(str, abstr['files']))\n            info_line = f\"- Index {i}: {abstr['name']} (Relevant file indices: [{file_indices_str}])\\n  Description: {abstr['description']}\"\n            context += info_line + \"\\n\"\n            abstraction_info_for_prompt.append(f\"{i} # {abstr['name']}\")\n            all_relevant_indices.update(abstr['files'])\n\n        context += \"\\nRelevant File Snippets (Referenced by Index and Path):\\n\"\n        # Get content for relevant files using helper\n        relevant_files_content_map = get_content_for_indices(\n            files_data,\n            sorted(list(all_relevant_indices))\n        )\n        # Format file content for context\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path} ---\\n{content}\"\n            for idx_path, content in relevant_files_content_map.items()\n        )\n        context += file_context_str\n\n        return context, \"\\n\".join(abstraction_info_for_prompt), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, abstraction_listing, project_name = prep_res  # Unpack project name\n        print(\"Analyzing relationships using LLM...\")\n        prompt = f\"\"\"\nBased on the following abstractions and relevant code snippets from the project `{project_name}`:\n\nList of Abstraction Indices and Names:\n{abstraction_listing}\n\nContext (Abstractions, Descriptions, Code):\n{context}\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        relationships_data = yaml.safe_load(yaml_str)\n\n        if not isinstance(relationships_data, dict) or not all(k in relationships_data for k in [\"summary\", \"relationships\"]):\n            raise ValueError(\"LLM output is not a dict or missing keys ('summary', 'relationships')\")\n        if not isinstance(relationships_data[\"summary\"], str):\n             raise ValueError(\"summary is not a string\")\n        if not isinstance(relationships_data[\"relationships\"], list):\n             raise ValueError(\"relationships is not a list\")\n\n        # Validate relationships structure\n        validated_relationships = []\n        num_abstractions = len(abstraction_listing.split('\\n'))\n        for rel in relationships_data[\"relationships\"]:\n             # Check for 'label' key\n             if not isinstance(rel, dict) or not all(k in rel for k in [\"from_abstraction\", \"to_abstraction\", \"label\"]):\n                  raise ValueError(f\"Missing keys (expected from_abstraction, to_abstraction, label) in relationship item: {rel}\")\n             # Validate 'label' is a string\n             if not isinstance(rel[\"label\"], str):\n                  raise ValueError(f\"Relationship label is not a string: {rel}\")\n\n             # Validate indices\n             try:\n                 from_idx = int(str(rel[\"from_abstraction\"]).split('#')[0].strip())\n                 to_idx = int(str(rel[\"to_abstraction\"]).split('#')[0].strip())\n                 if not (0 <= from_idx < num_abstractions and 0 <= to_idx < num_abstractions):\n                      raise ValueError(f\"Invalid index in relationship: from={from_idx}, to={to_idx}. Max index is {num_abstractions-1}.\")\n                 validated_relationships.append({\n                     \"from\": from_idx,\n                     \"to\": to_idx,\n                     \"label\": rel[\"label\"] \n                 })\n             except (ValueError, TypeError):\n                  raise ValueError(f\"Could not parse indices from relationship: {rel}\")\n\n        print(\"Generated project summary and relationship details.\")\n        return {\n            \"summary\": relationships_data[\"summary\"],\n            \"details\": validated_relationships # Store validated, index-based relationships\n        }\n\n\n    def post(self, shared, prep_res, exec_res):\n        # Structure is now {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        shared[\"relationships\"] = exec_res\n\nclass OrderChapters(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"]\n        relationships = shared[\"relationships\"]\n        project_name = shared[\"project_name\"]  # Get project name\n\n        # Prepare context for the LLM\n        abstraction_info_for_prompt = []\n        for i, a in enumerate(abstractions):\n            abstraction_info_for_prompt.append(f\"- {i} # {a['name']}\")\n        abstraction_listing = \"\\n\".join(abstraction_info_for_prompt)\n\n        context = f\"Project Summary:\\n{relationships['summary']}\\n\\n\"\n        context += \"Relationships (Indices refer to abstractions above):\\n\"\n        for rel in relationships['details']:\n             from_name = abstractions[rel['from']]['name']\n             to_name = abstractions[rel['to']]['name']\n             # Use 'label' instead of 'desc'\n             context += f\"- From {rel['from']} ({from_name}) to {rel['to']} ({to_name}): {rel['label']}\\n\"\n\n        return abstraction_listing, context, len(abstractions), project_name\n\n    def exec(self, prep_res):\n        abstraction_listing, context, num_abstractions, project_name = prep_res\n        print(\"Determining chapter order using LLM...\")\n        prompt = f\"\"\"\nGiven the following project abstractions and their relationships for the project ```` {project_name} ````:\n\nAbstractions (Index # Name):\n{abstraction_listing}\n\nContext about relationships and project summary:\n{context}\n\nIf you are going to make a tutorial for ```` {project_name} ````, what is the best order to explain these abstractions, from first to last?\nIdeally, first explain those that are the most important or foundational, perhaps user-facing concepts or entry points. Then move to more detailed, lower-level implementation details or supporting concepts.\n\nOutput the ordered list of abstraction indices, including the name in a comment for clarity. Use the format `idx # AbstractionName`.\n\n```yaml\n- 2 # FoundationalConcept\n- 0 # CoreClassA\n- 1 # CoreClassB (uses CoreClassA)\n- ...\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        # Rely on Node's built-in retry/fallback\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        ordered_indices_raw = yaml.safe_load(yaml_str)\n\n        if not isinstance(ordered_indices_raw, list):\n            raise ValueError(\"LLM output is not a list\")\n\n        ordered_indices = []\n        seen_indices = set()\n        for entry in ordered_indices_raw:\n            try:\n                 if isinstance(entry, int):\n                     idx = entry\n                 elif isinstance(entry, str) and '#' in entry:\n                      idx = int(entry.split('#')[0].strip())\n                 else:\n                      idx = int(str(entry).strip())\n\n                 if not (0 <= idx < num_abstractions):\n                      raise ValueError(f\"Invalid index {idx} in ordered list. Max index is {num_abstractions-1}.\")\n                 if idx in seen_indices:\n                     raise ValueError(f\"Duplicate index {idx} found in ordered list.\")\n                 ordered_indices.append(idx)\n                 seen_indices.add(idx)\n\n            except (ValueError, TypeError):\n                 raise ValueError(f\"Could not parse index from ordered list entry: {entry}\")\n\n        # Check if all abstractions are included\n        if len(ordered_indices) != num_abstractions:\n             raise ValueError(f\"Ordered list length ({len(ordered_indices)}) does not match number of abstractions ({num_abstractions}). Missing indices: {set(range(num_abstractions)) - seen_indices}\")\n\n        print(f\"Determined chapter order (indices): {ordered_indices}\")\n        return ordered_indices # Return the list of indices\n\n    def post(self, shared, prep_res, exec_res):\n        # exec_res is already the list of ordered indices\n        shared[\"chapter_order\"] = exec_res # List of indices\n\nclass WriteChapters(BatchNode):\n    def prep(self, shared):\n        chapter_order = shared[\"chapter_order\"] # List of indices\n        abstractions = shared[\"abstractions\"]   # List of dicts, now using 'files' with indices\n        files_data = shared[\"files\"]\n        # Get already written chapters to provide context\n        # We store them temporarily during the batch run, not in shared memory yet\n        # The 'previous_chapters_summary' will be built progressively in the exec context\n        self.chapters_written_so_far = [] # Use instance variable for temporary storage across exec calls\n\n        # Create a complete list of all chapters\n        all_chapters = []\n        chapter_filenames = {} # Store chapter filename mapping for linking\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                chapter_num = i + 1\n                chapter_name = abstractions[abstraction_index][\"name\"]\n                # Create safe filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in chapter_name).lower()\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                # Format with link\n                all_chapters.append(f\"{chapter_num}. [{chapter_name}]({filename})\")\n                # Store mapping of chapter index to filename for linking\n                chapter_filenames[abstraction_index] = {\"num\": chapter_num, \"name\": chapter_name, \"filename\": filename}\n        \n        # Create a formatted string with all chapters\n        full_chapter_listing = \"\\n\".join(all_chapters)\n\n        items_to_process = []\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                abstraction_details = abstractions[abstraction_index]\n                # Use 'files' (list of indices) directly\n                related_file_indices = abstraction_details.get(\"files\", [])\n                # Get content using helper, passing indices\n                related_files_content_map = get_content_for_indices(files_data, related_file_indices)\n                \n                # Get previous chapter info for transitions\n                prev_chapter = None\n                if i > 0:\n                    prev_idx = chapter_order[i-1]\n                    prev_chapter = chapter_filenames[prev_idx]\n                \n                # Get next chapter info for transitions\n                next_chapter = None\n                if i < len(chapter_order) - 1:\n                    next_idx = chapter_order[i+1]\n                    next_chapter = chapter_filenames[next_idx]\n\n                items_to_process.append({\n                    \"chapter_num\": i + 1,\n                    \"abstraction_index\": abstraction_index,\n                    \"abstraction_details\": abstraction_details,\n                    \"related_files_content_map\": related_files_content_map,\n                    \"project_name\": shared[\"project_name\"],  # Add project name\n                    \"full_chapter_listing\": full_chapter_listing,  # Add the full chapter listing\n                    \"chapter_filenames\": chapter_filenames,  # Add chapter filenames mapping\n                    \"prev_chapter\": prev_chapter,  # Add previous chapter info\n                    \"next_chapter\": next_chapter,  # Add next chapter info\n                    # previous_chapters_summary will be added dynamically in exec\n                })\n            else:\n                print(f\"Warning: Invalid abstraction index {abstraction_index} in chapter_order. Skipping.\")\n\n        print(f\"Preparing to write {len(items_to_process)} chapters...\")\n        return items_to_process # Iterable for BatchNode\n\n    def exec(self, item):\n        # This runs for each item prepared above\n        abstraction_name = item[\"abstraction_details\"][\"name\"]\n        chapter_num = item[\"chapter_num\"]\n        project_name = item.get(\"project_name\")  # Get from item\n        print(f\"Writing chapter {chapter_num} for: {abstraction_name} using LLM...\")\n\n        # Prepare file context string from the map\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path.split('# ')[1] if '# ' in idx_path else idx_path} ---\\n{content}\"\n            for idx_path, content in item[\"related_files_content_map\"].items()\n        )\n\n        # Get summary of chapters written *before* this one\n        # Use the temporary instance variable\n        previous_chapters_summary = \"\\n---\\n\".join(self.chapters_written_so_far)\n\n\n        prompt = f\"\"\"\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `{project_name}` about the concept: \"{abstraction_name}\". This is Chapter {chapter_num}.\n\nConcept Details:\n- Description:\n{item[\"abstraction_details\"][\"description\"]}\n\nComplete Tutorial Structure:\n{item[\"full_chapter_listing\"]}\n\nContext from previous chapters (summary):\n{previous_chapters_summary if previous_chapters_summary else \"This is the first chapter.\"}\n\nRelevant Code Snippets:\n{file_context_str if file_context_str else \"No specific code snippets provided for this abstraction.\"}\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter {chapter_num}: {abstraction_name}`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n\"\"\"\n        chapter_content = call_llm(prompt)\n        # Basic validation/cleanup\n        actual_heading = f\"# Chapter {chapter_num}: {abstraction_name}\"\n        if not chapter_content.strip().startswith(f\"# Chapter {chapter_num}\"):\n             # Add heading if missing or incorrect, trying to preserve content\n             lines = chapter_content.strip().split('\\n')\n             if lines and lines[0].strip().startswith(\"#\"): # If there's some heading, replace it\n                 lines[0] = actual_heading\n                 chapter_content = \"\\n\".join(lines)\n             else: # Otherwise, prepend it\n                 chapter_content = f\"{actual_heading}\\n\\n{chapter_content}\"\n\n        # Add the generated content to our temporary list for the next iteration's context\n        self.chapters_written_so_far.append(chapter_content)\n\n        return chapter_content # Return the Markdown string\n\n    def post(self, shared, prep_res, exec_res_list):\n        # exec_res_list contains the generated Markdown for each chapter, in order\n        shared[\"chapters\"] = exec_res_list\n        # Clean up the temporary instance variable\n        del self.chapters_written_so_far\n        print(f\"Finished writing {len(exec_res_list)} chapters.\")\n\nclass CombineTutorial(Node):\n    def prep(self, shared):\n        project_name = shared[\"project_name\"]\n        output_base_dir = shared.get(\"output_dir\", \"output\") # Default output dir\n        output_path = os.path.join(output_base_dir, project_name)\n        repo_url = shared[\"repo_url\"]  # Get the repository URL\n\n        # Use 'label' from relationships_data['details']\n        relationships_data = shared[\"relationships\"] # {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        chapter_order = shared[\"chapter_order\"] # indices\n        abstractions = shared[\"abstractions\"]   # list of dicts\n        chapters_content = shared[\"chapters\"]   # list of strings\n\n        # --- Generate Mermaid Diagram ---\n        mermaid_lines = [\"flowchart TD\"]\n        # Add nodes for each abstraction\n        for i, abstr in enumerate(abstractions):\n            # Sanitize name for Mermaid ID and label\n            node_id = f\"A{i}\"\n            sanitized_name = abstr['name'].replace('\"', '')\n            node_label = sanitized_name # Using sanitized name only, no index\n            mermaid_lines.append(f'    {node_id}[\"{node_label}\"]')\n        # Add edges for relationships using 'label'\n        for rel in relationships_data['details']:\n            from_node_id = f\"A{rel['from']}\"\n            to_node_id = f\"A{rel['to']}\"\n            # Sanitize 'label' for edge label\n            edge_label = rel['label'].replace('\"', '').replace('\\n', ' ') # Basic sanitization\n            # Limit edge label length for readability (optional, but good for diagrams)\n            max_label_len = 30 # Make it shorter for labels\n            if len(edge_label) > max_label_len:\n                edge_label = edge_label[:max_label_len-3] + \"...\"\n            mermaid_lines.append(f'    {from_node_id} -- \"{edge_label}\" --> {to_node_id}')\n\n        mermaid_diagram = \"\\n\".join(mermaid_lines)\n        # --- End Mermaid ---\n\n\n        # Prepare index.md content\n        index_content = f\"# Tutorial: {project_name}\\n\\n\"\n        index_content += f\"{relationships_data['summary']}\\n\\n\"\n        index_content += f\"**Source Repository:** [{repo_url}]({repo_url})\\n\\n\"\n\n        # Add Mermaid diagram for relationships\n        index_content += \"```mermaid\\n\"\n        index_content += mermaid_diagram + \"\\n\"\n        index_content += \"```\\n\\n\"\n\n        index_content += \"## Chapters\\n\\n\"\n\n        chapter_files = []\n        # Generate chapter links based on the determined order\n        for i, abstraction_index in enumerate(chapter_order):\n            # Ensure index is valid and we have content for it\n            if 0 <= abstraction_index < len(abstractions) and i < len(chapters_content):\n                abstraction_name = abstractions[abstraction_index][\"name\"]\n                # Sanitize name for filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in abstraction_name).lower()\n                # Use chapter number (i+1) for ordering filename\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                index_content += f\"{i+1}. [{abstraction_name}]({filename})\\n\"\n                \n                # Add attribution to chapter content\n                chapter_content = chapters_content[i]\n                if not chapter_content.endswith(\"\\n\\n\"):\n                    chapter_content += \"\\n\\n\"\n                chapter_content += \"---\\n\\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\"\n                \n                # Store filename and corresponding content\n                chapter_files.append({\"filename\": filename, \"content\": chapter_content})\n            else:\n                 print(f\"Warning: Mismatch between chapter order, abstractions, or content at index {i} (abstraction index {abstraction_index}). Skipping file generation for this entry.\")\n\n        # Add attribution to index content\n        index_content += \"\\n\\n---\\n\\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\"\n\n        return {\n            \"output_path\": output_path,\n            \"index_content\": index_content,\n            \"chapter_files\": chapter_files # List of {\"filename\": str, \"content\": str}\n        }\n\n    def exec(self, prep_res):\n        output_path = prep_res[\"output_path\"]\n        index_content = prep_res[\"index_content\"]\n        chapter_files = prep_res[\"chapter_files\"]\n\n        print(f\"Combining tutorial into directory: {output_path}\")\n        # Rely on Node's built-in retry/fallback\n        os.makedirs(output_path, exist_ok=True)\n\n        # Write index.md\n        index_filepath = os.path.join(output_path, \"index.md\")\n        with open(index_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(index_content)\n        print(f\"  - Wrote {index_filepath}\")\n\n        # Write chapter files\n        for chapter_info in chapter_files:\n            chapter_filepath = os.path.join(output_path, chapter_info[\"filename\"])\n            with open(chapter_filepath, \"w\", encoding=\"utf-8\") as f:\n                f.write(chapter_info[\"content\"])\n            print(f\"  - Wrote {chapter_filepath}\")\n\n        return output_path # Return the final path\n\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"final_output_dir\"] = exec_res # Store the output path\n        print(f\"\\nTutorial generation complete! Files are in: {exec_res}\")\n        convert_all_md_to_html(exec_res) # Convert all Markdown files to HTML\n        \n\n--- File: utils/call_llm.py ---\nfrom google import genai\nimport os\nimport logging\nimport json\nfrom datetime import datetime\n\n# Configure logging\nlog_directory = os.getenv(\"LOG_DIR\", \"logs\")\nos.makedirs(log_directory, exist_ok=True)\nlog_file = os.path.join(log_directory, f\"llm_calls_{datetime.now().strftime('%Y%m%d')}.log\")\n\n# Set up logger\nlogger = logging.getLogger(\"llm_logger\")\nlogger.setLevel(logging.INFO)\nlogger.propagate = False  # Prevent propagation to root logger\nfile_handler = logging.FileHandler(log_file)\nfile_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\nlogger.addHandler(file_handler)\n\n# Simple cache configuration\ncache_file = \"llm_cache.json\"\n\n# By default, we Google Gemini 2.5 pro, as it shows great performance for code understanding\ndef call_llm(prompt: str, use_cache: bool = True) -> str:\n    # Log the prompt\n    logger.info(f\"PROMPT: {prompt}\")\n    \n    # Check cache if enabled\n    if use_cache:\n        # Load cache from disk\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, 'r') as f:\n                    cache = json.load(f)\n            except:\n                logger.warning(f\"Failed to load cache, starting with empty cache\")\n        \n        # Return from cache if exists\n        if prompt in cache:\n            logger.info(f\"RESPONSE: {cache[prompt]}\")\n            return cache[prompt]\n    \n    # Call the LLM if not in cache or cache disabled\n    client = genai.Client(\n        vertexai=True, \n        # TODO: change to your own project id and location\n        project=os.getenv(\"GEMINI_PROJECT_ID\", \"your-project-id\"),\n        location=os.getenv(\"GEMINI_LOCATION\", \"us-central1\")\n    )\n    # You can comment the previous line and use the AI Studio key instead:\n    # client = genai.Client(\n    #     api_key=os.getenv(\"GEMINI_API_KEY\", \"your-api_key\"),\n    # )\n    model = os.getenv(\"GEMINI_MODEL\", \"gemini-2.5-pro-exp-03-25\")\n    response = client.models.generate_content(\n        model=model,\n        contents=[prompt]\n    )\n    response_text = response.text\n    \n    # Log the response\n    logger.info(f\"RESPONSE: {response_text}\")\n    \n    # Update cache if enabled\n    if use_cache:\n        # Load cache again to avoid overwrites\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, 'r') as f:\n                    cache = json.load(f)\n            except:\n                pass\n        \n        # Add to cache and save\n        cache[prompt] = response_text\n        try:\n            with open(cache_file, 'w') as f:\n                json.dump(cache, f)\n        except Exception as e:\n            logger.error(f\"Failed to save cache: {e}\")\n    \n    return response_text\n\n# # Use Anthropic Claude 3.7 Sonnet Extended Thinking\n# def call_llm(prompt, use_cache: bool = True):\n#     from anthropic import Anthropic\n#     client = Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\", \"your-api-key\"))\n#     response = client.messages.create(\n#         model=\"claude-3-7-sonnet-20250219\",\n#         max_tokens=21000,\n#         thinking={\n#             \"type\": \"enabled\",\n#             \"budget_tokens\": 20000\n#         },\n#         messages=[\n#             {\"role\": \"user\", \"content\": prompt}\n#         ]\n#     )\n#     return response.content[1].text\n\n# # Use OpenAI o1\n# def call_llm(prompt, use_cache: bool = True):    \n#     from openai import OpenAI\n#     client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n#     r = client.chat.completions.create(\n#         model=\"o1\",\n#         messages=[{\"role\": \"user\", \"content\": prompt}],\n#         response_format={\n#             \"type\": \"text\"\n#         },\n#         reasoning_effort=\"medium\",\n#         store=False\n#     )\n#     return r.choices[0].message.content\n\nif __name__ == \"__main__\":\n    test_prompt = \"Hello, how are you?\"\n    \n    # First call - should hit the API\n    print(\"Making call...\")\n    response1 = call_llm(test_prompt, use_cache=False)\n    print(f\"Response: {response1}\")\n    \n\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 5: Abstraction Identification`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "```markdown\n# Chapter 5: Abstraction Identification\n\nIn the previous chapter, [Codebase Crawling](04_codebase_crawling.md), we learned how to gather all the code files from a repository or local directory. Now, we need to figure out *what's important* in that code!  That's where Abstraction Identification comes in.\n\n## What is Abstraction Identification?\n\nImagine you're trying to learn a new language.  You wouldn't just start memorizing every single word in the dictionary, right? Instead, you'd focus on the most common and important words and grammar rules.\n\nAbstraction Identification is similar. It's the process of analyzing the codebase and identifying the most important concepts or components to explain in the tutorial.  These \"concepts\" are often called \"abstractions\" because they represent a higher-level, simplified view of the underlying code.\n\nThink of it like a detective piecing together clues from a crime scene to identify the key suspects and their roles in the crime.  The LLM is our detective, sifting through the code to find the most important pieces.\n\nWhy is this important?  Because if we tried to explain every single line of code, the tutorial would be overwhelming and confusing.  Abstraction Identification helps us focus on the core concepts that a beginner needs to understand.\n\n## Key Concepts\n\n*   **Abstractions:**  Simplified representations of complex code components. Think of them as key concepts or building blocks of the project. Examples could be \"API Endpoint,\" \"Database Connection,\" or \"User Authentication.\"\n*   **LLM (Large Language Model):**  The \"brain\" of our operation.  We use an LLM to analyze the code and identify the important abstractions.\n*   **Codebase Context:** The actual code files that the LLM analyzes.  This is the output of the [Codebase Crawling](04_codebase_crawling.md) step.\n*   **File Indices:** A list of index numbers which corresponds to file paths used for the LLM to refer back to the code.\n\n## How Abstraction Identification Works\n\nThe Abstraction Identification process is handled by the `IdentifyAbstractions` node in our flow.\n\nLet's look at the simplified steps in `nodes.py`:\n\n```python\nclass IdentifyAbstractions(Node):\n    def prep(self, shared):\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        \n        # Helper to create context from files, respecting limits (basic example)\n        def create_llm_context(files_data):\n            context = \"\"\n            file_info = [] # Store tuples of (index, path)\n            for i, (path, content) in enumerate(files_data):\n                entry = f\"--- File Index {i}: {path} ---\\n{content}\\n\\n\"\n                context += entry\n                file_info.append((i, path))\n\n            return context, file_info # file_info is list of (index, path)\n\n        context, file_info = create_llm_context(files_data)\n        # Format file info for the prompt (comment is just a hint for LLM)\n        file_listing_for_prompt = \"\\n\".join([f\"- {idx} # {path}\" for idx, path in file_info])\n        return context, file_listing_for_prompt, len(files_data), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, file_listing_for_prompt, file_count, project_name = prep_res  # Unpack project name\n        print(\"Identifying abstractions using LLM...\")\n        prompt = f\"\"\"\nFor the project `{project_name}`:\n\nCodebase Context:\n{context}\n\nAnalyze the codebase context.\nIdentify the top 5-10 core most important abstractions to help those new to the codebase.\n\nFor each abstraction, provide:\n1. A concise `name`.\n2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\nList of file indices and paths present in the context:\n{file_listing_for_prompt}\n\nFormat the output as a YAML list of dictionaries:\n\n```yaml\n- name: Query Processing\n  description: | \n    Explains what the abstraction does.\n    It's like a central dispatcher routing requests.\n  file_indices:\n    - 0 # path/to/file1.py\n    - 3 # path/to/related.py\n- name: Query Optimization\n  description: |\n    Another core concept, similar to a blueprint for objects.\n  file_indices:\n    - 5 # path/to/another.js\n# ... up to 10 abstractions\n```\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        abstractions = yaml.safe_load(yaml_str)\n\n        # Basic Validation\n        # Extract data and file indices to `shared` dictionary\n        # This is where we get the list of abstractions\n        validated_abstractions = []\n        # --- Add validation here as needed ---\n        print(f\"Identified {len(validated_abstractions)} abstractions.\")\n        return validated_abstractions\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"abstractions\"] = exec_res # List of {\"name\": str, \"description\": str, \"files\": [int]}\n```\n\nHere's a breakdown:\n\n1.  **`prep(shared)`:**\n    *   This method retrieves the `files` (the code) from the `shared` dictionary. This is the output of the [Codebase Crawling](04_codebase_crawling.md) step.\n    *   It creates an LLM-friendly context string by concatenating the contents of all the files.  This is what we'll feed to the LLM.\n    *   `file_listing_for_prompt` contains a list of file indices and their paths to tell the LLM where the context came from.\n\n2.  **`exec(prep_res)`:**\n    *   This method constructs a prompt for the LLM, telling it to analyze the code and identify the key abstractions. The prompt includes:\n        *   The codebase context (the concatenated code files).\n        *   Instructions on how to format the output (as a YAML list of dictionaries).\n    *   It calls the `call_llm` function (defined in `utils/call_llm.py`) to send the prompt to the LLM.\n    *   It receives the LLM's response, which should be a YAML string containing the list of abstractions, their descriptions, and their relevant file indices.\n    *   The YAML output is validated and parsed to store it in `shared[\"abstractions\"]`\n\n3.  **`post(shared, prep_res, exec_res)`:**\n    *   This method takes the list of abstractions and stores it in the `shared[\"abstractions\"]` variable. This makes the abstractions available to the next node in the flow, which is [Relationship Analysis](06_relationship_analysis.md).\n\n## Code Example: The LLM Prompt\n\nHere's an example of the prompt that we send to the LLM (from `nodes.py`):\n\n```python\nprompt = f\"\"\"\nFor the project `{project_name}`:\n\nCodebase Context:\n{context}\n\nAnalyze the codebase context.\nIdentify the top 5-10 core most important abstractions to help those new to the codebase.\n\nFor each abstraction, provide:\n1. A concise `name`.\n2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\nList of file indices and paths present in the context:\n{file_listing_for_prompt}\n\nFormat the output as a YAML list of dictionaries:\n\n```yaml\n- name: Query Processing\n  description: | \n    Explains what the abstraction does.\n    It's like a central dispatcher routing requests.\n  file_indices:\n    - 0 # path/to/file1.py\n    - 3 # path/to/related.py\n- name: Query Optimization\n  description: |\n    Another core concept, similar to a blueprint for objects.\n  file_indices:\n    - 5 # path/to/another.js\n# ... up to 10 abstractions\n```\"\"\"\n```\n\nThis prompt tells the LLM to:\n\n*   Analyze the code and identify the top 5-10 most important abstractions.\n*   For each abstraction, provide a name, a description, and a list of relevant file indices.\n*   Format the output as a YAML list of dictionaries.\n\nThe prompt is designed to be clear and concise, guiding the LLM to provide the information we need in a structured format.\n\n## Code Example: Validating the LLM Response\n\nAfter the LLM returns its response, we need to validate that it's in the correct format. Here's a simplified example of how we do this (from `nodes.py`):\n\n```python\nyaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\nabstractions = yaml.safe_load(yaml_str)\n\nif not isinstance(abstractions, list):\n    raise ValueError(\"LLM Output is not a list\")\n\nvalidated_abstractions = []\nfor item in abstractions:\n    if not isinstance(item, dict) or not all(k in item for k in [\"name\", \"description\", \"file_indices\"]):\n        raise ValueError(f\"Missing keys in abstraction item: {item}\")\n\n    # --- Add validation here as needed ---\n    validated_abstractions.append({\n        \"name\": item[\"name\"],\n        \"description\": item[\"description\"],\n        \"file_indices\": item[\"file_indices\"]\n    })\n```\n\nThis code checks that:\n\n*   The response is a valid YAML string.\n*   The YAML string represents a list.\n*   Each item in the list is a dictionary with the required keys (\"name\", \"description\", \"file_indices\").\n\nIf the response doesn't pass these checks, we raise a `ValueError` to indicate that there's a problem.\n\n## Using The Abstractions\n\nThe final output of the `IdentifyAbstractions` node is a list of dictionaries, where each dictionary represents an abstraction:\n\n```json\n[\n  {\n    \"name\": \"Query Processing\",\n    \"description\": \"A beginner-friendly explanation of what Query Processing is...\",\n    \"files\": [0, 3]\n  },\n  {\n    \"name\": \"Query Optimization\",\n    \"description\": \"A beginner-friendly explanation of what Query Optimization is...\",\n    \"files\": [5]\n  }\n]\n```\n\nThis list is stored in the `shared[\"abstractions\"]` variable and passed to the next node in the flow. The \"files\" key stores the list of integer indices, corresponding to paths from earlier in Codebase Crawling.\n\n## Why is Abstraction Identification Important?\n\n*   **Focus:** It helps us focus on the most important concepts in the codebase, making the tutorial more manageable and easier to understand.\n*   **Organization:** It provides a structure for the tutorial, allowing us to organize the content around the key abstractions.\n*   **Beginner-Friendliness:** By focusing on abstractions, we can explain the code in a more beginner-friendly way, using high-level concepts and analogies.\n\n## Conclusion\n\nIn this chapter, we've learned about Abstraction Identification, which is the process of analyzing the codebase and identifying the most important concepts to explain in the tutorial. We've seen how we use an LLM to perform this analysis, and how we validate the LLM's response to ensure that it's in the correct format.\n\nIn the next chapter, [Relationship Analysis](06_relationship_analysis.md), we'll explore how we analyze the relationships between these abstractions to understand the overall architecture of the codebase.\n```", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `Tutorial-Codebase-Knowledge` about the concept: \"Relationship Analysis\". This is Chapter 6.\n\nConcept Details:\n- Description:\nRelationship Analysis is the process of understanding how the identified abstractions interact with\neach other. This involves using an LLM to analyze the code and determine the key relationships\nbetween the abstractions. It's like a network analyst mapping out the connections between\ndifferent computers in a network to understand how data flows between them.\n\n\nComplete Tutorial Structure:\n1. [API Endpoint](01_api_endpoint.md)\n2. [Flow Orchestration](02_flow_orchestration.md)\n3. [Configuration Management](03_configuration_management.md)\n4. [Codebase Crawling](04_codebase_crawling.md)\n5. [Abstraction Identification](05_abstraction_identification.md)\n6. [Relationship Analysis](06_relationship_analysis.md)\n7. [LLM Interaction](07_llm_interaction.md)\n8. [Chapter Writing](08_chapter_writing.md)\n9. [Tutorial Combination](09_tutorial_combination.md)\n\nContext from previous chapters (summary):\n# Chapter 1: API Endpoint\n\nWelcome to the first step in our journey to understanding how to generate tutorials from codebases! In this chapter, we'll explore the **API Endpoint**, which acts like the \"front door\" to our tutorial generation system.\n\nImagine you have a robot that can build LEGO sets. You wouldn't want to open up the robot and start fiddling with its internal wires every time you want it to build something, right? Instead, you'd want a simple button or a control panel to tell it what to do. The API Endpoint is like that button for our tutorial generator.\n\n**Why do we need an API Endpoint?**\n\nThink of it this way: you might want to trigger the tutorial generation process from:\n\n*   Your web browser\n*   Another program running on your computer\n*   Even from a different computer across the internet!\n\nAn API Endpoint allows all these different \"clients\" to communicate with our tutorial generation system in a standardized way. It's the central point of contact.\n\n**Key Concepts:**\n\n1.  **API (Application Programming Interface):** Think of it as a contract between two pieces of software. It defines how they can talk to each other. In our case, it defines how external programs can ask our system to generate a tutorial.\n\n2.  **Endpoint:** A specific URL (web address) that represents a particular function or resource in an API. In our case, it's the URL you send a request to in order to start the tutorial generation. It's like the specific button on our robot's control panel that says \"Build this LEGO set!\"\n\n3.  **Request:** The message you send to the API Endpoint to tell it what to do. This includes information like the repository URL, output folder and options for the tutorial generation.\n\n4.  **Response:** The message the API Endpoint sends back to you after it has processed your request. This might include a success message, an error message, or the path to the generated tutorial.\n\n**Our Use Case: Generating a tutorial for a GitHub repository**\n\nLet's say you want to generate a tutorial for a project on GitHub. The API Endpoint allows you to do this with a simple request. You'll need to provide the URL of the repository and any specific options for how you want the tutorial to be generated.\n\n**How to Use the API Endpoint:**\n\nOur API Endpoint is designed to receive a `POST` request to the `/generate` path. This request contains information about the repository for which you want to generate a tutorial.\n\nHere's an example of how you might send a request (this is just an example, the actual implementation will depend on how you're sending the request, e.g., using `curl`, `Postman`, or a Python script):\n\n```json\n{\n  \"repo_url\": \"https://github.com/fastapi/fastapi\",\n  \"name\": \"FastAPI\",\n  \"output\": \"tutorials\"\n}\n```\n\nThis request tells the system to:\n\n*   Generate a tutorial for the `https://github.com/fastapi/fastapi` repository.\n*   Use \"FastAPI\" as the project name.\n*   Save the tutorial in the `tutorials` output folder.\n\nAfter sending this request, the API Endpoint will process it and, if successful, return a response like this:\n\n```json\n{\n  \"message\": \"Tutorial generation completed successfully.\",\n  \"final_output_directory\": \"tutorials/FastAPI/html\"\n}\n```\n\nThis tells you that the tutorial was generated successfully and is located in the `tutorials/FastAPI/html` directory.\n\n**Internal Implementation: What happens under the hood?**\n\nWhen the API Endpoint receives a request, here's a simplified breakdown of what happens:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant API Endpoint\n    participant Main Script\n    participant Codebase\n    participant LLM\n\n    User->>API Endpoint: Sends tutorial generation request (repo URL, options)\n    API Endpoint->>Main Script: Executes main.py with provided arguments\n    Main Script->>Codebase: Crawls and analyzes the codebase\n    Main Script->>LLM: Interacts with LLM to generate content\n    Main Script->>API Endpoint: Returns success/failure and output path\n    API Endpoint->>User: Sends back response with result and output path\n```\n\nHere's a slightly more detailed explanation:\n\n1.  **Request Reception:** The API Endpoint receives the request with the repository URL and other options.\n2.  **Command Construction:**  It constructs a command line command to run the main script (`main.py`) with the provided parameters.\n3.  **Script Execution:** It executes the `main.py` script, which does the heavy lifting of cloning the repository, analyzing the code, and generating the tutorial content.\n4.  **Response Generation:** After the script finishes (either successfully or with an error), the API Endpoint creates a response with the appropriate message and, if successful, the path to the generated tutorial.\n5.  **Response Delivery:** Finally, it sends the response back to the user.\n\n**Code Snippets:**\n\nLet's look at some key parts of the code in `api.py`:\n\n```python\nfrom fastapi import FastAPI, HTTPException, status\nfrom pydantic import BaseModel, Field\n```\n\nThis imports the necessary libraries for creating the API using FastAPI (a Python framework for building APIs) and Pydantic (for data validation). We are importing `FastAPI` to create the API application, `HTTPException` to handle errors and send them to the user, and `BaseModel` and `Field` from `pydantic` to define the structure of our input data.\n\n```python\nclass GenerationRequest(BaseModel):\n    repo_url: str\n    name: Optional[str] = None\n    token: Optional[str] = None\n    output: str = Field(default=\"output\")\n    include: Optional[List[str]] = None\n    exclude: Optional[List[str]] = None\n    max_size: Optional[int] = None\n```\n\nThis defines the structure of the data that the API Endpoint expects to receive in the request. This is crucial for validation and ensuring that the input data is in the correct format. `repo_url` is required, while other fields like `name`, `token`, `output`, `include`, `exclude`, and `max_size` are optional.\n\n```python\n@app.post(\n    \"/generate\",\n    summary=\"Trigger Tutorial Generation\",\n    description=\"Accepts repository details and options, then runs the tutorial generation script.\",\n    status_code=status.HTTP_200_OK\n)\nasync def generate_tutorial(request_data: GenerationRequest):\n    # ... implementation details ...\n    return {\n        \"message\": \"Tutorial generation completed successfully.\",\n        \"final_output_directory\": expected_final_path,\n    }\n```\n\nThis code defines the `/generate` endpoint using the `@app.post` decorator. This tells FastAPI that this function should be called when a `POST` request is sent to the `/generate` URL. It receives the `request_data` (which is of type `GenerationRequest` that we defined above), does some processing, and returns a response containing a success message and the path to the generated tutorial.  The `status_code=status.HTTP_200_OK` ensures that a successful response returns a 200 OK status code.\n\n**CORS Middleware**\n\n```python\norigins = [\n    \"http://127.0.0.1:5500\", # VS Code Live Server default\n    \"http://localhost\",\n    \"http://localhost:8080\",\n    \"http://127.0.0.1\",\n    \"http://127.0.0.1:8080\",\n    \"null\", # file:// origin\n]\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=origins,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n```\n\nThis section is responsible for configuring Cross-Origin Resource Sharing (CORS). CORS is a security feature implemented by web browsers to restrict web pages from making requests to a different domain than the one which served the web page. Without CORS, you might not be able to send requests to your API from a webpage running on your local machine (e.g. during development). The middleware specifies the origins that are allowed to make requests to the API, ensuring that requests from these origins are not blocked by the browser.\n\n**Error Handling**\n\nThe `api.py` file includes comprehensive error handling to gracefully manage different scenarios that might arise during script execution.\n\n```python\n    except subprocess.CalledProcessError as e:\n        logger.error(f\"Script failed with exit code {e.returncode}\")\n        logger.error(f\"Script stdout:\\n{e.stdout}\")\n        logger.error(f\"Script stderr:\\n{e.stderr}\")\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail={\n                \"error\": \"Script execution failed.\",\n                \"details\": \"The tutorial generation script exited with an error.\",\n                \"return_code\": e.returncode,\n                \"script_stdout\": e.stdout,\n                \"script_stderr\": e.stderr\n            }\n        )\n    except FileNotFoundError:\n        error_msg = f\"Error: Script '{MAIN_SCRIPT_PATH}' or Python '{PYTHON_EXECUTABLE}' not found.\"\n        logger.error(error_msg)\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail={\"error\": \"Server configuration error.\", \"details\": error_msg}\n        )\n    except Exception as e:\n        logger.exception(\"An unexpected error occurred during script execution.\")\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail={\"error\": \"An unexpected internal server error occurred.\", \"details\": str(e)}\n        )\n```\n\n*   **`subprocess.CalledProcessError`**: Catches errors when the `main.py` script fails during execution. It logs the error details and raises an `HTTPException` with a 500 status code, providing details like the return code, stdout, and stderr of the script.\n*   **`FileNotFoundError`**: Handles cases where the `main.py` script or Python executable is not found. It raises an `HTTPException` with a 500 status code, indicating a server configuration issue.\n*   **`Exception`**: Serves as a catch-all for unexpected errors during script execution. It logs the exception and raises an `HTTPException` with a 500 status code, providing details about the error.\n\n**Conclusion:**\n\nIn this chapter, we've learned about the API Endpoint, which serves as the entry point to our tutorial generation system. We saw how it allows external programs to trigger the tutorial generation process with a simple request. We also explored the internal implementation and understood how it orchestrates the execution of the `main.py` script.\n\nNow that we know how to trigger the process, let's dive into [Flow Orchestration](02_flow_orchestration.md) in the next chapter to understand the overall sequence of steps involved in generating a tutorial.\n\n---\n# Chapter 2: Flow Orchestration\n\nIn the previous chapter, [API Endpoint](01_api_endpoint.md), we learned how to trigger the tutorial generation process. Think of it like pressing the \"Start\" button on a machine. But what happens *after* you press that button? That's where Flow Orchestration comes in!\n\n## What is Flow Orchestration?\n\nImagine a factory that builds cars. You wouldn't just dump all the parts in a pile and hope a car magically appears, right? Instead, you'd have a carefully planned assembly line:\n\n1.  **Frame Assembly:** The car's frame is built.\n2.  **Engine Installation:** The engine is installed.\n3.  **Bodywork:** The body panels are attached.\n4.  **Painting:** The car gets its color.\n5.  **Interior Fitting:** Seats and other interior parts are added.\n6.  **Quality Control:** Final checks are performed.\n\nFlow Orchestration is like that assembly line for our tutorial generation system. It defines the *sequence of steps* needed to transform a codebase into a complete tutorial. It makes sure each task happens in the right order, passing the result of one task to the next.\n\nWithout flow orchestration, we'd have a chaotic mess. Chapters might be written before the codebase is even analyzed, or the tutorial might be combined before the chapters are finished! Flow Orchestration brings order to the process.\n\n## Key Concepts\n\n*   **Flow:** The overall process or pipeline of tasks. In our case, it's the entire tutorial generation process.\n*   **Node:** A single task or step in the flow. Each node performs a specific job, like fetching code, analyzing relationships, or writing a chapter.  Nodes can be thought of as functions in a programming language.\n*   **Sequence:** The order in which the nodes are executed.  This is crucial for ensuring the correct flow of data and dependencies.\n*   **Dependencies:**  A node might *depend* on the output of another node. For example, you can't write a chapter *about* an abstraction until you've identified *what* the abstractions *are*.\n\n## How Flow Orchestration Works in Our Project\n\nLet's look at the main steps in our tutorial generation flow:\n\n```mermaid\ngraph LR\n    A[Fetch Repo] --> B(Identify Abstractions)\n    B --> C{Analyze Relationships}\n    C --> D[Order Chapters]\n    D --> E((Write Chapters))\n    E --> F(Combine Tutorial)\n    style A fill:#f9f,stroke:#333,stroke-width:2px\n    style F fill:#f9f,stroke:#333,stroke-width:2px\n    style E fill:#ccf,stroke:#333,stroke-width:2px\n```\n\nHere's a breakdown of each node:\n\n1.  **Fetch Repo:**  Downloads the codebase from a remote repository (like GitHub) or reads it from a local directory. This node is responsible for getting the raw material needed to create the tutorial.  It outputs the files and their contents.\n2.  **Identify Abstractions:** Analyzes the codebase and identifies key concepts or abstractions that are important for understanding the project. This step essentially identifies the key topics to cover in the tutorial.  It outputs a list of abstractions, including their names, descriptions, and the files they're found in.\n3.  **Analyze Relationships:** Examines how the different abstractions interact with each other. This helps us understand the overall architecture of the codebase and how the different pieces fit together. The LLM creates a summary of the project and the relationships between the abstractions\n4.  **Order Chapters:** Determines the best order to present the abstractions in the tutorial.  Some abstractions might be foundational and need to be explained before others. The LLM figures out the order.\n5.  **Write Chapters:**  Generates the actual content for each chapter of the tutorial. It uses the abstraction details, relationships, and chapter order to create a beginner-friendly explanation of each concept. This node is marked with double parentheses `((...))` because it's a *BatchNode*. A `BatchNode` handles independent steps and can be run in parallel.\n6.  **Combine Tutorial:**  Assembles all the individual chapters into a complete tutorial, creating an index page and table of contents.\n\n## Code Example: Defining the Flow\n\nThe core of our flow orchestration is defined in `flow.py`:\n\n```python\nfrom pocketflow import Flow\n# Import all node classes from nodes.py\nfrom nodes import (\n    FetchRepo,\n    IdentifyAbstractions,\n    AnalyzeRelationships,\n    OrderChapters,\n    WriteChapters,\n    CombineTutorial\n)\n\ndef create_tutorial_flow():\n    \"\"\"Creates and returns the codebase tutorial generation flow.\"\"\"\n\n    # Instantiate nodes\n    fetch_repo = FetchRepo()\n    identify_abstractions = IdentifyAbstractions(max_retries=3, wait=10)\n    analyze_relationships = AnalyzeRelationships(max_retries=3, wait=10)\n    order_chapters = OrderChapters(max_retries=3, wait=10)\n    write_chapters = WriteChapters(max_retries=3, wait=10) # This is a BatchNode\n    combine_tutorial = CombineTutorial()\n\n    # Connect nodes in sequence based on the design\n    fetch_repo >> identify_abstractions\n    identify_abstractions >> analyze_relationships\n    analyze_relationships >> order_chapters\n    order_chapters >> write_chapters\n    write_chapters >> combine_tutorial\n\n    # Create the flow starting with FetchRepo\n    tutorial_flow = Flow(start=fetch_repo)\n\n    return tutorial_flow\n```\n\nLet's break down this code:\n\n*   **Importing Nodes:** We import all the node classes (like `FetchRepo`, `IdentifyAbstractions`, etc.) from the `nodes.py` file.  Think of these as importing different tools or functions that we need for our assembly line.\n*   **Instantiating Nodes:**  We create instances of each node class. For example, `fetch_repo = FetchRepo()` creates a specific instance of the `FetchRepo` node. This is where the maximum number of retries and wait time between retries is set for nodes that interact with the LLM. This helps handle cases where the LLM might be temporarily unavailable or returns an invalid response.\n*   **Connecting Nodes:** The `>>` operator defines the sequence of the flow. `fetch_repo >> identify_abstractions` means that the `IdentifyAbstractions` node will execute *after* the `FetchRepo` node completes.  The output of `FetchRepo` will be passed as input to `IdentifyAbstractions`.\n*   **Creating the Flow:** We use the `Flow` class from the `pocketflow` library to create the flow. We specify the `start` node, which is the first node to be executed.\n\nThis simple code defines the entire structure of our tutorial generation process!\n\n## Code Example: Running the Flow\n\nThe `main.py` file is responsible for setting up the flow with all the command line input parameters and calling the `Flow` to execute:\n\n```python\nimport dotenv\nimport os\nimport argparse\n# Import the function that creates the flow\nfrom flow import create_tutorial_flow\n\ndotenv.load_dotenv()\n\n# Default file patterns\nDEFAULT_INCLUDE_PATTERNS = {\n    \"*.py\", \"*.js\", \"*.jsx\", \"*.ts\", \"*.tsx\", \"*.go\", \"*.java\", \"*.pyi\", \"*.pyx\", \n    \"*.c\", \"*.cc\", \"*.cpp\", \"*.h\", \"*.md\", \"*.rst\", \"Dockerfile\", \n    \"Makefile\", \"*.yaml\", \"*.yml\",\n}\n\nDEFAULT_EXCLUDE_PATTERNS = {\n    \"*test*\", \"tests/*\", \"docs/*\", \"examples/*\", \"v1/*\", \n    \"dist/*\", \"build/*\", \"experimental/*\", \"deprecated/*\", \n    \"legacy/*\", \".git/*\", \".github/*\", \".next/*\", \".vscode/*\", \"obj/*\", \"bin/*\", \"node_modules/*\", \"*.log\"\n}\n\n# --- Main Function ---\ndef main():\n    parser = argparse.ArgumentParser(description=\"Generate a tutorial for a GitHub codebase or local directory.\")\n    \n    # Create mutually exclusive group for source\n    source_group = parser.add_mutually_exclusive_group(required=True)\n    source_group.add_argument(\"--repo\", help=\"URL of the public GitHub repository.\")\n    source_group.add_argument(\"--dir\", help=\"Path to local directory.\")\n    \n    parser.add_argument(\"-n\", \"--name\", help=\"Project name (optional, derived from repo/directory if omitted).\")\n    parser.add_argument(\"-t\", \"--token\", help=\"GitHub personal access token (optional, reads from GITHUB_TOKEN env var if not provided).\")\n    parser.add_argument(\"-o\", \"--output\", default=\"output\", help=\"Base directory for output (default: ./output).\")\n    parser.add_argument(\"-i\", \"--include\", nargs=\"+\", help=\"Include file patterns (e.g. '*.py' '*.js'). Defaults to common code files if not specified.\")\n    parser.add_argument(\"-e\", \"--exclude\", nargs=\"+\", help=\"Exclude file patterns (e.g. 'tests/*' 'docs/*'). Defaults to test/build directories if not specified.\")\n    parser.add_argument(\"-s\", \"--max-size\", type=int, default=100000, help=\"Maximum file size in bytes (default: 100000, about 100KB).\")\n\n    args = parser.parse_args()\n\n    # Get GitHub token from argument or environment variable if using repo\n    github_token = None\n    if args.repo:\n        github_token = args.token or os.environ.get('GITHUB_TOKEN')\n        if not github_token:\n            print(\"Warning: No GitHub token provided. You might hit rate limits for public repositories.\")\n\n    # Initialize the shared dictionary with inputs\n    shared = {\n        \"repo_url\": args.repo,\n        \"local_dir\": args.dir,\n        \"project_name\": args.name, # Can be None, FetchRepo will derive it\n        \"github_token\": github_token,\n        \"output_dir\": args.output, # Base directory for CombineTutorial output\n\n        # Add include/exclude patterns and max file size\n        \"include_patterns\": set(args.include) if args.include else DEFAULT_INCLUDE_PATTERNS,\n        \"exclude_patterns\": set(args.exclude) if args.exclude else DEFAULT_EXCLUDE_PATTERNS,\n        \"max_file_size\": args.max_size,\n\n        # Outputs will be populated by the nodes\n        \"files\": [],\n        \"abstractions\": [],\n        \"relationships\": {},\n        \"chapter_order\": [],\n        \"chapters\": [],\n        \"final_output_dir\": None\n    }\n\n    print(f\"Starting tutorial generation for: {args.repo or args.dir}\")\n\n    # Create the flow instance\n    tutorial_flow = create_tutorial_flow()\n\n    # Run the flow\n    tutorial_flow.run(shared)\n    \nif __name__ == \"__main__\":\n    main()\n```\n\nHere are the important parts:\n\n1.  **Parsing Arguments:** We use `argparse` to handle command-line arguments like the repository URL, project name, and output directory.\n2.  **Shared Dictionary:** The `shared` dictionary acts as a central store for data that is passed between the nodes. Each node can read data from this dictionary and write its results back to it.\n3.  **Creating the Flow:** We call `create_tutorial_flow()` to get an instance of the flow we defined in `flow.py`.\n4.  **Running the Flow:**  Finally, we call `tutorial_flow.run(shared)` to start the flow. The `shared` dictionary is passed to the `run` method so that the nodes can access and modify the data.\n\n## What Happens Inside a Node?\n\nEach node consists of three important methods. Looking at the example of `FetchRepo` in `nodes.py`:\n\n```python\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"local_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        if not project_name:\n            # Basic name derivation from URL or directory\n            if repo_url:\n                project_name = repo_url.split('/')[-1].replace('.git', '')\n            else:\n                project_name = os.path.basename(os.path.abspath(local_dir))\n            shared[\"project_name\"] = project_name\n\n        # Get file patterns directly from shared\n        include_patterns = shared[\"include_patterns\"]\n        exclude_patterns = shared[\"exclude_patterns\"]\n        max_file_size = shared[\"max_file_size\"]\n\n        return {\n            \"repo_url\": repo_url,\n            \"local_dir\": local_dir,\n            \"token\": shared.get(\"github_token\"),\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns,\n            \"max_file_size\": max_file_size,\n            \"use_relative_paths\": True\n        }\n\n    def exec(self, prep_res):\n        if prep_res[\"repo_url\"]:\n            print(f\"Crawling repository: {prep_res['repo_url']}...\")\n            result = crawl_github_files(\n                repo_url=prep_res[\"repo_url\"],\n                token=prep_res[\"token\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        else:\n            print(f\"Crawling directory: {prep_res['local_dir']}...\")\n            result = crawl_local_files(\n                directory=prep_res[\"local_dir\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n            \n        # Convert dict to list of tuples: [(path, content), ...]\n        files_list = list(result.get(\"files\", {}).items())\n        print(f\"Fetched {len(files_list)} files.\")\n        return files_list\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"files\"] = exec_res # List of (path, content) tuples\n```\n\n*   **`prep(shared)`:** This method prepares the input for the node's execution. It reads data from the `shared` dictionary and transforms it into a format that the `exec` method can use. It also performs any necessary setup or validation. Returns a `prep_res` object that contains everything necessary for the `exec` function to work.\n*   **`exec(prep_res)`:** This method performs the core logic of the node. It receives the prepared input from the `prep` method and executes the task. In the case of `FetchRepo`, it downloads the codebase from GitHub or reads it from the local directory. It returns the `exec_res` which is the output of the node.\n*   **`post(shared, prep_res, exec_res)`:** This method is executed *after* the `exec` method completes. It takes the output of the `exec` method (`exec_res`) and writes it back to the `shared` dictionary, making it available to the next node in the flow.  In the case of `FetchRepo`, it writes the list of files and their contents to the `shared[\"files\"]` variable.\n\n## Why Use Flow Orchestration?\n\n*   **Organization:** It provides a clear and structured way to define complex processes.\n*   **Maintainability:** It makes the code easier to understand and maintain by breaking it down into smaller, independent nodes.\n*   **Reusability:**  Nodes can be reused in different flows, making it easier to build new features or modify existing ones.\n*   **Testability:** Each node can be tested independently, making it easier to ensure the correctness of the entire system.\n*   **Error Handling:** `pocketflow` supports retries, fallbacks, error handling, and graceful termination, which are useful for nodes that interact with external resources.\n\n## Conclusion\n\nFlow Orchestration is a powerful technique for managing complex processes like our tutorial generation system. It provides a structured way to define the sequence of steps, making the code more organized, maintainable, and testable.\n\nIn the next chapter, [Configuration Management](03_configuration_management.md), we'll explore how to manage the different configuration settings that control the behavior of our system. This will allow us to customize the tutorial generation process for different codebases and use cases.\n\n---\n# Chapter 3: Configuration Management\n\nIn the previous chapter, [Flow Orchestration](02_flow_orchestration.md), we learned how to define the overall sequence of steps for generating a tutorial. Now, let's talk about how to *customize* that process. Think of it like this: you have a recipe for baking a cake, but you might want to adjust the recipe to make it chocolate or vanilla. That's where Configuration Management comes in!\n\n## What is Configuration Management?\n\nConfiguration Management is like having a control panel that allows you to adjust the settings of our tutorial generation \"machine\" to achieve the desired output. It lets you change things like:\n\n*   Which repository to generate a tutorial for.\n*   What name to give the project in the tutorial.\n*   Which files to include or exclude from analysis.\n*   Where to save the generated tutorial.\n\nWithout configuration management, we'd be stuck with a single, inflexible way of generating tutorials. It's like trying to bake every cake with the exact same recipe, even if you want different flavors!\n\n## Key Concepts\n\n1.  **Environment Variables:** These are like global settings that can be accessed by any program running on your computer. They're often used to store things like API keys or default values.\n2.  **Argument Parsing:** This allows you to pass settings directly to a program when you run it from the command line. It's like specifying the ingredients for your cake when you give the recipe to the baker.\n\n## Our Use Case: Generating a Tutorial for a Specific GitHub Repository with a Custom Name\n\nLet's say you want to generate a tutorial for the `requests` library on GitHub and name the project \"Python Requests Tutorial\". With Configuration Management, you can easily do this!\n\n## How to Use Configuration Management\n\nOur project uses both environment variables and argument parsing to manage configuration.\n\n### 1. Environment Variables\n\nEnvironment variables are useful for settings that you might want to reuse across multiple runs of the program, or that contain sensitive information you don't want to type directly into the command line.\n\nTo set an environment variable, you can use the following command in your terminal (replace `<your_token>` with your actual token):\n\n```bash\nexport GITHUB_TOKEN=<your_token>\n```\n\nOr, you can create a `.env` file in the project root directory:\n\n```\nGITHUB_TOKEN=<your_token>\n```\n\nThe `dotenv` library will automatically load these variables when the script runs.\n\nIn our `main.py` file, we use the `os.environ.get()` function to read the value of the `GITHUB_TOKEN` environment variable:\n\n```python\nimport os\ngithub_token = args.token or os.environ.get('GITHUB_TOKEN')\n```\n\nThis code first checks if the token was provided as a command-line argument (`args.token`). If not, it looks for the `GITHUB_TOKEN` environment variable.\n\n### 2. Argument Parsing\n\nArgument parsing allows you to specify settings directly when you run the `main.py` script from the command line.\n\nHere's how you would generate a tutorial for the `requests` library with a custom name:\n\n```bash\npython main.py --repo https://github.com/psf/requests -n \"Python Requests Tutorial\" -o tutorials\n```\n\nLet's break down this command:\n\n*   `python main.py`: This tells your computer to run the `main.py` script using Python.\n*   `--repo https://github.com/psf/requests`: This specifies the URL of the GitHub repository to use. `--repo` is the *argument* name, and `https://github.com/psf/requests` is the *value* of that argument.\n*   `-n \"Python Requests Tutorial\"`: This sets the project name to \"Python Requests Tutorial\". `-n` is a *short* argument name, and `\"Python Requests Tutorial\"` is the value.\n*   `-o tutorials`: This specifies that the tutorial output will be written to the `tutorials` directory.\n\nThe `argparse` module in Python handles the parsing of these command-line arguments. In `main.py`, we define the arguments that our script accepts:\n\n```python\nimport argparse\n\nparser = argparse.ArgumentParser(description=\"Generate a tutorial for a GitHub codebase or local directory.\")\nparser.add_argument(\"--repo\", help=\"URL of the public GitHub repository.\")\nparser.add_argument(\"-n\", \"--name\", help=\"Project name (optional, derived from repo/directory if omitted).\")\nparser.add_argument(\"-o\", \"--output\", default=\"output\", help=\"Base directory for output (default: ./output).\")\n# ... other arguments\nargs = parser.parse_args()\n```\n\nThis code defines three arguments: `--repo`, `--name` (with the short form `-n`), and `--output` (with the short form `-o`). The `help` parameter provides a description of each argument that is displayed when you run the script with the `--help` flag:\n\n```bash\npython main.py --help\n```\n\n### How Include and Exclude Patterns Work\n\nThe `-i` (or `--include`) and `-e` (or `--exclude`) options are used to specify which files to include or exclude from the tutorial generation process. This is helpful if you want to focus on specific parts of the codebase or ignore irrelevant files (like test files or documentation).\n\nFor example, to only include Python files and exclude test directories, you could use the following command:\n\n```bash\npython main.py --repo https://github.com/psf/requests -i \"*.py\" -e \"tests/*\"\n```\n\n## Internal Implementation\n\nHere's a simplified view of how Configuration Management works internally:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Command Line\n    participant Main Script\n    participant Args Namespace\n    User->>Command Line: Runs script with arguments: python main.py --repo ...\n    Command Line->>Main Script: Executes main.py\n    Main Script->>Args Namespace: Parses command-line arguments using argparse\n    Args Namespace->>Main Script: Returns arguments as an object (e.g., args.repo, args.name)\n    Main Script->>Main Script: Accesses argument values and uses them\n```\n\nHere's a breakdown:\n\n1.  **User Interaction:** The user runs the `main.py` script from the command line, providing arguments like the repository URL and project name.\n2.  **Argument Parsing:** The `argparse` module in `main.py` parses these arguments and stores them in an `Args Namespace` object.\n3.  **Accessing Arguments:** The `main.py` script accesses the argument values from the `Args Namespace` object (e.g., `args.repo`, `args.name`).\n4.  **Using Arguments:** The script uses these values to configure the tutorial generation process. For example, the `repo_url` value is passed to the [Codebase Crawling](04_codebase_crawling.md) node to download the codebase.\n\n## Code Example: Passing Configuration to Nodes\n\nThe parsed arguments are stored in the `shared` dictionary and passed to the nodes in the flow. For example, in `main.py`:\n\n```python\nshared = {\n    \"repo_url\": args.repo,\n    \"project_name\": args.name,\n    \"output_dir\": args.output,\n    # ... other arguments\n}\n\ntutorial_flow.run(shared)\n```\n\nEach node can then access the configuration values from the `shared` dictionary in its `prep` method. For example, in `nodes.py`:\n\n```python\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        project_name = shared.get(\"project_name\")\n        # ...\n        return {\n            \"repo_url\": repo_url,\n            # ...\n        }\n```\n\n## Why is Configuration Management Important?\n\n*   **Flexibility:** It allows you to customize the tutorial generation process for different codebases and use cases.\n*   **Reusability:** You can easily reuse the same script to generate tutorials for different projects by simply changing the configuration settings.\n*   **Automation:** You can automate the tutorial generation process by running the script with a predefined set of arguments.\n\n## Conclusion\n\nIn this chapter, we've learned about Configuration Management, which allows us to customize the tutorial generation process by using environment variables and argument parsing. This gives us the flexibility to generate tutorials for different codebases with different settings.\n\nIn the next chapter, [Codebase Crawling](04_codebase_crawling.md), we'll dive into the process of fetching and analyzing the codebase itself.\n\n---\n# Chapter 4: Codebase Crawling\n\nIn the previous chapter, [Configuration Management](03_configuration_management.md), we learned how to customize our tutorial generation process using command-line arguments and environment variables. Now, let's explore how we actually *get* the code that we'll be using to generate the tutorial! That's where Codebase Crawling comes in.\n\n## What is Codebase Crawling?\n\nImagine you're writing a report on a popular book. You can't write the report without actually *reading* the book first, right? Codebase Crawling is like reading the book (or, more accurately, the code) for our tutorial generation system.\n\nSpecifically, Codebase Crawling is the process of gathering all the necessary files from either:\n\n*   A remote GitHub repository, or\n*   A local directory on your computer.\n\nIt's like a diligent librarian collecting books from different shelves, ensuring they meet certain criteria before adding them to the collection. The criteria might include:\n\n*   **File Type:** Only including certain types of files (e.g., Python files, JavaScript files).\n*   **File Size:** Ignoring files that are too large.\n*   **Location:** Only including files in certain directories, or excluding files in others (like test directories).\n\nWithout Codebase Crawling, our system wouldn't have any code to analyze, and we wouldn't be able to generate a tutorial!\n\n## Key Concepts\n\n*   **Repository URL:** The address of the GitHub repository (e.g., `https://github.com/fastapi/fastapi`).\n*   **Local Directory:** The path to a directory on your computer (e.g., `/Users/myuser/myproject`).\n*   **Include Patterns:**  Patterns that specify which files to include (e.g., `*.py` for all Python files).  Think of these as \"keywords\" to search for in the library.\n*   **Exclude Patterns:** Patterns that specify which files to exclude (e.g., `tests/*` to exclude all files in the `tests` directory).  Think of these as \"forbidden words\" to avoid in the search.\n*   **Maximum File Size:** A limit on the size of the files to include. This helps prevent the system from trying to process very large files, which can be slow or cause errors.\n\n## How Codebase Crawling Works in Our Project\n\nThe Codebase Crawling process is handled by the `FetchRepo` node in our flow. Remember from [Flow Orchestration](02_flow_orchestration.md) that each node has three important steps: `prep`, `exec`, and `post`.\n\nLet's look at how Codebase Crawling is implemented in the `FetchRepo` node in `nodes.py`:\n\n```python\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"local_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        # ... (derive project name if not provided)\n\n        # Get file patterns directly from shared\n        include_patterns = shared[\"include_patterns\"]\n        exclude_patterns = shared[\"exclude_patterns\"]\n        max_file_size = shared[\"max_file_size\"]\n\n        return {\n            \"repo_url\": repo_url,\n            \"local_dir\": local_dir,\n            \"token\": shared.get(\"github_token\"),\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns,\n            \"max_file_size\": max_file_size,\n            \"use_relative_paths\": True\n        }\n\n    def exec(self, prep_res):\n        if prep_res[\"repo_url\"]:\n            print(f\"Crawling repository: {prep_res['repo_url']}...\")\n            result = crawl_github_files(\n                repo_url=prep_res[\"repo_url\"],\n                token=prep_res[\"token\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        else:\n            print(f\"Crawling directory: {prep_res['local_dir']}...\")\n            result = crawl_local_files(\n                directory=prep_res[\"local_dir\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n            \n        # Convert dict to list of tuples: [(path, content), ...]\n        files_list = list(result.get(\"files\", {}).items())\n        print(f\"Fetched {len(files_list)} files.\")\n        return files_list\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"files\"] = exec_res # List of (path, content) tuples\n```\n\nLet's break down what's happening:\n\n1.  **`prep(shared)`:**\n    *   This method retrieves the configuration values (repository URL, local directory, include patterns, exclude patterns, maximum file size) from the `shared` dictionary. This is how the node knows *what* to crawl and *how* to crawl it.\n    *   It prepares these values into a `prep_res` object which is a dictionary containing the configuration for the `exec` function to use.\n\n2.  **`exec(prep_res)`:**\n    *   This method checks if a `repo_url` is provided. If so, it calls the `crawl_github_files` function to crawl the GitHub repository.\n    *   If a `repo_url` is *not* provided (meaning we're crawling a local directory), it calls the `crawl_local_files` function to crawl the local directory.\n    *   The `crawl_github_files` and `crawl_local_files` functions do the actual work of fetching the files and their contents.\n    *   Finally, the result from the crawl is converted into a list of tuples where each tuple contains a file path and file content.\n\n3.  **`post(shared, prep_res, exec_res)`:**\n    *   This method takes the list of files (the `exec_res` output) and stores it in the `shared[\"files\"]` variable. This makes the files available to the next node in the flow, which is [Abstraction Identification](05_abstraction_identification.md).\n\n### Crawling GitHub Repositories: `crawl_github_files`\n\nThe `crawl_github_files` function in `utils/crawl_github_files.py` is responsible for fetching files from a GitHub repository.  Here's a simplified view of how it works:\n\n```python\ndef crawl_github_files(\n    repo_url, \n    token=None, \n    max_file_size: int = 1 * 1024 * 1024,  # 1 MB\n    use_relative_paths: bool = False,\n    include_patterns: Union[str, Set[str]] = None,\n    exclude_patterns: Union[str, Set[str]] = None\n):\n    # Parse the repo_url to extract owner, repo name, branch and path\n    # Uses the GitHub REST API to get the files and their content\n\n    files = {}\n    skipped_files = []\n\n    # Fetch contents of the repository at a specific path and commit\n    def fetch_contents(path):\n        # Use the GitHub REST API to get the contents of the given path\n        # If it's a file, download it and add it to the files dictionary\n        # If it's a directory, recursively call fetch_contents on the directory\n\n        # Check if file should be included based on patterns\n        def should_include_file(file_path: str, file_name: str) -> bool:\n            # If no include patterns are specified, include all files\n            # Otherwise, check if the file matches any of the include patterns\n            # If exclude patterns are specified, exclude files that match the exclude patterns\n            return True or False # Simplified logic\n\n    # Start crawling from the specified path\n    fetch_contents(specific_path)\n\n    return {\n        \"files\": files,\n        \"stats\": {\n            \"downloaded_count\": len(files),\n            \"skipped_count\": len(skipped_files),\n            \"skipped_files\": skipped_files,\n            \"base_path\": specific_path if use_relative_paths else None,\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns\n        }\n    }\n```\n\nHere's a breakdown:\n\n1.  **Parsing the Repository URL:** The function first parses the `repo_url` to extract the owner, repository name, branch/commit, and path within the repository. This information is used to construct the URLs for the GitHub API requests.\n2.  **Using the GitHub REST API:** It uses the GitHub REST API to fetch the contents of the repository. The API allows us to retrieve information about files and directories in the repository.\n3.  **Handling Different Content Types:** The API returns different data depending on whether the item is a file or a directory.\n    *   **Files:** For files, the function downloads the file content and stores it in the `files` dictionary.  It also checks the file size against the `max_file_size` limit.\n    *   **Directories:** For directories, the function recursively calls itself to process the contents of the subdirectory. This allows it to traverse the entire repository structure.\n4.  **Include/Exclude Patterns:** The function uses the `should_include_file` helper function to check if a file should be included based on the `include_patterns` and `exclude_patterns`. This allows you to filter the files that are included in the tutorial generation process.  If a file matches an exclude pattern, it will not be included.\n5.  **Error Handling:** The function includes error handling to deal with potential issues like rate limits (which can occur if you make too many requests to the GitHub API in a short period of time) and network errors.\n\n### Crawling Local Directories: `crawl_local_files`\n\nThe `crawl_local_files` function in `utils/crawl_local_files.py` is responsible for fetching files from a local directory. Here's a simplified view of how it works:\n\n```python\nimport os\nimport fnmatch\n\ndef crawl_local_files(directory, include_patterns=None, exclude_patterns=None, max_file_size=None, use_relative_paths=True):\n    \"\"\"\n    Crawl files in a local directory with similar interface as crawl_github_files.\n    \"\"\"\n    files_dict = {}\n\n    # Traverse through all files and folders in the directory\n    for root, _, files in os.walk(directory):\n        for filename in files:\n            filepath = os.path.join(root, filename) # Get the full path\n            relpath = os.path.relpath(filepath, directory) # Path relative to directory\n\n            # Check include and exclude patterns\n            included = True #Simplified\n            excluded = False #Simplified\n            if not included or excluded:\n                continue\n\n            # Check file size\n            if max_file_size and os.path.getsize(filepath) > max_file_size:\n                continue\n\n            try:\n                with open(filepath, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                files_dict[relpath] = content #Store content\n            except Exception as e:\n                print(f\"Warning: Could not read file {filepath}: {e}\")\n\n    return {\"files\": files_dict}\n```\n\nHere's a breakdown:\n\n1.  **Traversing the Directory:** The function uses `os.walk` to traverse the directory tree, visiting each directory and file within the specified `directory`.\n2.  **Constructing File Paths:** For each file, the function constructs the full path to the file using `os.path.join`. It also calculates the relative path to the file (relative to the specified `directory`) using `os.path.relpath`.\n3.  **Include/Exclude Patterns:** Similar to `crawl_github_files`, the function checks if a file should be included based on the `include_patterns` and `exclude_patterns`.\n4.  **File Size Check:** The function checks the file size against the `max_file_size` limit.\n5.  **Reading File Content:** If the file passes all the checks, the function reads the content of the file and stores it in the `files_dict` dictionary, using the file path as the key.\n6.  **Error Handling:** The function includes basic error handling to catch potential issues like file read errors.\n\n## How to Use Codebase Crawling\n\nYou don't directly call `crawl_github_files` or `crawl_local_files` yourself. Instead, you provide the necessary configuration (repository URL or local directory, include patterns, exclude patterns, maximum file size) when you run the `main.py` script.  The `FetchRepo` node then uses this configuration to crawl the codebase.\n\nFor example, to generate a tutorial for the `fastapi` repository, including only Python files and excluding the `tests` directory, you would use the following command:\n\n```bash\npython main.py --repo https://github.com/fastapi/fastapi -i \"*.py\" -e \"tests/*\"\n```\n\nTo generate a tutorial for a local directory, you would use the `--dir` option instead of `--repo`:\n\n```bash\npython main.py --dir /path/to/my/project -i \"*.py\" -e \"tests/*\"\n```\n\n## Why is Codebase Crawling Important?\n\n*   **Data Acquisition:** It's the first step in the tutorial generation process, providing the raw data (the codebase) that is needed to generate the tutorial.\n*   **Filtering:** It allows you to filter the files that are included in the tutorial, focusing on the most relevant parts of the codebase and ignoring irrelevant files (like test files or documentation).\n*   **Efficiency:** By limiting the maximum file size, it helps prevent the system from trying to process very large files, which can be slow or cause errors.\n\n## Conclusion\n\nIn this chapter, we've learned about Codebase Crawling, which is the process of gathering the necessary files from either a GitHub repository or a local directory. We've seen how it uses include and exclude patterns to filter files, and how it limits the maximum file size to prevent performance issues.\n\nIn the next chapter, [Abstraction Identification](05_abstraction_identification.md), we'll explore how we analyze the codebase to identify the key concepts that should be covered in the tutorial.\n\n---\n# Chapter 5: Abstraction Identification\n\n```markdown\n# Chapter 5: Abstraction Identification\n\nIn the previous chapter, [Codebase Crawling](04_codebase_crawling.md), we learned how to gather all the code files from a repository or local directory. Now, we need to figure out *what's important* in that code!  That's where Abstraction Identification comes in.\n\n## What is Abstraction Identification?\n\nImagine you're trying to learn a new language.  You wouldn't just start memorizing every single word in the dictionary, right? Instead, you'd focus on the most common and important words and grammar rules.\n\nAbstraction Identification is similar. It's the process of analyzing the codebase and identifying the most important concepts or components to explain in the tutorial.  These \"concepts\" are often called \"abstractions\" because they represent a higher-level, simplified view of the underlying code.\n\nThink of it like a detective piecing together clues from a crime scene to identify the key suspects and their roles in the crime.  The LLM is our detective, sifting through the code to find the most important pieces.\n\nWhy is this important?  Because if we tried to explain every single line of code, the tutorial would be overwhelming and confusing.  Abstraction Identification helps us focus on the core concepts that a beginner needs to understand.\n\n## Key Concepts\n\n*   **Abstractions:**  Simplified representations of complex code components. Think of them as key concepts or building blocks of the project. Examples could be \"API Endpoint,\" \"Database Connection,\" or \"User Authentication.\"\n*   **LLM (Large Language Model):**  The \"brain\" of our operation.  We use an LLM to analyze the code and identify the important abstractions.\n*   **Codebase Context:** The actual code files that the LLM analyzes.  This is the output of the [Codebase Crawling](04_codebase_crawling.md) step.\n*   **File Indices:** A list of index numbers which corresponds to file paths used for the LLM to refer back to the code.\n\n## How Abstraction Identification Works\n\nThe Abstraction Identification process is handled by the `IdentifyAbstractions` node in our flow.\n\nLet's look at the simplified steps in `nodes.py`:\n\n```python\nclass IdentifyAbstractions(Node):\n    def prep(self, shared):\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        \n        # Helper to create context from files, respecting limits (basic example)\n        def create_llm_context(files_data):\n            context = \"\"\n            file_info = [] # Store tuples of (index, path)\n            for i, (path, content) in enumerate(files_data):\n                entry = f\"--- File Index {i}: {path} ---\\n{content}\\n\\n\"\n                context += entry\n                file_info.append((i, path))\n\n            return context, file_info # file_info is list of (index, path)\n\n        context, file_info = create_llm_context(files_data)\n        # Format file info for the prompt (comment is just a hint for LLM)\n        file_listing_for_prompt = \"\\n\".join([f\"- {idx} # {path}\" for idx, path in file_info])\n        return context, file_listing_for_prompt, len(files_data), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, file_listing_for_prompt, file_count, project_name = prep_res  # Unpack project name\n        print(\"Identifying abstractions using LLM...\")\n        prompt = f\"\"\"\nFor the project `{project_name}`:\n\nCodebase Context:\n{context}\n\nAnalyze the codebase context.\nIdentify the top 5-10 core most important abstractions to help those new to the codebase.\n\nFor each abstraction, provide:\n1. A concise `name`.\n2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\nList of file indices and paths present in the context:\n{file_listing_for_prompt}\n\nFormat the output as a YAML list of dictionaries:\n\n```yaml\n- name: Query Processing\n  description: | \n    Explains what the abstraction does.\n    It's like a central dispatcher routing requests.\n  file_indices:\n    - 0 # path/to/file1.py\n    - 3 # path/to/related.py\n- name: Query Optimization\n  description: |\n    Another core concept, similar to a blueprint for objects.\n  file_indices:\n    - 5 # path/to/another.js\n# ... up to 10 abstractions\n```\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        abstractions = yaml.safe_load(yaml_str)\n\n        # Basic Validation\n        # Extract data and file indices to `shared` dictionary\n        # This is where we get the list of abstractions\n        validated_abstractions = []\n        # --- Add validation here as needed ---\n        print(f\"Identified {len(validated_abstractions)} abstractions.\")\n        return validated_abstractions\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"abstractions\"] = exec_res # List of {\"name\": str, \"description\": str, \"files\": [int]}\n```\n\nHere's a breakdown:\n\n1.  **`prep(shared)`:**\n    *   This method retrieves the `files` (the code) from the `shared` dictionary. This is the output of the [Codebase Crawling](04_codebase_crawling.md) step.\n    *   It creates an LLM-friendly context string by concatenating the contents of all the files.  This is what we'll feed to the LLM.\n    *   `file_listing_for_prompt` contains a list of file indices and their paths to tell the LLM where the context came from.\n\n2.  **`exec(prep_res)`:**\n    *   This method constructs a prompt for the LLM, telling it to analyze the code and identify the key abstractions. The prompt includes:\n        *   The codebase context (the concatenated code files).\n        *   Instructions on how to format the output (as a YAML list of dictionaries).\n    *   It calls the `call_llm` function (defined in `utils/call_llm.py`) to send the prompt to the LLM.\n    *   It receives the LLM's response, which should be a YAML string containing the list of abstractions, their descriptions, and their relevant file indices.\n    *   The YAML output is validated and parsed to store it in `shared[\"abstractions\"]`\n\n3.  **`post(shared, prep_res, exec_res)`:**\n    *   This method takes the list of abstractions and stores it in the `shared[\"abstractions\"]` variable. This makes the abstractions available to the next node in the flow, which is [Relationship Analysis](06_relationship_analysis.md).\n\n## Code Example: The LLM Prompt\n\nHere's an example of the prompt that we send to the LLM (from `nodes.py`):\n\n```python\nprompt = f\"\"\"\nFor the project `{project_name}`:\n\nCodebase Context:\n{context}\n\nAnalyze the codebase context.\nIdentify the top 5-10 core most important abstractions to help those new to the codebase.\n\nFor each abstraction, provide:\n1. A concise `name`.\n2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\nList of file indices and paths present in the context:\n{file_listing_for_prompt}\n\nFormat the output as a YAML list of dictionaries:\n\n```yaml\n- name: Query Processing\n  description: | \n    Explains what the abstraction does.\n    It's like a central dispatcher routing requests.\n  file_indices:\n    - 0 # path/to/file1.py\n    - 3 # path/to/related.py\n- name: Query Optimization\n  description: |\n    Another core concept, similar to a blueprint for objects.\n  file_indices:\n    - 5 # path/to/another.js\n# ... up to 10 abstractions\n```\"\"\"\n```\n\nThis prompt tells the LLM to:\n\n*   Analyze the code and identify the top 5-10 most important abstractions.\n*   For each abstraction, provide a name, a description, and a list of relevant file indices.\n*   Format the output as a YAML list of dictionaries.\n\nThe prompt is designed to be clear and concise, guiding the LLM to provide the information we need in a structured format.\n\n## Code Example: Validating the LLM Response\n\nAfter the LLM returns its response, we need to validate that it's in the correct format. Here's a simplified example of how we do this (from `nodes.py`):\n\n```python\nyaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\nabstractions = yaml.safe_load(yaml_str)\n\nif not isinstance(abstractions, list):\n    raise ValueError(\"LLM Output is not a list\")\n\nvalidated_abstractions = []\nfor item in abstractions:\n    if not isinstance(item, dict) or not all(k in item for k in [\"name\", \"description\", \"file_indices\"]):\n        raise ValueError(f\"Missing keys in abstraction item: {item}\")\n\n    # --- Add validation here as needed ---\n    validated_abstractions.append({\n        \"name\": item[\"name\"],\n        \"description\": item[\"description\"],\n        \"file_indices\": item[\"file_indices\"]\n    })\n```\n\nThis code checks that:\n\n*   The response is a valid YAML string.\n*   The YAML string represents a list.\n*   Each item in the list is a dictionary with the required keys (\"name\", \"description\", \"file_indices\").\n\nIf the response doesn't pass these checks, we raise a `ValueError` to indicate that there's a problem.\n\n## Using The Abstractions\n\nThe final output of the `IdentifyAbstractions` node is a list of dictionaries, where each dictionary represents an abstraction:\n\n```json\n[\n  {\n    \"name\": \"Query Processing\",\n    \"description\": \"A beginner-friendly explanation of what Query Processing is...\",\n    \"files\": [0, 3]\n  },\n  {\n    \"name\": \"Query Optimization\",\n    \"description\": \"A beginner-friendly explanation of what Query Optimization is...\",\n    \"files\": [5]\n  }\n]\n```\n\nThis list is stored in the `shared[\"abstractions\"]` variable and passed to the next node in the flow. The \"files\" key stores the list of integer indices, corresponding to paths from earlier in Codebase Crawling.\n\n## Why is Abstraction Identification Important?\n\n*   **Focus:** It helps us focus on the most important concepts in the codebase, making the tutorial more manageable and easier to understand.\n*   **Organization:** It provides a structure for the tutorial, allowing us to organize the content around the key abstractions.\n*   **Beginner-Friendliness:** By focusing on abstractions, we can explain the code in a more beginner-friendly way, using high-level concepts and analogies.\n\n## Conclusion\n\nIn this chapter, we've learned about Abstraction Identification, which is the process of analyzing the codebase and identifying the most important concepts to explain in the tutorial. We've seen how we use an LLM to perform this analysis, and how we validate the LLM's response to ensure that it's in the correct format.\n\nIn the next chapter, [Relationship Analysis](06_relationship_analysis.md), we'll explore how we analyze the relationships between these abstractions to understand the overall architecture of the codebase.\n```\n\nRelevant Code Snippets:\n--- File: nodes.py ---\nimport os\nimport yaml\nfrom pocketflow import Node, BatchNode\nfrom utils.crawl_github_files import crawl_github_files\nfrom utils.call_llm import call_llm # Assuming you have this utility\nfrom utils.crawl_local_files import crawl_local_files\nfrom converter.md_to_html import convert_all_md_to_html\n\n\n# Helper to get content for specific file indices\ndef get_content_for_indices(files_data, indices):\n    content_map = {}\n    for i in indices:\n        if 0 <= i < len(files_data):\n            path, content = files_data[i]\n            content_map[f\"{i} # {path}\"] = content # Use index + path as key for context\n    return content_map\n\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"local_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        if not project_name:\n            # Basic name derivation from URL or directory\n            if repo_url:\n                project_name = repo_url.split('/')[-1].replace('.git', '')\n            else:\n                project_name = os.path.basename(os.path.abspath(local_dir))\n            shared[\"project_name\"] = project_name\n\n        # Get file patterns directly from shared\n        include_patterns = shared[\"include_patterns\"]\n        exclude_patterns = shared[\"exclude_patterns\"]\n        max_file_size = shared[\"max_file_size\"]\n\n        return {\n            \"repo_url\": repo_url,\n            \"local_dir\": local_dir,\n            \"token\": shared.get(\"github_token\"),\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns,\n            \"max_file_size\": max_file_size,\n            \"use_relative_paths\": True\n        }\n\n    def exec(self, prep_res):\n        if prep_res[\"repo_url\"]:\n            print(f\"Crawling repository: {prep_res['repo_url']}...\")\n            result = crawl_github_files(\n                repo_url=prep_res[\"repo_url\"],\n                token=prep_res[\"token\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        else:\n            print(f\"Crawling directory: {prep_res['local_dir']}...\")\n            result = crawl_local_files(\n                directory=prep_res[\"local_dir\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n            \n        # Convert dict to list of tuples: [(path, content), ...]\n        files_list = list(result.get(\"files\", {}).items())\n        print(f\"Fetched {len(files_list)} files.\")\n        return files_list\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"files\"] = exec_res # List of (path, content) tuples\n\nclass IdentifyAbstractions(Node):\n    def prep(self, shared):\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        \n        # Helper to create context from files, respecting limits (basic example)\n        def create_llm_context(files_data):\n            context = \"\"\n            file_info = [] # Store tuples of (index, path)\n            for i, (path, content) in enumerate(files_data):\n                entry = f\"--- File Index {i}: {path} ---\\n{content}\\n\\n\"\n                context += entry\n                file_info.append((i, path))\n\n            return context, file_info # file_info is list of (index, path)\n\n        context, file_info = create_llm_context(files_data)\n        # Format file info for the prompt (comment is just a hint for LLM)\n        file_listing_for_prompt = \"\\n\".join([f\"- {idx} # {path}\" for idx, path in file_info])\n        return context, file_listing_for_prompt, len(files_data), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, file_listing_for_prompt, file_count, project_name = prep_res  # Unpack project name\n        print(\"Identifying abstractions using LLM...\")\n        prompt = f\"\"\"\nFor the project `{project_name}`:\n\nCodebase Context:\n{context}\n\nAnalyze the codebase context.\nIdentify the top 5-10 core most important abstractions to help those new to the codebase.\n\nFor each abstraction, provide:\n1. A concise `name`.\n2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\nList of file indices and paths present in the context:\n{file_listing_for_prompt}\n\nFormat the output as a YAML list of dictionaries:\n\n```yaml\n- name: Query Processing\n  description: | \n    Explains what the abstraction does.\n    It's like a central dispatcher routing requests.\n  file_indices:\n    - 0 # path/to/file1.py\n    - 3 # path/to/related.py\n- name: Query Optimization\n  description: |\n    Another core concept, similar to a blueprint for objects.\n  file_indices:\n    - 5 # path/to/another.js\n# ... up to 10 abstractions\n```\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        abstractions = yaml.safe_load(yaml_str)\n\n        if not isinstance(abstractions, list):\n            raise ValueError(\"LLM Output is not a list\")\n\n        validated_abstractions = []\n        for item in abstractions:\n            if not isinstance(item, dict) or not all(k in item for k in [\"name\", \"description\", \"file_indices\"]):\n                raise ValueError(f\"Missing keys in abstraction item: {item}\")\n            if not isinstance(item[\"description\"], str):\n                 raise ValueError(f\"description is not a string in item: {item}\")\n            if not isinstance(item[\"file_indices\"], list):\n                 raise ValueError(f\"file_indices is not a list in item: {item}\")\n\n            # Validate indices\n            validated_indices = []\n            for idx_entry in item[\"file_indices\"]:\n                 try:\n                     if isinstance(idx_entry, int):\n                         idx = idx_entry\n                     elif isinstance(idx_entry, str) and '#' in idx_entry:\n                          idx = int(idx_entry.split('#')[0].strip())\n                     else:\n                          idx = int(str(idx_entry).strip())\n\n                     if not (0 <= idx < file_count):\n                         raise ValueError(f\"Invalid file index {idx} found in item {item['name']}. Max index is {file_count - 1}.\")\n                     validated_indices.append(idx)\n                 except (ValueError, TypeError):\n                      raise ValueError(f\"Could not parse index from entry: {idx_entry} in item {item['name']}\")\n\n            item[\"files\"] = sorted(list(set(validated_indices)))\n            # Store only the required fields\n            validated_abstractions.append({\n                \"name\": item[\"name\"],\n                \"description\": item[\"description\"],\n                \"files\": item[\"files\"]\n            })\n\n        print(f\"Identified {len(validated_abstractions)} abstractions.\")\n        return validated_abstractions\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"abstractions\"] = exec_res # List of {\"name\": str, \"description\": str, \"files\": [int]}\n\nclass AnalyzeRelationships(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"] # Now contains 'files' list of indices\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n\n        # Create context with abstraction names, indices, descriptions, and relevant file snippets\n        context = \"Identified Abstractions:\\n\"\n        all_relevant_indices = set()\n        abstraction_info_for_prompt = []\n        for i, abstr in enumerate(abstractions):\n            # Use 'files' which contains indices directly\n            file_indices_str = \", \".join(map(str, abstr['files']))\n            info_line = f\"- Index {i}: {abstr['name']} (Relevant file indices: [{file_indices_str}])\\n  Description: {abstr['description']}\"\n            context += info_line + \"\\n\"\n            abstraction_info_for_prompt.append(f\"{i} # {abstr['name']}\")\n            all_relevant_indices.update(abstr['files'])\n\n        context += \"\\nRelevant File Snippets (Referenced by Index and Path):\\n\"\n        # Get content for relevant files using helper\n        relevant_files_content_map = get_content_for_indices(\n            files_data,\n            sorted(list(all_relevant_indices))\n        )\n        # Format file content for context\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path} ---\\n{content}\"\n            for idx_path, content in relevant_files_content_map.items()\n        )\n        context += file_context_str\n\n        return context, \"\\n\".join(abstraction_info_for_prompt), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, abstraction_listing, project_name = prep_res  # Unpack project name\n        print(\"Analyzing relationships using LLM...\")\n        prompt = f\"\"\"\nBased on the following abstractions and relevant code snippets from the project `{project_name}`:\n\nList of Abstraction Indices and Names:\n{abstraction_listing}\n\nContext (Abstractions, Descriptions, Code):\n{context}\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        relationships_data = yaml.safe_load(yaml_str)\n\n        if not isinstance(relationships_data, dict) or not all(k in relationships_data for k in [\"summary\", \"relationships\"]):\n            raise ValueError(\"LLM output is not a dict or missing keys ('summary', 'relationships')\")\n        if not isinstance(relationships_data[\"summary\"], str):\n             raise ValueError(\"summary is not a string\")\n        if not isinstance(relationships_data[\"relationships\"], list):\n             raise ValueError(\"relationships is not a list\")\n\n        # Validate relationships structure\n        validated_relationships = []\n        num_abstractions = len(abstraction_listing.split('\\n'))\n        for rel in relationships_data[\"relationships\"]:\n             # Check for 'label' key\n             if not isinstance(rel, dict) or not all(k in rel for k in [\"from_abstraction\", \"to_abstraction\", \"label\"]):\n                  raise ValueError(f\"Missing keys (expected from_abstraction, to_abstraction, label) in relationship item: {rel}\")\n             # Validate 'label' is a string\n             if not isinstance(rel[\"label\"], str):\n                  raise ValueError(f\"Relationship label is not a string: {rel}\")\n\n             # Validate indices\n             try:\n                 from_idx = int(str(rel[\"from_abstraction\"]).split('#')[0].strip())\n                 to_idx = int(str(rel[\"to_abstraction\"]).split('#')[0].strip())\n                 if not (0 <= from_idx < num_abstractions and 0 <= to_idx < num_abstractions):\n                      raise ValueError(f\"Invalid index in relationship: from={from_idx}, to={to_idx}. Max index is {num_abstractions-1}.\")\n                 validated_relationships.append({\n                     \"from\": from_idx,\n                     \"to\": to_idx,\n                     \"label\": rel[\"label\"] \n                 })\n             except (ValueError, TypeError):\n                  raise ValueError(f\"Could not parse indices from relationship: {rel}\")\n\n        print(\"Generated project summary and relationship details.\")\n        return {\n            \"summary\": relationships_data[\"summary\"],\n            \"details\": validated_relationships # Store validated, index-based relationships\n        }\n\n\n    def post(self, shared, prep_res, exec_res):\n        # Structure is now {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        shared[\"relationships\"] = exec_res\n\nclass OrderChapters(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"]\n        relationships = shared[\"relationships\"]\n        project_name = shared[\"project_name\"]  # Get project name\n\n        # Prepare context for the LLM\n        abstraction_info_for_prompt = []\n        for i, a in enumerate(abstractions):\n            abstraction_info_for_prompt.append(f\"- {i} # {a['name']}\")\n        abstraction_listing = \"\\n\".join(abstraction_info_for_prompt)\n\n        context = f\"Project Summary:\\n{relationships['summary']}\\n\\n\"\n        context += \"Relationships (Indices refer to abstractions above):\\n\"\n        for rel in relationships['details']:\n             from_name = abstractions[rel['from']]['name']\n             to_name = abstractions[rel['to']]['name']\n             # Use 'label' instead of 'desc'\n             context += f\"- From {rel['from']} ({from_name}) to {rel['to']} ({to_name}): {rel['label']}\\n\"\n\n        return abstraction_listing, context, len(abstractions), project_name\n\n    def exec(self, prep_res):\n        abstraction_listing, context, num_abstractions, project_name = prep_res\n        print(\"Determining chapter order using LLM...\")\n        prompt = f\"\"\"\nGiven the following project abstractions and their relationships for the project ```` {project_name} ````:\n\nAbstractions (Index # Name):\n{abstraction_listing}\n\nContext about relationships and project summary:\n{context}\n\nIf you are going to make a tutorial for ```` {project_name} ````, what is the best order to explain these abstractions, from first to last?\nIdeally, first explain those that are the most important or foundational, perhaps user-facing concepts or entry points. Then move to more detailed, lower-level implementation details or supporting concepts.\n\nOutput the ordered list of abstraction indices, including the name in a comment for clarity. Use the format `idx # AbstractionName`.\n\n```yaml\n- 2 # FoundationalConcept\n- 0 # CoreClassA\n- 1 # CoreClassB (uses CoreClassA)\n- ...\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        # Rely on Node's built-in retry/fallback\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        ordered_indices_raw = yaml.safe_load(yaml_str)\n\n        if not isinstance(ordered_indices_raw, list):\n            raise ValueError(\"LLM output is not a list\")\n\n        ordered_indices = []\n        seen_indices = set()\n        for entry in ordered_indices_raw:\n            try:\n                 if isinstance(entry, int):\n                     idx = entry\n                 elif isinstance(entry, str) and '#' in entry:\n                      idx = int(entry.split('#')[0].strip())\n                 else:\n                      idx = int(str(entry).strip())\n\n                 if not (0 <= idx < num_abstractions):\n                      raise ValueError(f\"Invalid index {idx} in ordered list. Max index is {num_abstractions-1}.\")\n                 if idx in seen_indices:\n                     raise ValueError(f\"Duplicate index {idx} found in ordered list.\")\n                 ordered_indices.append(idx)\n                 seen_indices.add(idx)\n\n            except (ValueError, TypeError):\n                 raise ValueError(f\"Could not parse index from ordered list entry: {entry}\")\n\n        # Check if all abstractions are included\n        if len(ordered_indices) != num_abstractions:\n             raise ValueError(f\"Ordered list length ({len(ordered_indices)}) does not match number of abstractions ({num_abstractions}). Missing indices: {set(range(num_abstractions)) - seen_indices}\")\n\n        print(f\"Determined chapter order (indices): {ordered_indices}\")\n        return ordered_indices # Return the list of indices\n\n    def post(self, shared, prep_res, exec_res):\n        # exec_res is already the list of ordered indices\n        shared[\"chapter_order\"] = exec_res # List of indices\n\nclass WriteChapters(BatchNode):\n    def prep(self, shared):\n        chapter_order = shared[\"chapter_order\"] # List of indices\n        abstractions = shared[\"abstractions\"]   # List of dicts, now using 'files' with indices\n        files_data = shared[\"files\"]\n        # Get already written chapters to provide context\n        # We store them temporarily during the batch run, not in shared memory yet\n        # The 'previous_chapters_summary' will be built progressively in the exec context\n        self.chapters_written_so_far = [] # Use instance variable for temporary storage across exec calls\n\n        # Create a complete list of all chapters\n        all_chapters = []\n        chapter_filenames = {} # Store chapter filename mapping for linking\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                chapter_num = i + 1\n                chapter_name = abstractions[abstraction_index][\"name\"]\n                # Create safe filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in chapter_name).lower()\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                # Format with link\n                all_chapters.append(f\"{chapter_num}. [{chapter_name}]({filename})\")\n                # Store mapping of chapter index to filename for linking\n                chapter_filenames[abstraction_index] = {\"num\": chapter_num, \"name\": chapter_name, \"filename\": filename}\n        \n        # Create a formatted string with all chapters\n        full_chapter_listing = \"\\n\".join(all_chapters)\n\n        items_to_process = []\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                abstraction_details = abstractions[abstraction_index]\n                # Use 'files' (list of indices) directly\n                related_file_indices = abstraction_details.get(\"files\", [])\n                # Get content using helper, passing indices\n                related_files_content_map = get_content_for_indices(files_data, related_file_indices)\n                \n                # Get previous chapter info for transitions\n                prev_chapter = None\n                if i > 0:\n                    prev_idx = chapter_order[i-1]\n                    prev_chapter = chapter_filenames[prev_idx]\n                \n                # Get next chapter info for transitions\n                next_chapter = None\n                if i < len(chapter_order) - 1:\n                    next_idx = chapter_order[i+1]\n                    next_chapter = chapter_filenames[next_idx]\n\n                items_to_process.append({\n                    \"chapter_num\": i + 1,\n                    \"abstraction_index\": abstraction_index,\n                    \"abstraction_details\": abstraction_details,\n                    \"related_files_content_map\": related_files_content_map,\n                    \"project_name\": shared[\"project_name\"],  # Add project name\n                    \"full_chapter_listing\": full_chapter_listing,  # Add the full chapter listing\n                    \"chapter_filenames\": chapter_filenames,  # Add chapter filenames mapping\n                    \"prev_chapter\": prev_chapter,  # Add previous chapter info\n                    \"next_chapter\": next_chapter,  # Add next chapter info\n                    # previous_chapters_summary will be added dynamically in exec\n                })\n            else:\n                print(f\"Warning: Invalid abstraction index {abstraction_index} in chapter_order. Skipping.\")\n\n        print(f\"Preparing to write {len(items_to_process)} chapters...\")\n        return items_to_process # Iterable for BatchNode\n\n    def exec(self, item):\n        # This runs for each item prepared above\n        abstraction_name = item[\"abstraction_details\"][\"name\"]\n        chapter_num = item[\"chapter_num\"]\n        project_name = item.get(\"project_name\")  # Get from item\n        print(f\"Writing chapter {chapter_num} for: {abstraction_name} using LLM...\")\n\n        # Prepare file context string from the map\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path.split('# ')[1] if '# ' in idx_path else idx_path} ---\\n{content}\"\n            for idx_path, content in item[\"related_files_content_map\"].items()\n        )\n\n        # Get summary of chapters written *before* this one\n        # Use the temporary instance variable\n        previous_chapters_summary = \"\\n---\\n\".join(self.chapters_written_so_far)\n\n\n        prompt = f\"\"\"\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `{project_name}` about the concept: \"{abstraction_name}\". This is Chapter {chapter_num}.\n\nConcept Details:\n- Description:\n{item[\"abstraction_details\"][\"description\"]}\n\nComplete Tutorial Structure:\n{item[\"full_chapter_listing\"]}\n\nContext from previous chapters (summary):\n{previous_chapters_summary if previous_chapters_summary else \"This is the first chapter.\"}\n\nRelevant Code Snippets:\n{file_context_str if file_context_str else \"No specific code snippets provided for this abstraction.\"}\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter {chapter_num}: {abstraction_name}`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n\"\"\"\n        chapter_content = call_llm(prompt)\n        # Basic validation/cleanup\n        actual_heading = f\"# Chapter {chapter_num}: {abstraction_name}\"\n        if not chapter_content.strip().startswith(f\"# Chapter {chapter_num}\"):\n             # Add heading if missing or incorrect, trying to preserve content\n             lines = chapter_content.strip().split('\\n')\n             if lines and lines[0].strip().startswith(\"#\"): # If there's some heading, replace it\n                 lines[0] = actual_heading\n                 chapter_content = \"\\n\".join(lines)\n             else: # Otherwise, prepend it\n                 chapter_content = f\"{actual_heading}\\n\\n{chapter_content}\"\n\n        # Add the generated content to our temporary list for the next iteration's context\n        self.chapters_written_so_far.append(chapter_content)\n\n        return chapter_content # Return the Markdown string\n\n    def post(self, shared, prep_res, exec_res_list):\n        # exec_res_list contains the generated Markdown for each chapter, in order\n        shared[\"chapters\"] = exec_res_list\n        # Clean up the temporary instance variable\n        del self.chapters_written_so_far\n        print(f\"Finished writing {len(exec_res_list)} chapters.\")\n\nclass CombineTutorial(Node):\n    def prep(self, shared):\n        project_name = shared[\"project_name\"]\n        output_base_dir = shared.get(\"output_dir\", \"output\") # Default output dir\n        output_path = os.path.join(output_base_dir, project_name)\n        repo_url = shared[\"repo_url\"]  # Get the repository URL\n\n        # Use 'label' from relationships_data['details']\n        relationships_data = shared[\"relationships\"] # {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        chapter_order = shared[\"chapter_order\"] # indices\n        abstractions = shared[\"abstractions\"]   # list of dicts\n        chapters_content = shared[\"chapters\"]   # list of strings\n\n        # --- Generate Mermaid Diagram ---\n        mermaid_lines = [\"flowchart TD\"]\n        # Add nodes for each abstraction\n        for i, abstr in enumerate(abstractions):\n            # Sanitize name for Mermaid ID and label\n            node_id = f\"A{i}\"\n            sanitized_name = abstr['name'].replace('\"', '')\n            node_label = sanitized_name # Using sanitized name only, no index\n            mermaid_lines.append(f'    {node_id}[\"{node_label}\"]')\n        # Add edges for relationships using 'label'\n        for rel in relationships_data['details']:\n            from_node_id = f\"A{rel['from']}\"\n            to_node_id = f\"A{rel['to']}\"\n            # Sanitize 'label' for edge label\n            edge_label = rel['label'].replace('\"', '').replace('\\n', ' ') # Basic sanitization\n            # Limit edge label length for readability (optional, but good for diagrams)\n            max_label_len = 30 # Make it shorter for labels\n            if len(edge_label) > max_label_len:\n                edge_label = edge_label[:max_label_len-3] + \"...\"\n            mermaid_lines.append(f'    {from_node_id} -- \"{edge_label}\" --> {to_node_id}')\n\n        mermaid_diagram = \"\\n\".join(mermaid_lines)\n        # --- End Mermaid ---\n\n\n        # Prepare index.md content\n        index_content = f\"# Tutorial: {project_name}\\n\\n\"\n        index_content += f\"{relationships_data['summary']}\\n\\n\"\n        index_content += f\"**Source Repository:** [{repo_url}]({repo_url})\\n\\n\"\n\n        # Add Mermaid diagram for relationships\n        index_content += \"```mermaid\\n\"\n        index_content += mermaid_diagram + \"\\n\"\n        index_content += \"```\\n\\n\"\n\n        index_content += \"## Chapters\\n\\n\"\n\n        chapter_files = []\n        # Generate chapter links based on the determined order\n        for i, abstraction_index in enumerate(chapter_order):\n            # Ensure index is valid and we have content for it\n            if 0 <= abstraction_index < len(abstractions) and i < len(chapters_content):\n                abstraction_name = abstractions[abstraction_index][\"name\"]\n                # Sanitize name for filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in abstraction_name).lower()\n                # Use chapter number (i+1) for ordering filename\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                index_content += f\"{i+1}. [{abstraction_name}]({filename})\\n\"\n                \n                # Add attribution to chapter content\n                chapter_content = chapters_content[i]\n                if not chapter_content.endswith(\"\\n\\n\"):\n                    chapter_content += \"\\n\\n\"\n                chapter_content += \"---\\n\\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\"\n                \n                # Store filename and corresponding content\n                chapter_files.append({\"filename\": filename, \"content\": chapter_content})\n            else:\n                 print(f\"Warning: Mismatch between chapter order, abstractions, or content at index {i} (abstraction index {abstraction_index}). Skipping file generation for this entry.\")\n\n        # Add attribution to index content\n        index_content += \"\\n\\n---\\n\\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\"\n\n        return {\n            \"output_path\": output_path,\n            \"index_content\": index_content,\n            \"chapter_files\": chapter_files # List of {\"filename\": str, \"content\": str}\n        }\n\n    def exec(self, prep_res):\n        output_path = prep_res[\"output_path\"]\n        index_content = prep_res[\"index_content\"]\n        chapter_files = prep_res[\"chapter_files\"]\n\n        print(f\"Combining tutorial into directory: {output_path}\")\n        # Rely on Node's built-in retry/fallback\n        os.makedirs(output_path, exist_ok=True)\n\n        # Write index.md\n        index_filepath = os.path.join(output_path, \"index.md\")\n        with open(index_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(index_content)\n        print(f\"  - Wrote {index_filepath}\")\n\n        # Write chapter files\n        for chapter_info in chapter_files:\n            chapter_filepath = os.path.join(output_path, chapter_info[\"filename\"])\n            with open(chapter_filepath, \"w\", encoding=\"utf-8\") as f:\n                f.write(chapter_info[\"content\"])\n            print(f\"  - Wrote {chapter_filepath}\")\n\n        return output_path # Return the final path\n\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"final_output_dir\"] = exec_res # Store the output path\n        print(f\"\\nTutorial generation complete! Files are in: {exec_res}\")\n        convert_all_md_to_html(exec_res) # Convert all Markdown files to HTML\n        \n\n--- File: utils/call_llm.py ---\nfrom google import genai\nimport os\nimport logging\nimport json\nfrom datetime import datetime\n\n# Configure logging\nlog_directory = os.getenv(\"LOG_DIR\", \"logs\")\nos.makedirs(log_directory, exist_ok=True)\nlog_file = os.path.join(log_directory, f\"llm_calls_{datetime.now().strftime('%Y%m%d')}.log\")\n\n# Set up logger\nlogger = logging.getLogger(\"llm_logger\")\nlogger.setLevel(logging.INFO)\nlogger.propagate = False  # Prevent propagation to root logger\nfile_handler = logging.FileHandler(log_file)\nfile_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\nlogger.addHandler(file_handler)\n\n# Simple cache configuration\ncache_file = \"llm_cache.json\"\n\n# By default, we Google Gemini 2.5 pro, as it shows great performance for code understanding\ndef call_llm(prompt: str, use_cache: bool = True) -> str:\n    # Log the prompt\n    logger.info(f\"PROMPT: {prompt}\")\n    \n    # Check cache if enabled\n    if use_cache:\n        # Load cache from disk\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, 'r') as f:\n                    cache = json.load(f)\n            except:\n                logger.warning(f\"Failed to load cache, starting with empty cache\")\n        \n        # Return from cache if exists\n        if prompt in cache:\n            logger.info(f\"RESPONSE: {cache[prompt]}\")\n            return cache[prompt]\n    \n    # Call the LLM if not in cache or cache disabled\n    client = genai.Client(\n        vertexai=True, \n        # TODO: change to your own project id and location\n        project=os.getenv(\"GEMINI_PROJECT_ID\", \"your-project-id\"),\n        location=os.getenv(\"GEMINI_LOCATION\", \"us-central1\")\n    )\n    # You can comment the previous line and use the AI Studio key instead:\n    # client = genai.Client(\n    #     api_key=os.getenv(\"GEMINI_API_KEY\", \"your-api_key\"),\n    # )\n    model = os.getenv(\"GEMINI_MODEL\", \"gemini-2.5-pro-exp-03-25\")\n    response = client.models.generate_content(\n        model=model,\n        contents=[prompt]\n    )\n    response_text = response.text\n    \n    # Log the response\n    logger.info(f\"RESPONSE: {response_text}\")\n    \n    # Update cache if enabled\n    if use_cache:\n        # Load cache again to avoid overwrites\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, 'r') as f:\n                    cache = json.load(f)\n            except:\n                pass\n        \n        # Add to cache and save\n        cache[prompt] = response_text\n        try:\n            with open(cache_file, 'w') as f:\n                json.dump(cache, f)\n        except Exception as e:\n            logger.error(f\"Failed to save cache: {e}\")\n    \n    return response_text\n\n# # Use Anthropic Claude 3.7 Sonnet Extended Thinking\n# def call_llm(prompt, use_cache: bool = True):\n#     from anthropic import Anthropic\n#     client = Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\", \"your-api-key\"))\n#     response = client.messages.create(\n#         model=\"claude-3-7-sonnet-20250219\",\n#         max_tokens=21000,\n#         thinking={\n#             \"type\": \"enabled\",\n#             \"budget_tokens\": 20000\n#         },\n#         messages=[\n#             {\"role\": \"user\", \"content\": prompt}\n#         ]\n#     )\n#     return response.content[1].text\n\n# # Use OpenAI o1\n# def call_llm(prompt, use_cache: bool = True):    \n#     from openai import OpenAI\n#     client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n#     r = client.chat.completions.create(\n#         model=\"o1\",\n#         messages=[{\"role\": \"user\", \"content\": prompt}],\n#         response_format={\n#             \"type\": \"text\"\n#         },\n#         reasoning_effort=\"medium\",\n#         store=False\n#     )\n#     return r.choices[0].message.content\n\nif __name__ == \"__main__\":\n    test_prompt = \"Hello, how are you?\"\n    \n    # First call - should hit the API\n    print(\"Making call...\")\n    response1 = call_llm(test_prompt, use_cache=False)\n    print(f\"Response: {response1}\")\n    \n\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 6: Relationship Analysis`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "```markdown\n# Chapter 6: Relationship Analysis\n\nIn the previous chapter, [Abstraction Identification](05_abstraction_identification.md), we identified the key concepts or \"abstractions\" within a codebase. Now, it's time to connect the dots!  Relationship Analysis helps us understand how these abstractions interact with each other to form the overall system.\n\n## What is Relationship Analysis?\n\nImagine you've identified all the important people in a city: the mayor, the police chief, the school superintendent, and the business owners.  Knowing who they are is useful, but you also need to understand *how they interact*.  Who reports to whom? Who depends on whom?  What are the power dynamics?\n\nRelationship Analysis is the process of understanding how the identified abstractions interact with each other. This involves using an LLM to analyze the code and determine the key relationships between the abstractions. It's like a network analyst mapping out the connections between different computers in a network to understand how data flows between them.\n\nWithout Relationship Analysis, we'd have a collection of isolated concepts, but we wouldn't understand how they work together to achieve the project's goals. It provides a crucial high-level summary and interaction diagram to give beginners a big picture understanding.\n\n## Key Concepts\n\n*   **Relationships:** The connections or interactions between abstractions.  Examples could be \"uses,\" \"manages,\" \"inherits from,\" or \"depends on.\"\n*   **Directed Graph:** A visual representation of the relationships between abstractions, where the direction of the arrow indicates the flow of interaction. We use a Mermaid diagram to represent this.\n*   **Project Summary:**  A high-level description of the project's purpose and functionality, based on the identified abstractions and their relationships.\n*   **File Indices:** A list of index numbers which corresponds to file paths used for the LLM to refer back to the code.\n*   **Abstraction Index:** The numbered index for a given abstraction.\n\n## How Relationship Analysis Works\n\nThe Relationship Analysis process is handled by the `AnalyzeRelationships` node in our flow.\n\nLet's look at the simplified steps in `nodes.py`:\n\n```python\nclass AnalyzeRelationships(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"] # Now contains 'files' list of indices\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n\n        # Create context with abstraction names, indices, descriptions, and relevant file snippets\n        context = \"Identified Abstractions:\\n\"\n        all_relevant_indices = set()\n        abstraction_info_for_prompt = []\n        for i, abstr in enumerate(abstractions):\n            # Use 'files' which contains indices directly\n            file_indices_str = \", \".join(map(str, abstr['files']))\n            info_line = f\"- Index {i}: {abstr['name']} (Relevant file indices: [{file_indices_str}])\\n  Description: {abstr['description']}\"\n            context += info_line + \"\\n\"\n            abstraction_info_for_prompt.append(f\"{i} # {abstr['name']}\")\n            all_relevant_indices.update(abstr['files'])\n\n        context += \"\\nRelevant File Snippets (Referenced by Index and Path):\\n\"\n        # Get content for relevant files using helper\n        relevant_files_content_map = get_content_for_indices(\n            files_data,\n            sorted(list(all_relevant_indices))\n        )\n        # Format file content for context\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path} ---\\n{content}\"\n            for idx_path, content in relevant_files_content_map.items()\n        )\n        context += file_context_str\n\n        return context, \"\\n\".join(abstraction_info_for_prompt), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, abstraction_listing, project_name = prep_res  # Unpack project name\n        print(\"Analyzing relationships using LLM...\")\n        prompt = f\"\"\"\nBased on the following abstractions and relevant code snippets from the project `{project_name}`:\n\nList of Abstraction Indices and Names:\n{abstraction_listing}\n\nContext (Abstractions, Descriptions, Code):\n{context}\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        relationships_data = yaml.safe_load(yaml_str)\n\n        if not isinstance(relationships_data, dict) or not all(k in relationships_data for k in [\"summary\", \"relationships\"]):\n            raise ValueError(\"LLM output is not a dict or missing keys ('summary', 'relationships')\")\n        if not isinstance(relationships_data[\"summary\"], str):\n             raise ValueError(\"summary is not a string\")\n        if not isinstance(relationships_data[\"relationships\"], list):\n             raise ValueError(\"relationships is not a list\")\n\n        # Validate relationships structure\n        validated_relationships = []\n        num_abstractions = len(abstraction_listing.split('\\n'))\n        for rel in relationships_data[\"relationships\"]:\n             # Check for 'label' key\n             if not isinstance(rel, dict) or not all(k in rel for k in [\"from_abstraction\", \"to_abstraction\", \"label\"]):\n                  raise ValueError(f\"Missing keys (expected from_abstraction, to_abstraction, label) in relationship item: {rel}\")\n             # Validate 'label' is a string\n             if not isinstance(rel[\"label\"], str):\n                  raise ValueError(f\"Relationship label is not a string: {rel}\")\n\n             # Validate indices\n             try:\n                 from_idx = int(str(rel[\"from_abstraction\"]).split('#')[0].strip())\n                 to_idx = int(str(rel[\"to_abstraction\"]).split('#')[0].strip())\n                 if not (0 <= from_idx < num_abstractions and 0 <= to_idx < num_abstractions):\n                      raise ValueError(f\"Invalid index in relationship: from={from_idx}, to={to_idx}. Max index is {num_abstractions-1}.\")\n                 validated_relationships.append({\n                     \"from\": from_idx,\n                     \"to\": to_idx,\n                     \"label\": rel[\"label\"] \n                 })\n             except (ValueError, TypeError):\n                  raise ValueError(f\"Could not parse indices from relationship: {rel}\")\n\n        print(\"Generated project summary and relationship details.\")\n        return {\n            \"summary\": relationships_data[\"summary\"],\n            \"details\": validated_relationships # Store validated, index-based relationships\n        }\n\n\n    def post(self, shared, prep_res, exec_res):\n        # Structure is now {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        shared[\"relationships\"] = exec_res\n```\n\nHere's a breakdown:\n\n1.  **`prep(shared)`:**\n    *   This method retrieves the `abstractions` and `files` from the `shared` dictionary.  The `abstractions` are the output of the [Abstraction Identification](05_abstraction_identification.md) step. The files are the output from [Codebase Crawling](04_codebase_crawling.md).\n    *   It creates a context string for the LLM.  This context includes:\n        *   A list of the identified abstractions, including their names, descriptions, and file indices.\n        *   The content of the relevant code files.\n2.  **`exec(prep_res)`:**\n    *   This method constructs a prompt for the LLM, telling it to analyze the abstractions and their relationships.  The prompt includes:\n        *   The context string created in the `prep` method.\n        *   Instructions on how to format the output (as a YAML string containing the project summary and the relationships between abstractions).\n    *   It calls the `call_llm` function (defined in `utils/call_llm.py`) to send the prompt to the LLM.\n    *   It receives the LLM's response, which should be a YAML string containing the project summary and the relationships between abstractions.\n    *   The output is validated and parsed to store it in `shared[\"relationships\"]`.\n3.  **`post(shared, prep_res, exec_res)`:**\n    *   This method takes the project summary and relationships and stores them in the `shared[\"relationships\"]` variable. This makes the relationships available to the next node in the flow, which is [Order Chapters](07_llm_interaction.md).\n\n## Code Example: The LLM Prompt\n\nHere's an example of the prompt that we send to the LLM (from `nodes.py`):\n\n```python\nprompt = f\"\"\"\nBased on the following abstractions and relevant code snippets from the project `{project_name}`:\n\nList of Abstraction Indices and Names:\n{abstraction_listing}\n\nContext (Abstractions, Descriptions, Code):\n{context}\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n\"\"\"\n```\n\nThis prompt tells the LLM to:\n\n*   Provide a high-level summary of the project's purpose and functionality.\n*   Identify the key relationships between the abstractions.\n*   For each relationship, specify the source abstraction, the target abstraction, and a brief label describing the relationship.\n*   Format the output as a YAML string.\n\n## Code Example: The Output of Relationship Analysis\n\nThe `exec` function returns a dictionary that contains the output of the `AnalyzeRelationships` node:\n\n```json\n{\n  \"summary\": \"This project is a web framework that provides a simple and efficient way to build APIs.\",\n  \"details\": [\n    {\n      \"from\": 0,\n      \"to\": 1,\n      \"label\": \"Manages\"\n    },\n    {\n      \"from\": 2,\n      \"to\": 0,\n      \"label\": \"Provides config\"\n    }\n  ]\n}\n```\n\nHere's a breakdown:\n\n*   `\"summary\"`: A short, beginner-friendly explanation of the project's goal. This is used in the introduction of the tutorial.\n*   `\"details\"`: A list of the relationships between abstractions.\n    *   `\"from\"`: The index of the source abstraction.\n    *   `\"to\"`: The index of the target abstraction.\n    *   `\"label\"`: A short description of the relationship.\n\n## Using The Relationships\n\nThe final output of the `AnalyzeRelationships` node is a dictionary containing the summary and relationship details. This dictionary is stored in the `shared[\"relationships\"]` variable and is used by later nodes.\n\nFor example, the `OrderChapters` node uses the relationships to determine the best order to present the abstractions in the tutorial.  The `CombineTutorial` node uses the relationships to generate a Mermaid diagram visualizing the connections between the abstractions, and to generate the tutorial index page.\n\n## Why is Relationship Analysis Important?\n\n*   **Context:** It provides a high-level overview of the project's purpose and functionality.\n*   **Understanding:** It helps to understand how the different abstractions interact with each other to form the overall system.\n*   **Visualization:** It allows us to visualize the relationships between abstractions, making it easier to grasp the overall architecture of the codebase.\n*   **Ordering:** It informs the ordering of chapters in the tutorial, ensuring that concepts are presented in a logical and easy-to-understand sequence.\n\n## Conclusion\n\nIn this chapter, we've learned about Relationship Analysis, which is the process of understanding how the identified abstractions interact with each other. We've seen how we use an LLM to perform this analysis, and how we use the results to generate a project summary and visualize the relationships between abstractions.\n\nIn the next chapter, [LLM Interaction](07_llm_interaction.md), we'll explore how we interact with the LLM to generate the actual content for the tutorial chapters.\n```", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `Tutorial-Codebase-Knowledge` about the concept: \"LLM Interaction\". This is Chapter 7.\n\nConcept Details:\n- Description:\nThe LLM Interaction module handles all calls to the Large Language Model (LLM). This includes\ncaching responses for efficiency, logging prompts and responses for debugging, and providing\na consistent interface for other modules to interact with the LLM. It's like a dedicated\ninterpreter translating human instructions into machine-understandable code.\n\n\nComplete Tutorial Structure:\n1. [API Endpoint](01_api_endpoint.md)\n2. [Flow Orchestration](02_flow_orchestration.md)\n3. [Configuration Management](03_configuration_management.md)\n4. [Codebase Crawling](04_codebase_crawling.md)\n5. [Abstraction Identification](05_abstraction_identification.md)\n6. [Relationship Analysis](06_relationship_analysis.md)\n7. [LLM Interaction](07_llm_interaction.md)\n8. [Chapter Writing](08_chapter_writing.md)\n9. [Tutorial Combination](09_tutorial_combination.md)\n\nContext from previous chapters (summary):\n# Chapter 1: API Endpoint\n\nWelcome to the first step in our journey to understanding how to generate tutorials from codebases! In this chapter, we'll explore the **API Endpoint**, which acts like the \"front door\" to our tutorial generation system.\n\nImagine you have a robot that can build LEGO sets. You wouldn't want to open up the robot and start fiddling with its internal wires every time you want it to build something, right? Instead, you'd want a simple button or a control panel to tell it what to do. The API Endpoint is like that button for our tutorial generator.\n\n**Why do we need an API Endpoint?**\n\nThink of it this way: you might want to trigger the tutorial generation process from:\n\n*   Your web browser\n*   Another program running on your computer\n*   Even from a different computer across the internet!\n\nAn API Endpoint allows all these different \"clients\" to communicate with our tutorial generation system in a standardized way. It's the central point of contact.\n\n**Key Concepts:**\n\n1.  **API (Application Programming Interface):** Think of it as a contract between two pieces of software. It defines how they can talk to each other. In our case, it defines how external programs can ask our system to generate a tutorial.\n\n2.  **Endpoint:** A specific URL (web address) that represents a particular function or resource in an API. In our case, it's the URL you send a request to in order to start the tutorial generation. It's like the specific button on our robot's control panel that says \"Build this LEGO set!\"\n\n3.  **Request:** The message you send to the API Endpoint to tell it what to do. This includes information like the repository URL, output folder and options for the tutorial generation.\n\n4.  **Response:** The message the API Endpoint sends back to you after it has processed your request. This might include a success message, an error message, or the path to the generated tutorial.\n\n**Our Use Case: Generating a tutorial for a GitHub repository**\n\nLet's say you want to generate a tutorial for a project on GitHub. The API Endpoint allows you to do this with a simple request. You'll need to provide the URL of the repository and any specific options for how you want the tutorial to be generated.\n\n**How to Use the API Endpoint:**\n\nOur API Endpoint is designed to receive a `POST` request to the `/generate` path. This request contains information about the repository for which you want to generate a tutorial.\n\nHere's an example of how you might send a request (this is just an example, the actual implementation will depend on how you're sending the request, e.g., using `curl`, `Postman`, or a Python script):\n\n```json\n{\n  \"repo_url\": \"https://github.com/fastapi/fastapi\",\n  \"name\": \"FastAPI\",\n  \"output\": \"tutorials\"\n}\n```\n\nThis request tells the system to:\n\n*   Generate a tutorial for the `https://github.com/fastapi/fastapi` repository.\n*   Use \"FastAPI\" as the project name.\n*   Save the tutorial in the `tutorials` output folder.\n\nAfter sending this request, the API Endpoint will process it and, if successful, return a response like this:\n\n```json\n{\n  \"message\": \"Tutorial generation completed successfully.\",\n  \"final_output_directory\": \"tutorials/FastAPI/html\"\n}\n```\n\nThis tells you that the tutorial was generated successfully and is located in the `tutorials/FastAPI/html` directory.\n\n**Internal Implementation: What happens under the hood?**\n\nWhen the API Endpoint receives a request, here's a simplified breakdown of what happens:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant API Endpoint\n    participant Main Script\n    participant Codebase\n    participant LLM\n\n    User->>API Endpoint: Sends tutorial generation request (repo URL, options)\n    API Endpoint->>Main Script: Executes main.py with provided arguments\n    Main Script->>Codebase: Crawls and analyzes the codebase\n    Main Script->>LLM: Interacts with LLM to generate content\n    Main Script->>API Endpoint: Returns success/failure and output path\n    API Endpoint->>User: Sends back response with result and output path\n```\n\nHere's a slightly more detailed explanation:\n\n1.  **Request Reception:** The API Endpoint receives the request with the repository URL and other options.\n2.  **Command Construction:**  It constructs a command line command to run the main script (`main.py`) with the provided parameters.\n3.  **Script Execution:** It executes the `main.py` script, which does the heavy lifting of cloning the repository, analyzing the code, and generating the tutorial content.\n4.  **Response Generation:** After the script finishes (either successfully or with an error), the API Endpoint creates a response with the appropriate message and, if successful, the path to the generated tutorial.\n5.  **Response Delivery:** Finally, it sends the response back to the user.\n\n**Code Snippets:**\n\nLet's look at some key parts of the code in `api.py`:\n\n```python\nfrom fastapi import FastAPI, HTTPException, status\nfrom pydantic import BaseModel, Field\n```\n\nThis imports the necessary libraries for creating the API using FastAPI (a Python framework for building APIs) and Pydantic (for data validation). We are importing `FastAPI` to create the API application, `HTTPException` to handle errors and send them to the user, and `BaseModel` and `Field` from `pydantic` to define the structure of our input data.\n\n```python\nclass GenerationRequest(BaseModel):\n    repo_url: str\n    name: Optional[str] = None\n    token: Optional[str] = None\n    output: str = Field(default=\"output\")\n    include: Optional[List[str]] = None\n    exclude: Optional[List[str]] = None\n    max_size: Optional[int] = None\n```\n\nThis defines the structure of the data that the API Endpoint expects to receive in the request. This is crucial for validation and ensuring that the input data is in the correct format. `repo_url` is required, while other fields like `name`, `token`, `output`, `include`, `exclude`, and `max_size` are optional.\n\n```python\n@app.post(\n    \"/generate\",\n    summary=\"Trigger Tutorial Generation\",\n    description=\"Accepts repository details and options, then runs the tutorial generation script.\",\n    status_code=status.HTTP_200_OK\n)\nasync def generate_tutorial(request_data: GenerationRequest):\n    # ... implementation details ...\n    return {\n        \"message\": \"Tutorial generation completed successfully.\",\n        \"final_output_directory\": expected_final_path,\n    }\n```\n\nThis code defines the `/generate` endpoint using the `@app.post` decorator. This tells FastAPI that this function should be called when a `POST` request is sent to the `/generate` URL. It receives the `request_data` (which is of type `GenerationRequest` that we defined above), does some processing, and returns a response containing a success message and the path to the generated tutorial.  The `status_code=status.HTTP_200_OK` ensures that a successful response returns a 200 OK status code.\n\n**CORS Middleware**\n\n```python\norigins = [\n    \"http://127.0.0.1:5500\", # VS Code Live Server default\n    \"http://localhost\",\n    \"http://localhost:8080\",\n    \"http://127.0.0.1\",\n    \"http://127.0.0.1:8080\",\n    \"null\", # file:// origin\n]\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=origins,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n```\n\nThis section is responsible for configuring Cross-Origin Resource Sharing (CORS). CORS is a security feature implemented by web browsers to restrict web pages from making requests to a different domain than the one which served the web page. Without CORS, you might not be able to send requests to your API from a webpage running on your local machine (e.g. during development). The middleware specifies the origins that are allowed to make requests to the API, ensuring that requests from these origins are not blocked by the browser.\n\n**Error Handling**\n\nThe `api.py` file includes comprehensive error handling to gracefully manage different scenarios that might arise during script execution.\n\n```python\n    except subprocess.CalledProcessError as e:\n        logger.error(f\"Script failed with exit code {e.returncode}\")\n        logger.error(f\"Script stdout:\\n{e.stdout}\")\n        logger.error(f\"Script stderr:\\n{e.stderr}\")\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail={\n                \"error\": \"Script execution failed.\",\n                \"details\": \"The tutorial generation script exited with an error.\",\n                \"return_code\": e.returncode,\n                \"script_stdout\": e.stdout,\n                \"script_stderr\": e.stderr\n            }\n        )\n    except FileNotFoundError:\n        error_msg = f\"Error: Script '{MAIN_SCRIPT_PATH}' or Python '{PYTHON_EXECUTABLE}' not found.\"\n        logger.error(error_msg)\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail={\"error\": \"Server configuration error.\", \"details\": error_msg}\n        )\n    except Exception as e:\n        logger.exception(\"An unexpected error occurred during script execution.\")\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail={\"error\": \"An unexpected internal server error occurred.\", \"details\": str(e)}\n        )\n```\n\n*   **`subprocess.CalledProcessError`**: Catches errors when the `main.py` script fails during execution. It logs the error details and raises an `HTTPException` with a 500 status code, providing details like the return code, stdout, and stderr of the script.\n*   **`FileNotFoundError`**: Handles cases where the `main.py` script or Python executable is not found. It raises an `HTTPException` with a 500 status code, indicating a server configuration issue.\n*   **`Exception`**: Serves as a catch-all for unexpected errors during script execution. It logs the exception and raises an `HTTPException` with a 500 status code, providing details about the error.\n\n**Conclusion:**\n\nIn this chapter, we've learned about the API Endpoint, which serves as the entry point to our tutorial generation system. We saw how it allows external programs to trigger the tutorial generation process with a simple request. We also explored the internal implementation and understood how it orchestrates the execution of the `main.py` script.\n\nNow that we know how to trigger the process, let's dive into [Flow Orchestration](02_flow_orchestration.md) in the next chapter to understand the overall sequence of steps involved in generating a tutorial.\n\n---\n# Chapter 2: Flow Orchestration\n\nIn the previous chapter, [API Endpoint](01_api_endpoint.md), we learned how to trigger the tutorial generation process. Think of it like pressing the \"Start\" button on a machine. But what happens *after* you press that button? That's where Flow Orchestration comes in!\n\n## What is Flow Orchestration?\n\nImagine a factory that builds cars. You wouldn't just dump all the parts in a pile and hope a car magically appears, right? Instead, you'd have a carefully planned assembly line:\n\n1.  **Frame Assembly:** The car's frame is built.\n2.  **Engine Installation:** The engine is installed.\n3.  **Bodywork:** The body panels are attached.\n4.  **Painting:** The car gets its color.\n5.  **Interior Fitting:** Seats and other interior parts are added.\n6.  **Quality Control:** Final checks are performed.\n\nFlow Orchestration is like that assembly line for our tutorial generation system. It defines the *sequence of steps* needed to transform a codebase into a complete tutorial. It makes sure each task happens in the right order, passing the result of one task to the next.\n\nWithout flow orchestration, we'd have a chaotic mess. Chapters might be written before the codebase is even analyzed, or the tutorial might be combined before the chapters are finished! Flow Orchestration brings order to the process.\n\n## Key Concepts\n\n*   **Flow:** The overall process or pipeline of tasks. In our case, it's the entire tutorial generation process.\n*   **Node:** A single task or step in the flow. Each node performs a specific job, like fetching code, analyzing relationships, or writing a chapter.  Nodes can be thought of as functions in a programming language.\n*   **Sequence:** The order in which the nodes are executed.  This is crucial for ensuring the correct flow of data and dependencies.\n*   **Dependencies:**  A node might *depend* on the output of another node. For example, you can't write a chapter *about* an abstraction until you've identified *what* the abstractions *are*.\n\n## How Flow Orchestration Works in Our Project\n\nLet's look at the main steps in our tutorial generation flow:\n\n```mermaid\ngraph LR\n    A[Fetch Repo] --> B(Identify Abstractions)\n    B --> C{Analyze Relationships}\n    C --> D[Order Chapters]\n    D --> E((Write Chapters))\n    E --> F(Combine Tutorial)\n    style A fill:#f9f,stroke:#333,stroke-width:2px\n    style F fill:#f9f,stroke:#333,stroke-width:2px\n    style E fill:#ccf,stroke:#333,stroke-width:2px\n```\n\nHere's a breakdown of each node:\n\n1.  **Fetch Repo:**  Downloads the codebase from a remote repository (like GitHub) or reads it from a local directory. This node is responsible for getting the raw material needed to create the tutorial.  It outputs the files and their contents.\n2.  **Identify Abstractions:** Analyzes the codebase and identifies key concepts or abstractions that are important for understanding the project. This step essentially identifies the key topics to cover in the tutorial.  It outputs a list of abstractions, including their names, descriptions, and the files they're found in.\n3.  **Analyze Relationships:** Examines how the different abstractions interact with each other. This helps us understand the overall architecture of the codebase and how the different pieces fit together. The LLM creates a summary of the project and the relationships between the abstractions\n4.  **Order Chapters:** Determines the best order to present the abstractions in the tutorial.  Some abstractions might be foundational and need to be explained before others. The LLM figures out the order.\n5.  **Write Chapters:**  Generates the actual content for each chapter of the tutorial. It uses the abstraction details, relationships, and chapter order to create a beginner-friendly explanation of each concept. This node is marked with double parentheses `((...))` because it's a *BatchNode*. A `BatchNode` handles independent steps and can be run in parallel.\n6.  **Combine Tutorial:**  Assembles all the individual chapters into a complete tutorial, creating an index page and table of contents.\n\n## Code Example: Defining the Flow\n\nThe core of our flow orchestration is defined in `flow.py`:\n\n```python\nfrom pocketflow import Flow\n# Import all node classes from nodes.py\nfrom nodes import (\n    FetchRepo,\n    IdentifyAbstractions,\n    AnalyzeRelationships,\n    OrderChapters,\n    WriteChapters,\n    CombineTutorial\n)\n\ndef create_tutorial_flow():\n    \"\"\"Creates and returns the codebase tutorial generation flow.\"\"\"\n\n    # Instantiate nodes\n    fetch_repo = FetchRepo()\n    identify_abstractions = IdentifyAbstractions(max_retries=3, wait=10)\n    analyze_relationships = AnalyzeRelationships(max_retries=3, wait=10)\n    order_chapters = OrderChapters(max_retries=3, wait=10)\n    write_chapters = WriteChapters(max_retries=3, wait=10) # This is a BatchNode\n    combine_tutorial = CombineTutorial()\n\n    # Connect nodes in sequence based on the design\n    fetch_repo >> identify_abstractions\n    identify_abstractions >> analyze_relationships\n    analyze_relationships >> order_chapters\n    order_chapters >> write_chapters\n    write_chapters >> combine_tutorial\n\n    # Create the flow starting with FetchRepo\n    tutorial_flow = Flow(start=fetch_repo)\n\n    return tutorial_flow\n```\n\nLet's break down this code:\n\n*   **Importing Nodes:** We import all the node classes (like `FetchRepo`, `IdentifyAbstractions`, etc.) from the `nodes.py` file.  Think of these as importing different tools or functions that we need for our assembly line.\n*   **Instantiating Nodes:**  We create instances of each node class. For example, `fetch_repo = FetchRepo()` creates a specific instance of the `FetchRepo` node. This is where the maximum number of retries and wait time between retries is set for nodes that interact with the LLM. This helps handle cases where the LLM might be temporarily unavailable or returns an invalid response.\n*   **Connecting Nodes:** The `>>` operator defines the sequence of the flow. `fetch_repo >> identify_abstractions` means that the `IdentifyAbstractions` node will execute *after* the `FetchRepo` node completes.  The output of `FetchRepo` will be passed as input to `IdentifyAbstractions`.\n*   **Creating the Flow:** We use the `Flow` class from the `pocketflow` library to create the flow. We specify the `start` node, which is the first node to be executed.\n\nThis simple code defines the entire structure of our tutorial generation process!\n\n## Code Example: Running the Flow\n\nThe `main.py` file is responsible for setting up the flow with all the command line input parameters and calling the `Flow` to execute:\n\n```python\nimport dotenv\nimport os\nimport argparse\n# Import the function that creates the flow\nfrom flow import create_tutorial_flow\n\ndotenv.load_dotenv()\n\n# Default file patterns\nDEFAULT_INCLUDE_PATTERNS = {\n    \"*.py\", \"*.js\", \"*.jsx\", \"*.ts\", \"*.tsx\", \"*.go\", \"*.java\", \"*.pyi\", \"*.pyx\", \n    \"*.c\", \"*.cc\", \"*.cpp\", \"*.h\", \"*.md\", \"*.rst\", \"Dockerfile\", \n    \"Makefile\", \"*.yaml\", \"*.yml\",\n}\n\nDEFAULT_EXCLUDE_PATTERNS = {\n    \"*test*\", \"tests/*\", \"docs/*\", \"examples/*\", \"v1/*\", \n    \"dist/*\", \"build/*\", \"experimental/*\", \"deprecated/*\", \n    \"legacy/*\", \".git/*\", \".github/*\", \".next/*\", \".vscode/*\", \"obj/*\", \"bin/*\", \"node_modules/*\", \"*.log\"\n}\n\n# --- Main Function ---\ndef main():\n    parser = argparse.ArgumentParser(description=\"Generate a tutorial for a GitHub codebase or local directory.\")\n    \n    # Create mutually exclusive group for source\n    source_group = parser.add_mutually_exclusive_group(required=True)\n    source_group.add_argument(\"--repo\", help=\"URL of the public GitHub repository.\")\n    source_group.add_argument(\"--dir\", help=\"Path to local directory.\")\n    \n    parser.add_argument(\"-n\", \"--name\", help=\"Project name (optional, derived from repo/directory if omitted).\")\n    parser.add_argument(\"-t\", \"--token\", help=\"GitHub personal access token (optional, reads from GITHUB_TOKEN env var if not provided).\")\n    parser.add_argument(\"-o\", \"--output\", default=\"output\", help=\"Base directory for output (default: ./output).\")\n    parser.add_argument(\"-i\", \"--include\", nargs=\"+\", help=\"Include file patterns (e.g. '*.py' '*.js'). Defaults to common code files if not specified.\")\n    parser.add_argument(\"-e\", \"--exclude\", nargs=\"+\", help=\"Exclude file patterns (e.g. 'tests/*' 'docs/*'). Defaults to test/build directories if not specified.\")\n    parser.add_argument(\"-s\", \"--max-size\", type=int, default=100000, help=\"Maximum file size in bytes (default: 100000, about 100KB).\")\n\n    args = parser.parse_args()\n\n    # Get GitHub token from argument or environment variable if using repo\n    github_token = None\n    if args.repo:\n        github_token = args.token or os.environ.get('GITHUB_TOKEN')\n        if not github_token:\n            print(\"Warning: No GitHub token provided. You might hit rate limits for public repositories.\")\n\n    # Initialize the shared dictionary with inputs\n    shared = {\n        \"repo_url\": args.repo,\n        \"local_dir\": args.dir,\n        \"project_name\": args.name, # Can be None, FetchRepo will derive it\n        \"github_token\": github_token,\n        \"output_dir\": args.output, # Base directory for CombineTutorial output\n\n        # Add include/exclude patterns and max file size\n        \"include_patterns\": set(args.include) if args.include else DEFAULT_INCLUDE_PATTERNS,\n        \"exclude_patterns\": set(args.exclude) if args.exclude else DEFAULT_EXCLUDE_PATTERNS,\n        \"max_file_size\": args.max_size,\n\n        # Outputs will be populated by the nodes\n        \"files\": [],\n        \"abstractions\": [],\n        \"relationships\": {},\n        \"chapter_order\": [],\n        \"chapters\": [],\n        \"final_output_dir\": None\n    }\n\n    print(f\"Starting tutorial generation for: {args.repo or args.dir}\")\n\n    # Create the flow instance\n    tutorial_flow = create_tutorial_flow()\n\n    # Run the flow\n    tutorial_flow.run(shared)\n    \nif __name__ == \"__main__\":\n    main()\n```\n\nHere are the important parts:\n\n1.  **Parsing Arguments:** We use `argparse` to handle command-line arguments like the repository URL, project name, and output directory.\n2.  **Shared Dictionary:** The `shared` dictionary acts as a central store for data that is passed between the nodes. Each node can read data from this dictionary and write its results back to it.\n3.  **Creating the Flow:** We call `create_tutorial_flow()` to get an instance of the flow we defined in `flow.py`.\n4.  **Running the Flow:**  Finally, we call `tutorial_flow.run(shared)` to start the flow. The `shared` dictionary is passed to the `run` method so that the nodes can access and modify the data.\n\n## What Happens Inside a Node?\n\nEach node consists of three important methods. Looking at the example of `FetchRepo` in `nodes.py`:\n\n```python\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"local_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        if not project_name:\n            # Basic name derivation from URL or directory\n            if repo_url:\n                project_name = repo_url.split('/')[-1].replace('.git', '')\n            else:\n                project_name = os.path.basename(os.path.abspath(local_dir))\n            shared[\"project_name\"] = project_name\n\n        # Get file patterns directly from shared\n        include_patterns = shared[\"include_patterns\"]\n        exclude_patterns = shared[\"exclude_patterns\"]\n        max_file_size = shared[\"max_file_size\"]\n\n        return {\n            \"repo_url\": repo_url,\n            \"local_dir\": local_dir,\n            \"token\": shared.get(\"github_token\"),\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns,\n            \"max_file_size\": max_file_size,\n            \"use_relative_paths\": True\n        }\n\n    def exec(self, prep_res):\n        if prep_res[\"repo_url\"]:\n            print(f\"Crawling repository: {prep_res['repo_url']}...\")\n            result = crawl_github_files(\n                repo_url=prep_res[\"repo_url\"],\n                token=prep_res[\"token\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        else:\n            print(f\"Crawling directory: {prep_res['local_dir']}...\")\n            result = crawl_local_files(\n                directory=prep_res[\"local_dir\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n            \n        # Convert dict to list of tuples: [(path, content), ...]\n        files_list = list(result.get(\"files\", {}).items())\n        print(f\"Fetched {len(files_list)} files.\")\n        return files_list\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"files\"] = exec_res # List of (path, content) tuples\n```\n\n*   **`prep(shared)`:** This method prepares the input for the node's execution. It reads data from the `shared` dictionary and transforms it into a format that the `exec` method can use. It also performs any necessary setup or validation. Returns a `prep_res` object that contains everything necessary for the `exec` function to work.\n*   **`exec(prep_res)`:** This method performs the core logic of the node. It receives the prepared input from the `prep` method and executes the task. In the case of `FetchRepo`, it downloads the codebase from GitHub or reads it from the local directory. It returns the `exec_res` which is the output of the node.\n*   **`post(shared, prep_res, exec_res)`:** This method is executed *after* the `exec` method completes. It takes the output of the `exec` method (`exec_res`) and writes it back to the `shared` dictionary, making it available to the next node in the flow.  In the case of `FetchRepo`, it writes the list of files and their contents to the `shared[\"files\"]` variable.\n\n## Why Use Flow Orchestration?\n\n*   **Organization:** It provides a clear and structured way to define complex processes.\n*   **Maintainability:** It makes the code easier to understand and maintain by breaking it down into smaller, independent nodes.\n*   **Reusability:**  Nodes can be reused in different flows, making it easier to build new features or modify existing ones.\n*   **Testability:** Each node can be tested independently, making it easier to ensure the correctness of the entire system.\n*   **Error Handling:** `pocketflow` supports retries, fallbacks, error handling, and graceful termination, which are useful for nodes that interact with external resources.\n\n## Conclusion\n\nFlow Orchestration is a powerful technique for managing complex processes like our tutorial generation system. It provides a structured way to define the sequence of steps, making the code more organized, maintainable, and testable.\n\nIn the next chapter, [Configuration Management](03_configuration_management.md), we'll explore how to manage the different configuration settings that control the behavior of our system. This will allow us to customize the tutorial generation process for different codebases and use cases.\n\n---\n# Chapter 3: Configuration Management\n\nIn the previous chapter, [Flow Orchestration](02_flow_orchestration.md), we learned how to define the overall sequence of steps for generating a tutorial. Now, let's talk about how to *customize* that process. Think of it like this: you have a recipe for baking a cake, but you might want to adjust the recipe to make it chocolate or vanilla. That's where Configuration Management comes in!\n\n## What is Configuration Management?\n\nConfiguration Management is like having a control panel that allows you to adjust the settings of our tutorial generation \"machine\" to achieve the desired output. It lets you change things like:\n\n*   Which repository to generate a tutorial for.\n*   What name to give the project in the tutorial.\n*   Which files to include or exclude from analysis.\n*   Where to save the generated tutorial.\n\nWithout configuration management, we'd be stuck with a single, inflexible way of generating tutorials. It's like trying to bake every cake with the exact same recipe, even if you want different flavors!\n\n## Key Concepts\n\n1.  **Environment Variables:** These are like global settings that can be accessed by any program running on your computer. They're often used to store things like API keys or default values.\n2.  **Argument Parsing:** This allows you to pass settings directly to a program when you run it from the command line. It's like specifying the ingredients for your cake when you give the recipe to the baker.\n\n## Our Use Case: Generating a Tutorial for a Specific GitHub Repository with a Custom Name\n\nLet's say you want to generate a tutorial for the `requests` library on GitHub and name the project \"Python Requests Tutorial\". With Configuration Management, you can easily do this!\n\n## How to Use Configuration Management\n\nOur project uses both environment variables and argument parsing to manage configuration.\n\n### 1. Environment Variables\n\nEnvironment variables are useful for settings that you might want to reuse across multiple runs of the program, or that contain sensitive information you don't want to type directly into the command line.\n\nTo set an environment variable, you can use the following command in your terminal (replace `<your_token>` with your actual token):\n\n```bash\nexport GITHUB_TOKEN=<your_token>\n```\n\nOr, you can create a `.env` file in the project root directory:\n\n```\nGITHUB_TOKEN=<your_token>\n```\n\nThe `dotenv` library will automatically load these variables when the script runs.\n\nIn our `main.py` file, we use the `os.environ.get()` function to read the value of the `GITHUB_TOKEN` environment variable:\n\n```python\nimport os\ngithub_token = args.token or os.environ.get('GITHUB_TOKEN')\n```\n\nThis code first checks if the token was provided as a command-line argument (`args.token`). If not, it looks for the `GITHUB_TOKEN` environment variable.\n\n### 2. Argument Parsing\n\nArgument parsing allows you to specify settings directly when you run the `main.py` script from the command line.\n\nHere's how you would generate a tutorial for the `requests` library with a custom name:\n\n```bash\npython main.py --repo https://github.com/psf/requests -n \"Python Requests Tutorial\" -o tutorials\n```\n\nLet's break down this command:\n\n*   `python main.py`: This tells your computer to run the `main.py` script using Python.\n*   `--repo https://github.com/psf/requests`: This specifies the URL of the GitHub repository to use. `--repo` is the *argument* name, and `https://github.com/psf/requests` is the *value* of that argument.\n*   `-n \"Python Requests Tutorial\"`: This sets the project name to \"Python Requests Tutorial\". `-n` is a *short* argument name, and `\"Python Requests Tutorial\"` is the value.\n*   `-o tutorials`: This specifies that the tutorial output will be written to the `tutorials` directory.\n\nThe `argparse` module in Python handles the parsing of these command-line arguments. In `main.py`, we define the arguments that our script accepts:\n\n```python\nimport argparse\n\nparser = argparse.ArgumentParser(description=\"Generate a tutorial for a GitHub codebase or local directory.\")\nparser.add_argument(\"--repo\", help=\"URL of the public GitHub repository.\")\nparser.add_argument(\"-n\", \"--name\", help=\"Project name (optional, derived from repo/directory if omitted).\")\nparser.add_argument(\"-o\", \"--output\", default=\"output\", help=\"Base directory for output (default: ./output).\")\n# ... other arguments\nargs = parser.parse_args()\n```\n\nThis code defines three arguments: `--repo`, `--name` (with the short form `-n`), and `--output` (with the short form `-o`). The `help` parameter provides a description of each argument that is displayed when you run the script with the `--help` flag:\n\n```bash\npython main.py --help\n```\n\n### How Include and Exclude Patterns Work\n\nThe `-i` (or `--include`) and `-e` (or `--exclude`) options are used to specify which files to include or exclude from the tutorial generation process. This is helpful if you want to focus on specific parts of the codebase or ignore irrelevant files (like test files or documentation).\n\nFor example, to only include Python files and exclude test directories, you could use the following command:\n\n```bash\npython main.py --repo https://github.com/psf/requests -i \"*.py\" -e \"tests/*\"\n```\n\n## Internal Implementation\n\nHere's a simplified view of how Configuration Management works internally:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Command Line\n    participant Main Script\n    participant Args Namespace\n    User->>Command Line: Runs script with arguments: python main.py --repo ...\n    Command Line->>Main Script: Executes main.py\n    Main Script->>Args Namespace: Parses command-line arguments using argparse\n    Args Namespace->>Main Script: Returns arguments as an object (e.g., args.repo, args.name)\n    Main Script->>Main Script: Accesses argument values and uses them\n```\n\nHere's a breakdown:\n\n1.  **User Interaction:** The user runs the `main.py` script from the command line, providing arguments like the repository URL and project name.\n2.  **Argument Parsing:** The `argparse` module in `main.py` parses these arguments and stores them in an `Args Namespace` object.\n3.  **Accessing Arguments:** The `main.py` script accesses the argument values from the `Args Namespace` object (e.g., `args.repo`, `args.name`).\n4.  **Using Arguments:** The script uses these values to configure the tutorial generation process. For example, the `repo_url` value is passed to the [Codebase Crawling](04_codebase_crawling.md) node to download the codebase.\n\n## Code Example: Passing Configuration to Nodes\n\nThe parsed arguments are stored in the `shared` dictionary and passed to the nodes in the flow. For example, in `main.py`:\n\n```python\nshared = {\n    \"repo_url\": args.repo,\n    \"project_name\": args.name,\n    \"output_dir\": args.output,\n    # ... other arguments\n}\n\ntutorial_flow.run(shared)\n```\n\nEach node can then access the configuration values from the `shared` dictionary in its `prep` method. For example, in `nodes.py`:\n\n```python\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        project_name = shared.get(\"project_name\")\n        # ...\n        return {\n            \"repo_url\": repo_url,\n            # ...\n        }\n```\n\n## Why is Configuration Management Important?\n\n*   **Flexibility:** It allows you to customize the tutorial generation process for different codebases and use cases.\n*   **Reusability:** You can easily reuse the same script to generate tutorials for different projects by simply changing the configuration settings.\n*   **Automation:** You can automate the tutorial generation process by running the script with a predefined set of arguments.\n\n## Conclusion\n\nIn this chapter, we've learned about Configuration Management, which allows us to customize the tutorial generation process by using environment variables and argument parsing. This gives us the flexibility to generate tutorials for different codebases with different settings.\n\nIn the next chapter, [Codebase Crawling](04_codebase_crawling.md), we'll dive into the process of fetching and analyzing the codebase itself.\n\n---\n# Chapter 4: Codebase Crawling\n\nIn the previous chapter, [Configuration Management](03_configuration_management.md), we learned how to customize our tutorial generation process using command-line arguments and environment variables. Now, let's explore how we actually *get* the code that we'll be using to generate the tutorial! That's where Codebase Crawling comes in.\n\n## What is Codebase Crawling?\n\nImagine you're writing a report on a popular book. You can't write the report without actually *reading* the book first, right? Codebase Crawling is like reading the book (or, more accurately, the code) for our tutorial generation system.\n\nSpecifically, Codebase Crawling is the process of gathering all the necessary files from either:\n\n*   A remote GitHub repository, or\n*   A local directory on your computer.\n\nIt's like a diligent librarian collecting books from different shelves, ensuring they meet certain criteria before adding them to the collection. The criteria might include:\n\n*   **File Type:** Only including certain types of files (e.g., Python files, JavaScript files).\n*   **File Size:** Ignoring files that are too large.\n*   **Location:** Only including files in certain directories, or excluding files in others (like test directories).\n\nWithout Codebase Crawling, our system wouldn't have any code to analyze, and we wouldn't be able to generate a tutorial!\n\n## Key Concepts\n\n*   **Repository URL:** The address of the GitHub repository (e.g., `https://github.com/fastapi/fastapi`).\n*   **Local Directory:** The path to a directory on your computer (e.g., `/Users/myuser/myproject`).\n*   **Include Patterns:**  Patterns that specify which files to include (e.g., `*.py` for all Python files).  Think of these as \"keywords\" to search for in the library.\n*   **Exclude Patterns:** Patterns that specify which files to exclude (e.g., `tests/*` to exclude all files in the `tests` directory).  Think of these as \"forbidden words\" to avoid in the search.\n*   **Maximum File Size:** A limit on the size of the files to include. This helps prevent the system from trying to process very large files, which can be slow or cause errors.\n\n## How Codebase Crawling Works in Our Project\n\nThe Codebase Crawling process is handled by the `FetchRepo` node in our flow. Remember from [Flow Orchestration](02_flow_orchestration.md) that each node has three important steps: `prep`, `exec`, and `post`.\n\nLet's look at how Codebase Crawling is implemented in the `FetchRepo` node in `nodes.py`:\n\n```python\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"local_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        # ... (derive project name if not provided)\n\n        # Get file patterns directly from shared\n        include_patterns = shared[\"include_patterns\"]\n        exclude_patterns = shared[\"exclude_patterns\"]\n        max_file_size = shared[\"max_file_size\"]\n\n        return {\n            \"repo_url\": repo_url,\n            \"local_dir\": local_dir,\n            \"token\": shared.get(\"github_token\"),\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns,\n            \"max_file_size\": max_file_size,\n            \"use_relative_paths\": True\n        }\n\n    def exec(self, prep_res):\n        if prep_res[\"repo_url\"]:\n            print(f\"Crawling repository: {prep_res['repo_url']}...\")\n            result = crawl_github_files(\n                repo_url=prep_res[\"repo_url\"],\n                token=prep_res[\"token\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        else:\n            print(f\"Crawling directory: {prep_res['local_dir']}...\")\n            result = crawl_local_files(\n                directory=prep_res[\"local_dir\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n            \n        # Convert dict to list of tuples: [(path, content), ...]\n        files_list = list(result.get(\"files\", {}).items())\n        print(f\"Fetched {len(files_list)} files.\")\n        return files_list\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"files\"] = exec_res # List of (path, content) tuples\n```\n\nLet's break down what's happening:\n\n1.  **`prep(shared)`:**\n    *   This method retrieves the configuration values (repository URL, local directory, include patterns, exclude patterns, maximum file size) from the `shared` dictionary. This is how the node knows *what* to crawl and *how* to crawl it.\n    *   It prepares these values into a `prep_res` object which is a dictionary containing the configuration for the `exec` function to use.\n\n2.  **`exec(prep_res)`:**\n    *   This method checks if a `repo_url` is provided. If so, it calls the `crawl_github_files` function to crawl the GitHub repository.\n    *   If a `repo_url` is *not* provided (meaning we're crawling a local directory), it calls the `crawl_local_files` function to crawl the local directory.\n    *   The `crawl_github_files` and `crawl_local_files` functions do the actual work of fetching the files and their contents.\n    *   Finally, the result from the crawl is converted into a list of tuples where each tuple contains a file path and file content.\n\n3.  **`post(shared, prep_res, exec_res)`:**\n    *   This method takes the list of files (the `exec_res` output) and stores it in the `shared[\"files\"]` variable. This makes the files available to the next node in the flow, which is [Abstraction Identification](05_abstraction_identification.md).\n\n### Crawling GitHub Repositories: `crawl_github_files`\n\nThe `crawl_github_files` function in `utils/crawl_github_files.py` is responsible for fetching files from a GitHub repository.  Here's a simplified view of how it works:\n\n```python\ndef crawl_github_files(\n    repo_url, \n    token=None, \n    max_file_size: int = 1 * 1024 * 1024,  # 1 MB\n    use_relative_paths: bool = False,\n    include_patterns: Union[str, Set[str]] = None,\n    exclude_patterns: Union[str, Set[str]] = None\n):\n    # Parse the repo_url to extract owner, repo name, branch and path\n    # Uses the GitHub REST API to get the files and their content\n\n    files = {}\n    skipped_files = []\n\n    # Fetch contents of the repository at a specific path and commit\n    def fetch_contents(path):\n        # Use the GitHub REST API to get the contents of the given path\n        # If it's a file, download it and add it to the files dictionary\n        # If it's a directory, recursively call fetch_contents on the directory\n\n        # Check if file should be included based on patterns\n        def should_include_file(file_path: str, file_name: str) -> bool:\n            # If no include patterns are specified, include all files\n            # Otherwise, check if the file matches any of the include patterns\n            # If exclude patterns are specified, exclude files that match the exclude patterns\n            return True or False # Simplified logic\n\n    # Start crawling from the specified path\n    fetch_contents(specific_path)\n\n    return {\n        \"files\": files,\n        \"stats\": {\n            \"downloaded_count\": len(files),\n            \"skipped_count\": len(skipped_files),\n            \"skipped_files\": skipped_files,\n            \"base_path\": specific_path if use_relative_paths else None,\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns\n        }\n    }\n```\n\nHere's a breakdown:\n\n1.  **Parsing the Repository URL:** The function first parses the `repo_url` to extract the owner, repository name, branch/commit, and path within the repository. This information is used to construct the URLs for the GitHub API requests.\n2.  **Using the GitHub REST API:** It uses the GitHub REST API to fetch the contents of the repository. The API allows us to retrieve information about files and directories in the repository.\n3.  **Handling Different Content Types:** The API returns different data depending on whether the item is a file or a directory.\n    *   **Files:** For files, the function downloads the file content and stores it in the `files` dictionary.  It also checks the file size against the `max_file_size` limit.\n    *   **Directories:** For directories, the function recursively calls itself to process the contents of the subdirectory. This allows it to traverse the entire repository structure.\n4.  **Include/Exclude Patterns:** The function uses the `should_include_file` helper function to check if a file should be included based on the `include_patterns` and `exclude_patterns`. This allows you to filter the files that are included in the tutorial generation process.  If a file matches an exclude pattern, it will not be included.\n5.  **Error Handling:** The function includes error handling to deal with potential issues like rate limits (which can occur if you make too many requests to the GitHub API in a short period of time) and network errors.\n\n### Crawling Local Directories: `crawl_local_files`\n\nThe `crawl_local_files` function in `utils/crawl_local_files.py` is responsible for fetching files from a local directory. Here's a simplified view of how it works:\n\n```python\nimport os\nimport fnmatch\n\ndef crawl_local_files(directory, include_patterns=None, exclude_patterns=None, max_file_size=None, use_relative_paths=True):\n    \"\"\"\n    Crawl files in a local directory with similar interface as crawl_github_files.\n    \"\"\"\n    files_dict = {}\n\n    # Traverse through all files and folders in the directory\n    for root, _, files in os.walk(directory):\n        for filename in files:\n            filepath = os.path.join(root, filename) # Get the full path\n            relpath = os.path.relpath(filepath, directory) # Path relative to directory\n\n            # Check include and exclude patterns\n            included = True #Simplified\n            excluded = False #Simplified\n            if not included or excluded:\n                continue\n\n            # Check file size\n            if max_file_size and os.path.getsize(filepath) > max_file_size:\n                continue\n\n            try:\n                with open(filepath, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                files_dict[relpath] = content #Store content\n            except Exception as e:\n                print(f\"Warning: Could not read file {filepath}: {e}\")\n\n    return {\"files\": files_dict}\n```\n\nHere's a breakdown:\n\n1.  **Traversing the Directory:** The function uses `os.walk` to traverse the directory tree, visiting each directory and file within the specified `directory`.\n2.  **Constructing File Paths:** For each file, the function constructs the full path to the file using `os.path.join`. It also calculates the relative path to the file (relative to the specified `directory`) using `os.path.relpath`.\n3.  **Include/Exclude Patterns:** Similar to `crawl_github_files`, the function checks if a file should be included based on the `include_patterns` and `exclude_patterns`.\n4.  **File Size Check:** The function checks the file size against the `max_file_size` limit.\n5.  **Reading File Content:** If the file passes all the checks, the function reads the content of the file and stores it in the `files_dict` dictionary, using the file path as the key.\n6.  **Error Handling:** The function includes basic error handling to catch potential issues like file read errors.\n\n## How to Use Codebase Crawling\n\nYou don't directly call `crawl_github_files` or `crawl_local_files` yourself. Instead, you provide the necessary configuration (repository URL or local directory, include patterns, exclude patterns, maximum file size) when you run the `main.py` script.  The `FetchRepo` node then uses this configuration to crawl the codebase.\n\nFor example, to generate a tutorial for the `fastapi` repository, including only Python files and excluding the `tests` directory, you would use the following command:\n\n```bash\npython main.py --repo https://github.com/fastapi/fastapi -i \"*.py\" -e \"tests/*\"\n```\n\nTo generate a tutorial for a local directory, you would use the `--dir` option instead of `--repo`:\n\n```bash\npython main.py --dir /path/to/my/project -i \"*.py\" -e \"tests/*\"\n```\n\n## Why is Codebase Crawling Important?\n\n*   **Data Acquisition:** It's the first step in the tutorial generation process, providing the raw data (the codebase) that is needed to generate the tutorial.\n*   **Filtering:** It allows you to filter the files that are included in the tutorial, focusing on the most relevant parts of the codebase and ignoring irrelevant files (like test files or documentation).\n*   **Efficiency:** By limiting the maximum file size, it helps prevent the system from trying to process very large files, which can be slow or cause errors.\n\n## Conclusion\n\nIn this chapter, we've learned about Codebase Crawling, which is the process of gathering the necessary files from either a GitHub repository or a local directory. We've seen how it uses include and exclude patterns to filter files, and how it limits the maximum file size to prevent performance issues.\n\nIn the next chapter, [Abstraction Identification](05_abstraction_identification.md), we'll explore how we analyze the codebase to identify the key concepts that should be covered in the tutorial.\n\n---\n# Chapter 5: Abstraction Identification\n\n```markdown\n# Chapter 5: Abstraction Identification\n\nIn the previous chapter, [Codebase Crawling](04_codebase_crawling.md), we learned how to gather all the code files from a repository or local directory. Now, we need to figure out *what's important* in that code!  That's where Abstraction Identification comes in.\n\n## What is Abstraction Identification?\n\nImagine you're trying to learn a new language.  You wouldn't just start memorizing every single word in the dictionary, right? Instead, you'd focus on the most common and important words and grammar rules.\n\nAbstraction Identification is similar. It's the process of analyzing the codebase and identifying the most important concepts or components to explain in the tutorial.  These \"concepts\" are often called \"abstractions\" because they represent a higher-level, simplified view of the underlying code.\n\nThink of it like a detective piecing together clues from a crime scene to identify the key suspects and their roles in the crime.  The LLM is our detective, sifting through the code to find the most important pieces.\n\nWhy is this important?  Because if we tried to explain every single line of code, the tutorial would be overwhelming and confusing.  Abstraction Identification helps us focus on the core concepts that a beginner needs to understand.\n\n## Key Concepts\n\n*   **Abstractions:**  Simplified representations of complex code components. Think of them as key concepts or building blocks of the project. Examples could be \"API Endpoint,\" \"Database Connection,\" or \"User Authentication.\"\n*   **LLM (Large Language Model):**  The \"brain\" of our operation.  We use an LLM to analyze the code and identify the important abstractions.\n*   **Codebase Context:** The actual code files that the LLM analyzes.  This is the output of the [Codebase Crawling](04_codebase_crawling.md) step.\n*   **File Indices:** A list of index numbers which corresponds to file paths used for the LLM to refer back to the code.\n\n## How Abstraction Identification Works\n\nThe Abstraction Identification process is handled by the `IdentifyAbstractions` node in our flow.\n\nLet's look at the simplified steps in `nodes.py`:\n\n```python\nclass IdentifyAbstractions(Node):\n    def prep(self, shared):\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        \n        # Helper to create context from files, respecting limits (basic example)\n        def create_llm_context(files_data):\n            context = \"\"\n            file_info = [] # Store tuples of (index, path)\n            for i, (path, content) in enumerate(files_data):\n                entry = f\"--- File Index {i}: {path} ---\\n{content}\\n\\n\"\n                context += entry\n                file_info.append((i, path))\n\n            return context, file_info # file_info is list of (index, path)\n\n        context, file_info = create_llm_context(files_data)\n        # Format file info for the prompt (comment is just a hint for LLM)\n        file_listing_for_prompt = \"\\n\".join([f\"- {idx} # {path}\" for idx, path in file_info])\n        return context, file_listing_for_prompt, len(files_data), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, file_listing_for_prompt, file_count, project_name = prep_res  # Unpack project name\n        print(\"Identifying abstractions using LLM...\")\n        prompt = f\"\"\"\nFor the project `{project_name}`:\n\nCodebase Context:\n{context}\n\nAnalyze the codebase context.\nIdentify the top 5-10 core most important abstractions to help those new to the codebase.\n\nFor each abstraction, provide:\n1. A concise `name`.\n2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\nList of file indices and paths present in the context:\n{file_listing_for_prompt}\n\nFormat the output as a YAML list of dictionaries:\n\n```yaml\n- name: Query Processing\n  description: | \n    Explains what the abstraction does.\n    It's like a central dispatcher routing requests.\n  file_indices:\n    - 0 # path/to/file1.py\n    - 3 # path/to/related.py\n- name: Query Optimization\n  description: |\n    Another core concept, similar to a blueprint for objects.\n  file_indices:\n    - 5 # path/to/another.js\n# ... up to 10 abstractions\n```\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        abstractions = yaml.safe_load(yaml_str)\n\n        # Basic Validation\n        # Extract data and file indices to `shared` dictionary\n        # This is where we get the list of abstractions\n        validated_abstractions = []\n        # --- Add validation here as needed ---\n        print(f\"Identified {len(validated_abstractions)} abstractions.\")\n        return validated_abstractions\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"abstractions\"] = exec_res # List of {\"name\": str, \"description\": str, \"files\": [int]}\n```\n\nHere's a breakdown:\n\n1.  **`prep(shared)`:**\n    *   This method retrieves the `files` (the code) from the `shared` dictionary. This is the output of the [Codebase Crawling](04_codebase_crawling.md) step.\n    *   It creates an LLM-friendly context string by concatenating the contents of all the files.  This is what we'll feed to the LLM.\n    *   `file_listing_for_prompt` contains a list of file indices and their paths to tell the LLM where the context came from.\n\n2.  **`exec(prep_res)`:**\n    *   This method constructs a prompt for the LLM, telling it to analyze the code and identify the key abstractions. The prompt includes:\n        *   The codebase context (the concatenated code files).\n        *   Instructions on how to format the output (as a YAML list of dictionaries).\n    *   It calls the `call_llm` function (defined in `utils/call_llm.py`) to send the prompt to the LLM.\n    *   It receives the LLM's response, which should be a YAML string containing the list of abstractions, their descriptions, and their relevant file indices.\n    *   The YAML output is validated and parsed to store it in `shared[\"abstractions\"]`\n\n3.  **`post(shared, prep_res, exec_res)`:**\n    *   This method takes the list of abstractions and stores it in the `shared[\"abstractions\"]` variable. This makes the abstractions available to the next node in the flow, which is [Relationship Analysis](06_relationship_analysis.md).\n\n## Code Example: The LLM Prompt\n\nHere's an example of the prompt that we send to the LLM (from `nodes.py`):\n\n```python\nprompt = f\"\"\"\nFor the project `{project_name}`:\n\nCodebase Context:\n{context}\n\nAnalyze the codebase context.\nIdentify the top 5-10 core most important abstractions to help those new to the codebase.\n\nFor each abstraction, provide:\n1. A concise `name`.\n2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\nList of file indices and paths present in the context:\n{file_listing_for_prompt}\n\nFormat the output as a YAML list of dictionaries:\n\n```yaml\n- name: Query Processing\n  description: | \n    Explains what the abstraction does.\n    It's like a central dispatcher routing requests.\n  file_indices:\n    - 0 # path/to/file1.py\n    - 3 # path/to/related.py\n- name: Query Optimization\n  description: |\n    Another core concept, similar to a blueprint for objects.\n  file_indices:\n    - 5 # path/to/another.js\n# ... up to 10 abstractions\n```\"\"\"\n```\n\nThis prompt tells the LLM to:\n\n*   Analyze the code and identify the top 5-10 most important abstractions.\n*   For each abstraction, provide a name, a description, and a list of relevant file indices.\n*   Format the output as a YAML list of dictionaries.\n\nThe prompt is designed to be clear and concise, guiding the LLM to provide the information we need in a structured format.\n\n## Code Example: Validating the LLM Response\n\nAfter the LLM returns its response, we need to validate that it's in the correct format. Here's a simplified example of how we do this (from `nodes.py`):\n\n```python\nyaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\nabstractions = yaml.safe_load(yaml_str)\n\nif not isinstance(abstractions, list):\n    raise ValueError(\"LLM Output is not a list\")\n\nvalidated_abstractions = []\nfor item in abstractions:\n    if not isinstance(item, dict) or not all(k in item for k in [\"name\", \"description\", \"file_indices\"]):\n        raise ValueError(f\"Missing keys in abstraction item: {item}\")\n\n    # --- Add validation here as needed ---\n    validated_abstractions.append({\n        \"name\": item[\"name\"],\n        \"description\": item[\"description\"],\n        \"file_indices\": item[\"file_indices\"]\n    })\n```\n\nThis code checks that:\n\n*   The response is a valid YAML string.\n*   The YAML string represents a list.\n*   Each item in the list is a dictionary with the required keys (\"name\", \"description\", \"file_indices\").\n\nIf the response doesn't pass these checks, we raise a `ValueError` to indicate that there's a problem.\n\n## Using The Abstractions\n\nThe final output of the `IdentifyAbstractions` node is a list of dictionaries, where each dictionary represents an abstraction:\n\n```json\n[\n  {\n    \"name\": \"Query Processing\",\n    \"description\": \"A beginner-friendly explanation of what Query Processing is...\",\n    \"files\": [0, 3]\n  },\n  {\n    \"name\": \"Query Optimization\",\n    \"description\": \"A beginner-friendly explanation of what Query Optimization is...\",\n    \"files\": [5]\n  }\n]\n```\n\nThis list is stored in the `shared[\"abstractions\"]` variable and passed to the next node in the flow. The \"files\" key stores the list of integer indices, corresponding to paths from earlier in Codebase Crawling.\n\n## Why is Abstraction Identification Important?\n\n*   **Focus:** It helps us focus on the most important concepts in the codebase, making the tutorial more manageable and easier to understand.\n*   **Organization:** It provides a structure for the tutorial, allowing us to organize the content around the key abstractions.\n*   **Beginner-Friendliness:** By focusing on abstractions, we can explain the code in a more beginner-friendly way, using high-level concepts and analogies.\n\n## Conclusion\n\nIn this chapter, we've learned about Abstraction Identification, which is the process of analyzing the codebase and identifying the most important concepts to explain in the tutorial. We've seen how we use an LLM to perform this analysis, and how we validate the LLM's response to ensure that it's in the correct format.\n\nIn the next chapter, [Relationship Analysis](06_relationship_analysis.md), we'll explore how we analyze the relationships between these abstractions to understand the overall architecture of the codebase.\n```\n---\n# Chapter 6: Relationship Analysis\n\n```markdown\n# Chapter 6: Relationship Analysis\n\nIn the previous chapter, [Abstraction Identification](05_abstraction_identification.md), we identified the key concepts or \"abstractions\" within a codebase. Now, it's time to connect the dots!  Relationship Analysis helps us understand how these abstractions interact with each other to form the overall system.\n\n## What is Relationship Analysis?\n\nImagine you've identified all the important people in a city: the mayor, the police chief, the school superintendent, and the business owners.  Knowing who they are is useful, but you also need to understand *how they interact*.  Who reports to whom? Who depends on whom?  What are the power dynamics?\n\nRelationship Analysis is the process of understanding how the identified abstractions interact with each other. This involves using an LLM to analyze the code and determine the key relationships between the abstractions. It's like a network analyst mapping out the connections between different computers in a network to understand how data flows between them.\n\nWithout Relationship Analysis, we'd have a collection of isolated concepts, but we wouldn't understand how they work together to achieve the project's goals. It provides a crucial high-level summary and interaction diagram to give beginners a big picture understanding.\n\n## Key Concepts\n\n*   **Relationships:** The connections or interactions between abstractions.  Examples could be \"uses,\" \"manages,\" \"inherits from,\" or \"depends on.\"\n*   **Directed Graph:** A visual representation of the relationships between abstractions, where the direction of the arrow indicates the flow of interaction. We use a Mermaid diagram to represent this.\n*   **Project Summary:**  A high-level description of the project's purpose and functionality, based on the identified abstractions and their relationships.\n*   **File Indices:** A list of index numbers which corresponds to file paths used for the LLM to refer back to the code.\n*   **Abstraction Index:** The numbered index for a given abstraction.\n\n## How Relationship Analysis Works\n\nThe Relationship Analysis process is handled by the `AnalyzeRelationships` node in our flow.\n\nLet's look at the simplified steps in `nodes.py`:\n\n```python\nclass AnalyzeRelationships(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"] # Now contains 'files' list of indices\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n\n        # Create context with abstraction names, indices, descriptions, and relevant file snippets\n        context = \"Identified Abstractions:\\n\"\n        all_relevant_indices = set()\n        abstraction_info_for_prompt = []\n        for i, abstr in enumerate(abstractions):\n            # Use 'files' which contains indices directly\n            file_indices_str = \", \".join(map(str, abstr['files']))\n            info_line = f\"- Index {i}: {abstr['name']} (Relevant file indices: [{file_indices_str}])\\n  Description: {abstr['description']}\"\n            context += info_line + \"\\n\"\n            abstraction_info_for_prompt.append(f\"{i} # {abstr['name']}\")\n            all_relevant_indices.update(abstr['files'])\n\n        context += \"\\nRelevant File Snippets (Referenced by Index and Path):\\n\"\n        # Get content for relevant files using helper\n        relevant_files_content_map = get_content_for_indices(\n            files_data,\n            sorted(list(all_relevant_indices))\n        )\n        # Format file content for context\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path} ---\\n{content}\"\n            for idx_path, content in relevant_files_content_map.items()\n        )\n        context += file_context_str\n\n        return context, \"\\n\".join(abstraction_info_for_prompt), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, abstraction_listing, project_name = prep_res  # Unpack project name\n        print(\"Analyzing relationships using LLM...\")\n        prompt = f\"\"\"\nBased on the following abstractions and relevant code snippets from the project `{project_name}`:\n\nList of Abstraction Indices and Names:\n{abstraction_listing}\n\nContext (Abstractions, Descriptions, Code):\n{context}\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        relationships_data = yaml.safe_load(yaml_str)\n\n        if not isinstance(relationships_data, dict) or not all(k in relationships_data for k in [\"summary\", \"relationships\"]):\n            raise ValueError(\"LLM output is not a dict or missing keys ('summary', 'relationships')\")\n        if not isinstance(relationships_data[\"summary\"], str):\n             raise ValueError(\"summary is not a string\")\n        if not isinstance(relationships_data[\"relationships\"], list):\n             raise ValueError(\"relationships is not a list\")\n\n        # Validate relationships structure\n        validated_relationships = []\n        num_abstractions = len(abstraction_listing.split('\\n'))\n        for rel in relationships_data[\"relationships\"]:\n             # Check for 'label' key\n             if not isinstance(rel, dict) or not all(k in rel for k in [\"from_abstraction\", \"to_abstraction\", \"label\"]):\n                  raise ValueError(f\"Missing keys (expected from_abstraction, to_abstraction, label) in relationship item: {rel}\")\n             # Validate 'label' is a string\n             if not isinstance(rel[\"label\"], str):\n                  raise ValueError(f\"Relationship label is not a string: {rel}\")\n\n             # Validate indices\n             try:\n                 from_idx = int(str(rel[\"from_abstraction\"]).split('#')[0].strip())\n                 to_idx = int(str(rel[\"to_abstraction\"]).split('#')[0].strip())\n                 if not (0 <= from_idx < num_abstractions and 0 <= to_idx < num_abstractions):\n                      raise ValueError(f\"Invalid index in relationship: from={from_idx}, to={to_idx}. Max index is {num_abstractions-1}.\")\n                 validated_relationships.append({\n                     \"from\": from_idx,\n                     \"to\": to_idx,\n                     \"label\": rel[\"label\"] \n                 })\n             except (ValueError, TypeError):\n                  raise ValueError(f\"Could not parse indices from relationship: {rel}\")\n\n        print(\"Generated project summary and relationship details.\")\n        return {\n            \"summary\": relationships_data[\"summary\"],\n            \"details\": validated_relationships # Store validated, index-based relationships\n        }\n\n\n    def post(self, shared, prep_res, exec_res):\n        # Structure is now {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        shared[\"relationships\"] = exec_res\n```\n\nHere's a breakdown:\n\n1.  **`prep(shared)`:**\n    *   This method retrieves the `abstractions` and `files` from the `shared` dictionary.  The `abstractions` are the output of the [Abstraction Identification](05_abstraction_identification.md) step. The files are the output from [Codebase Crawling](04_codebase_crawling.md).\n    *   It creates a context string for the LLM.  This context includes:\n        *   A list of the identified abstractions, including their names, descriptions, and file indices.\n        *   The content of the relevant code files.\n2.  **`exec(prep_res)`:**\n    *   This method constructs a prompt for the LLM, telling it to analyze the abstractions and their relationships.  The prompt includes:\n        *   The context string created in the `prep` method.\n        *   Instructions on how to format the output (as a YAML string containing the project summary and the relationships between abstractions).\n    *   It calls the `call_llm` function (defined in `utils/call_llm.py`) to send the prompt to the LLM.\n    *   It receives the LLM's response, which should be a YAML string containing the project summary and the relationships between abstractions.\n    *   The output is validated and parsed to store it in `shared[\"relationships\"]`.\n3.  **`post(shared, prep_res, exec_res)`:**\n    *   This method takes the project summary and relationships and stores them in the `shared[\"relationships\"]` variable. This makes the relationships available to the next node in the flow, which is [Order Chapters](07_llm_interaction.md).\n\n## Code Example: The LLM Prompt\n\nHere's an example of the prompt that we send to the LLM (from `nodes.py`):\n\n```python\nprompt = f\"\"\"\nBased on the following abstractions and relevant code snippets from the project `{project_name}`:\n\nList of Abstraction Indices and Names:\n{abstraction_listing}\n\nContext (Abstractions, Descriptions, Code):\n{context}\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n\"\"\"\n```\n\nThis prompt tells the LLM to:\n\n*   Provide a high-level summary of the project's purpose and functionality.\n*   Identify the key relationships between the abstractions.\n*   For each relationship, specify the source abstraction, the target abstraction, and a brief label describing the relationship.\n*   Format the output as a YAML string.\n\n## Code Example: The Output of Relationship Analysis\n\nThe `exec` function returns a dictionary that contains the output of the `AnalyzeRelationships` node:\n\n```json\n{\n  \"summary\": \"This project is a web framework that provides a simple and efficient way to build APIs.\",\n  \"details\": [\n    {\n      \"from\": 0,\n      \"to\": 1,\n      \"label\": \"Manages\"\n    },\n    {\n      \"from\": 2,\n      \"to\": 0,\n      \"label\": \"Provides config\"\n    }\n  ]\n}\n```\n\nHere's a breakdown:\n\n*   `\"summary\"`: A short, beginner-friendly explanation of the project's goal. This is used in the introduction of the tutorial.\n*   `\"details\"`: A list of the relationships between abstractions.\n    *   `\"from\"`: The index of the source abstraction.\n    *   `\"to\"`: The index of the target abstraction.\n    *   `\"label\"`: A short description of the relationship.\n\n## Using The Relationships\n\nThe final output of the `AnalyzeRelationships` node is a dictionary containing the summary and relationship details. This dictionary is stored in the `shared[\"relationships\"]` variable and is used by later nodes.\n\nFor example, the `OrderChapters` node uses the relationships to determine the best order to present the abstractions in the tutorial.  The `CombineTutorial` node uses the relationships to generate a Mermaid diagram visualizing the connections between the abstractions, and to generate the tutorial index page.\n\n## Why is Relationship Analysis Important?\n\n*   **Context:** It provides a high-level overview of the project's purpose and functionality.\n*   **Understanding:** It helps to understand how the different abstractions interact with each other to form the overall system.\n*   **Visualization:** It allows us to visualize the relationships between abstractions, making it easier to grasp the overall architecture of the codebase.\n*   **Ordering:** It informs the ordering of chapters in the tutorial, ensuring that concepts are presented in a logical and easy-to-understand sequence.\n\n## Conclusion\n\nIn this chapter, we've learned about Relationship Analysis, which is the process of understanding how the identified abstractions interact with each other. We've seen how we use an LLM to perform this analysis, and how we use the results to generate a project summary and visualize the relationships between abstractions.\n\nIn the next chapter, [LLM Interaction](07_llm_interaction.md), we'll explore how we interact with the LLM to generate the actual content for the tutorial chapters.\n```\n\nRelevant Code Snippets:\n--- File: nodes.py ---\nimport os\nimport yaml\nfrom pocketflow import Node, BatchNode\nfrom utils.crawl_github_files import crawl_github_files\nfrom utils.call_llm import call_llm # Assuming you have this utility\nfrom utils.crawl_local_files import crawl_local_files\nfrom converter.md_to_html import convert_all_md_to_html\n\n\n# Helper to get content for specific file indices\ndef get_content_for_indices(files_data, indices):\n    content_map = {}\n    for i in indices:\n        if 0 <= i < len(files_data):\n            path, content = files_data[i]\n            content_map[f\"{i} # {path}\"] = content # Use index + path as key for context\n    return content_map\n\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"local_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        if not project_name:\n            # Basic name derivation from URL or directory\n            if repo_url:\n                project_name = repo_url.split('/')[-1].replace('.git', '')\n            else:\n                project_name = os.path.basename(os.path.abspath(local_dir))\n            shared[\"project_name\"] = project_name\n\n        # Get file patterns directly from shared\n        include_patterns = shared[\"include_patterns\"]\n        exclude_patterns = shared[\"exclude_patterns\"]\n        max_file_size = shared[\"max_file_size\"]\n\n        return {\n            \"repo_url\": repo_url,\n            \"local_dir\": local_dir,\n            \"token\": shared.get(\"github_token\"),\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns,\n            \"max_file_size\": max_file_size,\n            \"use_relative_paths\": True\n        }\n\n    def exec(self, prep_res):\n        if prep_res[\"repo_url\"]:\n            print(f\"Crawling repository: {prep_res['repo_url']}...\")\n            result = crawl_github_files(\n                repo_url=prep_res[\"repo_url\"],\n                token=prep_res[\"token\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        else:\n            print(f\"Crawling directory: {prep_res['local_dir']}...\")\n            result = crawl_local_files(\n                directory=prep_res[\"local_dir\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n            \n        # Convert dict to list of tuples: [(path, content), ...]\n        files_list = list(result.get(\"files\", {}).items())\n        print(f\"Fetched {len(files_list)} files.\")\n        return files_list\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"files\"] = exec_res # List of (path, content) tuples\n\nclass IdentifyAbstractions(Node):\n    def prep(self, shared):\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        \n        # Helper to create context from files, respecting limits (basic example)\n        def create_llm_context(files_data):\n            context = \"\"\n            file_info = [] # Store tuples of (index, path)\n            for i, (path, content) in enumerate(files_data):\n                entry = f\"--- File Index {i}: {path} ---\\n{content}\\n\\n\"\n                context += entry\n                file_info.append((i, path))\n\n            return context, file_info # file_info is list of (index, path)\n\n        context, file_info = create_llm_context(files_data)\n        # Format file info for the prompt (comment is just a hint for LLM)\n        file_listing_for_prompt = \"\\n\".join([f\"- {idx} # {path}\" for idx, path in file_info])\n        return context, file_listing_for_prompt, len(files_data), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, file_listing_for_prompt, file_count, project_name = prep_res  # Unpack project name\n        print(\"Identifying abstractions using LLM...\")\n        prompt = f\"\"\"\nFor the project `{project_name}`:\n\nCodebase Context:\n{context}\n\nAnalyze the codebase context.\nIdentify the top 5-10 core most important abstractions to help those new to the codebase.\n\nFor each abstraction, provide:\n1. A concise `name`.\n2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\nList of file indices and paths present in the context:\n{file_listing_for_prompt}\n\nFormat the output as a YAML list of dictionaries:\n\n```yaml\n- name: Query Processing\n  description: | \n    Explains what the abstraction does.\n    It's like a central dispatcher routing requests.\n  file_indices:\n    - 0 # path/to/file1.py\n    - 3 # path/to/related.py\n- name: Query Optimization\n  description: |\n    Another core concept, similar to a blueprint for objects.\n  file_indices:\n    - 5 # path/to/another.js\n# ... up to 10 abstractions\n```\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        abstractions = yaml.safe_load(yaml_str)\n\n        if not isinstance(abstractions, list):\n            raise ValueError(\"LLM Output is not a list\")\n\n        validated_abstractions = []\n        for item in abstractions:\n            if not isinstance(item, dict) or not all(k in item for k in [\"name\", \"description\", \"file_indices\"]):\n                raise ValueError(f\"Missing keys in abstraction item: {item}\")\n            if not isinstance(item[\"description\"], str):\n                 raise ValueError(f\"description is not a string in item: {item}\")\n            if not isinstance(item[\"file_indices\"], list):\n                 raise ValueError(f\"file_indices is not a list in item: {item}\")\n\n            # Validate indices\n            validated_indices = []\n            for idx_entry in item[\"file_indices\"]:\n                 try:\n                     if isinstance(idx_entry, int):\n                         idx = idx_entry\n                     elif isinstance(idx_entry, str) and '#' in idx_entry:\n                          idx = int(idx_entry.split('#')[0].strip())\n                     else:\n                          idx = int(str(idx_entry).strip())\n\n                     if not (0 <= idx < file_count):\n                         raise ValueError(f\"Invalid file index {idx} found in item {item['name']}. Max index is {file_count - 1}.\")\n                     validated_indices.append(idx)\n                 except (ValueError, TypeError):\n                      raise ValueError(f\"Could not parse index from entry: {idx_entry} in item {item['name']}\")\n\n            item[\"files\"] = sorted(list(set(validated_indices)))\n            # Store only the required fields\n            validated_abstractions.append({\n                \"name\": item[\"name\"],\n                \"description\": item[\"description\"],\n                \"files\": item[\"files\"]\n            })\n\n        print(f\"Identified {len(validated_abstractions)} abstractions.\")\n        return validated_abstractions\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"abstractions\"] = exec_res # List of {\"name\": str, \"description\": str, \"files\": [int]}\n\nclass AnalyzeRelationships(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"] # Now contains 'files' list of indices\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n\n        # Create context with abstraction names, indices, descriptions, and relevant file snippets\n        context = \"Identified Abstractions:\\n\"\n        all_relevant_indices = set()\n        abstraction_info_for_prompt = []\n        for i, abstr in enumerate(abstractions):\n            # Use 'files' which contains indices directly\n            file_indices_str = \", \".join(map(str, abstr['files']))\n            info_line = f\"- Index {i}: {abstr['name']} (Relevant file indices: [{file_indices_str}])\\n  Description: {abstr['description']}\"\n            context += info_line + \"\\n\"\n            abstraction_info_for_prompt.append(f\"{i} # {abstr['name']}\")\n            all_relevant_indices.update(abstr['files'])\n\n        context += \"\\nRelevant File Snippets (Referenced by Index and Path):\\n\"\n        # Get content for relevant files using helper\n        relevant_files_content_map = get_content_for_indices(\n            files_data,\n            sorted(list(all_relevant_indices))\n        )\n        # Format file content for context\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path} ---\\n{content}\"\n            for idx_path, content in relevant_files_content_map.items()\n        )\n        context += file_context_str\n\n        return context, \"\\n\".join(abstraction_info_for_prompt), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, abstraction_listing, project_name = prep_res  # Unpack project name\n        print(\"Analyzing relationships using LLM...\")\n        prompt = f\"\"\"\nBased on the following abstractions and relevant code snippets from the project `{project_name}`:\n\nList of Abstraction Indices and Names:\n{abstraction_listing}\n\nContext (Abstractions, Descriptions, Code):\n{context}\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        relationships_data = yaml.safe_load(yaml_str)\n\n        if not isinstance(relationships_data, dict) or not all(k in relationships_data for k in [\"summary\", \"relationships\"]):\n            raise ValueError(\"LLM output is not a dict or missing keys ('summary', 'relationships')\")\n        if not isinstance(relationships_data[\"summary\"], str):\n             raise ValueError(\"summary is not a string\")\n        if not isinstance(relationships_data[\"relationships\"], list):\n             raise ValueError(\"relationships is not a list\")\n\n        # Validate relationships structure\n        validated_relationships = []\n        num_abstractions = len(abstraction_listing.split('\\n'))\n        for rel in relationships_data[\"relationships\"]:\n             # Check for 'label' key\n             if not isinstance(rel, dict) or not all(k in rel for k in [\"from_abstraction\", \"to_abstraction\", \"label\"]):\n                  raise ValueError(f\"Missing keys (expected from_abstraction, to_abstraction, label) in relationship item: {rel}\")\n             # Validate 'label' is a string\n             if not isinstance(rel[\"label\"], str):\n                  raise ValueError(f\"Relationship label is not a string: {rel}\")\n\n             # Validate indices\n             try:\n                 from_idx = int(str(rel[\"from_abstraction\"]).split('#')[0].strip())\n                 to_idx = int(str(rel[\"to_abstraction\"]).split('#')[0].strip())\n                 if not (0 <= from_idx < num_abstractions and 0 <= to_idx < num_abstractions):\n                      raise ValueError(f\"Invalid index in relationship: from={from_idx}, to={to_idx}. Max index is {num_abstractions-1}.\")\n                 validated_relationships.append({\n                     \"from\": from_idx,\n                     \"to\": to_idx,\n                     \"label\": rel[\"label\"] \n                 })\n             except (ValueError, TypeError):\n                  raise ValueError(f\"Could not parse indices from relationship: {rel}\")\n\n        print(\"Generated project summary and relationship details.\")\n        return {\n            \"summary\": relationships_data[\"summary\"],\n            \"details\": validated_relationships # Store validated, index-based relationships\n        }\n\n\n    def post(self, shared, prep_res, exec_res):\n        # Structure is now {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        shared[\"relationships\"] = exec_res\n\nclass OrderChapters(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"]\n        relationships = shared[\"relationships\"]\n        project_name = shared[\"project_name\"]  # Get project name\n\n        # Prepare context for the LLM\n        abstraction_info_for_prompt = []\n        for i, a in enumerate(abstractions):\n            abstraction_info_for_prompt.append(f\"- {i} # {a['name']}\")\n        abstraction_listing = \"\\n\".join(abstraction_info_for_prompt)\n\n        context = f\"Project Summary:\\n{relationships['summary']}\\n\\n\"\n        context += \"Relationships (Indices refer to abstractions above):\\n\"\n        for rel in relationships['details']:\n             from_name = abstractions[rel['from']]['name']\n             to_name = abstractions[rel['to']]['name']\n             # Use 'label' instead of 'desc'\n             context += f\"- From {rel['from']} ({from_name}) to {rel['to']} ({to_name}): {rel['label']}\\n\"\n\n        return abstraction_listing, context, len(abstractions), project_name\n\n    def exec(self, prep_res):\n        abstraction_listing, context, num_abstractions, project_name = prep_res\n        print(\"Determining chapter order using LLM...\")\n        prompt = f\"\"\"\nGiven the following project abstractions and their relationships for the project ```` {project_name} ````:\n\nAbstractions (Index # Name):\n{abstraction_listing}\n\nContext about relationships and project summary:\n{context}\n\nIf you are going to make a tutorial for ```` {project_name} ````, what is the best order to explain these abstractions, from first to last?\nIdeally, first explain those that are the most important or foundational, perhaps user-facing concepts or entry points. Then move to more detailed, lower-level implementation details or supporting concepts.\n\nOutput the ordered list of abstraction indices, including the name in a comment for clarity. Use the format `idx # AbstractionName`.\n\n```yaml\n- 2 # FoundationalConcept\n- 0 # CoreClassA\n- 1 # CoreClassB (uses CoreClassA)\n- ...\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        # Rely on Node's built-in retry/fallback\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        ordered_indices_raw = yaml.safe_load(yaml_str)\n\n        if not isinstance(ordered_indices_raw, list):\n            raise ValueError(\"LLM output is not a list\")\n\n        ordered_indices = []\n        seen_indices = set()\n        for entry in ordered_indices_raw:\n            try:\n                 if isinstance(entry, int):\n                     idx = entry\n                 elif isinstance(entry, str) and '#' in entry:\n                      idx = int(entry.split('#')[0].strip())\n                 else:\n                      idx = int(str(entry).strip())\n\n                 if not (0 <= idx < num_abstractions):\n                      raise ValueError(f\"Invalid index {idx} in ordered list. Max index is {num_abstractions-1}.\")\n                 if idx in seen_indices:\n                     raise ValueError(f\"Duplicate index {idx} found in ordered list.\")\n                 ordered_indices.append(idx)\n                 seen_indices.add(idx)\n\n            except (ValueError, TypeError):\n                 raise ValueError(f\"Could not parse index from ordered list entry: {entry}\")\n\n        # Check if all abstractions are included\n        if len(ordered_indices) != num_abstractions:\n             raise ValueError(f\"Ordered list length ({len(ordered_indices)}) does not match number of abstractions ({num_abstractions}). Missing indices: {set(range(num_abstractions)) - seen_indices}\")\n\n        print(f\"Determined chapter order (indices): {ordered_indices}\")\n        return ordered_indices # Return the list of indices\n\n    def post(self, shared, prep_res, exec_res):\n        # exec_res is already the list of ordered indices\n        shared[\"chapter_order\"] = exec_res # List of indices\n\nclass WriteChapters(BatchNode):\n    def prep(self, shared):\n        chapter_order = shared[\"chapter_order\"] # List of indices\n        abstractions = shared[\"abstractions\"]   # List of dicts, now using 'files' with indices\n        files_data = shared[\"files\"]\n        # Get already written chapters to provide context\n        # We store them temporarily during the batch run, not in shared memory yet\n        # The 'previous_chapters_summary' will be built progressively in the exec context\n        self.chapters_written_so_far = [] # Use instance variable for temporary storage across exec calls\n\n        # Create a complete list of all chapters\n        all_chapters = []\n        chapter_filenames = {} # Store chapter filename mapping for linking\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                chapter_num = i + 1\n                chapter_name = abstractions[abstraction_index][\"name\"]\n                # Create safe filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in chapter_name).lower()\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                # Format with link\n                all_chapters.append(f\"{chapter_num}. [{chapter_name}]({filename})\")\n                # Store mapping of chapter index to filename for linking\n                chapter_filenames[abstraction_index] = {\"num\": chapter_num, \"name\": chapter_name, \"filename\": filename}\n        \n        # Create a formatted string with all chapters\n        full_chapter_listing = \"\\n\".join(all_chapters)\n\n        items_to_process = []\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                abstraction_details = abstractions[abstraction_index]\n                # Use 'files' (list of indices) directly\n                related_file_indices = abstraction_details.get(\"files\", [])\n                # Get content using helper, passing indices\n                related_files_content_map = get_content_for_indices(files_data, related_file_indices)\n                \n                # Get previous chapter info for transitions\n                prev_chapter = None\n                if i > 0:\n                    prev_idx = chapter_order[i-1]\n                    prev_chapter = chapter_filenames[prev_idx]\n                \n                # Get next chapter info for transitions\n                next_chapter = None\n                if i < len(chapter_order) - 1:\n                    next_idx = chapter_order[i+1]\n                    next_chapter = chapter_filenames[next_idx]\n\n                items_to_process.append({\n                    \"chapter_num\": i + 1,\n                    \"abstraction_index\": abstraction_index,\n                    \"abstraction_details\": abstraction_details,\n                    \"related_files_content_map\": related_files_content_map,\n                    \"project_name\": shared[\"project_name\"],  # Add project name\n                    \"full_chapter_listing\": full_chapter_listing,  # Add the full chapter listing\n                    \"chapter_filenames\": chapter_filenames,  # Add chapter filenames mapping\n                    \"prev_chapter\": prev_chapter,  # Add previous chapter info\n                    \"next_chapter\": next_chapter,  # Add next chapter info\n                    # previous_chapters_summary will be added dynamically in exec\n                })\n            else:\n                print(f\"Warning: Invalid abstraction index {abstraction_index} in chapter_order. Skipping.\")\n\n        print(f\"Preparing to write {len(items_to_process)} chapters...\")\n        return items_to_process # Iterable for BatchNode\n\n    def exec(self, item):\n        # This runs for each item prepared above\n        abstraction_name = item[\"abstraction_details\"][\"name\"]\n        chapter_num = item[\"chapter_num\"]\n        project_name = item.get(\"project_name\")  # Get from item\n        print(f\"Writing chapter {chapter_num} for: {abstraction_name} using LLM...\")\n\n        # Prepare file context string from the map\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path.split('# ')[1] if '# ' in idx_path else idx_path} ---\\n{content}\"\n            for idx_path, content in item[\"related_files_content_map\"].items()\n        )\n\n        # Get summary of chapters written *before* this one\n        # Use the temporary instance variable\n        previous_chapters_summary = \"\\n---\\n\".join(self.chapters_written_so_far)\n\n\n        prompt = f\"\"\"\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `{project_name}` about the concept: \"{abstraction_name}\". This is Chapter {chapter_num}.\n\nConcept Details:\n- Description:\n{item[\"abstraction_details\"][\"description\"]}\n\nComplete Tutorial Structure:\n{item[\"full_chapter_listing\"]}\n\nContext from previous chapters (summary):\n{previous_chapters_summary if previous_chapters_summary else \"This is the first chapter.\"}\n\nRelevant Code Snippets:\n{file_context_str if file_context_str else \"No specific code snippets provided for this abstraction.\"}\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter {chapter_num}: {abstraction_name}`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n\"\"\"\n        chapter_content = call_llm(prompt)\n        # Basic validation/cleanup\n        actual_heading = f\"# Chapter {chapter_num}: {abstraction_name}\"\n        if not chapter_content.strip().startswith(f\"# Chapter {chapter_num}\"):\n             # Add heading if missing or incorrect, trying to preserve content\n             lines = chapter_content.strip().split('\\n')\n             if lines and lines[0].strip().startswith(\"#\"): # If there's some heading, replace it\n                 lines[0] = actual_heading\n                 chapter_content = \"\\n\".join(lines)\n             else: # Otherwise, prepend it\n                 chapter_content = f\"{actual_heading}\\n\\n{chapter_content}\"\n\n        # Add the generated content to our temporary list for the next iteration's context\n        self.chapters_written_so_far.append(chapter_content)\n\n        return chapter_content # Return the Markdown string\n\n    def post(self, shared, prep_res, exec_res_list):\n        # exec_res_list contains the generated Markdown for each chapter, in order\n        shared[\"chapters\"] = exec_res_list\n        # Clean up the temporary instance variable\n        del self.chapters_written_so_far\n        print(f\"Finished writing {len(exec_res_list)} chapters.\")\n\nclass CombineTutorial(Node):\n    def prep(self, shared):\n        project_name = shared[\"project_name\"]\n        output_base_dir = shared.get(\"output_dir\", \"output\") # Default output dir\n        output_path = os.path.join(output_base_dir, project_name)\n        repo_url = shared[\"repo_url\"]  # Get the repository URL\n\n        # Use 'label' from relationships_data['details']\n        relationships_data = shared[\"relationships\"] # {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        chapter_order = shared[\"chapter_order\"] # indices\n        abstractions = shared[\"abstractions\"]   # list of dicts\n        chapters_content = shared[\"chapters\"]   # list of strings\n\n        # --- Generate Mermaid Diagram ---\n        mermaid_lines = [\"flowchart TD\"]\n        # Add nodes for each abstraction\n        for i, abstr in enumerate(abstractions):\n            # Sanitize name for Mermaid ID and label\n            node_id = f\"A{i}\"\n            sanitized_name = abstr['name'].replace('\"', '')\n            node_label = sanitized_name # Using sanitized name only, no index\n            mermaid_lines.append(f'    {node_id}[\"{node_label}\"]')\n        # Add edges for relationships using 'label'\n        for rel in relationships_data['details']:\n            from_node_id = f\"A{rel['from']}\"\n            to_node_id = f\"A{rel['to']}\"\n            # Sanitize 'label' for edge label\n            edge_label = rel['label'].replace('\"', '').replace('\\n', ' ') # Basic sanitization\n            # Limit edge label length for readability (optional, but good for diagrams)\n            max_label_len = 30 # Make it shorter for labels\n            if len(edge_label) > max_label_len:\n                edge_label = edge_label[:max_label_len-3] + \"...\"\n            mermaid_lines.append(f'    {from_node_id} -- \"{edge_label}\" --> {to_node_id}')\n\n        mermaid_diagram = \"\\n\".join(mermaid_lines)\n        # --- End Mermaid ---\n\n\n        # Prepare index.md content\n        index_content = f\"# Tutorial: {project_name}\\n\\n\"\n        index_content += f\"{relationships_data['summary']}\\n\\n\"\n        index_content += f\"**Source Repository:** [{repo_url}]({repo_url})\\n\\n\"\n\n        # Add Mermaid diagram for relationships\n        index_content += \"```mermaid\\n\"\n        index_content += mermaid_diagram + \"\\n\"\n        index_content += \"```\\n\\n\"\n\n        index_content += \"## Chapters\\n\\n\"\n\n        chapter_files = []\n        # Generate chapter links based on the determined order\n        for i, abstraction_index in enumerate(chapter_order):\n            # Ensure index is valid and we have content for it\n            if 0 <= abstraction_index < len(abstractions) and i < len(chapters_content):\n                abstraction_name = abstractions[abstraction_index][\"name\"]\n                # Sanitize name for filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in abstraction_name).lower()\n                # Use chapter number (i+1) for ordering filename\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                index_content += f\"{i+1}. [{abstraction_name}]({filename})\\n\"\n                \n                # Add attribution to chapter content\n                chapter_content = chapters_content[i]\n                if not chapter_content.endswith(\"\\n\\n\"):\n                    chapter_content += \"\\n\\n\"\n                chapter_content += \"---\\n\\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\"\n                \n                # Store filename and corresponding content\n                chapter_files.append({\"filename\": filename, \"content\": chapter_content})\n            else:\n                 print(f\"Warning: Mismatch between chapter order, abstractions, or content at index {i} (abstraction index {abstraction_index}). Skipping file generation for this entry.\")\n\n        # Add attribution to index content\n        index_content += \"\\n\\n---\\n\\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\"\n\n        return {\n            \"output_path\": output_path,\n            \"index_content\": index_content,\n            \"chapter_files\": chapter_files # List of {\"filename\": str, \"content\": str}\n        }\n\n    def exec(self, prep_res):\n        output_path = prep_res[\"output_path\"]\n        index_content = prep_res[\"index_content\"]\n        chapter_files = prep_res[\"chapter_files\"]\n\n        print(f\"Combining tutorial into directory: {output_path}\")\n        # Rely on Node's built-in retry/fallback\n        os.makedirs(output_path, exist_ok=True)\n\n        # Write index.md\n        index_filepath = os.path.join(output_path, \"index.md\")\n        with open(index_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(index_content)\n        print(f\"  - Wrote {index_filepath}\")\n\n        # Write chapter files\n        for chapter_info in chapter_files:\n            chapter_filepath = os.path.join(output_path, chapter_info[\"filename\"])\n            with open(chapter_filepath, \"w\", encoding=\"utf-8\") as f:\n                f.write(chapter_info[\"content\"])\n            print(f\"  - Wrote {chapter_filepath}\")\n\n        return output_path # Return the final path\n\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"final_output_dir\"] = exec_res # Store the output path\n        print(f\"\\nTutorial generation complete! Files are in: {exec_res}\")\n        convert_all_md_to_html(exec_res) # Convert all Markdown files to HTML\n        \n\n--- File: utils/call_llm.py ---\nfrom google import genai\nimport os\nimport logging\nimport json\nfrom datetime import datetime\n\n# Configure logging\nlog_directory = os.getenv(\"LOG_DIR\", \"logs\")\nos.makedirs(log_directory, exist_ok=True)\nlog_file = os.path.join(log_directory, f\"llm_calls_{datetime.now().strftime('%Y%m%d')}.log\")\n\n# Set up logger\nlogger = logging.getLogger(\"llm_logger\")\nlogger.setLevel(logging.INFO)\nlogger.propagate = False  # Prevent propagation to root logger\nfile_handler = logging.FileHandler(log_file)\nfile_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\nlogger.addHandler(file_handler)\n\n# Simple cache configuration\ncache_file = \"llm_cache.json\"\n\n# By default, we Google Gemini 2.5 pro, as it shows great performance for code understanding\ndef call_llm(prompt: str, use_cache: bool = True) -> str:\n    # Log the prompt\n    logger.info(f\"PROMPT: {prompt}\")\n    \n    # Check cache if enabled\n    if use_cache:\n        # Load cache from disk\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, 'r') as f:\n                    cache = json.load(f)\n            except:\n                logger.warning(f\"Failed to load cache, starting with empty cache\")\n        \n        # Return from cache if exists\n        if prompt in cache:\n            logger.info(f\"RESPONSE: {cache[prompt]}\")\n            return cache[prompt]\n    \n    # Call the LLM if not in cache or cache disabled\n    client = genai.Client(\n        vertexai=True, \n        # TODO: change to your own project id and location\n        project=os.getenv(\"GEMINI_PROJECT_ID\", \"your-project-id\"),\n        location=os.getenv(\"GEMINI_LOCATION\", \"us-central1\")\n    )\n    # You can comment the previous line and use the AI Studio key instead:\n    # client = genai.Client(\n    #     api_key=os.getenv(\"GEMINI_API_KEY\", \"your-api_key\"),\n    # )\n    model = os.getenv(\"GEMINI_MODEL\", \"gemini-2.5-pro-exp-03-25\")\n    response = client.models.generate_content(\n        model=model,\n        contents=[prompt]\n    )\n    response_text = response.text\n    \n    # Log the response\n    logger.info(f\"RESPONSE: {response_text}\")\n    \n    # Update cache if enabled\n    if use_cache:\n        # Load cache again to avoid overwrites\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, 'r') as f:\n                    cache = json.load(f)\n            except:\n                pass\n        \n        # Add to cache and save\n        cache[prompt] = response_text\n        try:\n            with open(cache_file, 'w') as f:\n                json.dump(cache, f)\n        except Exception as e:\n            logger.error(f\"Failed to save cache: {e}\")\n    \n    return response_text\n\n# # Use Anthropic Claude 3.7 Sonnet Extended Thinking\n# def call_llm(prompt, use_cache: bool = True):\n#     from anthropic import Anthropic\n#     client = Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\", \"your-api-key\"))\n#     response = client.messages.create(\n#         model=\"claude-3-7-sonnet-20250219\",\n#         max_tokens=21000,\n#         thinking={\n#             \"type\": \"enabled\",\n#             \"budget_tokens\": 20000\n#         },\n#         messages=[\n#             {\"role\": \"user\", \"content\": prompt}\n#         ]\n#     )\n#     return response.content[1].text\n\n# # Use OpenAI o1\n# def call_llm(prompt, use_cache: bool = True):    \n#     from openai import OpenAI\n#     client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n#     r = client.chat.completions.create(\n#         model=\"o1\",\n#         messages=[{\"role\": \"user\", \"content\": prompt}],\n#         response_format={\n#             \"type\": \"text\"\n#         },\n#         reasoning_effort=\"medium\",\n#         store=False\n#     )\n#     return r.choices[0].message.content\n\nif __name__ == \"__main__\":\n    test_prompt = \"Hello, how are you?\"\n    \n    # First call - should hit the API\n    print(\"Making call...\")\n    response1 = call_llm(test_prompt, use_cache=False)\n    print(f\"Response: {response1}\")\n    \n\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 7: LLM Interaction`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "```markdown\n# Chapter 7: LLM Interaction\n\nIn the previous chapter, [Relationship Analysis](06_relationship_analysis.md), we learned how to understand the connections between the key concepts in a codebase. Now, we're going to focus on how we use a Large Language Model (LLM) to actually *generate* the content for our tutorial! This is where the magic happens!\n\n## What is LLM Interaction?\n\nImagine you're a writer, but you have a super-smart AI assistant that can help you research, outline, and even write entire sections of your book. This assistant is the LLM Interaction module. It's the part of our system that talks to the LLM, sends it instructions, and receives its responses.\n\nThe LLM Interaction module handles all calls to the Large Language Model (LLM). This includes:\n\n*   **Prompt Engineering:** Carefully crafting the instructions we send to the LLM to get the best possible results.\n*   **Caching Responses:** Saving the LLM's responses so we don't have to ask it the same question multiple times. This saves time and money.\n*   **Logging Prompts and Responses:** Keeping a record of everything we send to and receive from the LLM for debugging and analysis.\n*   **Providing a Consistent Interface:** Making it easy for other modules to interact with the LLM without having to worry about the details of how the LLM works.\n\nThink of it as a dedicated interpreter translating human instructions into machine-understandable code and back again! Without the LLM Interaction module, we'd have no way to leverage the power of LLMs to generate our tutorials.\n\n## Key Concepts\n\n*   **Prompt:** The instruction or question we send to the LLM. It's like telling your AI assistant what you want it to do.\n*   **Response:** The output we receive from the LLM. It's like your AI assistant's answer to your question.\n*   **Caching:** Storing the LLM's responses so we can reuse them later. This is like remembering the answer to a question so you don't have to look it up again.\n*   **Logging:** Recording all the interactions with the LLM for debugging and analysis. This is like keeping a notebook of all your conversations with your AI assistant.\n*   **API Key:** A secret code that allows us to access the LLM. It's like the password you need to log into your AI assistant.\n\n## How LLM Interaction Works\n\nThe LLM Interaction is handled by the `call_llm` function in `utils/call_llm.py`. All LLM Interaction nodes will call this central function.\n\nHere's a simplified view of how it works:\n\n```python\nfrom google import genai\nimport os\nimport logging\nimport json\nfrom datetime import datetime\n\n# Configure logging\nlog_directory = os.getenv(\"LOG_DIR\", \"logs\")\nos.makedirs(log_directory, exist_ok=True)\nlog_file = os.path.join(log_directory, f\"llm_calls_{datetime.now().strftime('%Y%m%d')}.log\")\n\n# Set up logger\nlogger = logging.getLogger(\"llm_logger\")\nlogger.setLevel(logging.INFO)\nlogger.propagate = False  # Prevent propagation to root logger\nfile_handler = logging.FileHandler(log_file)\nfile_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\nlogger.addHandler(file_handler)\n\n# Simple cache configuration\ncache_file = \"llm_cache.json\"\n\n# By default, we Google Gemini 2.5 pro, as it shows great performance for code understanding\ndef call_llm(prompt: str, use_cache: bool = True) -> str:\n    # Log the prompt\n    logger.info(f\"PROMPT: {prompt}\")\n    \n    # Check cache if enabled\n    if use_cache:\n        # Load cache from disk\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, 'r') as f:\n                    cache = json.load(f)\n            except:\n                logger.warning(f\"Failed to load cache, starting with empty cache\")\n        \n        # Return from cache if exists\n        if prompt in cache:\n            logger.info(f\"RESPONSE: {cache[prompt]}\")\n            return cache[prompt]\n    \n    # Call the LLM if not in cache or cache disabled\n    client = genai.Client(\n        vertexai=True, \n        # TODO: change to your own project id and location\n        project=os.getenv(\"GEMINI_PROJECT_ID\", \"your-project-id\"),\n        location=os.getenv(\"GEMINI_LOCATION\", \"us-central1\")\n    )\n    # You can comment the previous line and use the AI Studio key instead:\n    # client = genai.Client(\n    #     api_key=os.getenv(\"GEMINI_API_KEY\", \"your-api_key\"),\n    # )\n    model = os.getenv(\"GEMINI_MODEL\", \"gemini-2.5-pro-exp-03-25\")\n    response = client.models.generate_content(\n        model=model,\n        contents=[prompt]\n    )\n    response_text = response.text\n    \n    # Log the response\n    logger.info(f\"RESPONSE: {response_text}\")\n    \n    # Update cache if enabled\n    if use_cache:\n        # Load cache again to avoid overwrites\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, 'r') as f:\n                    cache = json.load(f)\n            except:\n                pass\n        \n        # Add to cache and save\n        cache[prompt] = response_text\n        try:\n            with open(cache_file, 'w') as f:\n                json.dump(cache, f)\n        except Exception as e:\n            logger.error(f\"Failed to save cache: {e}\")\n    \n    return response_text\n```\n\nLet's break down the code:\n\n1.  **Importing Libraries:** We import the necessary libraries for interacting with the LLM, handling environment variables, logging, and caching.\n2.  **Configuring Logging:** We set up a logger to record all the prompts and responses. This helps us debug any issues and analyze the LLM's behavior.\n3.  **Configuring Caching:** We set up a simple cache to store the LLM's responses. This saves us from having to make the same API calls multiple times.\n4.  **Calling the LLM:** The `call_llm` function takes a prompt as input and sends it to the LLM. It handles authentication, error handling, and response parsing.\n5.  **Logging the Prompt and Response:** The function logs the prompt and response to the log file.\n6.  **Updating the Cache:** If caching is enabled, the function updates the cache with the prompt and response.\n7. **Choosing LLM Model:** The Gemini family of models are used by default, but the code has examples for how to call other models like Anthropic Claude or OpenAI.\n\n## Code Example: Writing a Chapter\n\nThe `WriteChapters` node uses the `call_llm` function to generate the content for each chapter of the tutorial.\n\n```python\nclass WriteChapters(BatchNode):\n    def prep(self, shared):\n        # ... (preparation logic) ...\n\n    def exec(self, item):\n        # ... (item preparation) ...\n\n        prompt = f\"\"\"\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `{project_name}` about the concept: \"{abstraction_name}\". This is Chapter {chapter_num}.\n\nConcept Details:\n- Description:\n{item[\"abstraction_details\"][\"description\"]}\n\nComplete Tutorial Structure:\n{item[\"full_chapter_listing\"]}\n\nContext from previous chapters (summary):\n{previous_chapters_summary if previous_chapters_summary else \"This is the first chapter.\"}\n\nRelevant Code Snippets:\n{file_context_str if file_context_str else \"No specific code snippets provided for this abstraction.\"}\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter {chapter_num}: {abstraction_name}`).\n        \n        # ... (more instructions) ...\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n\"\"\"\n        chapter_content = call_llm(prompt)\n        # ... (validation and cleanup) ...\n        return chapter_content # Return the Markdown string\n\n    def post(self, shared, prep_res, exec_res_list):\n        # ... (post-processing logic) ...\n```\n\nHere's a breakdown:\n\n1.  **Preparing the Prompt:** The `prep` and `exec` methods construct a detailed prompt that tells the LLM exactly what we want it to do:\n    *   Write a tutorial chapter about a specific concept.\n    *   Use a beginner-friendly tone.\n    *   Follow a specific structure.\n    *   Incorporate relevant code snippets.\n    *   Reference previous chapters.\n2.  **Calling the LLM:** The `exec` method calls the `call_llm` function to send the prompt to the LLM.\n3.  **Receiving the Response:** The `call_llm` function returns the LLM's response, which is the generated Markdown content for the chapter.\n4.  **Validating and Cleaning Up:** The `exec` method validates the LLM's response to make sure it's in the correct format and cleans up any unwanted characters.\n5.  **Returning the Result:** The `exec` method returns the validated and cleaned-up Markdown content for the chapter.\n\n## Setting Up the LLM API Key\n\nTo use the LLM Interaction module, you'll need to set up an API key for the LLM you want to use. The default call_llm uses Google's Gemini API, but code samples are shown for Anthropic Claude and OpenAI. The Gemini API can be set up with the Google AI Studio key or using Google Cloud.\n\nFor Google AI Studio:\n\n1.  Go to the [Google AI Studio website](https://makersuite.google.com/).\n2.  Create an account or log in.\n3.  Follow the instructions to create an API key.\n4.  Set the `GEMINI_API_KEY` environment variable to your API key.\n\n    ```bash\n    export GEMINI_API_KEY=<your_api_key>\n    ```\n\nFor Google Cloud:\n\n1.  Create a Google Cloud project.\n2.  Enable the Gemini API for your project.\n3.  Create a service account with the necessary permissions.\n4.  Download the service account key file.\n5.  Set the `GEMINI_PROJECT_ID` environment variable to your project ID.\n6.  Set the `GEMINI_LOCATION` environment variable to your project location.\n\n## Caching and Logging\n\nThe `call_llm` function automatically caches the LLM's responses to a JSON file called `llm_cache.json`. This helps to speed up the tutorial generation process and reduce the cost of calling the LLM.\n\nThe `call_llm` function also logs all the prompts and responses to a log file in the `logs` directory. This helps to debug any issues and analyze the LLM's behavior.\n\n## Why is LLM Interaction Important?\n\n*   **Content Generation:** It allows us to automatically generate the content for our tutorials, saving us a lot of time and effort.\n*   **Scalability:** It allows us to generate tutorials for any codebase, no matter how large or complex.\n*   **Consistency:** It ensures that the tutorials are written in a consistent style and tone.\n*   **Beginner-Friendly:** By providing detailed instructions to the LLM, we can ensure that the tutorials are easy for beginners to understand.\n\n## Conclusion\n\nIn this chapter, we've learned about LLM Interaction, which is the process of using a Large Language Model (LLM) to generate the content for our tutorials. We've seen how we prepare the prompts, call the LLM, receive the responses, and validate the results.\n\nIn the next chapter, [Chapter Writing](08_chapter_writing.md), we'll delve deeper into the techniques for crafting effective and engaging tutorial chapters.\n```", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `Tutorial-Codebase-Knowledge` about the concept: \"Chapter Writing\". This is Chapter 8.\n\nConcept Details:\n- Description:\nChapter Writing is the process of creating the actual tutorial content, explaining each abstraction\nin a beginner-friendly way. This involves using an LLM to generate the chapter content and\nincorporating relevant code snippets and diagrams. It's like a skilled storyteller weaving a\ncompelling narrative around different characters and events to educate and entertain the audience.\n\n\nComplete Tutorial Structure:\n1. [API Endpoint](01_api_endpoint.md)\n2. [Flow Orchestration](02_flow_orchestration.md)\n3. [Configuration Management](03_configuration_management.md)\n4. [Codebase Crawling](04_codebase_crawling.md)\n5. [Abstraction Identification](05_abstraction_identification.md)\n6. [Relationship Analysis](06_relationship_analysis.md)\n7. [LLM Interaction](07_llm_interaction.md)\n8. [Chapter Writing](08_chapter_writing.md)\n9. [Tutorial Combination](09_tutorial_combination.md)\n\nContext from previous chapters (summary):\n# Chapter 1: API Endpoint\n\nWelcome to the first step in our journey to understanding how to generate tutorials from codebases! In this chapter, we'll explore the **API Endpoint**, which acts like the \"front door\" to our tutorial generation system.\n\nImagine you have a robot that can build LEGO sets. You wouldn't want to open up the robot and start fiddling with its internal wires every time you want it to build something, right? Instead, you'd want a simple button or a control panel to tell it what to do. The API Endpoint is like that button for our tutorial generator.\n\n**Why do we need an API Endpoint?**\n\nThink of it this way: you might want to trigger the tutorial generation process from:\n\n*   Your web browser\n*   Another program running on your computer\n*   Even from a different computer across the internet!\n\nAn API Endpoint allows all these different \"clients\" to communicate with our tutorial generation system in a standardized way. It's the central point of contact.\n\n**Key Concepts:**\n\n1.  **API (Application Programming Interface):** Think of it as a contract between two pieces of software. It defines how they can talk to each other. In our case, it defines how external programs can ask our system to generate a tutorial.\n\n2.  **Endpoint:** A specific URL (web address) that represents a particular function or resource in an API. In our case, it's the URL you send a request to in order to start the tutorial generation. It's like the specific button on our robot's control panel that says \"Build this LEGO set!\"\n\n3.  **Request:** The message you send to the API Endpoint to tell it what to do. This includes information like the repository URL, output folder and options for the tutorial generation.\n\n4.  **Response:** The message the API Endpoint sends back to you after it has processed your request. This might include a success message, an error message, or the path to the generated tutorial.\n\n**Our Use Case: Generating a tutorial for a GitHub repository**\n\nLet's say you want to generate a tutorial for a project on GitHub. The API Endpoint allows you to do this with a simple request. You'll need to provide the URL of the repository and any specific options for how you want the tutorial to be generated.\n\n**How to Use the API Endpoint:**\n\nOur API Endpoint is designed to receive a `POST` request to the `/generate` path. This request contains information about the repository for which you want to generate a tutorial.\n\nHere's an example of how you might send a request (this is just an example, the actual implementation will depend on how you're sending the request, e.g., using `curl`, `Postman`, or a Python script):\n\n```json\n{\n  \"repo_url\": \"https://github.com/fastapi/fastapi\",\n  \"name\": \"FastAPI\",\n  \"output\": \"tutorials\"\n}\n```\n\nThis request tells the system to:\n\n*   Generate a tutorial for the `https://github.com/fastapi/fastapi` repository.\n*   Use \"FastAPI\" as the project name.\n*   Save the tutorial in the `tutorials` output folder.\n\nAfter sending this request, the API Endpoint will process it and, if successful, return a response like this:\n\n```json\n{\n  \"message\": \"Tutorial generation completed successfully.\",\n  \"final_output_directory\": \"tutorials/FastAPI/html\"\n}\n```\n\nThis tells you that the tutorial was generated successfully and is located in the `tutorials/FastAPI/html` directory.\n\n**Internal Implementation: What happens under the hood?**\n\nWhen the API Endpoint receives a request, here's a simplified breakdown of what happens:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant API Endpoint\n    participant Main Script\n    participant Codebase\n    participant LLM\n\n    User->>API Endpoint: Sends tutorial generation request (repo URL, options)\n    API Endpoint->>Main Script: Executes main.py with provided arguments\n    Main Script->>Codebase: Crawls and analyzes the codebase\n    Main Script->>LLM: Interacts with LLM to generate content\n    Main Script->>API Endpoint: Returns success/failure and output path\n    API Endpoint->>User: Sends back response with result and output path\n```\n\nHere's a slightly more detailed explanation:\n\n1.  **Request Reception:** The API Endpoint receives the request with the repository URL and other options.\n2.  **Command Construction:**  It constructs a command line command to run the main script (`main.py`) with the provided parameters.\n3.  **Script Execution:** It executes the `main.py` script, which does the heavy lifting of cloning the repository, analyzing the code, and generating the tutorial content.\n4.  **Response Generation:** After the script finishes (either successfully or with an error), the API Endpoint creates a response with the appropriate message and, if successful, the path to the generated tutorial.\n5.  **Response Delivery:** Finally, it sends the response back to the user.\n\n**Code Snippets:**\n\nLet's look at some key parts of the code in `api.py`:\n\n```python\nfrom fastapi import FastAPI, HTTPException, status\nfrom pydantic import BaseModel, Field\n```\n\nThis imports the necessary libraries for creating the API using FastAPI (a Python framework for building APIs) and Pydantic (for data validation). We are importing `FastAPI` to create the API application, `HTTPException` to handle errors and send them to the user, and `BaseModel` and `Field` from `pydantic` to define the structure of our input data.\n\n```python\nclass GenerationRequest(BaseModel):\n    repo_url: str\n    name: Optional[str] = None\n    token: Optional[str] = None\n    output: str = Field(default=\"output\")\n    include: Optional[List[str]] = None\n    exclude: Optional[List[str]] = None\n    max_size: Optional[int] = None\n```\n\nThis defines the structure of the data that the API Endpoint expects to receive in the request. This is crucial for validation and ensuring that the input data is in the correct format. `repo_url` is required, while other fields like `name`, `token`, `output`, `include`, `exclude`, and `max_size` are optional.\n\n```python\n@app.post(\n    \"/generate\",\n    summary=\"Trigger Tutorial Generation\",\n    description=\"Accepts repository details and options, then runs the tutorial generation script.\",\n    status_code=status.HTTP_200_OK\n)\nasync def generate_tutorial(request_data: GenerationRequest):\n    # ... implementation details ...\n    return {\n        \"message\": \"Tutorial generation completed successfully.\",\n        \"final_output_directory\": expected_final_path,\n    }\n```\n\nThis code defines the `/generate` endpoint using the `@app.post` decorator. This tells FastAPI that this function should be called when a `POST` request is sent to the `/generate` URL. It receives the `request_data` (which is of type `GenerationRequest` that we defined above), does some processing, and returns a response containing a success message and the path to the generated tutorial.  The `status_code=status.HTTP_200_OK` ensures that a successful response returns a 200 OK status code.\n\n**CORS Middleware**\n\n```python\norigins = [\n    \"http://127.0.0.1:5500\", # VS Code Live Server default\n    \"http://localhost\",\n    \"http://localhost:8080\",\n    \"http://127.0.0.1\",\n    \"http://127.0.0.1:8080\",\n    \"null\", # file:// origin\n]\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=origins,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n```\n\nThis section is responsible for configuring Cross-Origin Resource Sharing (CORS). CORS is a security feature implemented by web browsers to restrict web pages from making requests to a different domain than the one which served the web page. Without CORS, you might not be able to send requests to your API from a webpage running on your local machine (e.g. during development). The middleware specifies the origins that are allowed to make requests to the API, ensuring that requests from these origins are not blocked by the browser.\n\n**Error Handling**\n\nThe `api.py` file includes comprehensive error handling to gracefully manage different scenarios that might arise during script execution.\n\n```python\n    except subprocess.CalledProcessError as e:\n        logger.error(f\"Script failed with exit code {e.returncode}\")\n        logger.error(f\"Script stdout:\\n{e.stdout}\")\n        logger.error(f\"Script stderr:\\n{e.stderr}\")\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail={\n                \"error\": \"Script execution failed.\",\n                \"details\": \"The tutorial generation script exited with an error.\",\n                \"return_code\": e.returncode,\n                \"script_stdout\": e.stdout,\n                \"script_stderr\": e.stderr\n            }\n        )\n    except FileNotFoundError:\n        error_msg = f\"Error: Script '{MAIN_SCRIPT_PATH}' or Python '{PYTHON_EXECUTABLE}' not found.\"\n        logger.error(error_msg)\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail={\"error\": \"Server configuration error.\", \"details\": error_msg}\n        )\n    except Exception as e:\n        logger.exception(\"An unexpected error occurred during script execution.\")\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail={\"error\": \"An unexpected internal server error occurred.\", \"details\": str(e)}\n        )\n```\n\n*   **`subprocess.CalledProcessError`**: Catches errors when the `main.py` script fails during execution. It logs the error details and raises an `HTTPException` with a 500 status code, providing details like the return code, stdout, and stderr of the script.\n*   **`FileNotFoundError`**: Handles cases where the `main.py` script or Python executable is not found. It raises an `HTTPException` with a 500 status code, indicating a server configuration issue.\n*   **`Exception`**: Serves as a catch-all for unexpected errors during script execution. It logs the exception and raises an `HTTPException` with a 500 status code, providing details about the error.\n\n**Conclusion:**\n\nIn this chapter, we've learned about the API Endpoint, which serves as the entry point to our tutorial generation system. We saw how it allows external programs to trigger the tutorial generation process with a simple request. We also explored the internal implementation and understood how it orchestrates the execution of the `main.py` script.\n\nNow that we know how to trigger the process, let's dive into [Flow Orchestration](02_flow_orchestration.md) in the next chapter to understand the overall sequence of steps involved in generating a tutorial.\n\n---\n# Chapter 2: Flow Orchestration\n\nIn the previous chapter, [API Endpoint](01_api_endpoint.md), we learned how to trigger the tutorial generation process. Think of it like pressing the \"Start\" button on a machine. But what happens *after* you press that button? That's where Flow Orchestration comes in!\n\n## What is Flow Orchestration?\n\nImagine a factory that builds cars. You wouldn't just dump all the parts in a pile and hope a car magically appears, right? Instead, you'd have a carefully planned assembly line:\n\n1.  **Frame Assembly:** The car's frame is built.\n2.  **Engine Installation:** The engine is installed.\n3.  **Bodywork:** The body panels are attached.\n4.  **Painting:** The car gets its color.\n5.  **Interior Fitting:** Seats and other interior parts are added.\n6.  **Quality Control:** Final checks are performed.\n\nFlow Orchestration is like that assembly line for our tutorial generation system. It defines the *sequence of steps* needed to transform a codebase into a complete tutorial. It makes sure each task happens in the right order, passing the result of one task to the next.\n\nWithout flow orchestration, we'd have a chaotic mess. Chapters might be written before the codebase is even analyzed, or the tutorial might be combined before the chapters are finished! Flow Orchestration brings order to the process.\n\n## Key Concepts\n\n*   **Flow:** The overall process or pipeline of tasks. In our case, it's the entire tutorial generation process.\n*   **Node:** A single task or step in the flow. Each node performs a specific job, like fetching code, analyzing relationships, or writing a chapter.  Nodes can be thought of as functions in a programming language.\n*   **Sequence:** The order in which the nodes are executed.  This is crucial for ensuring the correct flow of data and dependencies.\n*   **Dependencies:**  A node might *depend* on the output of another node. For example, you can't write a chapter *about* an abstraction until you've identified *what* the abstractions *are*.\n\n## How Flow Orchestration Works in Our Project\n\nLet's look at the main steps in our tutorial generation flow:\n\n```mermaid\ngraph LR\n    A[Fetch Repo] --> B(Identify Abstractions)\n    B --> C{Analyze Relationships}\n    C --> D[Order Chapters]\n    D --> E((Write Chapters))\n    E --> F(Combine Tutorial)\n    style A fill:#f9f,stroke:#333,stroke-width:2px\n    style F fill:#f9f,stroke:#333,stroke-width:2px\n    style E fill:#ccf,stroke:#333,stroke-width:2px\n```\n\nHere's a breakdown of each node:\n\n1.  **Fetch Repo:**  Downloads the codebase from a remote repository (like GitHub) or reads it from a local directory. This node is responsible for getting the raw material needed to create the tutorial.  It outputs the files and their contents.\n2.  **Identify Abstractions:** Analyzes the codebase and identifies key concepts or abstractions that are important for understanding the project. This step essentially identifies the key topics to cover in the tutorial.  It outputs a list of abstractions, including their names, descriptions, and the files they're found in.\n3.  **Analyze Relationships:** Examines how the different abstractions interact with each other. This helps us understand the overall architecture of the codebase and how the different pieces fit together. The LLM creates a summary of the project and the relationships between the abstractions\n4.  **Order Chapters:** Determines the best order to present the abstractions in the tutorial.  Some abstractions might be foundational and need to be explained before others. The LLM figures out the order.\n5.  **Write Chapters:**  Generates the actual content for each chapter of the tutorial. It uses the abstraction details, relationships, and chapter order to create a beginner-friendly explanation of each concept. This node is marked with double parentheses `((...))` because it's a *BatchNode*. A `BatchNode` handles independent steps and can be run in parallel.\n6.  **Combine Tutorial:**  Assembles all the individual chapters into a complete tutorial, creating an index page and table of contents.\n\n## Code Example: Defining the Flow\n\nThe core of our flow orchestration is defined in `flow.py`:\n\n```python\nfrom pocketflow import Flow\n# Import all node classes from nodes.py\nfrom nodes import (\n    FetchRepo,\n    IdentifyAbstractions,\n    AnalyzeRelationships,\n    OrderChapters,\n    WriteChapters,\n    CombineTutorial\n)\n\ndef create_tutorial_flow():\n    \"\"\"Creates and returns the codebase tutorial generation flow.\"\"\"\n\n    # Instantiate nodes\n    fetch_repo = FetchRepo()\n    identify_abstractions = IdentifyAbstractions(max_retries=3, wait=10)\n    analyze_relationships = AnalyzeRelationships(max_retries=3, wait=10)\n    order_chapters = OrderChapters(max_retries=3, wait=10)\n    write_chapters = WriteChapters(max_retries=3, wait=10) # This is a BatchNode\n    combine_tutorial = CombineTutorial()\n\n    # Connect nodes in sequence based on the design\n    fetch_repo >> identify_abstractions\n    identify_abstractions >> analyze_relationships\n    analyze_relationships >> order_chapters\n    order_chapters >> write_chapters\n    write_chapters >> combine_tutorial\n\n    # Create the flow starting with FetchRepo\n    tutorial_flow = Flow(start=fetch_repo)\n\n    return tutorial_flow\n```\n\nLet's break down this code:\n\n*   **Importing Nodes:** We import all the node classes (like `FetchRepo`, `IdentifyAbstractions`, etc.) from the `nodes.py` file.  Think of these as importing different tools or functions that we need for our assembly line.\n*   **Instantiating Nodes:**  We create instances of each node class. For example, `fetch_repo = FetchRepo()` creates a specific instance of the `FetchRepo` node. This is where the maximum number of retries and wait time between retries is set for nodes that interact with the LLM. This helps handle cases where the LLM might be temporarily unavailable or returns an invalid response.\n*   **Connecting Nodes:** The `>>` operator defines the sequence of the flow. `fetch_repo >> identify_abstractions` means that the `IdentifyAbstractions` node will execute *after* the `FetchRepo` node completes.  The output of `FetchRepo` will be passed as input to `IdentifyAbstractions`.\n*   **Creating the Flow:** We use the `Flow` class from the `pocketflow` library to create the flow. We specify the `start` node, which is the first node to be executed.\n\nThis simple code defines the entire structure of our tutorial generation process!\n\n## Code Example: Running the Flow\n\nThe `main.py` file is responsible for setting up the flow with all the command line input parameters and calling the `Flow` to execute:\n\n```python\nimport dotenv\nimport os\nimport argparse\n# Import the function that creates the flow\nfrom flow import create_tutorial_flow\n\ndotenv.load_dotenv()\n\n# Default file patterns\nDEFAULT_INCLUDE_PATTERNS = {\n    \"*.py\", \"*.js\", \"*.jsx\", \"*.ts\", \"*.tsx\", \"*.go\", \"*.java\", \"*.pyi\", \"*.pyx\", \n    \"*.c\", \"*.cc\", \"*.cpp\", \"*.h\", \"*.md\", \"*.rst\", \"Dockerfile\", \n    \"Makefile\", \"*.yaml\", \"*.yml\",\n}\n\nDEFAULT_EXCLUDE_PATTERNS = {\n    \"*test*\", \"tests/*\", \"docs/*\", \"examples/*\", \"v1/*\", \n    \"dist/*\", \"build/*\", \"experimental/*\", \"deprecated/*\", \n    \"legacy/*\", \".git/*\", \".github/*\", \".next/*\", \".vscode/*\", \"obj/*\", \"bin/*\", \"node_modules/*\", \"*.log\"\n}\n\n# --- Main Function ---\ndef main():\n    parser = argparse.ArgumentParser(description=\"Generate a tutorial for a GitHub codebase or local directory.\")\n    \n    # Create mutually exclusive group for source\n    source_group = parser.add_mutually_exclusive_group(required=True)\n    source_group.add_argument(\"--repo\", help=\"URL of the public GitHub repository.\")\n    source_group.add_argument(\"--dir\", help=\"Path to local directory.\")\n    \n    parser.add_argument(\"-n\", \"--name\", help=\"Project name (optional, derived from repo/directory if omitted).\")\n    parser.add_argument(\"-t\", \"--token\", help=\"GitHub personal access token (optional, reads from GITHUB_TOKEN env var if not provided).\")\n    parser.add_argument(\"-o\", \"--output\", default=\"output\", help=\"Base directory for output (default: ./output).\")\n    parser.add_argument(\"-i\", \"--include\", nargs=\"+\", help=\"Include file patterns (e.g. '*.py' '*.js'). Defaults to common code files if not specified.\")\n    parser.add_argument(\"-e\", \"--exclude\", nargs=\"+\", help=\"Exclude file patterns (e.g. 'tests/*' 'docs/*'). Defaults to test/build directories if not specified.\")\n    parser.add_argument(\"-s\", \"--max-size\", type=int, default=100000, help=\"Maximum file size in bytes (default: 100000, about 100KB).\")\n\n    args = parser.parse_args()\n\n    # Get GitHub token from argument or environment variable if using repo\n    github_token = None\n    if args.repo:\n        github_token = args.token or os.environ.get('GITHUB_TOKEN')\n        if not github_token:\n            print(\"Warning: No GitHub token provided. You might hit rate limits for public repositories.\")\n\n    # Initialize the shared dictionary with inputs\n    shared = {\n        \"repo_url\": args.repo,\n        \"local_dir\": args.dir,\n        \"project_name\": args.name, # Can be None, FetchRepo will derive it\n        \"github_token\": github_token,\n        \"output_dir\": args.output, # Base directory for CombineTutorial output\n\n        # Add include/exclude patterns and max file size\n        \"include_patterns\": set(args.include) if args.include else DEFAULT_INCLUDE_PATTERNS,\n        \"exclude_patterns\": set(args.exclude) if args.exclude else DEFAULT_EXCLUDE_PATTERNS,\n        \"max_file_size\": args.max_size,\n\n        # Outputs will be populated by the nodes\n        \"files\": [],\n        \"abstractions\": [],\n        \"relationships\": {},\n        \"chapter_order\": [],\n        \"chapters\": [],\n        \"final_output_dir\": None\n    }\n\n    print(f\"Starting tutorial generation for: {args.repo or args.dir}\")\n\n    # Create the flow instance\n    tutorial_flow = create_tutorial_flow()\n\n    # Run the flow\n    tutorial_flow.run(shared)\n    \nif __name__ == \"__main__\":\n    main()\n```\n\nHere are the important parts:\n\n1.  **Parsing Arguments:** We use `argparse` to handle command-line arguments like the repository URL, project name, and output directory.\n2.  **Shared Dictionary:** The `shared` dictionary acts as a central store for data that is passed between the nodes. Each node can read data from this dictionary and write its results back to it.\n3.  **Creating the Flow:** We call `create_tutorial_flow()` to get an instance of the flow we defined in `flow.py`.\n4.  **Running the Flow:**  Finally, we call `tutorial_flow.run(shared)` to start the flow. The `shared` dictionary is passed to the `run` method so that the nodes can access and modify the data.\n\n## What Happens Inside a Node?\n\nEach node consists of three important methods. Looking at the example of `FetchRepo` in `nodes.py`:\n\n```python\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"local_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        if not project_name:\n            # Basic name derivation from URL or directory\n            if repo_url:\n                project_name = repo_url.split('/')[-1].replace('.git', '')\n            else:\n                project_name = os.path.basename(os.path.abspath(local_dir))\n            shared[\"project_name\"] = project_name\n\n        # Get file patterns directly from shared\n        include_patterns = shared[\"include_patterns\"]\n        exclude_patterns = shared[\"exclude_patterns\"]\n        max_file_size = shared[\"max_file_size\"]\n\n        return {\n            \"repo_url\": repo_url,\n            \"local_dir\": local_dir,\n            \"token\": shared.get(\"github_token\"),\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns,\n            \"max_file_size\": max_file_size,\n            \"use_relative_paths\": True\n        }\n\n    def exec(self, prep_res):\n        if prep_res[\"repo_url\"]:\n            print(f\"Crawling repository: {prep_res['repo_url']}...\")\n            result = crawl_github_files(\n                repo_url=prep_res[\"repo_url\"],\n                token=prep_res[\"token\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        else:\n            print(f\"Crawling directory: {prep_res['local_dir']}...\")\n            result = crawl_local_files(\n                directory=prep_res[\"local_dir\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n            \n        # Convert dict to list of tuples: [(path, content), ...]\n        files_list = list(result.get(\"files\", {}).items())\n        print(f\"Fetched {len(files_list)} files.\")\n        return files_list\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"files\"] = exec_res # List of (path, content) tuples\n```\n\n*   **`prep(shared)`:** This method prepares the input for the node's execution. It reads data from the `shared` dictionary and transforms it into a format that the `exec` method can use. It also performs any necessary setup or validation. Returns a `prep_res` object that contains everything necessary for the `exec` function to work.\n*   **`exec(prep_res)`:** This method performs the core logic of the node. It receives the prepared input from the `prep` method and executes the task. In the case of `FetchRepo`, it downloads the codebase from GitHub or reads it from the local directory. It returns the `exec_res` which is the output of the node.\n*   **`post(shared, prep_res, exec_res)`:** This method is executed *after* the `exec` method completes. It takes the output of the `exec` method (`exec_res`) and writes it back to the `shared` dictionary, making it available to the next node in the flow.  In the case of `FetchRepo`, it writes the list of files and their contents to the `shared[\"files\"]` variable.\n\n## Why Use Flow Orchestration?\n\n*   **Organization:** It provides a clear and structured way to define complex processes.\n*   **Maintainability:** It makes the code easier to understand and maintain by breaking it down into smaller, independent nodes.\n*   **Reusability:**  Nodes can be reused in different flows, making it easier to build new features or modify existing ones.\n*   **Testability:** Each node can be tested independently, making it easier to ensure the correctness of the entire system.\n*   **Error Handling:** `pocketflow` supports retries, fallbacks, error handling, and graceful termination, which are useful for nodes that interact with external resources.\n\n## Conclusion\n\nFlow Orchestration is a powerful technique for managing complex processes like our tutorial generation system. It provides a structured way to define the sequence of steps, making the code more organized, maintainable, and testable.\n\nIn the next chapter, [Configuration Management](03_configuration_management.md), we'll explore how to manage the different configuration settings that control the behavior of our system. This will allow us to customize the tutorial generation process for different codebases and use cases.\n\n---\n# Chapter 3: Configuration Management\n\nIn the previous chapter, [Flow Orchestration](02_flow_orchestration.md), we learned how to define the overall sequence of steps for generating a tutorial. Now, let's talk about how to *customize* that process. Think of it like this: you have a recipe for baking a cake, but you might want to adjust the recipe to make it chocolate or vanilla. That's where Configuration Management comes in!\n\n## What is Configuration Management?\n\nConfiguration Management is like having a control panel that allows you to adjust the settings of our tutorial generation \"machine\" to achieve the desired output. It lets you change things like:\n\n*   Which repository to generate a tutorial for.\n*   What name to give the project in the tutorial.\n*   Which files to include or exclude from analysis.\n*   Where to save the generated tutorial.\n\nWithout configuration management, we'd be stuck with a single, inflexible way of generating tutorials. It's like trying to bake every cake with the exact same recipe, even if you want different flavors!\n\n## Key Concepts\n\n1.  **Environment Variables:** These are like global settings that can be accessed by any program running on your computer. They're often used to store things like API keys or default values.\n2.  **Argument Parsing:** This allows you to pass settings directly to a program when you run it from the command line. It's like specifying the ingredients for your cake when you give the recipe to the baker.\n\n## Our Use Case: Generating a Tutorial for a Specific GitHub Repository with a Custom Name\n\nLet's say you want to generate a tutorial for the `requests` library on GitHub and name the project \"Python Requests Tutorial\". With Configuration Management, you can easily do this!\n\n## How to Use Configuration Management\n\nOur project uses both environment variables and argument parsing to manage configuration.\n\n### 1. Environment Variables\n\nEnvironment variables are useful for settings that you might want to reuse across multiple runs of the program, or that contain sensitive information you don't want to type directly into the command line.\n\nTo set an environment variable, you can use the following command in your terminal (replace `<your_token>` with your actual token):\n\n```bash\nexport GITHUB_TOKEN=<your_token>\n```\n\nOr, you can create a `.env` file in the project root directory:\n\n```\nGITHUB_TOKEN=<your_token>\n```\n\nThe `dotenv` library will automatically load these variables when the script runs.\n\nIn our `main.py` file, we use the `os.environ.get()` function to read the value of the `GITHUB_TOKEN` environment variable:\n\n```python\nimport os\ngithub_token = args.token or os.environ.get('GITHUB_TOKEN')\n```\n\nThis code first checks if the token was provided as a command-line argument (`args.token`). If not, it looks for the `GITHUB_TOKEN` environment variable.\n\n### 2. Argument Parsing\n\nArgument parsing allows you to specify settings directly when you run the `main.py` script from the command line.\n\nHere's how you would generate a tutorial for the `requests` library with a custom name:\n\n```bash\npython main.py --repo https://github.com/psf/requests -n \"Python Requests Tutorial\" -o tutorials\n```\n\nLet's break down this command:\n\n*   `python main.py`: This tells your computer to run the `main.py` script using Python.\n*   `--repo https://github.com/psf/requests`: This specifies the URL of the GitHub repository to use. `--repo` is the *argument* name, and `https://github.com/psf/requests` is the *value* of that argument.\n*   `-n \"Python Requests Tutorial\"`: This sets the project name to \"Python Requests Tutorial\". `-n` is a *short* argument name, and `\"Python Requests Tutorial\"` is the value.\n*   `-o tutorials`: This specifies that the tutorial output will be written to the `tutorials` directory.\n\nThe `argparse` module in Python handles the parsing of these command-line arguments. In `main.py`, we define the arguments that our script accepts:\n\n```python\nimport argparse\n\nparser = argparse.ArgumentParser(description=\"Generate a tutorial for a GitHub codebase or local directory.\")\nparser.add_argument(\"--repo\", help=\"URL of the public GitHub repository.\")\nparser.add_argument(\"-n\", \"--name\", help=\"Project name (optional, derived from repo/directory if omitted).\")\nparser.add_argument(\"-o\", \"--output\", default=\"output\", help=\"Base directory for output (default: ./output).\")\n# ... other arguments\nargs = parser.parse_args()\n```\n\nThis code defines three arguments: `--repo`, `--name` (with the short form `-n`), and `--output` (with the short form `-o`). The `help` parameter provides a description of each argument that is displayed when you run the script with the `--help` flag:\n\n```bash\npython main.py --help\n```\n\n### How Include and Exclude Patterns Work\n\nThe `-i` (or `--include`) and `-e` (or `--exclude`) options are used to specify which files to include or exclude from the tutorial generation process. This is helpful if you want to focus on specific parts of the codebase or ignore irrelevant files (like test files or documentation).\n\nFor example, to only include Python files and exclude test directories, you could use the following command:\n\n```bash\npython main.py --repo https://github.com/psf/requests -i \"*.py\" -e \"tests/*\"\n```\n\n## Internal Implementation\n\nHere's a simplified view of how Configuration Management works internally:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Command Line\n    participant Main Script\n    participant Args Namespace\n    User->>Command Line: Runs script with arguments: python main.py --repo ...\n    Command Line->>Main Script: Executes main.py\n    Main Script->>Args Namespace: Parses command-line arguments using argparse\n    Args Namespace->>Main Script: Returns arguments as an object (e.g., args.repo, args.name)\n    Main Script->>Main Script: Accesses argument values and uses them\n```\n\nHere's a breakdown:\n\n1.  **User Interaction:** The user runs the `main.py` script from the command line, providing arguments like the repository URL and project name.\n2.  **Argument Parsing:** The `argparse` module in `main.py` parses these arguments and stores them in an `Args Namespace` object.\n3.  **Accessing Arguments:** The `main.py` script accesses the argument values from the `Args Namespace` object (e.g., `args.repo`, `args.name`).\n4.  **Using Arguments:** The script uses these values to configure the tutorial generation process. For example, the `repo_url` value is passed to the [Codebase Crawling](04_codebase_crawling.md) node to download the codebase.\n\n## Code Example: Passing Configuration to Nodes\n\nThe parsed arguments are stored in the `shared` dictionary and passed to the nodes in the flow. For example, in `main.py`:\n\n```python\nshared = {\n    \"repo_url\": args.repo,\n    \"project_name\": args.name,\n    \"output_dir\": args.output,\n    # ... other arguments\n}\n\ntutorial_flow.run(shared)\n```\n\nEach node can then access the configuration values from the `shared` dictionary in its `prep` method. For example, in `nodes.py`:\n\n```python\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        project_name = shared.get(\"project_name\")\n        # ...\n        return {\n            \"repo_url\": repo_url,\n            # ...\n        }\n```\n\n## Why is Configuration Management Important?\n\n*   **Flexibility:** It allows you to customize the tutorial generation process for different codebases and use cases.\n*   **Reusability:** You can easily reuse the same script to generate tutorials for different projects by simply changing the configuration settings.\n*   **Automation:** You can automate the tutorial generation process by running the script with a predefined set of arguments.\n\n## Conclusion\n\nIn this chapter, we've learned about Configuration Management, which allows us to customize the tutorial generation process by using environment variables and argument parsing. This gives us the flexibility to generate tutorials for different codebases with different settings.\n\nIn the next chapter, [Codebase Crawling](04_codebase_crawling.md), we'll dive into the process of fetching and analyzing the codebase itself.\n\n---\n# Chapter 4: Codebase Crawling\n\nIn the previous chapter, [Configuration Management](03_configuration_management.md), we learned how to customize our tutorial generation process using command-line arguments and environment variables. Now, let's explore how we actually *get* the code that we'll be using to generate the tutorial! That's where Codebase Crawling comes in.\n\n## What is Codebase Crawling?\n\nImagine you're writing a report on a popular book. You can't write the report without actually *reading* the book first, right? Codebase Crawling is like reading the book (or, more accurately, the code) for our tutorial generation system.\n\nSpecifically, Codebase Crawling is the process of gathering all the necessary files from either:\n\n*   A remote GitHub repository, or\n*   A local directory on your computer.\n\nIt's like a diligent librarian collecting books from different shelves, ensuring they meet certain criteria before adding them to the collection. The criteria might include:\n\n*   **File Type:** Only including certain types of files (e.g., Python files, JavaScript files).\n*   **File Size:** Ignoring files that are too large.\n*   **Location:** Only including files in certain directories, or excluding files in others (like test directories).\n\nWithout Codebase Crawling, our system wouldn't have any code to analyze, and we wouldn't be able to generate a tutorial!\n\n## Key Concepts\n\n*   **Repository URL:** The address of the GitHub repository (e.g., `https://github.com/fastapi/fastapi`).\n*   **Local Directory:** The path to a directory on your computer (e.g., `/Users/myuser/myproject`).\n*   **Include Patterns:**  Patterns that specify which files to include (e.g., `*.py` for all Python files).  Think of these as \"keywords\" to search for in the library.\n*   **Exclude Patterns:** Patterns that specify which files to exclude (e.g., `tests/*` to exclude all files in the `tests` directory).  Think of these as \"forbidden words\" to avoid in the search.\n*   **Maximum File Size:** A limit on the size of the files to include. This helps prevent the system from trying to process very large files, which can be slow or cause errors.\n\n## How Codebase Crawling Works in Our Project\n\nThe Codebase Crawling process is handled by the `FetchRepo` node in our flow. Remember from [Flow Orchestration](02_flow_orchestration.md) that each node has three important steps: `prep`, `exec`, and `post`.\n\nLet's look at how Codebase Crawling is implemented in the `FetchRepo` node in `nodes.py`:\n\n```python\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"local_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        # ... (derive project name if not provided)\n\n        # Get file patterns directly from shared\n        include_patterns = shared[\"include_patterns\"]\n        exclude_patterns = shared[\"exclude_patterns\"]\n        max_file_size = shared[\"max_file_size\"]\n\n        return {\n            \"repo_url\": repo_url,\n            \"local_dir\": local_dir,\n            \"token\": shared.get(\"github_token\"),\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns,\n            \"max_file_size\": max_file_size,\n            \"use_relative_paths\": True\n        }\n\n    def exec(self, prep_res):\n        if prep_res[\"repo_url\"]:\n            print(f\"Crawling repository: {prep_res['repo_url']}...\")\n            result = crawl_github_files(\n                repo_url=prep_res[\"repo_url\"],\n                token=prep_res[\"token\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        else:\n            print(f\"Crawling directory: {prep_res['local_dir']}...\")\n            result = crawl_local_files(\n                directory=prep_res[\"local_dir\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n            \n        # Convert dict to list of tuples: [(path, content), ...]\n        files_list = list(result.get(\"files\", {}).items())\n        print(f\"Fetched {len(files_list)} files.\")\n        return files_list\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"files\"] = exec_res # List of (path, content) tuples\n```\n\nLet's break down what's happening:\n\n1.  **`prep(shared)`:**\n    *   This method retrieves the configuration values (repository URL, local directory, include patterns, exclude patterns, maximum file size) from the `shared` dictionary. This is how the node knows *what* to crawl and *how* to crawl it.\n    *   It prepares these values into a `prep_res` object which is a dictionary containing the configuration for the `exec` function to use.\n\n2.  **`exec(prep_res)`:**\n    *   This method checks if a `repo_url` is provided. If so, it calls the `crawl_github_files` function to crawl the GitHub repository.\n    *   If a `repo_url` is *not* provided (meaning we're crawling a local directory), it calls the `crawl_local_files` function to crawl the local directory.\n    *   The `crawl_github_files` and `crawl_local_files` functions do the actual work of fetching the files and their contents.\n    *   Finally, the result from the crawl is converted into a list of tuples where each tuple contains a file path and file content.\n\n3.  **`post(shared, prep_res, exec_res)`:**\n    *   This method takes the list of files (the `exec_res` output) and stores it in the `shared[\"files\"]` variable. This makes the files available to the next node in the flow, which is [Abstraction Identification](05_abstraction_identification.md).\n\n### Crawling GitHub Repositories: `crawl_github_files`\n\nThe `crawl_github_files` function in `utils/crawl_github_files.py` is responsible for fetching files from a GitHub repository.  Here's a simplified view of how it works:\n\n```python\ndef crawl_github_files(\n    repo_url, \n    token=None, \n    max_file_size: int = 1 * 1024 * 1024,  # 1 MB\n    use_relative_paths: bool = False,\n    include_patterns: Union[str, Set[str]] = None,\n    exclude_patterns: Union[str, Set[str]] = None\n):\n    # Parse the repo_url to extract owner, repo name, branch and path\n    # Uses the GitHub REST API to get the files and their content\n\n    files = {}\n    skipped_files = []\n\n    # Fetch contents of the repository at a specific path and commit\n    def fetch_contents(path):\n        # Use the GitHub REST API to get the contents of the given path\n        # If it's a file, download it and add it to the files dictionary\n        # If it's a directory, recursively call fetch_contents on the directory\n\n        # Check if file should be included based on patterns\n        def should_include_file(file_path: str, file_name: str) -> bool:\n            # If no include patterns are specified, include all files\n            # Otherwise, check if the file matches any of the include patterns\n            # If exclude patterns are specified, exclude files that match the exclude patterns\n            return True or False # Simplified logic\n\n    # Start crawling from the specified path\n    fetch_contents(specific_path)\n\n    return {\n        \"files\": files,\n        \"stats\": {\n            \"downloaded_count\": len(files),\n            \"skipped_count\": len(skipped_files),\n            \"skipped_files\": skipped_files,\n            \"base_path\": specific_path if use_relative_paths else None,\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns\n        }\n    }\n```\n\nHere's a breakdown:\n\n1.  **Parsing the Repository URL:** The function first parses the `repo_url` to extract the owner, repository name, branch/commit, and path within the repository. This information is used to construct the URLs for the GitHub API requests.\n2.  **Using the GitHub REST API:** It uses the GitHub REST API to fetch the contents of the repository. The API allows us to retrieve information about files and directories in the repository.\n3.  **Handling Different Content Types:** The API returns different data depending on whether the item is a file or a directory.\n    *   **Files:** For files, the function downloads the file content and stores it in the `files` dictionary.  It also checks the file size against the `max_file_size` limit.\n    *   **Directories:** For directories, the function recursively calls itself to process the contents of the subdirectory. This allows it to traverse the entire repository structure.\n4.  **Include/Exclude Patterns:** The function uses the `should_include_file` helper function to check if a file should be included based on the `include_patterns` and `exclude_patterns`. This allows you to filter the files that are included in the tutorial generation process.  If a file matches an exclude pattern, it will not be included.\n5.  **Error Handling:** The function includes error handling to deal with potential issues like rate limits (which can occur if you make too many requests to the GitHub API in a short period of time) and network errors.\n\n### Crawling Local Directories: `crawl_local_files`\n\nThe `crawl_local_files` function in `utils/crawl_local_files.py` is responsible for fetching files from a local directory. Here's a simplified view of how it works:\n\n```python\nimport os\nimport fnmatch\n\ndef crawl_local_files(directory, include_patterns=None, exclude_patterns=None, max_file_size=None, use_relative_paths=True):\n    \"\"\"\n    Crawl files in a local directory with similar interface as crawl_github_files.\n    \"\"\"\n    files_dict = {}\n\n    # Traverse through all files and folders in the directory\n    for root, _, files in os.walk(directory):\n        for filename in files:\n            filepath = os.path.join(root, filename) # Get the full path\n            relpath = os.path.relpath(filepath, directory) # Path relative to directory\n\n            # Check include and exclude patterns\n            included = True #Simplified\n            excluded = False #Simplified\n            if not included or excluded:\n                continue\n\n            # Check file size\n            if max_file_size and os.path.getsize(filepath) > max_file_size:\n                continue\n\n            try:\n                with open(filepath, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                files_dict[relpath] = content #Store content\n            except Exception as e:\n                print(f\"Warning: Could not read file {filepath}: {e}\")\n\n    return {\"files\": files_dict}\n```\n\nHere's a breakdown:\n\n1.  **Traversing the Directory:** The function uses `os.walk` to traverse the directory tree, visiting each directory and file within the specified `directory`.\n2.  **Constructing File Paths:** For each file, the function constructs the full path to the file using `os.path.join`. It also calculates the relative path to the file (relative to the specified `directory`) using `os.path.relpath`.\n3.  **Include/Exclude Patterns:** Similar to `crawl_github_files`, the function checks if a file should be included based on the `include_patterns` and `exclude_patterns`.\n4.  **File Size Check:** The function checks the file size against the `max_file_size` limit.\n5.  **Reading File Content:** If the file passes all the checks, the function reads the content of the file and stores it in the `files_dict` dictionary, using the file path as the key.\n6.  **Error Handling:** The function includes basic error handling to catch potential issues like file read errors.\n\n## How to Use Codebase Crawling\n\nYou don't directly call `crawl_github_files` or `crawl_local_files` yourself. Instead, you provide the necessary configuration (repository URL or local directory, include patterns, exclude patterns, maximum file size) when you run the `main.py` script.  The `FetchRepo` node then uses this configuration to crawl the codebase.\n\nFor example, to generate a tutorial for the `fastapi` repository, including only Python files and excluding the `tests` directory, you would use the following command:\n\n```bash\npython main.py --repo https://github.com/fastapi/fastapi -i \"*.py\" -e \"tests/*\"\n```\n\nTo generate a tutorial for a local directory, you would use the `--dir` option instead of `--repo`:\n\n```bash\npython main.py --dir /path/to/my/project -i \"*.py\" -e \"tests/*\"\n```\n\n## Why is Codebase Crawling Important?\n\n*   **Data Acquisition:** It's the first step in the tutorial generation process, providing the raw data (the codebase) that is needed to generate the tutorial.\n*   **Filtering:** It allows you to filter the files that are included in the tutorial, focusing on the most relevant parts of the codebase and ignoring irrelevant files (like test files or documentation).\n*   **Efficiency:** By limiting the maximum file size, it helps prevent the system from trying to process very large files, which can be slow or cause errors.\n\n## Conclusion\n\nIn this chapter, we've learned about Codebase Crawling, which is the process of gathering the necessary files from either a GitHub repository or a local directory. We've seen how it uses include and exclude patterns to filter files, and how it limits the maximum file size to prevent performance issues.\n\nIn the next chapter, [Abstraction Identification](05_abstraction_identification.md), we'll explore how we analyze the codebase to identify the key concepts that should be covered in the tutorial.\n\n---\n# Chapter 5: Abstraction Identification\n\n```markdown\n# Chapter 5: Abstraction Identification\n\nIn the previous chapter, [Codebase Crawling](04_codebase_crawling.md), we learned how to gather all the code files from a repository or local directory. Now, we need to figure out *what's important* in that code!  That's where Abstraction Identification comes in.\n\n## What is Abstraction Identification?\n\nImagine you're trying to learn a new language.  You wouldn't just start memorizing every single word in the dictionary, right? Instead, you'd focus on the most common and important words and grammar rules.\n\nAbstraction Identification is similar. It's the process of analyzing the codebase and identifying the most important concepts or components to explain in the tutorial.  These \"concepts\" are often called \"abstractions\" because they represent a higher-level, simplified view of the underlying code.\n\nThink of it like a detective piecing together clues from a crime scene to identify the key suspects and their roles in the crime.  The LLM is our detective, sifting through the code to find the most important pieces.\n\nWhy is this important?  Because if we tried to explain every single line of code, the tutorial would be overwhelming and confusing.  Abstraction Identification helps us focus on the core concepts that a beginner needs to understand.\n\n## Key Concepts\n\n*   **Abstractions:**  Simplified representations of complex code components. Think of them as key concepts or building blocks of the project. Examples could be \"API Endpoint,\" \"Database Connection,\" or \"User Authentication.\"\n*   **LLM (Large Language Model):**  The \"brain\" of our operation.  We use an LLM to analyze the code and identify the important abstractions.\n*   **Codebase Context:** The actual code files that the LLM analyzes.  This is the output of the [Codebase Crawling](04_codebase_crawling.md) step.\n*   **File Indices:** A list of index numbers which corresponds to file paths used for the LLM to refer back to the code.\n\n## How Abstraction Identification Works\n\nThe Abstraction Identification process is handled by the `IdentifyAbstractions` node in our flow.\n\nLet's look at the simplified steps in `nodes.py`:\n\n```python\nclass IdentifyAbstractions(Node):\n    def prep(self, shared):\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        \n        # Helper to create context from files, respecting limits (basic example)\n        def create_llm_context(files_data):\n            context = \"\"\n            file_info = [] # Store tuples of (index, path)\n            for i, (path, content) in enumerate(files_data):\n                entry = f\"--- File Index {i}: {path} ---\\n{content}\\n\\n\"\n                context += entry\n                file_info.append((i, path))\n\n            return context, file_info # file_info is list of (index, path)\n\n        context, file_info = create_llm_context(files_data)\n        # Format file info for the prompt (comment is just a hint for LLM)\n        file_listing_for_prompt = \"\\n\".join([f\"- {idx} # {path}\" for idx, path in file_info])\n        return context, file_listing_for_prompt, len(files_data), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, file_listing_for_prompt, file_count, project_name = prep_res  # Unpack project name\n        print(\"Identifying abstractions using LLM...\")\n        prompt = f\"\"\"\nFor the project `{project_name}`:\n\nCodebase Context:\n{context}\n\nAnalyze the codebase context.\nIdentify the top 5-10 core most important abstractions to help those new to the codebase.\n\nFor each abstraction, provide:\n1. A concise `name`.\n2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\nList of file indices and paths present in the context:\n{file_listing_for_prompt}\n\nFormat the output as a YAML list of dictionaries:\n\n```yaml\n- name: Query Processing\n  description: | \n    Explains what the abstraction does.\n    It's like a central dispatcher routing requests.\n  file_indices:\n    - 0 # path/to/file1.py\n    - 3 # path/to/related.py\n- name: Query Optimization\n  description: |\n    Another core concept, similar to a blueprint for objects.\n  file_indices:\n    - 5 # path/to/another.js\n# ... up to 10 abstractions\n```\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        abstractions = yaml.safe_load(yaml_str)\n\n        # Basic Validation\n        # Extract data and file indices to `shared` dictionary\n        # This is where we get the list of abstractions\n        validated_abstractions = []\n        # --- Add validation here as needed ---\n        print(f\"Identified {len(validated_abstractions)} abstractions.\")\n        return validated_abstractions\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"abstractions\"] = exec_res # List of {\"name\": str, \"description\": str, \"files\": [int]}\n```\n\nHere's a breakdown:\n\n1.  **`prep(shared)`:**\n    *   This method retrieves the `files` (the code) from the `shared` dictionary. This is the output of the [Codebase Crawling](04_codebase_crawling.md) step.\n    *   It creates an LLM-friendly context string by concatenating the contents of all the files.  This is what we'll feed to the LLM.\n    *   `file_listing_for_prompt` contains a list of file indices and their paths to tell the LLM where the context came from.\n\n2.  **`exec(prep_res)`:**\n    *   This method constructs a prompt for the LLM, telling it to analyze the code and identify the key abstractions. The prompt includes:\n        *   The codebase context (the concatenated code files).\n        *   Instructions on how to format the output (as a YAML list of dictionaries).\n    *   It calls the `call_llm` function (defined in `utils/call_llm.py`) to send the prompt to the LLM.\n    *   It receives the LLM's response, which should be a YAML string containing the list of abstractions, their descriptions, and their relevant file indices.\n    *   The YAML output is validated and parsed to store it in `shared[\"abstractions\"]`\n\n3.  **`post(shared, prep_res, exec_res)`:**\n    *   This method takes the list of abstractions and stores it in the `shared[\"abstractions\"]` variable. This makes the abstractions available to the next node in the flow, which is [Relationship Analysis](06_relationship_analysis.md).\n\n## Code Example: The LLM Prompt\n\nHere's an example of the prompt that we send to the LLM (from `nodes.py`):\n\n```python\nprompt = f\"\"\"\nFor the project `{project_name}`:\n\nCodebase Context:\n{context}\n\nAnalyze the codebase context.\nIdentify the top 5-10 core most important abstractions to help those new to the codebase.\n\nFor each abstraction, provide:\n1. A concise `name`.\n2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\nList of file indices and paths present in the context:\n{file_listing_for_prompt}\n\nFormat the output as a YAML list of dictionaries:\n\n```yaml\n- name: Query Processing\n  description: | \n    Explains what the abstraction does.\n    It's like a central dispatcher routing requests.\n  file_indices:\n    - 0 # path/to/file1.py\n    - 3 # path/to/related.py\n- name: Query Optimization\n  description: |\n    Another core concept, similar to a blueprint for objects.\n  file_indices:\n    - 5 # path/to/another.js\n# ... up to 10 abstractions\n```\"\"\"\n```\n\nThis prompt tells the LLM to:\n\n*   Analyze the code and identify the top 5-10 most important abstractions.\n*   For each abstraction, provide a name, a description, and a list of relevant file indices.\n*   Format the output as a YAML list of dictionaries.\n\nThe prompt is designed to be clear and concise, guiding the LLM to provide the information we need in a structured format.\n\n## Code Example: Validating the LLM Response\n\nAfter the LLM returns its response, we need to validate that it's in the correct format. Here's a simplified example of how we do this (from `nodes.py`):\n\n```python\nyaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\nabstractions = yaml.safe_load(yaml_str)\n\nif not isinstance(abstractions, list):\n    raise ValueError(\"LLM Output is not a list\")\n\nvalidated_abstractions = []\nfor item in abstractions:\n    if not isinstance(item, dict) or not all(k in item for k in [\"name\", \"description\", \"file_indices\"]):\n        raise ValueError(f\"Missing keys in abstraction item: {item}\")\n\n    # --- Add validation here as needed ---\n    validated_abstractions.append({\n        \"name\": item[\"name\"],\n        \"description\": item[\"description\"],\n        \"file_indices\": item[\"file_indices\"]\n    })\n```\n\nThis code checks that:\n\n*   The response is a valid YAML string.\n*   The YAML string represents a list.\n*   Each item in the list is a dictionary with the required keys (\"name\", \"description\", \"file_indices\").\n\nIf the response doesn't pass these checks, we raise a `ValueError` to indicate that there's a problem.\n\n## Using The Abstractions\n\nThe final output of the `IdentifyAbstractions` node is a list of dictionaries, where each dictionary represents an abstraction:\n\n```json\n[\n  {\n    \"name\": \"Query Processing\",\n    \"description\": \"A beginner-friendly explanation of what Query Processing is...\",\n    \"files\": [0, 3]\n  },\n  {\n    \"name\": \"Query Optimization\",\n    \"description\": \"A beginner-friendly explanation of what Query Optimization is...\",\n    \"files\": [5]\n  }\n]\n```\n\nThis list is stored in the `shared[\"abstractions\"]` variable and passed to the next node in the flow. The \"files\" key stores the list of integer indices, corresponding to paths from earlier in Codebase Crawling.\n\n## Why is Abstraction Identification Important?\n\n*   **Focus:** It helps us focus on the most important concepts in the codebase, making the tutorial more manageable and easier to understand.\n*   **Organization:** It provides a structure for the tutorial, allowing us to organize the content around the key abstractions.\n*   **Beginner-Friendliness:** By focusing on abstractions, we can explain the code in a more beginner-friendly way, using high-level concepts and analogies.\n\n## Conclusion\n\nIn this chapter, we've learned about Abstraction Identification, which is the process of analyzing the codebase and identifying the most important concepts to explain in the tutorial. We've seen how we use an LLM to perform this analysis, and how we validate the LLM's response to ensure that it's in the correct format.\n\nIn the next chapter, [Relationship Analysis](06_relationship_analysis.md), we'll explore how we analyze the relationships between these abstractions to understand the overall architecture of the codebase.\n```\n---\n# Chapter 6: Relationship Analysis\n\n```markdown\n# Chapter 6: Relationship Analysis\n\nIn the previous chapter, [Abstraction Identification](05_abstraction_identification.md), we identified the key concepts or \"abstractions\" within a codebase. Now, it's time to connect the dots!  Relationship Analysis helps us understand how these abstractions interact with each other to form the overall system.\n\n## What is Relationship Analysis?\n\nImagine you've identified all the important people in a city: the mayor, the police chief, the school superintendent, and the business owners.  Knowing who they are is useful, but you also need to understand *how they interact*.  Who reports to whom? Who depends on whom?  What are the power dynamics?\n\nRelationship Analysis is the process of understanding how the identified abstractions interact with each other. This involves using an LLM to analyze the code and determine the key relationships between the abstractions. It's like a network analyst mapping out the connections between different computers in a network to understand how data flows between them.\n\nWithout Relationship Analysis, we'd have a collection of isolated concepts, but we wouldn't understand how they work together to achieve the project's goals. It provides a crucial high-level summary and interaction diagram to give beginners a big picture understanding.\n\n## Key Concepts\n\n*   **Relationships:** The connections or interactions between abstractions.  Examples could be \"uses,\" \"manages,\" \"inherits from,\" or \"depends on.\"\n*   **Directed Graph:** A visual representation of the relationships between abstractions, where the direction of the arrow indicates the flow of interaction. We use a Mermaid diagram to represent this.\n*   **Project Summary:**  A high-level description of the project's purpose and functionality, based on the identified abstractions and their relationships.\n*   **File Indices:** A list of index numbers which corresponds to file paths used for the LLM to refer back to the code.\n*   **Abstraction Index:** The numbered index for a given abstraction.\n\n## How Relationship Analysis Works\n\nThe Relationship Analysis process is handled by the `AnalyzeRelationships` node in our flow.\n\nLet's look at the simplified steps in `nodes.py`:\n\n```python\nclass AnalyzeRelationships(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"] # Now contains 'files' list of indices\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n\n        # Create context with abstraction names, indices, descriptions, and relevant file snippets\n        context = \"Identified Abstractions:\\n\"\n        all_relevant_indices = set()\n        abstraction_info_for_prompt = []\n        for i, abstr in enumerate(abstractions):\n            # Use 'files' which contains indices directly\n            file_indices_str = \", \".join(map(str, abstr['files']))\n            info_line = f\"- Index {i}: {abstr['name']} (Relevant file indices: [{file_indices_str}])\\n  Description: {abstr['description']}\"\n            context += info_line + \"\\n\"\n            abstraction_info_for_prompt.append(f\"{i} # {abstr['name']}\")\n            all_relevant_indices.update(abstr['files'])\n\n        context += \"\\nRelevant File Snippets (Referenced by Index and Path):\\n\"\n        # Get content for relevant files using helper\n        relevant_files_content_map = get_content_for_indices(\n            files_data,\n            sorted(list(all_relevant_indices))\n        )\n        # Format file content for context\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path} ---\\n{content}\"\n            for idx_path, content in relevant_files_content_map.items()\n        )\n        context += file_context_str\n\n        return context, \"\\n\".join(abstraction_info_for_prompt), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, abstraction_listing, project_name = prep_res  # Unpack project name\n        print(\"Analyzing relationships using LLM...\")\n        prompt = f\"\"\"\nBased on the following abstractions and relevant code snippets from the project `{project_name}`:\n\nList of Abstraction Indices and Names:\n{abstraction_listing}\n\nContext (Abstractions, Descriptions, Code):\n{context}\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        relationships_data = yaml.safe_load(yaml_str)\n\n        if not isinstance(relationships_data, dict) or not all(k in relationships_data for k in [\"summary\", \"relationships\"]):\n            raise ValueError(\"LLM output is not a dict or missing keys ('summary', 'relationships')\")\n        if not isinstance(relationships_data[\"summary\"], str):\n             raise ValueError(\"summary is not a string\")\n        if not isinstance(relationships_data[\"relationships\"], list):\n             raise ValueError(\"relationships is not a list\")\n\n        # Validate relationships structure\n        validated_relationships = []\n        num_abstractions = len(abstraction_listing.split('\\n'))\n        for rel in relationships_data[\"relationships\"]:\n             # Check for 'label' key\n             if not isinstance(rel, dict) or not all(k in rel for k in [\"from_abstraction\", \"to_abstraction\", \"label\"]):\n                  raise ValueError(f\"Missing keys (expected from_abstraction, to_abstraction, label) in relationship item: {rel}\")\n             # Validate 'label' is a string\n             if not isinstance(rel[\"label\"], str):\n                  raise ValueError(f\"Relationship label is not a string: {rel}\")\n\n             # Validate indices\n             try:\n                 from_idx = int(str(rel[\"from_abstraction\"]).split('#')[0].strip())\n                 to_idx = int(str(rel[\"to_abstraction\"]).split('#')[0].strip())\n                 if not (0 <= from_idx < num_abstractions and 0 <= to_idx < num_abstractions):\n                      raise ValueError(f\"Invalid index in relationship: from={from_idx}, to={to_idx}. Max index is {num_abstractions-1}.\")\n                 validated_relationships.append({\n                     \"from\": from_idx,\n                     \"to\": to_idx,\n                     \"label\": rel[\"label\"] \n                 })\n             except (ValueError, TypeError):\n                  raise ValueError(f\"Could not parse indices from relationship: {rel}\")\n\n        print(\"Generated project summary and relationship details.\")\n        return {\n            \"summary\": relationships_data[\"summary\"],\n            \"details\": validated_relationships # Store validated, index-based relationships\n        }\n\n\n    def post(self, shared, prep_res, exec_res):\n        # Structure is now {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        shared[\"relationships\"] = exec_res\n```\n\nHere's a breakdown:\n\n1.  **`prep(shared)`:**\n    *   This method retrieves the `abstractions` and `files` from the `shared` dictionary.  The `abstractions` are the output of the [Abstraction Identification](05_abstraction_identification.md) step. The files are the output from [Codebase Crawling](04_codebase_crawling.md).\n    *   It creates a context string for the LLM.  This context includes:\n        *   A list of the identified abstractions, including their names, descriptions, and file indices.\n        *   The content of the relevant code files.\n2.  **`exec(prep_res)`:**\n    *   This method constructs a prompt for the LLM, telling it to analyze the abstractions and their relationships.  The prompt includes:\n        *   The context string created in the `prep` method.\n        *   Instructions on how to format the output (as a YAML string containing the project summary and the relationships between abstractions).\n    *   It calls the `call_llm` function (defined in `utils/call_llm.py`) to send the prompt to the LLM.\n    *   It receives the LLM's response, which should be a YAML string containing the project summary and the relationships between abstractions.\n    *   The output is validated and parsed to store it in `shared[\"relationships\"]`.\n3.  **`post(shared, prep_res, exec_res)`:**\n    *   This method takes the project summary and relationships and stores them in the `shared[\"relationships\"]` variable. This makes the relationships available to the next node in the flow, which is [Order Chapters](07_llm_interaction.md).\n\n## Code Example: The LLM Prompt\n\nHere's an example of the prompt that we send to the LLM (from `nodes.py`):\n\n```python\nprompt = f\"\"\"\nBased on the following abstractions and relevant code snippets from the project `{project_name}`:\n\nList of Abstraction Indices and Names:\n{abstraction_listing}\n\nContext (Abstractions, Descriptions, Code):\n{context}\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n\"\"\"\n```\n\nThis prompt tells the LLM to:\n\n*   Provide a high-level summary of the project's purpose and functionality.\n*   Identify the key relationships between the abstractions.\n*   For each relationship, specify the source abstraction, the target abstraction, and a brief label describing the relationship.\n*   Format the output as a YAML string.\n\n## Code Example: The Output of Relationship Analysis\n\nThe `exec` function returns a dictionary that contains the output of the `AnalyzeRelationships` node:\n\n```json\n{\n  \"summary\": \"This project is a web framework that provides a simple and efficient way to build APIs.\",\n  \"details\": [\n    {\n      \"from\": 0,\n      \"to\": 1,\n      \"label\": \"Manages\"\n    },\n    {\n      \"from\": 2,\n      \"to\": 0,\n      \"label\": \"Provides config\"\n    }\n  ]\n}\n```\n\nHere's a breakdown:\n\n*   `\"summary\"`: A short, beginner-friendly explanation of the project's goal. This is used in the introduction of the tutorial.\n*   `\"details\"`: A list of the relationships between abstractions.\n    *   `\"from\"`: The index of the source abstraction.\n    *   `\"to\"`: The index of the target abstraction.\n    *   `\"label\"`: A short description of the relationship.\n\n## Using The Relationships\n\nThe final output of the `AnalyzeRelationships` node is a dictionary containing the summary and relationship details. This dictionary is stored in the `shared[\"relationships\"]` variable and is used by later nodes.\n\nFor example, the `OrderChapters` node uses the relationships to determine the best order to present the abstractions in the tutorial.  The `CombineTutorial` node uses the relationships to generate a Mermaid diagram visualizing the connections between the abstractions, and to generate the tutorial index page.\n\n## Why is Relationship Analysis Important?\n\n*   **Context:** It provides a high-level overview of the project's purpose and functionality.\n*   **Understanding:** It helps to understand how the different abstractions interact with each other to form the overall system.\n*   **Visualization:** It allows us to visualize the relationships between abstractions, making it easier to grasp the overall architecture of the codebase.\n*   **Ordering:** It informs the ordering of chapters in the tutorial, ensuring that concepts are presented in a logical and easy-to-understand sequence.\n\n## Conclusion\n\nIn this chapter, we've learned about Relationship Analysis, which is the process of understanding how the identified abstractions interact with each other. We've seen how we use an LLM to perform this analysis, and how we use the results to generate a project summary and visualize the relationships between abstractions.\n\nIn the next chapter, [LLM Interaction](07_llm_interaction.md), we'll explore how we interact with the LLM to generate the actual content for the tutorial chapters.\n```\n---\n# Chapter 7: LLM Interaction\n\n```markdown\n# Chapter 7: LLM Interaction\n\nIn the previous chapter, [Relationship Analysis](06_relationship_analysis.md), we learned how to understand the connections between the key concepts in a codebase. Now, we're going to focus on how we use a Large Language Model (LLM) to actually *generate* the content for our tutorial! This is where the magic happens!\n\n## What is LLM Interaction?\n\nImagine you're a writer, but you have a super-smart AI assistant that can help you research, outline, and even write entire sections of your book. This assistant is the LLM Interaction module. It's the part of our system that talks to the LLM, sends it instructions, and receives its responses.\n\nThe LLM Interaction module handles all calls to the Large Language Model (LLM). This includes:\n\n*   **Prompt Engineering:** Carefully crafting the instructions we send to the LLM to get the best possible results.\n*   **Caching Responses:** Saving the LLM's responses so we don't have to ask it the same question multiple times. This saves time and money.\n*   **Logging Prompts and Responses:** Keeping a record of everything we send to and receive from the LLM for debugging and analysis.\n*   **Providing a Consistent Interface:** Making it easy for other modules to interact with the LLM without having to worry about the details of how the LLM works.\n\nThink of it as a dedicated interpreter translating human instructions into machine-understandable code and back again! Without the LLM Interaction module, we'd have no way to leverage the power of LLMs to generate our tutorials.\n\n## Key Concepts\n\n*   **Prompt:** The instruction or question we send to the LLM. It's like telling your AI assistant what you want it to do.\n*   **Response:** The output we receive from the LLM. It's like your AI assistant's answer to your question.\n*   **Caching:** Storing the LLM's responses so we can reuse them later. This is like remembering the answer to a question so you don't have to look it up again.\n*   **Logging:** Recording all the interactions with the LLM for debugging and analysis. This is like keeping a notebook of all your conversations with your AI assistant.\n*   **API Key:** A secret code that allows us to access the LLM. It's like the password you need to log into your AI assistant.\n\n## How LLM Interaction Works\n\nThe LLM Interaction is handled by the `call_llm` function in `utils/call_llm.py`. All LLM Interaction nodes will call this central function.\n\nHere's a simplified view of how it works:\n\n```python\nfrom google import genai\nimport os\nimport logging\nimport json\nfrom datetime import datetime\n\n# Configure logging\nlog_directory = os.getenv(\"LOG_DIR\", \"logs\")\nos.makedirs(log_directory, exist_ok=True)\nlog_file = os.path.join(log_directory, f\"llm_calls_{datetime.now().strftime('%Y%m%d')}.log\")\n\n# Set up logger\nlogger = logging.getLogger(\"llm_logger\")\nlogger.setLevel(logging.INFO)\nlogger.propagate = False  # Prevent propagation to root logger\nfile_handler = logging.FileHandler(log_file)\nfile_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\nlogger.addHandler(file_handler)\n\n# Simple cache configuration\ncache_file = \"llm_cache.json\"\n\n# By default, we Google Gemini 2.5 pro, as it shows great performance for code understanding\ndef call_llm(prompt: str, use_cache: bool = True) -> str:\n    # Log the prompt\n    logger.info(f\"PROMPT: {prompt}\")\n    \n    # Check cache if enabled\n    if use_cache:\n        # Load cache from disk\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, 'r') as f:\n                    cache = json.load(f)\n            except:\n                logger.warning(f\"Failed to load cache, starting with empty cache\")\n        \n        # Return from cache if exists\n        if prompt in cache:\n            logger.info(f\"RESPONSE: {cache[prompt]}\")\n            return cache[prompt]\n    \n    # Call the LLM if not in cache or cache disabled\n    client = genai.Client(\n        vertexai=True, \n        # TODO: change to your own project id and location\n        project=os.getenv(\"GEMINI_PROJECT_ID\", \"your-project-id\"),\n        location=os.getenv(\"GEMINI_LOCATION\", \"us-central1\")\n    )\n    # You can comment the previous line and use the AI Studio key instead:\n    # client = genai.Client(\n    #     api_key=os.getenv(\"GEMINI_API_KEY\", \"your-api_key\"),\n    # )\n    model = os.getenv(\"GEMINI_MODEL\", \"gemini-2.5-pro-exp-03-25\")\n    response = client.models.generate_content(\n        model=model,\n        contents=[prompt]\n    )\n    response_text = response.text\n    \n    # Log the response\n    logger.info(f\"RESPONSE: {response_text}\")\n    \n    # Update cache if enabled\n    if use_cache:\n        # Load cache again to avoid overwrites\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, 'r') as f:\n                    cache = json.load(f)\n            except:\n                pass\n        \n        # Add to cache and save\n        cache[prompt] = response_text\n        try:\n            with open(cache_file, 'w') as f:\n                json.dump(cache, f)\n        except Exception as e:\n            logger.error(f\"Failed to save cache: {e}\")\n    \n    return response_text\n```\n\nLet's break down the code:\n\n1.  **Importing Libraries:** We import the necessary libraries for interacting with the LLM, handling environment variables, logging, and caching.\n2.  **Configuring Logging:** We set up a logger to record all the prompts and responses. This helps us debug any issues and analyze the LLM's behavior.\n3.  **Configuring Caching:** We set up a simple cache to store the LLM's responses. This saves us from having to make the same API calls multiple times.\n4.  **Calling the LLM:** The `call_llm` function takes a prompt as input and sends it to the LLM. It handles authentication, error handling, and response parsing.\n5.  **Logging the Prompt and Response:** The function logs the prompt and response to the log file.\n6.  **Updating the Cache:** If caching is enabled, the function updates the cache with the prompt and response.\n7. **Choosing LLM Model:** The Gemini family of models are used by default, but the code has examples for how to call other models like Anthropic Claude or OpenAI.\n\n## Code Example: Writing a Chapter\n\nThe `WriteChapters` node uses the `call_llm` function to generate the content for each chapter of the tutorial.\n\n```python\nclass WriteChapters(BatchNode):\n    def prep(self, shared):\n        # ... (preparation logic) ...\n\n    def exec(self, item):\n        # ... (item preparation) ...\n\n        prompt = f\"\"\"\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `{project_name}` about the concept: \"{abstraction_name}\". This is Chapter {chapter_num}.\n\nConcept Details:\n- Description:\n{item[\"abstraction_details\"][\"description\"]}\n\nComplete Tutorial Structure:\n{item[\"full_chapter_listing\"]}\n\nContext from previous chapters (summary):\n{previous_chapters_summary if previous_chapters_summary else \"This is the first chapter.\"}\n\nRelevant Code Snippets:\n{file_context_str if file_context_str else \"No specific code snippets provided for this abstraction.\"}\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter {chapter_num}: {abstraction_name}`).\n        \n        # ... (more instructions) ...\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n\"\"\"\n        chapter_content = call_llm(prompt)\n        # ... (validation and cleanup) ...\n        return chapter_content # Return the Markdown string\n\n    def post(self, shared, prep_res, exec_res_list):\n        # ... (post-processing logic) ...\n```\n\nHere's a breakdown:\n\n1.  **Preparing the Prompt:** The `prep` and `exec` methods construct a detailed prompt that tells the LLM exactly what we want it to do:\n    *   Write a tutorial chapter about a specific concept.\n    *   Use a beginner-friendly tone.\n    *   Follow a specific structure.\n    *   Incorporate relevant code snippets.\n    *   Reference previous chapters.\n2.  **Calling the LLM:** The `exec` method calls the `call_llm` function to send the prompt to the LLM.\n3.  **Receiving the Response:** The `call_llm` function returns the LLM's response, which is the generated Markdown content for the chapter.\n4.  **Validating and Cleaning Up:** The `exec` method validates the LLM's response to make sure it's in the correct format and cleans up any unwanted characters.\n5.  **Returning the Result:** The `exec` method returns the validated and cleaned-up Markdown content for the chapter.\n\n## Setting Up the LLM API Key\n\nTo use the LLM Interaction module, you'll need to set up an API key for the LLM you want to use. The default call_llm uses Google's Gemini API, but code samples are shown for Anthropic Claude and OpenAI. The Gemini API can be set up with the Google AI Studio key or using Google Cloud.\n\nFor Google AI Studio:\n\n1.  Go to the [Google AI Studio website](https://makersuite.google.com/).\n2.  Create an account or log in.\n3.  Follow the instructions to create an API key.\n4.  Set the `GEMINI_API_KEY` environment variable to your API key.\n\n    ```bash\n    export GEMINI_API_KEY=<your_api_key>\n    ```\n\nFor Google Cloud:\n\n1.  Create a Google Cloud project.\n2.  Enable the Gemini API for your project.\n3.  Create a service account with the necessary permissions.\n4.  Download the service account key file.\n5.  Set the `GEMINI_PROJECT_ID` environment variable to your project ID.\n6.  Set the `GEMINI_LOCATION` environment variable to your project location.\n\n## Caching and Logging\n\nThe `call_llm` function automatically caches the LLM's responses to a JSON file called `llm_cache.json`. This helps to speed up the tutorial generation process and reduce the cost of calling the LLM.\n\nThe `call_llm` function also logs all the prompts and responses to a log file in the `logs` directory. This helps to debug any issues and analyze the LLM's behavior.\n\n## Why is LLM Interaction Important?\n\n*   **Content Generation:** It allows us to automatically generate the content for our tutorials, saving us a lot of time and effort.\n*   **Scalability:** It allows us to generate tutorials for any codebase, no matter how large or complex.\n*   **Consistency:** It ensures that the tutorials are written in a consistent style and tone.\n*   **Beginner-Friendly:** By providing detailed instructions to the LLM, we can ensure that the tutorials are easy for beginners to understand.\n\n## Conclusion\n\nIn this chapter, we've learned about LLM Interaction, which is the process of using a Large Language Model (LLM) to generate the content for our tutorials. We've seen how we prepare the prompts, call the LLM, receive the responses, and validate the results.\n\nIn the next chapter, [Chapter Writing](08_chapter_writing.md), we'll delve deeper into the techniques for crafting effective and engaging tutorial chapters.\n```\n\nRelevant Code Snippets:\n--- File: nodes.py ---\nimport os\nimport yaml\nfrom pocketflow import Node, BatchNode\nfrom utils.crawl_github_files import crawl_github_files\nfrom utils.call_llm import call_llm # Assuming you have this utility\nfrom utils.crawl_local_files import crawl_local_files\nfrom converter.md_to_html import convert_all_md_to_html\n\n\n# Helper to get content for specific file indices\ndef get_content_for_indices(files_data, indices):\n    content_map = {}\n    for i in indices:\n        if 0 <= i < len(files_data):\n            path, content = files_data[i]\n            content_map[f\"{i} # {path}\"] = content # Use index + path as key for context\n    return content_map\n\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"local_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        if not project_name:\n            # Basic name derivation from URL or directory\n            if repo_url:\n                project_name = repo_url.split('/')[-1].replace('.git', '')\n            else:\n                project_name = os.path.basename(os.path.abspath(local_dir))\n            shared[\"project_name\"] = project_name\n\n        # Get file patterns directly from shared\n        include_patterns = shared[\"include_patterns\"]\n        exclude_patterns = shared[\"exclude_patterns\"]\n        max_file_size = shared[\"max_file_size\"]\n\n        return {\n            \"repo_url\": repo_url,\n            \"local_dir\": local_dir,\n            \"token\": shared.get(\"github_token\"),\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns,\n            \"max_file_size\": max_file_size,\n            \"use_relative_paths\": True\n        }\n\n    def exec(self, prep_res):\n        if prep_res[\"repo_url\"]:\n            print(f\"Crawling repository: {prep_res['repo_url']}...\")\n            result = crawl_github_files(\n                repo_url=prep_res[\"repo_url\"],\n                token=prep_res[\"token\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        else:\n            print(f\"Crawling directory: {prep_res['local_dir']}...\")\n            result = crawl_local_files(\n                directory=prep_res[\"local_dir\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n            \n        # Convert dict to list of tuples: [(path, content), ...]\n        files_list = list(result.get(\"files\", {}).items())\n        print(f\"Fetched {len(files_list)} files.\")\n        return files_list\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"files\"] = exec_res # List of (path, content) tuples\n\nclass IdentifyAbstractions(Node):\n    def prep(self, shared):\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        \n        # Helper to create context from files, respecting limits (basic example)\n        def create_llm_context(files_data):\n            context = \"\"\n            file_info = [] # Store tuples of (index, path)\n            for i, (path, content) in enumerate(files_data):\n                entry = f\"--- File Index {i}: {path} ---\\n{content}\\n\\n\"\n                context += entry\n                file_info.append((i, path))\n\n            return context, file_info # file_info is list of (index, path)\n\n        context, file_info = create_llm_context(files_data)\n        # Format file info for the prompt (comment is just a hint for LLM)\n        file_listing_for_prompt = \"\\n\".join([f\"- {idx} # {path}\" for idx, path in file_info])\n        return context, file_listing_for_prompt, len(files_data), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, file_listing_for_prompt, file_count, project_name = prep_res  # Unpack project name\n        print(\"Identifying abstractions using LLM...\")\n        prompt = f\"\"\"\nFor the project `{project_name}`:\n\nCodebase Context:\n{context}\n\nAnalyze the codebase context.\nIdentify the top 5-10 core most important abstractions to help those new to the codebase.\n\nFor each abstraction, provide:\n1. A concise `name`.\n2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\nList of file indices and paths present in the context:\n{file_listing_for_prompt}\n\nFormat the output as a YAML list of dictionaries:\n\n```yaml\n- name: Query Processing\n  description: | \n    Explains what the abstraction does.\n    It's like a central dispatcher routing requests.\n  file_indices:\n    - 0 # path/to/file1.py\n    - 3 # path/to/related.py\n- name: Query Optimization\n  description: |\n    Another core concept, similar to a blueprint for objects.\n  file_indices:\n    - 5 # path/to/another.js\n# ... up to 10 abstractions\n```\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        abstractions = yaml.safe_load(yaml_str)\n\n        if not isinstance(abstractions, list):\n            raise ValueError(\"LLM Output is not a list\")\n\n        validated_abstractions = []\n        for item in abstractions:\n            if not isinstance(item, dict) or not all(k in item for k in [\"name\", \"description\", \"file_indices\"]):\n                raise ValueError(f\"Missing keys in abstraction item: {item}\")\n            if not isinstance(item[\"description\"], str):\n                 raise ValueError(f\"description is not a string in item: {item}\")\n            if not isinstance(item[\"file_indices\"], list):\n                 raise ValueError(f\"file_indices is not a list in item: {item}\")\n\n            # Validate indices\n            validated_indices = []\n            for idx_entry in item[\"file_indices\"]:\n                 try:\n                     if isinstance(idx_entry, int):\n                         idx = idx_entry\n                     elif isinstance(idx_entry, str) and '#' in idx_entry:\n                          idx = int(idx_entry.split('#')[0].strip())\n                     else:\n                          idx = int(str(idx_entry).strip())\n\n                     if not (0 <= idx < file_count):\n                         raise ValueError(f\"Invalid file index {idx} found in item {item['name']}. Max index is {file_count - 1}.\")\n                     validated_indices.append(idx)\n                 except (ValueError, TypeError):\n                      raise ValueError(f\"Could not parse index from entry: {idx_entry} in item {item['name']}\")\n\n            item[\"files\"] = sorted(list(set(validated_indices)))\n            # Store only the required fields\n            validated_abstractions.append({\n                \"name\": item[\"name\"],\n                \"description\": item[\"description\"],\n                \"files\": item[\"files\"]\n            })\n\n        print(f\"Identified {len(validated_abstractions)} abstractions.\")\n        return validated_abstractions\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"abstractions\"] = exec_res # List of {\"name\": str, \"description\": str, \"files\": [int]}\n\nclass AnalyzeRelationships(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"] # Now contains 'files' list of indices\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n\n        # Create context with abstraction names, indices, descriptions, and relevant file snippets\n        context = \"Identified Abstractions:\\n\"\n        all_relevant_indices = set()\n        abstraction_info_for_prompt = []\n        for i, abstr in enumerate(abstractions):\n            # Use 'files' which contains indices directly\n            file_indices_str = \", \".join(map(str, abstr['files']))\n            info_line = f\"- Index {i}: {abstr['name']} (Relevant file indices: [{file_indices_str}])\\n  Description: {abstr['description']}\"\n            context += info_line + \"\\n\"\n            abstraction_info_for_prompt.append(f\"{i} # {abstr['name']}\")\n            all_relevant_indices.update(abstr['files'])\n\n        context += \"\\nRelevant File Snippets (Referenced by Index and Path):\\n\"\n        # Get content for relevant files using helper\n        relevant_files_content_map = get_content_for_indices(\n            files_data,\n            sorted(list(all_relevant_indices))\n        )\n        # Format file content for context\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path} ---\\n{content}\"\n            for idx_path, content in relevant_files_content_map.items()\n        )\n        context += file_context_str\n\n        return context, \"\\n\".join(abstraction_info_for_prompt), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, abstraction_listing, project_name = prep_res  # Unpack project name\n        print(\"Analyzing relationships using LLM...\")\n        prompt = f\"\"\"\nBased on the following abstractions and relevant code snippets from the project `{project_name}`:\n\nList of Abstraction Indices and Names:\n{abstraction_listing}\n\nContext (Abstractions, Descriptions, Code):\n{context}\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        relationships_data = yaml.safe_load(yaml_str)\n\n        if not isinstance(relationships_data, dict) or not all(k in relationships_data for k in [\"summary\", \"relationships\"]):\n            raise ValueError(\"LLM output is not a dict or missing keys ('summary', 'relationships')\")\n        if not isinstance(relationships_data[\"summary\"], str):\n             raise ValueError(\"summary is not a string\")\n        if not isinstance(relationships_data[\"relationships\"], list):\n             raise ValueError(\"relationships is not a list\")\n\n        # Validate relationships structure\n        validated_relationships = []\n        num_abstractions = len(abstraction_listing.split('\\n'))\n        for rel in relationships_data[\"relationships\"]:\n             # Check for 'label' key\n             if not isinstance(rel, dict) or not all(k in rel for k in [\"from_abstraction\", \"to_abstraction\", \"label\"]):\n                  raise ValueError(f\"Missing keys (expected from_abstraction, to_abstraction, label) in relationship item: {rel}\")\n             # Validate 'label' is a string\n             if not isinstance(rel[\"label\"], str):\n                  raise ValueError(f\"Relationship label is not a string: {rel}\")\n\n             # Validate indices\n             try:\n                 from_idx = int(str(rel[\"from_abstraction\"]).split('#')[0].strip())\n                 to_idx = int(str(rel[\"to_abstraction\"]).split('#')[0].strip())\n                 if not (0 <= from_idx < num_abstractions and 0 <= to_idx < num_abstractions):\n                      raise ValueError(f\"Invalid index in relationship: from={from_idx}, to={to_idx}. Max index is {num_abstractions-1}.\")\n                 validated_relationships.append({\n                     \"from\": from_idx,\n                     \"to\": to_idx,\n                     \"label\": rel[\"label\"] \n                 })\n             except (ValueError, TypeError):\n                  raise ValueError(f\"Could not parse indices from relationship: {rel}\")\n\n        print(\"Generated project summary and relationship details.\")\n        return {\n            \"summary\": relationships_data[\"summary\"],\n            \"details\": validated_relationships # Store validated, index-based relationships\n        }\n\n\n    def post(self, shared, prep_res, exec_res):\n        # Structure is now {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        shared[\"relationships\"] = exec_res\n\nclass OrderChapters(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"]\n        relationships = shared[\"relationships\"]\n        project_name = shared[\"project_name\"]  # Get project name\n\n        # Prepare context for the LLM\n        abstraction_info_for_prompt = []\n        for i, a in enumerate(abstractions):\n            abstraction_info_for_prompt.append(f\"- {i} # {a['name']}\")\n        abstraction_listing = \"\\n\".join(abstraction_info_for_prompt)\n\n        context = f\"Project Summary:\\n{relationships['summary']}\\n\\n\"\n        context += \"Relationships (Indices refer to abstractions above):\\n\"\n        for rel in relationships['details']:\n             from_name = abstractions[rel['from']]['name']\n             to_name = abstractions[rel['to']]['name']\n             # Use 'label' instead of 'desc'\n             context += f\"- From {rel['from']} ({from_name}) to {rel['to']} ({to_name}): {rel['label']}\\n\"\n\n        return abstraction_listing, context, len(abstractions), project_name\n\n    def exec(self, prep_res):\n        abstraction_listing, context, num_abstractions, project_name = prep_res\n        print(\"Determining chapter order using LLM...\")\n        prompt = f\"\"\"\nGiven the following project abstractions and their relationships for the project ```` {project_name} ````:\n\nAbstractions (Index # Name):\n{abstraction_listing}\n\nContext about relationships and project summary:\n{context}\n\nIf you are going to make a tutorial for ```` {project_name} ````, what is the best order to explain these abstractions, from first to last?\nIdeally, first explain those that are the most important or foundational, perhaps user-facing concepts or entry points. Then move to more detailed, lower-level implementation details or supporting concepts.\n\nOutput the ordered list of abstraction indices, including the name in a comment for clarity. Use the format `idx # AbstractionName`.\n\n```yaml\n- 2 # FoundationalConcept\n- 0 # CoreClassA\n- 1 # CoreClassB (uses CoreClassA)\n- ...\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        # Rely on Node's built-in retry/fallback\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        ordered_indices_raw = yaml.safe_load(yaml_str)\n\n        if not isinstance(ordered_indices_raw, list):\n            raise ValueError(\"LLM output is not a list\")\n\n        ordered_indices = []\n        seen_indices = set()\n        for entry in ordered_indices_raw:\n            try:\n                 if isinstance(entry, int):\n                     idx = entry\n                 elif isinstance(entry, str) and '#' in entry:\n                      idx = int(entry.split('#')[0].strip())\n                 else:\n                      idx = int(str(entry).strip())\n\n                 if not (0 <= idx < num_abstractions):\n                      raise ValueError(f\"Invalid index {idx} in ordered list. Max index is {num_abstractions-1}.\")\n                 if idx in seen_indices:\n                     raise ValueError(f\"Duplicate index {idx} found in ordered list.\")\n                 ordered_indices.append(idx)\n                 seen_indices.add(idx)\n\n            except (ValueError, TypeError):\n                 raise ValueError(f\"Could not parse index from ordered list entry: {entry}\")\n\n        # Check if all abstractions are included\n        if len(ordered_indices) != num_abstractions:\n             raise ValueError(f\"Ordered list length ({len(ordered_indices)}) does not match number of abstractions ({num_abstractions}). Missing indices: {set(range(num_abstractions)) - seen_indices}\")\n\n        print(f\"Determined chapter order (indices): {ordered_indices}\")\n        return ordered_indices # Return the list of indices\n\n    def post(self, shared, prep_res, exec_res):\n        # exec_res is already the list of ordered indices\n        shared[\"chapter_order\"] = exec_res # List of indices\n\nclass WriteChapters(BatchNode):\n    def prep(self, shared):\n        chapter_order = shared[\"chapter_order\"] # List of indices\n        abstractions = shared[\"abstractions\"]   # List of dicts, now using 'files' with indices\n        files_data = shared[\"files\"]\n        # Get already written chapters to provide context\n        # We store them temporarily during the batch run, not in shared memory yet\n        # The 'previous_chapters_summary' will be built progressively in the exec context\n        self.chapters_written_so_far = [] # Use instance variable for temporary storage across exec calls\n\n        # Create a complete list of all chapters\n        all_chapters = []\n        chapter_filenames = {} # Store chapter filename mapping for linking\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                chapter_num = i + 1\n                chapter_name = abstractions[abstraction_index][\"name\"]\n                # Create safe filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in chapter_name).lower()\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                # Format with link\n                all_chapters.append(f\"{chapter_num}. [{chapter_name}]({filename})\")\n                # Store mapping of chapter index to filename for linking\n                chapter_filenames[abstraction_index] = {\"num\": chapter_num, \"name\": chapter_name, \"filename\": filename}\n        \n        # Create a formatted string with all chapters\n        full_chapter_listing = \"\\n\".join(all_chapters)\n\n        items_to_process = []\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                abstraction_details = abstractions[abstraction_index]\n                # Use 'files' (list of indices) directly\n                related_file_indices = abstraction_details.get(\"files\", [])\n                # Get content using helper, passing indices\n                related_files_content_map = get_content_for_indices(files_data, related_file_indices)\n                \n                # Get previous chapter info for transitions\n                prev_chapter = None\n                if i > 0:\n                    prev_idx = chapter_order[i-1]\n                    prev_chapter = chapter_filenames[prev_idx]\n                \n                # Get next chapter info for transitions\n                next_chapter = None\n                if i < len(chapter_order) - 1:\n                    next_idx = chapter_order[i+1]\n                    next_chapter = chapter_filenames[next_idx]\n\n                items_to_process.append({\n                    \"chapter_num\": i + 1,\n                    \"abstraction_index\": abstraction_index,\n                    \"abstraction_details\": abstraction_details,\n                    \"related_files_content_map\": related_files_content_map,\n                    \"project_name\": shared[\"project_name\"],  # Add project name\n                    \"full_chapter_listing\": full_chapter_listing,  # Add the full chapter listing\n                    \"chapter_filenames\": chapter_filenames,  # Add chapter filenames mapping\n                    \"prev_chapter\": prev_chapter,  # Add previous chapter info\n                    \"next_chapter\": next_chapter,  # Add next chapter info\n                    # previous_chapters_summary will be added dynamically in exec\n                })\n            else:\n                print(f\"Warning: Invalid abstraction index {abstraction_index} in chapter_order. Skipping.\")\n\n        print(f\"Preparing to write {len(items_to_process)} chapters...\")\n        return items_to_process # Iterable for BatchNode\n\n    def exec(self, item):\n        # This runs for each item prepared above\n        abstraction_name = item[\"abstraction_details\"][\"name\"]\n        chapter_num = item[\"chapter_num\"]\n        project_name = item.get(\"project_name\")  # Get from item\n        print(f\"Writing chapter {chapter_num} for: {abstraction_name} using LLM...\")\n\n        # Prepare file context string from the map\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path.split('# ')[1] if '# ' in idx_path else idx_path} ---\\n{content}\"\n            for idx_path, content in item[\"related_files_content_map\"].items()\n        )\n\n        # Get summary of chapters written *before* this one\n        # Use the temporary instance variable\n        previous_chapters_summary = \"\\n---\\n\".join(self.chapters_written_so_far)\n\n\n        prompt = f\"\"\"\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `{project_name}` about the concept: \"{abstraction_name}\". This is Chapter {chapter_num}.\n\nConcept Details:\n- Description:\n{item[\"abstraction_details\"][\"description\"]}\n\nComplete Tutorial Structure:\n{item[\"full_chapter_listing\"]}\n\nContext from previous chapters (summary):\n{previous_chapters_summary if previous_chapters_summary else \"This is the first chapter.\"}\n\nRelevant Code Snippets:\n{file_context_str if file_context_str else \"No specific code snippets provided for this abstraction.\"}\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter {chapter_num}: {abstraction_name}`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n\"\"\"\n        chapter_content = call_llm(prompt)\n        # Basic validation/cleanup\n        actual_heading = f\"# Chapter {chapter_num}: {abstraction_name}\"\n        if not chapter_content.strip().startswith(f\"# Chapter {chapter_num}\"):\n             # Add heading if missing or incorrect, trying to preserve content\n             lines = chapter_content.strip().split('\\n')\n             if lines and lines[0].strip().startswith(\"#\"): # If there's some heading, replace it\n                 lines[0] = actual_heading\n                 chapter_content = \"\\n\".join(lines)\n             else: # Otherwise, prepend it\n                 chapter_content = f\"{actual_heading}\\n\\n{chapter_content}\"\n\n        # Add the generated content to our temporary list for the next iteration's context\n        self.chapters_written_so_far.append(chapter_content)\n\n        return chapter_content # Return the Markdown string\n\n    def post(self, shared, prep_res, exec_res_list):\n        # exec_res_list contains the generated Markdown for each chapter, in order\n        shared[\"chapters\"] = exec_res_list\n        # Clean up the temporary instance variable\n        del self.chapters_written_so_far\n        print(f\"Finished writing {len(exec_res_list)} chapters.\")\n\nclass CombineTutorial(Node):\n    def prep(self, shared):\n        project_name = shared[\"project_name\"]\n        output_base_dir = shared.get(\"output_dir\", \"output\") # Default output dir\n        output_path = os.path.join(output_base_dir, project_name)\n        repo_url = shared[\"repo_url\"]  # Get the repository URL\n\n        # Use 'label' from relationships_data['details']\n        relationships_data = shared[\"relationships\"] # {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        chapter_order = shared[\"chapter_order\"] # indices\n        abstractions = shared[\"abstractions\"]   # list of dicts\n        chapters_content = shared[\"chapters\"]   # list of strings\n\n        # --- Generate Mermaid Diagram ---\n        mermaid_lines = [\"flowchart TD\"]\n        # Add nodes for each abstraction\n        for i, abstr in enumerate(abstractions):\n            # Sanitize name for Mermaid ID and label\n            node_id = f\"A{i}\"\n            sanitized_name = abstr['name'].replace('\"', '')\n            node_label = sanitized_name # Using sanitized name only, no index\n            mermaid_lines.append(f'    {node_id}[\"{node_label}\"]')\n        # Add edges for relationships using 'label'\n        for rel in relationships_data['details']:\n            from_node_id = f\"A{rel['from']}\"\n            to_node_id = f\"A{rel['to']}\"\n            # Sanitize 'label' for edge label\n            edge_label = rel['label'].replace('\"', '').replace('\\n', ' ') # Basic sanitization\n            # Limit edge label length for readability (optional, but good for diagrams)\n            max_label_len = 30 # Make it shorter for labels\n            if len(edge_label) > max_label_len:\n                edge_label = edge_label[:max_label_len-3] + \"...\"\n            mermaid_lines.append(f'    {from_node_id} -- \"{edge_label}\" --> {to_node_id}')\n\n        mermaid_diagram = \"\\n\".join(mermaid_lines)\n        # --- End Mermaid ---\n\n\n        # Prepare index.md content\n        index_content = f\"# Tutorial: {project_name}\\n\\n\"\n        index_content += f\"{relationships_data['summary']}\\n\\n\"\n        index_content += f\"**Source Repository:** [{repo_url}]({repo_url})\\n\\n\"\n\n        # Add Mermaid diagram for relationships\n        index_content += \"```mermaid\\n\"\n        index_content += mermaid_diagram + \"\\n\"\n        index_content += \"```\\n\\n\"\n\n        index_content += \"## Chapters\\n\\n\"\n\n        chapter_files = []\n        # Generate chapter links based on the determined order\n        for i, abstraction_index in enumerate(chapter_order):\n            # Ensure index is valid and we have content for it\n            if 0 <= abstraction_index < len(abstractions) and i < len(chapters_content):\n                abstraction_name = abstractions[abstraction_index][\"name\"]\n                # Sanitize name for filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in abstraction_name).lower()\n                # Use chapter number (i+1) for ordering filename\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                index_content += f\"{i+1}. [{abstraction_name}]({filename})\\n\"\n                \n                # Add attribution to chapter content\n                chapter_content = chapters_content[i]\n                if not chapter_content.endswith(\"\\n\\n\"):\n                    chapter_content += \"\\n\\n\"\n                chapter_content += \"---\\n\\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\"\n                \n                # Store filename and corresponding content\n                chapter_files.append({\"filename\": filename, \"content\": chapter_content})\n            else:\n                 print(f\"Warning: Mismatch between chapter order, abstractions, or content at index {i} (abstraction index {abstraction_index}). Skipping file generation for this entry.\")\n\n        # Add attribution to index content\n        index_content += \"\\n\\n---\\n\\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\"\n\n        return {\n            \"output_path\": output_path,\n            \"index_content\": index_content,\n            \"chapter_files\": chapter_files # List of {\"filename\": str, \"content\": str}\n        }\n\n    def exec(self, prep_res):\n        output_path = prep_res[\"output_path\"]\n        index_content = prep_res[\"index_content\"]\n        chapter_files = prep_res[\"chapter_files\"]\n\n        print(f\"Combining tutorial into directory: {output_path}\")\n        # Rely on Node's built-in retry/fallback\n        os.makedirs(output_path, exist_ok=True)\n\n        # Write index.md\n        index_filepath = os.path.join(output_path, \"index.md\")\n        with open(index_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(index_content)\n        print(f\"  - Wrote {index_filepath}\")\n\n        # Write chapter files\n        for chapter_info in chapter_files:\n            chapter_filepath = os.path.join(output_path, chapter_info[\"filename\"])\n            with open(chapter_filepath, \"w\", encoding=\"utf-8\") as f:\n                f.write(chapter_info[\"content\"])\n            print(f\"  - Wrote {chapter_filepath}\")\n\n        return output_path # Return the final path\n\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"final_output_dir\"] = exec_res # Store the output path\n        print(f\"\\nTutorial generation complete! Files are in: {exec_res}\")\n        convert_all_md_to_html(exec_res) # Convert all Markdown files to HTML\n        \n\n--- File: utils/call_llm.py ---\nfrom google import genai\nimport os\nimport logging\nimport json\nfrom datetime import datetime\n\n# Configure logging\nlog_directory = os.getenv(\"LOG_DIR\", \"logs\")\nos.makedirs(log_directory, exist_ok=True)\nlog_file = os.path.join(log_directory, f\"llm_calls_{datetime.now().strftime('%Y%m%d')}.log\")\n\n# Set up logger\nlogger = logging.getLogger(\"llm_logger\")\nlogger.setLevel(logging.INFO)\nlogger.propagate = False  # Prevent propagation to root logger\nfile_handler = logging.FileHandler(log_file)\nfile_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\nlogger.addHandler(file_handler)\n\n# Simple cache configuration\ncache_file = \"llm_cache.json\"\n\n# By default, we Google Gemini 2.5 pro, as it shows great performance for code understanding\ndef call_llm(prompt: str, use_cache: bool = True) -> str:\n    # Log the prompt\n    logger.info(f\"PROMPT: {prompt}\")\n    \n    # Check cache if enabled\n    if use_cache:\n        # Load cache from disk\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, 'r') as f:\n                    cache = json.load(f)\n            except:\n                logger.warning(f\"Failed to load cache, starting with empty cache\")\n        \n        # Return from cache if exists\n        if prompt in cache:\n            logger.info(f\"RESPONSE: {cache[prompt]}\")\n            return cache[prompt]\n    \n    # Call the LLM if not in cache or cache disabled\n    client = genai.Client(\n        vertexai=True, \n        # TODO: change to your own project id and location\n        project=os.getenv(\"GEMINI_PROJECT_ID\", \"your-project-id\"),\n        location=os.getenv(\"GEMINI_LOCATION\", \"us-central1\")\n    )\n    # You can comment the previous line and use the AI Studio key instead:\n    # client = genai.Client(\n    #     api_key=os.getenv(\"GEMINI_API_KEY\", \"your-api_key\"),\n    # )\n    model = os.getenv(\"GEMINI_MODEL\", \"gemini-2.5-pro-exp-03-25\")\n    response = client.models.generate_content(\n        model=model,\n        contents=[prompt]\n    )\n    response_text = response.text\n    \n    # Log the response\n    logger.info(f\"RESPONSE: {response_text}\")\n    \n    # Update cache if enabled\n    if use_cache:\n        # Load cache again to avoid overwrites\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, 'r') as f:\n                    cache = json.load(f)\n            except:\n                pass\n        \n        # Add to cache and save\n        cache[prompt] = response_text\n        try:\n            with open(cache_file, 'w') as f:\n                json.dump(cache, f)\n        except Exception as e:\n            logger.error(f\"Failed to save cache: {e}\")\n    \n    return response_text\n\n# # Use Anthropic Claude 3.7 Sonnet Extended Thinking\n# def call_llm(prompt, use_cache: bool = True):\n#     from anthropic import Anthropic\n#     client = Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\", \"your-api-key\"))\n#     response = client.messages.create(\n#         model=\"claude-3-7-sonnet-20250219\",\n#         max_tokens=21000,\n#         thinking={\n#             \"type\": \"enabled\",\n#             \"budget_tokens\": 20000\n#         },\n#         messages=[\n#             {\"role\": \"user\", \"content\": prompt}\n#         ]\n#     )\n#     return response.content[1].text\n\n# # Use OpenAI o1\n# def call_llm(prompt, use_cache: bool = True):    \n#     from openai import OpenAI\n#     client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"))\n#     r = client.chat.completions.create(\n#         model=\"o1\",\n#         messages=[{\"role\": \"user\", \"content\": prompt}],\n#         response_format={\n#             \"type\": \"text\"\n#         },\n#         reasoning_effort=\"medium\",\n#         store=False\n#     )\n#     return r.choices[0].message.content\n\nif __name__ == \"__main__\":\n    test_prompt = \"Hello, how are you?\"\n    \n    # First call - should hit the API\n    print(\"Making call...\")\n    response1 = call_llm(test_prompt, use_cache=False)\n    print(f\"Response: {response1}\")\n    \n\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 8: Chapter Writing`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "```markdown\n# Chapter 8: Chapter Writing\n\nIn the previous chapter, [LLM Interaction](07_llm_interaction.md), we learned how to communicate with Large Language Models (LLMs) to generate content for our tutorial. Now, let's dive deeper into the art and science of *crafting* those tutorial chapters! This is where we refine the raw output from the LLM into something truly engaging and educational.\n\n## What is Chapter Writing?\n\nChapter Writing is the process of taking the information and insights gathered from the previous steps (Codebase Crawling, Abstraction Identification, Relationship Analysis, and LLM Interaction) and transforming them into clear, concise, and beginner-friendly tutorial content. It's about weaving a compelling narrative around different concepts and code snippets to educate and entertain the audience.\n\nThink of it like a skilled storyteller weaving a compelling narrative around different characters and events. We're taking complex technical information and making it accessible and engaging for newcomers. It's not just about writing code; it's about *teaching* code.\n\n## Key Concepts\n\n*   **Beginner-Friendliness:**  Writing in a way that is easy for newcomers to understand, avoiding jargon and complex explanations.\n*   **Clear Structure:**  Organizing the chapter content in a logical and easy-to-follow manner, with clear headings, subheadings, and transitions.\n*   **Code Snippets:**  Incorporating relevant code snippets to illustrate the concepts being explained.\n*   **Diagrams:**  Using diagrams (like Mermaid diagrams) to visualize complex relationships and processes.\n*   **Analogies:**  Employing analogies and real-world examples to make abstract concepts more concrete.\n*   **Markdown Formatting:** Using Markdown to format the chapter content, making it readable and visually appealing.\n\n## How Chapter Writing Works\n\nThe actual \"writing\" is largely handled by the LLM in the `WriteChapters` node (as seen in the [LLM Interaction](07_llm_interaction.md) chapter). However, the *prompt engineering* that guides the LLM is crucial.\n\nThe `WriteChapters` node in `nodes.py` is responsible for generating the content for each chapter. Let's revisit the prompt:\n\n```python\nprompt = f\"\"\"\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `{project_name}` about the concept: \"{abstraction_name}\". This is Chapter {chapter_num}.\n\nConcept Details:\n- Description:\n{item[\"abstraction_details\"][\"description\"]}\n\nComplete Tutorial Structure:\n{item[\"full_chapter_listing\"]}\n\nContext from previous chapters (summary):\n{previous_chapters_summary if previous_chapters_summary else \"This is the first chapter.\"}\n\nRelevant Code Snippets:\n{file_context_str if file_context_str else \"No specific code snippets provided for this abstraction.\"}\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter {chapter_num}: {abstraction_name}`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n\"\"\"\n```\n\nLet's break down the key elements of this prompt:\n\n1.  **Context:** We provide the LLM with all the necessary context, including:\n    *   The project name.\n    *   The concept name.\n    *   The concept description.\n    *   A complete tutorial structure (list of chapters).\n    *   A summary of previous chapters (to maintain continuity).\n    *   Relevant code snippets.\n2.  **Instructions:** We give the LLM very specific instructions on how to write the chapter, including:\n    *   Start with a clear heading.\n    *   Provide a transition from the previous chapter.\n    *   Explain the motivation behind the concept.\n    *   Break down complex concepts into smaller parts.\n    *   Use code snippets to illustrate the concepts.\n    *   Explain the internal implementation.\n    *   Use Mermaid diagrams.\n    *   Use analogies and examples.\n    *   End with a conclusion and transition to the next chapter.\n    *   Maintain a welcoming and beginner-friendly tone.\n3.  **Formatting:** We specify that the output should be in Markdown format and that it should *only* include the Markdown content (no extra text or formatting).\n\n## Key Techniques for Effective Chapter Writing\n\nHere are some key techniques to keep in mind when crafting your prompts:\n\n1.  **Start with a Clear Heading:** Every chapter should begin with a clear and descriptive heading that tells the reader what the chapter is about. This helps the reader quickly understand the chapter's purpose and scope.\n\n    ```markdown\n    # Chapter 8: Chapter Writing\n    ```\n\n2.  **Provide a Transition from the Previous Chapter:** If the chapter is not the first one, it's important to provide a brief transition from the previous chapter. This helps the reader understand how the current chapter relates to what they've already learned.\n\n    ```markdown\n    In the previous chapter, [LLM Interaction](07_llm_interaction.md), we learned how to communicate with Large Language Models (LLMs) to generate content for our tutorial.\n    ```\n\n3.  **Explain the Motivation Behind the Concept:** Every chapter should explain why the concept being discussed is important. What problem does it solve? Why should the reader care?\n\n    ```markdown\n    Chapter Writing is the process of taking the information and insights gathered from the previous steps... and transforming them into clear, concise, and beginner-friendly tutorial content.\n    ```\n\n4.  **Break Down Complex Concepts:** If the concept is complex, break it down into smaller, more manageable parts. Explain each part one at a time, using clear and simple language.\n\n    ```markdown\n    ## Key Concepts\n\n    *   Beginner-Friendliness\n    *   Clear Structure\n    *   Code Snippets\n    *   Diagrams\n    *   Analogies\n    *   Markdown Formatting\n    ```\n\n5.  **Use Code Snippets to Illustrate the Concepts:** Code snippets are a powerful tool for teaching programming concepts. Use them to show the reader how the concept is used in practice.\n\n    ```python\n    def hello_world():\n        print(\"Hello, world!\")\n\n    hello_world() # Prints \"Hello, world!\"\n    ```\n\n6.  **Explain the Internal Implementation:** Understanding how something works under the hood can be very helpful for beginners. Provide a high-level overview of the internal implementation, using diagrams and code snippets.\n\n    ```mermaid\n    sequenceDiagram\n        participant User\n        participant Application\n        participant Database\n\n        User->>Application: Requests data\n        Application->>Database: Queries database\n        Database->>Application: Returns data\n        Application->>User: Displays data\n    ```\n\n7.  **Use Analogies and Examples:** Analogies and real-world examples can help make abstract concepts more concrete and easier to understand.\n\n    ```markdown\n    Think of it like a skilled storyteller weaving a compelling narrative around different characters and events.\n    ```\n\n8.  **End with a Conclusion and Transition to the Next Chapter:** Every chapter should end with a brief conclusion that summarizes what was learned and provides a transition to the next chapter.\n\n    ```markdown\n    In the next chapter, [Tutorial Combination](09_tutorial_combination.md), we'll learn how to combine all the individual chapters into a complete and polished tutorial.\n    ```\n\n## Example: Crafting a Section About Code Snippets\n\nLet's say we want to write a section about how to effectively use code snippets in a tutorial chapter. We might start with an introduction:\n\n```markdown\n## Using Code Snippets\n\nCode snippets are essential for illustrating the concepts you're explaining. They provide concrete examples of how the code works in practice, making it easier for beginners to understand.\n```\n\nThen, we might provide some guidelines on how to choose and format code snippets:\n\n```markdown\n*   **Keep it short:** Code snippets should be as short and focused as possible. Avoid including irrelevant code that might distract the reader.\n*   **Use comments:** Add comments to explain what the code is doing. This helps the reader understand the purpose of each line of code.\n*   **Provide context:** Explain where the code snippet fits within the larger project. This helps the reader understand how the code relates to the overall system.\n*   **Format consistently:** Use a consistent code style throughout the tutorial. This makes the code easier to read and understand.\n```\n\nFinally, we might provide an example of a well-formatted code snippet:\n\n```python\n# This function calculates the sum of two numbers.\ndef add(x, y):\n    return x + y\n\n# Example usage:\nresult = add(5, 3)\nprint(result)  # Output: 8\n```\n\n## Why is Chapter Writing Important?\n\n*   **Clarity:** It ensures that the tutorial content is clear, concise, and easy to understand.\n*   **Engagement:** It makes the tutorial more engaging and enjoyable to read.\n*   **Effectiveness:** It helps the reader learn the concepts more effectively.\n*   **Professionalism:** It gives the tutorial a polished and professional look.\n\n## Conclusion\n\nIn this chapter, we've explored the art and science of Chapter Writing. We've learned how to craft effective and engaging tutorial chapters by providing clear instructions to the LLM and incorporating key techniques like clear structure, code snippets, diagrams, and analogies. By following these guidelines, we can transform raw information into a valuable learning resource for beginners.\n\nIn the next chapter, [Tutorial Combination](09_tutorial_combination.md), we'll learn how to combine all the individual chapters into a complete and polished tutorial.\n```\n", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `Tutorial-Codebase-Knowledge` about the concept: \"Tutorial Combination\". This is Chapter 9.\n\nConcept Details:\n- Description:\nTutorial Combination is the final step in the tutorial generation process, where all the individual\nchapters are combined into a single, cohesive tutorial. This involves generating an index page,\ncreating a Mermaid diagram to visualize the relationships between the abstractions, and converting\nthe Markdown files to HTML. It's like a bookbinder assembling all the chapters of a book and\nadding a cover and table of contents to create a polished final product.\n\n\nComplete Tutorial Structure:\n1. [API Endpoint](01_api_endpoint.md)\n2. [Flow Orchestration](02_flow_orchestration.md)\n3. [Configuration Management](03_configuration_management.md)\n4. [Codebase Crawling](04_codebase_crawling.md)\n5. [Abstraction Identification](05_abstraction_identification.md)\n6. [Relationship Analysis](06_relationship_analysis.md)\n7. [LLM Interaction](07_llm_interaction.md)\n8. [Chapter Writing](08_chapter_writing.md)\n9. [Tutorial Combination](09_tutorial_combination.md)\n\nContext from previous chapters (summary):\n# Chapter 1: API Endpoint\n\nWelcome to the first step in our journey to understanding how to generate tutorials from codebases! In this chapter, we'll explore the **API Endpoint**, which acts like the \"front door\" to our tutorial generation system.\n\nImagine you have a robot that can build LEGO sets. You wouldn't want to open up the robot and start fiddling with its internal wires every time you want it to build something, right? Instead, you'd want a simple button or a control panel to tell it what to do. The API Endpoint is like that button for our tutorial generator.\n\n**Why do we need an API Endpoint?**\n\nThink of it this way: you might want to trigger the tutorial generation process from:\n\n*   Your web browser\n*   Another program running on your computer\n*   Even from a different computer across the internet!\n\nAn API Endpoint allows all these different \"clients\" to communicate with our tutorial generation system in a standardized way. It's the central point of contact.\n\n**Key Concepts:**\n\n1.  **API (Application Programming Interface):** Think of it as a contract between two pieces of software. It defines how they can talk to each other. In our case, it defines how external programs can ask our system to generate a tutorial.\n\n2.  **Endpoint:** A specific URL (web address) that represents a particular function or resource in an API. In our case, it's the URL you send a request to in order to start the tutorial generation. It's like the specific button on our robot's control panel that says \"Build this LEGO set!\"\n\n3.  **Request:** The message you send to the API Endpoint to tell it what to do. This includes information like the repository URL, output folder and options for the tutorial generation.\n\n4.  **Response:** The message the API Endpoint sends back to you after it has processed your request. This might include a success message, an error message, or the path to the generated tutorial.\n\n**Our Use Case: Generating a tutorial for a GitHub repository**\n\nLet's say you want to generate a tutorial for a project on GitHub. The API Endpoint allows you to do this with a simple request. You'll need to provide the URL of the repository and any specific options for how you want the tutorial to be generated.\n\n**How to Use the API Endpoint:**\n\nOur API Endpoint is designed to receive a `POST` request to the `/generate` path. This request contains information about the repository for which you want to generate a tutorial.\n\nHere's an example of how you might send a request (this is just an example, the actual implementation will depend on how you're sending the request, e.g., using `curl`, `Postman`, or a Python script):\n\n```json\n{\n  \"repo_url\": \"https://github.com/fastapi/fastapi\",\n  \"name\": \"FastAPI\",\n  \"output\": \"tutorials\"\n}\n```\n\nThis request tells the system to:\n\n*   Generate a tutorial for the `https://github.com/fastapi/fastapi` repository.\n*   Use \"FastAPI\" as the project name.\n*   Save the tutorial in the `tutorials` output folder.\n\nAfter sending this request, the API Endpoint will process it and, if successful, return a response like this:\n\n```json\n{\n  \"message\": \"Tutorial generation completed successfully.\",\n  \"final_output_directory\": \"tutorials/FastAPI/html\"\n}\n```\n\nThis tells you that the tutorial was generated successfully and is located in the `tutorials/FastAPI/html` directory.\n\n**Internal Implementation: What happens under the hood?**\n\nWhen the API Endpoint receives a request, here's a simplified breakdown of what happens:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant API Endpoint\n    participant Main Script\n    participant Codebase\n    participant LLM\n\n    User->>API Endpoint: Sends tutorial generation request (repo URL, options)\n    API Endpoint->>Main Script: Executes main.py with provided arguments\n    Main Script->>Codebase: Crawls and analyzes the codebase\n    Main Script->>LLM: Interacts with LLM to generate content\n    Main Script->>API Endpoint: Returns success/failure and output path\n    API Endpoint->>User: Sends back response with result and output path\n```\n\nHere's a slightly more detailed explanation:\n\n1.  **Request Reception:** The API Endpoint receives the request with the repository URL and other options.\n2.  **Command Construction:**  It constructs a command line command to run the main script (`main.py`) with the provided parameters.\n3.  **Script Execution:** It executes the `main.py` script, which does the heavy lifting of cloning the repository, analyzing the code, and generating the tutorial content.\n4.  **Response Generation:** After the script finishes (either successfully or with an error), the API Endpoint creates a response with the appropriate message and, if successful, the path to the generated tutorial.\n5.  **Response Delivery:** Finally, it sends the response back to the user.\n\n**Code Snippets:**\n\nLet's look at some key parts of the code in `api.py`:\n\n```python\nfrom fastapi import FastAPI, HTTPException, status\nfrom pydantic import BaseModel, Field\n```\n\nThis imports the necessary libraries for creating the API using FastAPI (a Python framework for building APIs) and Pydantic (for data validation). We are importing `FastAPI` to create the API application, `HTTPException` to handle errors and send them to the user, and `BaseModel` and `Field` from `pydantic` to define the structure of our input data.\n\n```python\nclass GenerationRequest(BaseModel):\n    repo_url: str\n    name: Optional[str] = None\n    token: Optional[str] = None\n    output: str = Field(default=\"output\")\n    include: Optional[List[str]] = None\n    exclude: Optional[List[str]] = None\n    max_size: Optional[int] = None\n```\n\nThis defines the structure of the data that the API Endpoint expects to receive in the request. This is crucial for validation and ensuring that the input data is in the correct format. `repo_url` is required, while other fields like `name`, `token`, `output`, `include`, `exclude`, and `max_size` are optional.\n\n```python\n@app.post(\n    \"/generate\",\n    summary=\"Trigger Tutorial Generation\",\n    description=\"Accepts repository details and options, then runs the tutorial generation script.\",\n    status_code=status.HTTP_200_OK\n)\nasync def generate_tutorial(request_data: GenerationRequest):\n    # ... implementation details ...\n    return {\n        \"message\": \"Tutorial generation completed successfully.\",\n        \"final_output_directory\": expected_final_path,\n    }\n```\n\nThis code defines the `/generate` endpoint using the `@app.post` decorator. This tells FastAPI that this function should be called when a `POST` request is sent to the `/generate` URL. It receives the `request_data` (which is of type `GenerationRequest` that we defined above), does some processing, and returns a response containing a success message and the path to the generated tutorial.  The `status_code=status.HTTP_200_OK` ensures that a successful response returns a 200 OK status code.\n\n**CORS Middleware**\n\n```python\norigins = [\n    \"http://127.0.0.1:5500\", # VS Code Live Server default\n    \"http://localhost\",\n    \"http://localhost:8080\",\n    \"http://127.0.0.1\",\n    \"http://127.0.0.1:8080\",\n    \"null\", # file:// origin\n]\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=origins,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n```\n\nThis section is responsible for configuring Cross-Origin Resource Sharing (CORS). CORS is a security feature implemented by web browsers to restrict web pages from making requests to a different domain than the one which served the web page. Without CORS, you might not be able to send requests to your API from a webpage running on your local machine (e.g. during development). The middleware specifies the origins that are allowed to make requests to the API, ensuring that requests from these origins are not blocked by the browser.\n\n**Error Handling**\n\nThe `api.py` file includes comprehensive error handling to gracefully manage different scenarios that might arise during script execution.\n\n```python\n    except subprocess.CalledProcessError as e:\n        logger.error(f\"Script failed with exit code {e.returncode}\")\n        logger.error(f\"Script stdout:\\n{e.stdout}\")\n        logger.error(f\"Script stderr:\\n{e.stderr}\")\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail={\n                \"error\": \"Script execution failed.\",\n                \"details\": \"The tutorial generation script exited with an error.\",\n                \"return_code\": e.returncode,\n                \"script_stdout\": e.stdout,\n                \"script_stderr\": e.stderr\n            }\n        )\n    except FileNotFoundError:\n        error_msg = f\"Error: Script '{MAIN_SCRIPT_PATH}' or Python '{PYTHON_EXECUTABLE}' not found.\"\n        logger.error(error_msg)\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail={\"error\": \"Server configuration error.\", \"details\": error_msg}\n        )\n    except Exception as e:\n        logger.exception(\"An unexpected error occurred during script execution.\")\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail={\"error\": \"An unexpected internal server error occurred.\", \"details\": str(e)}\n        )\n```\n\n*   **`subprocess.CalledProcessError`**: Catches errors when the `main.py` script fails during execution. It logs the error details and raises an `HTTPException` with a 500 status code, providing details like the return code, stdout, and stderr of the script.\n*   **`FileNotFoundError`**: Handles cases where the `main.py` script or Python executable is not found. It raises an `HTTPException` with a 500 status code, indicating a server configuration issue.\n*   **`Exception`**: Serves as a catch-all for unexpected errors during script execution. It logs the exception and raises an `HTTPException` with a 500 status code, providing details about the error.\n\n**Conclusion:**\n\nIn this chapter, we've learned about the API Endpoint, which serves as the entry point to our tutorial generation system. We saw how it allows external programs to trigger the tutorial generation process with a simple request. We also explored the internal implementation and understood how it orchestrates the execution of the `main.py` script.\n\nNow that we know how to trigger the process, let's dive into [Flow Orchestration](02_flow_orchestration.md) in the next chapter to understand the overall sequence of steps involved in generating a tutorial.\n\n---\n# Chapter 2: Flow Orchestration\n\nIn the previous chapter, [API Endpoint](01_api_endpoint.md), we learned how to trigger the tutorial generation process. Think of it like pressing the \"Start\" button on a machine. But what happens *after* you press that button? That's where Flow Orchestration comes in!\n\n## What is Flow Orchestration?\n\nImagine a factory that builds cars. You wouldn't just dump all the parts in a pile and hope a car magically appears, right? Instead, you'd have a carefully planned assembly line:\n\n1.  **Frame Assembly:** The car's frame is built.\n2.  **Engine Installation:** The engine is installed.\n3.  **Bodywork:** The body panels are attached.\n4.  **Painting:** The car gets its color.\n5.  **Interior Fitting:** Seats and other interior parts are added.\n6.  **Quality Control:** Final checks are performed.\n\nFlow Orchestration is like that assembly line for our tutorial generation system. It defines the *sequence of steps* needed to transform a codebase into a complete tutorial. It makes sure each task happens in the right order, passing the result of one task to the next.\n\nWithout flow orchestration, we'd have a chaotic mess. Chapters might be written before the codebase is even analyzed, or the tutorial might be combined before the chapters are finished! Flow Orchestration brings order to the process.\n\n## Key Concepts\n\n*   **Flow:** The overall process or pipeline of tasks. In our case, it's the entire tutorial generation process.\n*   **Node:** A single task or step in the flow. Each node performs a specific job, like fetching code, analyzing relationships, or writing a chapter.  Nodes can be thought of as functions in a programming language.\n*   **Sequence:** The order in which the nodes are executed.  This is crucial for ensuring the correct flow of data and dependencies.\n*   **Dependencies:**  A node might *depend* on the output of another node. For example, you can't write a chapter *about* an abstraction until you've identified *what* the abstractions *are*.\n\n## How Flow Orchestration Works in Our Project\n\nLet's look at the main steps in our tutorial generation flow:\n\n```mermaid\ngraph LR\n    A[Fetch Repo] --> B(Identify Abstractions)\n    B --> C{Analyze Relationships}\n    C --> D[Order Chapters]\n    D --> E((Write Chapters))\n    E --> F(Combine Tutorial)\n    style A fill:#f9f,stroke:#333,stroke-width:2px\n    style F fill:#f9f,stroke:#333,stroke-width:2px\n    style E fill:#ccf,stroke:#333,stroke-width:2px\n```\n\nHere's a breakdown of each node:\n\n1.  **Fetch Repo:**  Downloads the codebase from a remote repository (like GitHub) or reads it from a local directory. This node is responsible for getting the raw material needed to create the tutorial.  It outputs the files and their contents.\n2.  **Identify Abstractions:** Analyzes the codebase and identifies key concepts or abstractions that are important for understanding the project. This step essentially identifies the key topics to cover in the tutorial.  It outputs a list of abstractions, including their names, descriptions, and the files they're found in.\n3.  **Analyze Relationships:** Examines how the different abstractions interact with each other. This helps us understand the overall architecture of the codebase and how the different pieces fit together. The LLM creates a summary of the project and the relationships between the abstractions\n4.  **Order Chapters:** Determines the best order to present the abstractions in the tutorial.  Some abstractions might be foundational and need to be explained before others. The LLM figures out the order.\n5.  **Write Chapters:**  Generates the actual content for each chapter of the tutorial. It uses the abstraction details, relationships, and chapter order to create a beginner-friendly explanation of each concept. This node is marked with double parentheses `((...))` because it's a *BatchNode*. A `BatchNode` handles independent steps and can be run in parallel.\n6.  **Combine Tutorial:**  Assembles all the individual chapters into a complete tutorial, creating an index page and table of contents.\n\n## Code Example: Defining the Flow\n\nThe core of our flow orchestration is defined in `flow.py`:\n\n```python\nfrom pocketflow import Flow\n# Import all node classes from nodes.py\nfrom nodes import (\n    FetchRepo,\n    IdentifyAbstractions,\n    AnalyzeRelationships,\n    OrderChapters,\n    WriteChapters,\n    CombineTutorial\n)\n\ndef create_tutorial_flow():\n    \"\"\"Creates and returns the codebase tutorial generation flow.\"\"\"\n\n    # Instantiate nodes\n    fetch_repo = FetchRepo()\n    identify_abstractions = IdentifyAbstractions(max_retries=3, wait=10)\n    analyze_relationships = AnalyzeRelationships(max_retries=3, wait=10)\n    order_chapters = OrderChapters(max_retries=3, wait=10)\n    write_chapters = WriteChapters(max_retries=3, wait=10) # This is a BatchNode\n    combine_tutorial = CombineTutorial()\n\n    # Connect nodes in sequence based on the design\n    fetch_repo >> identify_abstractions\n    identify_abstractions >> analyze_relationships\n    analyze_relationships >> order_chapters\n    order_chapters >> write_chapters\n    write_chapters >> combine_tutorial\n\n    # Create the flow starting with FetchRepo\n    tutorial_flow = Flow(start=fetch_repo)\n\n    return tutorial_flow\n```\n\nLet's break down this code:\n\n*   **Importing Nodes:** We import all the node classes (like `FetchRepo`, `IdentifyAbstractions`, etc.) from the `nodes.py` file.  Think of these as importing different tools or functions that we need for our assembly line.\n*   **Instantiating Nodes:**  We create instances of each node class. For example, `fetch_repo = FetchRepo()` creates a specific instance of the `FetchRepo` node. This is where the maximum number of retries and wait time between retries is set for nodes that interact with the LLM. This helps handle cases where the LLM might be temporarily unavailable or returns an invalid response.\n*   **Connecting Nodes:** The `>>` operator defines the sequence of the flow. `fetch_repo >> identify_abstractions` means that the `IdentifyAbstractions` node will execute *after* the `FetchRepo` node completes.  The output of `FetchRepo` will be passed as input to `IdentifyAbstractions`.\n*   **Creating the Flow:** We use the `Flow` class from the `pocketflow` library to create the flow. We specify the `start` node, which is the first node to be executed.\n\nThis simple code defines the entire structure of our tutorial generation process!\n\n## Code Example: Running the Flow\n\nThe `main.py` file is responsible for setting up the flow with all the command line input parameters and calling the `Flow` to execute:\n\n```python\nimport dotenv\nimport os\nimport argparse\n# Import the function that creates the flow\nfrom flow import create_tutorial_flow\n\ndotenv.load_dotenv()\n\n# Default file patterns\nDEFAULT_INCLUDE_PATTERNS = {\n    \"*.py\", \"*.js\", \"*.jsx\", \"*.ts\", \"*.tsx\", \"*.go\", \"*.java\", \"*.pyi\", \"*.pyx\", \n    \"*.c\", \"*.cc\", \"*.cpp\", \"*.h\", \"*.md\", \"*.rst\", \"Dockerfile\", \n    \"Makefile\", \"*.yaml\", \"*.yml\",\n}\n\nDEFAULT_EXCLUDE_PATTERNS = {\n    \"*test*\", \"tests/*\", \"docs/*\", \"examples/*\", \"v1/*\", \n    \"dist/*\", \"build/*\", \"experimental/*\", \"deprecated/*\", \n    \"legacy/*\", \".git/*\", \".github/*\", \".next/*\", \".vscode/*\", \"obj/*\", \"bin/*\", \"node_modules/*\", \"*.log\"\n}\n\n# --- Main Function ---\ndef main():\n    parser = argparse.ArgumentParser(description=\"Generate a tutorial for a GitHub codebase or local directory.\")\n    \n    # Create mutually exclusive group for source\n    source_group = parser.add_mutually_exclusive_group(required=True)\n    source_group.add_argument(\"--repo\", help=\"URL of the public GitHub repository.\")\n    source_group.add_argument(\"--dir\", help=\"Path to local directory.\")\n    \n    parser.add_argument(\"-n\", \"--name\", help=\"Project name (optional, derived from repo/directory if omitted).\")\n    parser.add_argument(\"-t\", \"--token\", help=\"GitHub personal access token (optional, reads from GITHUB_TOKEN env var if not provided).\")\n    parser.add_argument(\"-o\", \"--output\", default=\"output\", help=\"Base directory for output (default: ./output).\")\n    parser.add_argument(\"-i\", \"--include\", nargs=\"+\", help=\"Include file patterns (e.g. '*.py' '*.js'). Defaults to common code files if not specified.\")\n    parser.add_argument(\"-e\", \"--exclude\", nargs=\"+\", help=\"Exclude file patterns (e.g. 'tests/*' 'docs/*'). Defaults to test/build directories if not specified.\")\n    parser.add_argument(\"-s\", \"--max-size\", type=int, default=100000, help=\"Maximum file size in bytes (default: 100000, about 100KB).\")\n\n    args = parser.parse_args()\n\n    # Get GitHub token from argument or environment variable if using repo\n    github_token = None\n    if args.repo:\n        github_token = args.token or os.environ.get('GITHUB_TOKEN')\n        if not github_token:\n            print(\"Warning: No GitHub token provided. You might hit rate limits for public repositories.\")\n\n    # Initialize the shared dictionary with inputs\n    shared = {\n        \"repo_url\": args.repo,\n        \"local_dir\": args.dir,\n        \"project_name\": args.name, # Can be None, FetchRepo will derive it\n        \"github_token\": github_token,\n        \"output_dir\": args.output, # Base directory for CombineTutorial output\n\n        # Add include/exclude patterns and max file size\n        \"include_patterns\": set(args.include) if args.include else DEFAULT_INCLUDE_PATTERNS,\n        \"exclude_patterns\": set(args.exclude) if args.exclude else DEFAULT_EXCLUDE_PATTERNS,\n        \"max_file_size\": args.max_size,\n\n        # Outputs will be populated by the nodes\n        \"files\": [],\n        \"abstractions\": [],\n        \"relationships\": {},\n        \"chapter_order\": [],\n        \"chapters\": [],\n        \"final_output_dir\": None\n    }\n\n    print(f\"Starting tutorial generation for: {args.repo or args.dir}\")\n\n    # Create the flow instance\n    tutorial_flow = create_tutorial_flow()\n\n    # Run the flow\n    tutorial_flow.run(shared)\n    \nif __name__ == \"__main__\":\n    main()\n```\n\nHere are the important parts:\n\n1.  **Parsing Arguments:** We use `argparse` to handle command-line arguments like the repository URL, project name, and output directory.\n2.  **Shared Dictionary:** The `shared` dictionary acts as a central store for data that is passed between the nodes. Each node can read data from this dictionary and write its results back to it.\n3.  **Creating the Flow:** We call `create_tutorial_flow()` to get an instance of the flow we defined in `flow.py`.\n4.  **Running the Flow:**  Finally, we call `tutorial_flow.run(shared)` to start the flow. The `shared` dictionary is passed to the `run` method so that the nodes can access and modify the data.\n\n## What Happens Inside a Node?\n\nEach node consists of three important methods. Looking at the example of `FetchRepo` in `nodes.py`:\n\n```python\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"local_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        if not project_name:\n            # Basic name derivation from URL or directory\n            if repo_url:\n                project_name = repo_url.split('/')[-1].replace('.git', '')\n            else:\n                project_name = os.path.basename(os.path.abspath(local_dir))\n            shared[\"project_name\"] = project_name\n\n        # Get file patterns directly from shared\n        include_patterns = shared[\"include_patterns\"]\n        exclude_patterns = shared[\"exclude_patterns\"]\n        max_file_size = shared[\"max_file_size\"]\n\n        return {\n            \"repo_url\": repo_url,\n            \"local_dir\": local_dir,\n            \"token\": shared.get(\"github_token\"),\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns,\n            \"max_file_size\": max_file_size,\n            \"use_relative_paths\": True\n        }\n\n    def exec(self, prep_res):\n        if prep_res[\"repo_url\"]:\n            print(f\"Crawling repository: {prep_res['repo_url']}...\")\n            result = crawl_github_files(\n                repo_url=prep_res[\"repo_url\"],\n                token=prep_res[\"token\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        else:\n            print(f\"Crawling directory: {prep_res['local_dir']}...\")\n            result = crawl_local_files(\n                directory=prep_res[\"local_dir\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n            \n        # Convert dict to list of tuples: [(path, content), ...]\n        files_list = list(result.get(\"files\", {}).items())\n        print(f\"Fetched {len(files_list)} files.\")\n        return files_list\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"files\"] = exec_res # List of (path, content) tuples\n```\n\n*   **`prep(shared)`:** This method prepares the input for the node's execution. It reads data from the `shared` dictionary and transforms it into a format that the `exec` method can use. It also performs any necessary setup or validation. Returns a `prep_res` object that contains everything necessary for the `exec` function to work.\n*   **`exec(prep_res)`:** This method performs the core logic of the node. It receives the prepared input from the `prep` method and executes the task. In the case of `FetchRepo`, it downloads the codebase from GitHub or reads it from the local directory. It returns the `exec_res` which is the output of the node.\n*   **`post(shared, prep_res, exec_res)`:** This method is executed *after* the `exec` method completes. It takes the output of the `exec` method (`exec_res`) and writes it back to the `shared` dictionary, making it available to the next node in the flow.  In the case of `FetchRepo`, it writes the list of files and their contents to the `shared[\"files\"]` variable.\n\n## Why Use Flow Orchestration?\n\n*   **Organization:** It provides a clear and structured way to define complex processes.\n*   **Maintainability:** It makes the code easier to understand and maintain by breaking it down into smaller, independent nodes.\n*   **Reusability:**  Nodes can be reused in different flows, making it easier to build new features or modify existing ones.\n*   **Testability:** Each node can be tested independently, making it easier to ensure the correctness of the entire system.\n*   **Error Handling:** `pocketflow` supports retries, fallbacks, error handling, and graceful termination, which are useful for nodes that interact with external resources.\n\n## Conclusion\n\nFlow Orchestration is a powerful technique for managing complex processes like our tutorial generation system. It provides a structured way to define the sequence of steps, making the code more organized, maintainable, and testable.\n\nIn the next chapter, [Configuration Management](03_configuration_management.md), we'll explore how to manage the different configuration settings that control the behavior of our system. This will allow us to customize the tutorial generation process for different codebases and use cases.\n\n---\n# Chapter 3: Configuration Management\n\nIn the previous chapter, [Flow Orchestration](02_flow_orchestration.md), we learned how to define the overall sequence of steps for generating a tutorial. Now, let's talk about how to *customize* that process. Think of it like this: you have a recipe for baking a cake, but you might want to adjust the recipe to make it chocolate or vanilla. That's where Configuration Management comes in!\n\n## What is Configuration Management?\n\nConfiguration Management is like having a control panel that allows you to adjust the settings of our tutorial generation \"machine\" to achieve the desired output. It lets you change things like:\n\n*   Which repository to generate a tutorial for.\n*   What name to give the project in the tutorial.\n*   Which files to include or exclude from analysis.\n*   Where to save the generated tutorial.\n\nWithout configuration management, we'd be stuck with a single, inflexible way of generating tutorials. It's like trying to bake every cake with the exact same recipe, even if you want different flavors!\n\n## Key Concepts\n\n1.  **Environment Variables:** These are like global settings that can be accessed by any program running on your computer. They're often used to store things like API keys or default values.\n2.  **Argument Parsing:** This allows you to pass settings directly to a program when you run it from the command line. It's like specifying the ingredients for your cake when you give the recipe to the baker.\n\n## Our Use Case: Generating a Tutorial for a Specific GitHub Repository with a Custom Name\n\nLet's say you want to generate a tutorial for the `requests` library on GitHub and name the project \"Python Requests Tutorial\". With Configuration Management, you can easily do this!\n\n## How to Use Configuration Management\n\nOur project uses both environment variables and argument parsing to manage configuration.\n\n### 1. Environment Variables\n\nEnvironment variables are useful for settings that you might want to reuse across multiple runs of the program, or that contain sensitive information you don't want to type directly into the command line.\n\nTo set an environment variable, you can use the following command in your terminal (replace `<your_token>` with your actual token):\n\n```bash\nexport GITHUB_TOKEN=<your_token>\n```\n\nOr, you can create a `.env` file in the project root directory:\n\n```\nGITHUB_TOKEN=<your_token>\n```\n\nThe `dotenv` library will automatically load these variables when the script runs.\n\nIn our `main.py` file, we use the `os.environ.get()` function to read the value of the `GITHUB_TOKEN` environment variable:\n\n```python\nimport os\ngithub_token = args.token or os.environ.get('GITHUB_TOKEN')\n```\n\nThis code first checks if the token was provided as a command-line argument (`args.token`). If not, it looks for the `GITHUB_TOKEN` environment variable.\n\n### 2. Argument Parsing\n\nArgument parsing allows you to specify settings directly when you run the `main.py` script from the command line.\n\nHere's how you would generate a tutorial for the `requests` library with a custom name:\n\n```bash\npython main.py --repo https://github.com/psf/requests -n \"Python Requests Tutorial\" -o tutorials\n```\n\nLet's break down this command:\n\n*   `python main.py`: This tells your computer to run the `main.py` script using Python.\n*   `--repo https://github.com/psf/requests`: This specifies the URL of the GitHub repository to use. `--repo` is the *argument* name, and `https://github.com/psf/requests` is the *value* of that argument.\n*   `-n \"Python Requests Tutorial\"`: This sets the project name to \"Python Requests Tutorial\". `-n` is a *short* argument name, and `\"Python Requests Tutorial\"` is the value.\n*   `-o tutorials`: This specifies that the tutorial output will be written to the `tutorials` directory.\n\nThe `argparse` module in Python handles the parsing of these command-line arguments. In `main.py`, we define the arguments that our script accepts:\n\n```python\nimport argparse\n\nparser = argparse.ArgumentParser(description=\"Generate a tutorial for a GitHub codebase or local directory.\")\nparser.add_argument(\"--repo\", help=\"URL of the public GitHub repository.\")\nparser.add_argument(\"-n\", \"--name\", help=\"Project name (optional, derived from repo/directory if omitted).\")\nparser.add_argument(\"-o\", \"--output\", default=\"output\", help=\"Base directory for output (default: ./output).\")\n# ... other arguments\nargs = parser.parse_args()\n```\n\nThis code defines three arguments: `--repo`, `--name` (with the short form `-n`), and `--output` (with the short form `-o`). The `help` parameter provides a description of each argument that is displayed when you run the script with the `--help` flag:\n\n```bash\npython main.py --help\n```\n\n### How Include and Exclude Patterns Work\n\nThe `-i` (or `--include`) and `-e` (or `--exclude`) options are used to specify which files to include or exclude from the tutorial generation process. This is helpful if you want to focus on specific parts of the codebase or ignore irrelevant files (like test files or documentation).\n\nFor example, to only include Python files and exclude test directories, you could use the following command:\n\n```bash\npython main.py --repo https://github.com/psf/requests -i \"*.py\" -e \"tests/*\"\n```\n\n## Internal Implementation\n\nHere's a simplified view of how Configuration Management works internally:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Command Line\n    participant Main Script\n    participant Args Namespace\n    User->>Command Line: Runs script with arguments: python main.py --repo ...\n    Command Line->>Main Script: Executes main.py\n    Main Script->>Args Namespace: Parses command-line arguments using argparse\n    Args Namespace->>Main Script: Returns arguments as an object (e.g., args.repo, args.name)\n    Main Script->>Main Script: Accesses argument values and uses them\n```\n\nHere's a breakdown:\n\n1.  **User Interaction:** The user runs the `main.py` script from the command line, providing arguments like the repository URL and project name.\n2.  **Argument Parsing:** The `argparse` module in `main.py` parses these arguments and stores them in an `Args Namespace` object.\n3.  **Accessing Arguments:** The `main.py` script accesses the argument values from the `Args Namespace` object (e.g., `args.repo`, `args.name`).\n4.  **Using Arguments:** The script uses these values to configure the tutorial generation process. For example, the `repo_url` value is passed to the [Codebase Crawling](04_codebase_crawling.md) node to download the codebase.\n\n## Code Example: Passing Configuration to Nodes\n\nThe parsed arguments are stored in the `shared` dictionary and passed to the nodes in the flow. For example, in `main.py`:\n\n```python\nshared = {\n    \"repo_url\": args.repo,\n    \"project_name\": args.name,\n    \"output_dir\": args.output,\n    # ... other arguments\n}\n\ntutorial_flow.run(shared)\n```\n\nEach node can then access the configuration values from the `shared` dictionary in its `prep` method. For example, in `nodes.py`:\n\n```python\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        project_name = shared.get(\"project_name\")\n        # ...\n        return {\n            \"repo_url\": repo_url,\n            # ...\n        }\n```\n\n## Why is Configuration Management Important?\n\n*   **Flexibility:** It allows you to customize the tutorial generation process for different codebases and use cases.\n*   **Reusability:** You can easily reuse the same script to generate tutorials for different projects by simply changing the configuration settings.\n*   **Automation:** You can automate the tutorial generation process by running the script with a predefined set of arguments.\n\n## Conclusion\n\nIn this chapter, we've learned about Configuration Management, which allows us to customize the tutorial generation process by using environment variables and argument parsing. This gives us the flexibility to generate tutorials for different codebases with different settings.\n\nIn the next chapter, [Codebase Crawling](04_codebase_crawling.md), we'll dive into the process of fetching and analyzing the codebase itself.\n\n---\n# Chapter 4: Codebase Crawling\n\nIn the previous chapter, [Configuration Management](03_configuration_management.md), we learned how to customize our tutorial generation process using command-line arguments and environment variables. Now, let's explore how we actually *get* the code that we'll be using to generate the tutorial! That's where Codebase Crawling comes in.\n\n## What is Codebase Crawling?\n\nImagine you're writing a report on a popular book. You can't write the report without actually *reading* the book first, right? Codebase Crawling is like reading the book (or, more accurately, the code) for our tutorial generation system.\n\nSpecifically, Codebase Crawling is the process of gathering all the necessary files from either:\n\n*   A remote GitHub repository, or\n*   A local directory on your computer.\n\nIt's like a diligent librarian collecting books from different shelves, ensuring they meet certain criteria before adding them to the collection. The criteria might include:\n\n*   **File Type:** Only including certain types of files (e.g., Python files, JavaScript files).\n*   **File Size:** Ignoring files that are too large.\n*   **Location:** Only including files in certain directories, or excluding files in others (like test directories).\n\nWithout Codebase Crawling, our system wouldn't have any code to analyze, and we wouldn't be able to generate a tutorial!\n\n## Key Concepts\n\n*   **Repository URL:** The address of the GitHub repository (e.g., `https://github.com/fastapi/fastapi`).\n*   **Local Directory:** The path to a directory on your computer (e.g., `/Users/myuser/myproject`).\n*   **Include Patterns:**  Patterns that specify which files to include (e.g., `*.py` for all Python files).  Think of these as \"keywords\" to search for in the library.\n*   **Exclude Patterns:** Patterns that specify which files to exclude (e.g., `tests/*` to exclude all files in the `tests` directory).  Think of these as \"forbidden words\" to avoid in the search.\n*   **Maximum File Size:** A limit on the size of the files to include. This helps prevent the system from trying to process very large files, which can be slow or cause errors.\n\n## How Codebase Crawling Works in Our Project\n\nThe Codebase Crawling process is handled by the `FetchRepo` node in our flow. Remember from [Flow Orchestration](02_flow_orchestration.md) that each node has three important steps: `prep`, `exec`, and `post`.\n\nLet's look at how Codebase Crawling is implemented in the `FetchRepo` node in `nodes.py`:\n\n```python\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"local_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        # ... (derive project name if not provided)\n\n        # Get file patterns directly from shared\n        include_patterns = shared[\"include_patterns\"]\n        exclude_patterns = shared[\"exclude_patterns\"]\n        max_file_size = shared[\"max_file_size\"]\n\n        return {\n            \"repo_url\": repo_url,\n            \"local_dir\": local_dir,\n            \"token\": shared.get(\"github_token\"),\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns,\n            \"max_file_size\": max_file_size,\n            \"use_relative_paths\": True\n        }\n\n    def exec(self, prep_res):\n        if prep_res[\"repo_url\"]:\n            print(f\"Crawling repository: {prep_res['repo_url']}...\")\n            result = crawl_github_files(\n                repo_url=prep_res[\"repo_url\"],\n                token=prep_res[\"token\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        else:\n            print(f\"Crawling directory: {prep_res['local_dir']}...\")\n            result = crawl_local_files(\n                directory=prep_res[\"local_dir\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n            \n        # Convert dict to list of tuples: [(path, content), ...]\n        files_list = list(result.get(\"files\", {}).items())\n        print(f\"Fetched {len(files_list)} files.\")\n        return files_list\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"files\"] = exec_res # List of (path, content) tuples\n```\n\nLet's break down what's happening:\n\n1.  **`prep(shared)`:**\n    *   This method retrieves the configuration values (repository URL, local directory, include patterns, exclude patterns, maximum file size) from the `shared` dictionary. This is how the node knows *what* to crawl and *how* to crawl it.\n    *   It prepares these values into a `prep_res` object which is a dictionary containing the configuration for the `exec` function to use.\n\n2.  **`exec(prep_res)`:**\n    *   This method checks if a `repo_url` is provided. If so, it calls the `crawl_github_files` function to crawl the GitHub repository.\n    *   If a `repo_url` is *not* provided (meaning we're crawling a local directory), it calls the `crawl_local_files` function to crawl the local directory.\n    *   The `crawl_github_files` and `crawl_local_files` functions do the actual work of fetching the files and their contents.\n    *   Finally, the result from the crawl is converted into a list of tuples where each tuple contains a file path and file content.\n\n3.  **`post(shared, prep_res, exec_res)`:**\n    *   This method takes the list of files (the `exec_res` output) and stores it in the `shared[\"files\"]` variable. This makes the files available to the next node in the flow, which is [Abstraction Identification](05_abstraction_identification.md).\n\n### Crawling GitHub Repositories: `crawl_github_files`\n\nThe `crawl_github_files` function in `utils/crawl_github_files.py` is responsible for fetching files from a GitHub repository.  Here's a simplified view of how it works:\n\n```python\ndef crawl_github_files(\n    repo_url, \n    token=None, \n    max_file_size: int = 1 * 1024 * 1024,  # 1 MB\n    use_relative_paths: bool = False,\n    include_patterns: Union[str, Set[str]] = None,\n    exclude_patterns: Union[str, Set[str]] = None\n):\n    # Parse the repo_url to extract owner, repo name, branch and path\n    # Uses the GitHub REST API to get the files and their content\n\n    files = {}\n    skipped_files = []\n\n    # Fetch contents of the repository at a specific path and commit\n    def fetch_contents(path):\n        # Use the GitHub REST API to get the contents of the given path\n        # If it's a file, download it and add it to the files dictionary\n        # If it's a directory, recursively call fetch_contents on the directory\n\n        # Check if file should be included based on patterns\n        def should_include_file(file_path: str, file_name: str) -> bool:\n            # If no include patterns are specified, include all files\n            # Otherwise, check if the file matches any of the include patterns\n            # If exclude patterns are specified, exclude files that match the exclude patterns\n            return True or False # Simplified logic\n\n    # Start crawling from the specified path\n    fetch_contents(specific_path)\n\n    return {\n        \"files\": files,\n        \"stats\": {\n            \"downloaded_count\": len(files),\n            \"skipped_count\": len(skipped_files),\n            \"skipped_files\": skipped_files,\n            \"base_path\": specific_path if use_relative_paths else None,\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns\n        }\n    }\n```\n\nHere's a breakdown:\n\n1.  **Parsing the Repository URL:** The function first parses the `repo_url` to extract the owner, repository name, branch/commit, and path within the repository. This information is used to construct the URLs for the GitHub API requests.\n2.  **Using the GitHub REST API:** It uses the GitHub REST API to fetch the contents of the repository. The API allows us to retrieve information about files and directories in the repository.\n3.  **Handling Different Content Types:** The API returns different data depending on whether the item is a file or a directory.\n    *   **Files:** For files, the function downloads the file content and stores it in the `files` dictionary.  It also checks the file size against the `max_file_size` limit.\n    *   **Directories:** For directories, the function recursively calls itself to process the contents of the subdirectory. This allows it to traverse the entire repository structure.\n4.  **Include/Exclude Patterns:** The function uses the `should_include_file` helper function to check if a file should be included based on the `include_patterns` and `exclude_patterns`. This allows you to filter the files that are included in the tutorial generation process.  If a file matches an exclude pattern, it will not be included.\n5.  **Error Handling:** The function includes error handling to deal with potential issues like rate limits (which can occur if you make too many requests to the GitHub API in a short period of time) and network errors.\n\n### Crawling Local Directories: `crawl_local_files`\n\nThe `crawl_local_files` function in `utils/crawl_local_files.py` is responsible for fetching files from a local directory. Here's a simplified view of how it works:\n\n```python\nimport os\nimport fnmatch\n\ndef crawl_local_files(directory, include_patterns=None, exclude_patterns=None, max_file_size=None, use_relative_paths=True):\n    \"\"\"\n    Crawl files in a local directory with similar interface as crawl_github_files.\n    \"\"\"\n    files_dict = {}\n\n    # Traverse through all files and folders in the directory\n    for root, _, files in os.walk(directory):\n        for filename in files:\n            filepath = os.path.join(root, filename) # Get the full path\n            relpath = os.path.relpath(filepath, directory) # Path relative to directory\n\n            # Check include and exclude patterns\n            included = True #Simplified\n            excluded = False #Simplified\n            if not included or excluded:\n                continue\n\n            # Check file size\n            if max_file_size and os.path.getsize(filepath) > max_file_size:\n                continue\n\n            try:\n                with open(filepath, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                files_dict[relpath] = content #Store content\n            except Exception as e:\n                print(f\"Warning: Could not read file {filepath}: {e}\")\n\n    return {\"files\": files_dict}\n```\n\nHere's a breakdown:\n\n1.  **Traversing the Directory:** The function uses `os.walk` to traverse the directory tree, visiting each directory and file within the specified `directory`.\n2.  **Constructing File Paths:** For each file, the function constructs the full path to the file using `os.path.join`. It also calculates the relative path to the file (relative to the specified `directory`) using `os.path.relpath`.\n3.  **Include/Exclude Patterns:** Similar to `crawl_github_files`, the function checks if a file should be included based on the `include_patterns` and `exclude_patterns`.\n4.  **File Size Check:** The function checks the file size against the `max_file_size` limit.\n5.  **Reading File Content:** If the file passes all the checks, the function reads the content of the file and stores it in the `files_dict` dictionary, using the file path as the key.\n6.  **Error Handling:** The function includes basic error handling to catch potential issues like file read errors.\n\n## How to Use Codebase Crawling\n\nYou don't directly call `crawl_github_files` or `crawl_local_files` yourself. Instead, you provide the necessary configuration (repository URL or local directory, include patterns, exclude patterns, maximum file size) when you run the `main.py` script.  The `FetchRepo` node then uses this configuration to crawl the codebase.\n\nFor example, to generate a tutorial for the `fastapi` repository, including only Python files and excluding the `tests` directory, you would use the following command:\n\n```bash\npython main.py --repo https://github.com/fastapi/fastapi -i \"*.py\" -e \"tests/*\"\n```\n\nTo generate a tutorial for a local directory, you would use the `--dir` option instead of `--repo`:\n\n```bash\npython main.py --dir /path/to/my/project -i \"*.py\" -e \"tests/*\"\n```\n\n## Why is Codebase Crawling Important?\n\n*   **Data Acquisition:** It's the first step in the tutorial generation process, providing the raw data (the codebase) that is needed to generate the tutorial.\n*   **Filtering:** It allows you to filter the files that are included in the tutorial, focusing on the most relevant parts of the codebase and ignoring irrelevant files (like test files or documentation).\n*   **Efficiency:** By limiting the maximum file size, it helps prevent the system from trying to process very large files, which can be slow or cause errors.\n\n## Conclusion\n\nIn this chapter, we've learned about Codebase Crawling, which is the process of gathering the necessary files from either a GitHub repository or a local directory. We've seen how it uses include and exclude patterns to filter files, and how it limits the maximum file size to prevent performance issues.\n\nIn the next chapter, [Abstraction Identification](05_abstraction_identification.md), we'll explore how we analyze the codebase to identify the key concepts that should be covered in the tutorial.\n\n---\n# Chapter 5: Abstraction Identification\n\n```markdown\n# Chapter 5: Abstraction Identification\n\nIn the previous chapter, [Codebase Crawling](04_codebase_crawling.md), we learned how to gather all the code files from a repository or local directory. Now, we need to figure out *what's important* in that code!  That's where Abstraction Identification comes in.\n\n## What is Abstraction Identification?\n\nImagine you're trying to learn a new language.  You wouldn't just start memorizing every single word in the dictionary, right? Instead, you'd focus on the most common and important words and grammar rules.\n\nAbstraction Identification is similar. It's the process of analyzing the codebase and identifying the most important concepts or components to explain in the tutorial.  These \"concepts\" are often called \"abstractions\" because they represent a higher-level, simplified view of the underlying code.\n\nThink of it like a detective piecing together clues from a crime scene to identify the key suspects and their roles in the crime.  The LLM is our detective, sifting through the code to find the most important pieces.\n\nWhy is this important?  Because if we tried to explain every single line of code, the tutorial would be overwhelming and confusing.  Abstraction Identification helps us focus on the core concepts that a beginner needs to understand.\n\n## Key Concepts\n\n*   **Abstractions:**  Simplified representations of complex code components. Think of them as key concepts or building blocks of the project. Examples could be \"API Endpoint,\" \"Database Connection,\" or \"User Authentication.\"\n*   **LLM (Large Language Model):**  The \"brain\" of our operation.  We use an LLM to analyze the code and identify the important abstractions.\n*   **Codebase Context:** The actual code files that the LLM analyzes.  This is the output of the [Codebase Crawling](04_codebase_crawling.md) step.\n*   **File Indices:** A list of index numbers which corresponds to file paths used for the LLM to refer back to the code.\n\n## How Abstraction Identification Works\n\nThe Abstraction Identification process is handled by the `IdentifyAbstractions` node in our flow.\n\nLet's look at the simplified steps in `nodes.py`:\n\n```python\nclass IdentifyAbstractions(Node):\n    def prep(self, shared):\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        \n        # Helper to create context from files, respecting limits (basic example)\n        def create_llm_context(files_data):\n            context = \"\"\n            file_info = [] # Store tuples of (index, path)\n            for i, (path, content) in enumerate(files_data):\n                entry = f\"--- File Index {i}: {path} ---\\n{content}\\n\\n\"\n                context += entry\n                file_info.append((i, path))\n\n            return context, file_info # file_info is list of (index, path)\n\n        context, file_info = create_llm_context(files_data)\n        # Format file info for the prompt (comment is just a hint for LLM)\n        file_listing_for_prompt = \"\\n\".join([f\"- {idx} # {path}\" for idx, path in file_info])\n        return context, file_listing_for_prompt, len(files_data), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, file_listing_for_prompt, file_count, project_name = prep_res  # Unpack project name\n        print(\"Identifying abstractions using LLM...\")\n        prompt = f\"\"\"\nFor the project `{project_name}`:\n\nCodebase Context:\n{context}\n\nAnalyze the codebase context.\nIdentify the top 5-10 core most important abstractions to help those new to the codebase.\n\nFor each abstraction, provide:\n1. A concise `name`.\n2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\nList of file indices and paths present in the context:\n{file_listing_for_prompt}\n\nFormat the output as a YAML list of dictionaries:\n\n```yaml\n- name: Query Processing\n  description: | \n    Explains what the abstraction does.\n    It's like a central dispatcher routing requests.\n  file_indices:\n    - 0 # path/to/file1.py\n    - 3 # path/to/related.py\n- name: Query Optimization\n  description: |\n    Another core concept, similar to a blueprint for objects.\n  file_indices:\n    - 5 # path/to/another.js\n# ... up to 10 abstractions\n```\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        abstractions = yaml.safe_load(yaml_str)\n\n        # Basic Validation\n        # Extract data and file indices to `shared` dictionary\n        # This is where we get the list of abstractions\n        validated_abstractions = []\n        # --- Add validation here as needed ---\n        print(f\"Identified {len(validated_abstractions)} abstractions.\")\n        return validated_abstractions\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"abstractions\"] = exec_res # List of {\"name\": str, \"description\": str, \"files\": [int]}\n```\n\nHere's a breakdown:\n\n1.  **`prep(shared)`:**\n    *   This method retrieves the `files` (the code) from the `shared` dictionary. This is the output of the [Codebase Crawling](04_codebase_crawling.md) step.\n    *   It creates an LLM-friendly context string by concatenating the contents of all the files.  This is what we'll feed to the LLM.\n    *   `file_listing_for_prompt` contains a list of file indices and their paths to tell the LLM where the context came from.\n\n2.  **`exec(prep_res)`:**\n    *   This method constructs a prompt for the LLM, telling it to analyze the code and identify the key abstractions. The prompt includes:\n        *   The codebase context (the concatenated code files).\n        *   Instructions on how to format the output (as a YAML list of dictionaries).\n    *   It calls the `call_llm` function (defined in `utils/call_llm.py`) to send the prompt to the LLM.\n    *   It receives the LLM's response, which should be a YAML string containing the list of abstractions, their descriptions, and their relevant file indices.\n    *   The YAML output is validated and parsed to store it in `shared[\"abstractions\"]`\n\n3.  **`post(shared, prep_res, exec_res)`:**\n    *   This method takes the list of abstractions and stores it in the `shared[\"abstractions\"]` variable. This makes the abstractions available to the next node in the flow, which is [Relationship Analysis](06_relationship_analysis.md).\n\n## Code Example: The LLM Prompt\n\nHere's an example of the prompt that we send to the LLM (from `nodes.py`):\n\n```python\nprompt = f\"\"\"\nFor the project `{project_name}`:\n\nCodebase Context:\n{context}\n\nAnalyze the codebase context.\nIdentify the top 5-10 core most important abstractions to help those new to the codebase.\n\nFor each abstraction, provide:\n1. A concise `name`.\n2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\nList of file indices and paths present in the context:\n{file_listing_for_prompt}\n\nFormat the output as a YAML list of dictionaries:\n\n```yaml\n- name: Query Processing\n  description: | \n    Explains what the abstraction does.\n    It's like a central dispatcher routing requests.\n  file_indices:\n    - 0 # path/to/file1.py\n    - 3 # path/to/related.py\n- name: Query Optimization\n  description: |\n    Another core concept, similar to a blueprint for objects.\n  file_indices:\n    - 5 # path/to/another.js\n# ... up to 10 abstractions\n```\"\"\"\n```\n\nThis prompt tells the LLM to:\n\n*   Analyze the code and identify the top 5-10 most important abstractions.\n*   For each abstraction, provide a name, a description, and a list of relevant file indices.\n*   Format the output as a YAML list of dictionaries.\n\nThe prompt is designed to be clear and concise, guiding the LLM to provide the information we need in a structured format.\n\n## Code Example: Validating the LLM Response\n\nAfter the LLM returns its response, we need to validate that it's in the correct format. Here's a simplified example of how we do this (from `nodes.py`):\n\n```python\nyaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\nabstractions = yaml.safe_load(yaml_str)\n\nif not isinstance(abstractions, list):\n    raise ValueError(\"LLM Output is not a list\")\n\nvalidated_abstractions = []\nfor item in abstractions:\n    if not isinstance(item, dict) or not all(k in item for k in [\"name\", \"description\", \"file_indices\"]):\n        raise ValueError(f\"Missing keys in abstraction item: {item}\")\n\n    # --- Add validation here as needed ---\n    validated_abstractions.append({\n        \"name\": item[\"name\"],\n        \"description\": item[\"description\"],\n        \"file_indices\": item[\"file_indices\"]\n    })\n```\n\nThis code checks that:\n\n*   The response is a valid YAML string.\n*   The YAML string represents a list.\n*   Each item in the list is a dictionary with the required keys (\"name\", \"description\", \"file_indices\").\n\nIf the response doesn't pass these checks, we raise a `ValueError` to indicate that there's a problem.\n\n## Using The Abstractions\n\nThe final output of the `IdentifyAbstractions` node is a list of dictionaries, where each dictionary represents an abstraction:\n\n```json\n[\n  {\n    \"name\": \"Query Processing\",\n    \"description\": \"A beginner-friendly explanation of what Query Processing is...\",\n    \"files\": [0, 3]\n  },\n  {\n    \"name\": \"Query Optimization\",\n    \"description\": \"A beginner-friendly explanation of what Query Optimization is...\",\n    \"files\": [5]\n  }\n]\n```\n\nThis list is stored in the `shared[\"abstractions\"]` variable and passed to the next node in the flow. The \"files\" key stores the list of integer indices, corresponding to paths from earlier in Codebase Crawling.\n\n## Why is Abstraction Identification Important?\n\n*   **Focus:** It helps us focus on the most important concepts in the codebase, making the tutorial more manageable and easier to understand.\n*   **Organization:** It provides a structure for the tutorial, allowing us to organize the content around the key abstractions.\n*   **Beginner-Friendliness:** By focusing on abstractions, we can explain the code in a more beginner-friendly way, using high-level concepts and analogies.\n\n## Conclusion\n\nIn this chapter, we've learned about Abstraction Identification, which is the process of analyzing the codebase and identifying the most important concepts to explain in the tutorial. We've seen how we use an LLM to perform this analysis, and how we validate the LLM's response to ensure that it's in the correct format.\n\nIn the next chapter, [Relationship Analysis](06_relationship_analysis.md), we'll explore how we analyze the relationships between these abstractions to understand the overall architecture of the codebase.\n```\n---\n# Chapter 6: Relationship Analysis\n\n```markdown\n# Chapter 6: Relationship Analysis\n\nIn the previous chapter, [Abstraction Identification](05_abstraction_identification.md), we identified the key concepts or \"abstractions\" within a codebase. Now, it's time to connect the dots!  Relationship Analysis helps us understand how these abstractions interact with each other to form the overall system.\n\n## What is Relationship Analysis?\n\nImagine you've identified all the important people in a city: the mayor, the police chief, the school superintendent, and the business owners.  Knowing who they are is useful, but you also need to understand *how they interact*.  Who reports to whom? Who depends on whom?  What are the power dynamics?\n\nRelationship Analysis is the process of understanding how the identified abstractions interact with each other. This involves using an LLM to analyze the code and determine the key relationships between the abstractions. It's like a network analyst mapping out the connections between different computers in a network to understand how data flows between them.\n\nWithout Relationship Analysis, we'd have a collection of isolated concepts, but we wouldn't understand how they work together to achieve the project's goals. It provides a crucial high-level summary and interaction diagram to give beginners a big picture understanding.\n\n## Key Concepts\n\n*   **Relationships:** The connections or interactions between abstractions.  Examples could be \"uses,\" \"manages,\" \"inherits from,\" or \"depends on.\"\n*   **Directed Graph:** A visual representation of the relationships between abstractions, where the direction of the arrow indicates the flow of interaction. We use a Mermaid diagram to represent this.\n*   **Project Summary:**  A high-level description of the project's purpose and functionality, based on the identified abstractions and their relationships.\n*   **File Indices:** A list of index numbers which corresponds to file paths used for the LLM to refer back to the code.\n*   **Abstraction Index:** The numbered index for a given abstraction.\n\n## How Relationship Analysis Works\n\nThe Relationship Analysis process is handled by the `AnalyzeRelationships` node in our flow.\n\nLet's look at the simplified steps in `nodes.py`:\n\n```python\nclass AnalyzeRelationships(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"] # Now contains 'files' list of indices\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n\n        # Create context with abstraction names, indices, descriptions, and relevant file snippets\n        context = \"Identified Abstractions:\\n\"\n        all_relevant_indices = set()\n        abstraction_info_for_prompt = []\n        for i, abstr in enumerate(abstractions):\n            # Use 'files' which contains indices directly\n            file_indices_str = \", \".join(map(str, abstr['files']))\n            info_line = f\"- Index {i}: {abstr['name']} (Relevant file indices: [{file_indices_str}])\\n  Description: {abstr['description']}\"\n            context += info_line + \"\\n\"\n            abstraction_info_for_prompt.append(f\"{i} # {abstr['name']}\")\n            all_relevant_indices.update(abstr['files'])\n\n        context += \"\\nRelevant File Snippets (Referenced by Index and Path):\\n\"\n        # Get content for relevant files using helper\n        relevant_files_content_map = get_content_for_indices(\n            files_data,\n            sorted(list(all_relevant_indices))\n        )\n        # Format file content for context\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path} ---\\n{content}\"\n            for idx_path, content in relevant_files_content_map.items()\n        )\n        context += file_context_str\n\n        return context, \"\\n\".join(abstraction_info_for_prompt), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, abstraction_listing, project_name = prep_res  # Unpack project name\n        print(\"Analyzing relationships using LLM...\")\n        prompt = f\"\"\"\nBased on the following abstractions and relevant code snippets from the project `{project_name}`:\n\nList of Abstraction Indices and Names:\n{abstraction_listing}\n\nContext (Abstractions, Descriptions, Code):\n{context}\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        relationships_data = yaml.safe_load(yaml_str)\n\n        if not isinstance(relationships_data, dict) or not all(k in relationships_data for k in [\"summary\", \"relationships\"]):\n            raise ValueError(\"LLM output is not a dict or missing keys ('summary', 'relationships')\")\n        if not isinstance(relationships_data[\"summary\"], str):\n             raise ValueError(\"summary is not a string\")\n        if not isinstance(relationships_data[\"relationships\"], list):\n             raise ValueError(\"relationships is not a list\")\n\n        # Validate relationships structure\n        validated_relationships = []\n        num_abstractions = len(abstraction_listing.split('\\n'))\n        for rel in relationships_data[\"relationships\"]:\n             # Check for 'label' key\n             if not isinstance(rel, dict) or not all(k in rel for k in [\"from_abstraction\", \"to_abstraction\", \"label\"]):\n                  raise ValueError(f\"Missing keys (expected from_abstraction, to_abstraction, label) in relationship item: {rel}\")\n             # Validate 'label' is a string\n             if not isinstance(rel[\"label\"], str):\n                  raise ValueError(f\"Relationship label is not a string: {rel}\")\n\n             # Validate indices\n             try:\n                 from_idx = int(str(rel[\"from_abstraction\"]).split('#')[0].strip())\n                 to_idx = int(str(rel[\"to_abstraction\"]).split('#')[0].strip())\n                 if not (0 <= from_idx < num_abstractions and 0 <= to_idx < num_abstractions):\n                      raise ValueError(f\"Invalid index in relationship: from={from_idx}, to={to_idx}. Max index is {num_abstractions-1}.\")\n                 validated_relationships.append({\n                     \"from\": from_idx,\n                     \"to\": to_idx,\n                     \"label\": rel[\"label\"] \n                 })\n             except (ValueError, TypeError):\n                  raise ValueError(f\"Could not parse indices from relationship: {rel}\")\n\n        print(\"Generated project summary and relationship details.\")\n        return {\n            \"summary\": relationships_data[\"summary\"],\n            \"details\": validated_relationships # Store validated, index-based relationships\n        }\n\n\n    def post(self, shared, prep_res, exec_res):\n        # Structure is now {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        shared[\"relationships\"] = exec_res\n```\n\nHere's a breakdown:\n\n1.  **`prep(shared)`:**\n    *   This method retrieves the `abstractions` and `files` from the `shared` dictionary.  The `abstractions` are the output of the [Abstraction Identification](05_abstraction_identification.md) step. The files are the output from [Codebase Crawling](04_codebase_crawling.md).\n    *   It creates a context string for the LLM.  This context includes:\n        *   A list of the identified abstractions, including their names, descriptions, and file indices.\n        *   The content of the relevant code files.\n2.  **`exec(prep_res)`:**\n    *   This method constructs a prompt for the LLM, telling it to analyze the abstractions and their relationships.  The prompt includes:\n        *   The context string created in the `prep` method.\n        *   Instructions on how to format the output (as a YAML string containing the project summary and the relationships between abstractions).\n    *   It calls the `call_llm` function (defined in `utils/call_llm.py`) to send the prompt to the LLM.\n    *   It receives the LLM's response, which should be a YAML string containing the project summary and the relationships between abstractions.\n    *   The output is validated and parsed to store it in `shared[\"relationships\"]`.\n3.  **`post(shared, prep_res, exec_res)`:**\n    *   This method takes the project summary and relationships and stores them in the `shared[\"relationships\"]` variable. This makes the relationships available to the next node in the flow, which is [Order Chapters](07_llm_interaction.md).\n\n## Code Example: The LLM Prompt\n\nHere's an example of the prompt that we send to the LLM (from `nodes.py`):\n\n```python\nprompt = f\"\"\"\nBased on the following abstractions and relevant code snippets from the project `{project_name}`:\n\nList of Abstraction Indices and Names:\n{abstraction_listing}\n\nContext (Abstractions, Descriptions, Code):\n{context}\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n\"\"\"\n```\n\nThis prompt tells the LLM to:\n\n*   Provide a high-level summary of the project's purpose and functionality.\n*   Identify the key relationships between the abstractions.\n*   For each relationship, specify the source abstraction, the target abstraction, and a brief label describing the relationship.\n*   Format the output as a YAML string.\n\n## Code Example: The Output of Relationship Analysis\n\nThe `exec` function returns a dictionary that contains the output of the `AnalyzeRelationships` node:\n\n```json\n{\n  \"summary\": \"This project is a web framework that provides a simple and efficient way to build APIs.\",\n  \"details\": [\n    {\n      \"from\": 0,\n      \"to\": 1,\n      \"label\": \"Manages\"\n    },\n    {\n      \"from\": 2,\n      \"to\": 0,\n      \"label\": \"Provides config\"\n    }\n  ]\n}\n```\n\nHere's a breakdown:\n\n*   `\"summary\"`: A short, beginner-friendly explanation of the project's goal. This is used in the introduction of the tutorial.\n*   `\"details\"`: A list of the relationships between abstractions.\n    *   `\"from\"`: The index of the source abstraction.\n    *   `\"to\"`: The index of the target abstraction.\n    *   `\"label\"`: A short description of the relationship.\n\n## Using The Relationships\n\nThe final output of the `AnalyzeRelationships` node is a dictionary containing the summary and relationship details. This dictionary is stored in the `shared[\"relationships\"]` variable and is used by later nodes.\n\nFor example, the `OrderChapters` node uses the relationships to determine the best order to present the abstractions in the tutorial.  The `CombineTutorial` node uses the relationships to generate a Mermaid diagram visualizing the connections between the abstractions, and to generate the tutorial index page.\n\n## Why is Relationship Analysis Important?\n\n*   **Context:** It provides a high-level overview of the project's purpose and functionality.\n*   **Understanding:** It helps to understand how the different abstractions interact with each other to form the overall system.\n*   **Visualization:** It allows us to visualize the relationships between abstractions, making it easier to grasp the overall architecture of the codebase.\n*   **Ordering:** It informs the ordering of chapters in the tutorial, ensuring that concepts are presented in a logical and easy-to-understand sequence.\n\n## Conclusion\n\nIn this chapter, we've learned about Relationship Analysis, which is the process of understanding how the identified abstractions interact with each other. We've seen how we use an LLM to perform this analysis, and how we use the results to generate a project summary and visualize the relationships between abstractions.\n\nIn the next chapter, [LLM Interaction](07_llm_interaction.md), we'll explore how we interact with the LLM to generate the actual content for the tutorial chapters.\n```\n---\n# Chapter 7: LLM Interaction\n\n```markdown\n# Chapter 7: LLM Interaction\n\nIn the previous chapter, [Relationship Analysis](06_relationship_analysis.md), we learned how to understand the connections between the key concepts in a codebase. Now, we're going to focus on how we use a Large Language Model (LLM) to actually *generate* the content for our tutorial! This is where the magic happens!\n\n## What is LLM Interaction?\n\nImagine you're a writer, but you have a super-smart AI assistant that can help you research, outline, and even write entire sections of your book. This assistant is the LLM Interaction module. It's the part of our system that talks to the LLM, sends it instructions, and receives its responses.\n\nThe LLM Interaction module handles all calls to the Large Language Model (LLM). This includes:\n\n*   **Prompt Engineering:** Carefully crafting the instructions we send to the LLM to get the best possible results.\n*   **Caching Responses:** Saving the LLM's responses so we don't have to ask it the same question multiple times. This saves time and money.\n*   **Logging Prompts and Responses:** Keeping a record of everything we send to and receive from the LLM for debugging and analysis.\n*   **Providing a Consistent Interface:** Making it easy for other modules to interact with the LLM without having to worry about the details of how the LLM works.\n\nThink of it as a dedicated interpreter translating human instructions into machine-understandable code and back again! Without the LLM Interaction module, we'd have no way to leverage the power of LLMs to generate our tutorials.\n\n## Key Concepts\n\n*   **Prompt:** The instruction or question we send to the LLM. It's like telling your AI assistant what you want it to do.\n*   **Response:** The output we receive from the LLM. It's like your AI assistant's answer to your question.\n*   **Caching:** Storing the LLM's responses so we can reuse them later. This is like remembering the answer to a question so you don't have to look it up again.\n*   **Logging:** Recording all the interactions with the LLM for debugging and analysis. This is like keeping a notebook of all your conversations with your AI assistant.\n*   **API Key:** A secret code that allows us to access the LLM. It's like the password you need to log into your AI assistant.\n\n## How LLM Interaction Works\n\nThe LLM Interaction is handled by the `call_llm` function in `utils/call_llm.py`. All LLM Interaction nodes will call this central function.\n\nHere's a simplified view of how it works:\n\n```python\nfrom google import genai\nimport os\nimport logging\nimport json\nfrom datetime import datetime\n\n# Configure logging\nlog_directory = os.getenv(\"LOG_DIR\", \"logs\")\nos.makedirs(log_directory, exist_ok=True)\nlog_file = os.path.join(log_directory, f\"llm_calls_{datetime.now().strftime('%Y%m%d')}.log\")\n\n# Set up logger\nlogger = logging.getLogger(\"llm_logger\")\nlogger.setLevel(logging.INFO)\nlogger.propagate = False  # Prevent propagation to root logger\nfile_handler = logging.FileHandler(log_file)\nfile_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\nlogger.addHandler(file_handler)\n\n# Simple cache configuration\ncache_file = \"llm_cache.json\"\n\n# By default, we Google Gemini 2.5 pro, as it shows great performance for code understanding\ndef call_llm(prompt: str, use_cache: bool = True) -> str:\n    # Log the prompt\n    logger.info(f\"PROMPT: {prompt}\")\n    \n    # Check cache if enabled\n    if use_cache:\n        # Load cache from disk\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, 'r') as f:\n                    cache = json.load(f)\n            except:\n                logger.warning(f\"Failed to load cache, starting with empty cache\")\n        \n        # Return from cache if exists\n        if prompt in cache:\n            logger.info(f\"RESPONSE: {cache[prompt]}\")\n            return cache[prompt]\n    \n    # Call the LLM if not in cache or cache disabled\n    client = genai.Client(\n        vertexai=True, \n        # TODO: change to your own project id and location\n        project=os.getenv(\"GEMINI_PROJECT_ID\", \"your-project-id\"),\n        location=os.getenv(\"GEMINI_LOCATION\", \"us-central1\")\n    )\n    # You can comment the previous line and use the AI Studio key instead:\n    # client = genai.Client(\n    #     api_key=os.getenv(\"GEMINI_API_KEY\", \"your-api_key\"),\n    # )\n    model = os.getenv(\"GEMINI_MODEL\", \"gemini-2.5-pro-exp-03-25\")\n    response = client.models.generate_content(\n        model=model,\n        contents=[prompt]\n    )\n    response_text = response.text\n    \n    # Log the response\n    logger.info(f\"RESPONSE: {response_text}\")\n    \n    # Update cache if enabled\n    if use_cache:\n        # Load cache again to avoid overwrites\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, 'r') as f:\n                    cache = json.load(f)\n            except:\n                pass\n        \n        # Add to cache and save\n        cache[prompt] = response_text\n        try:\n            with open(cache_file, 'w') as f:\n                json.dump(cache, f)\n        except Exception as e:\n            logger.error(f\"Failed to save cache: {e}\")\n    \n    return response_text\n```\n\nLet's break down the code:\n\n1.  **Importing Libraries:** We import the necessary libraries for interacting with the LLM, handling environment variables, logging, and caching.\n2.  **Configuring Logging:** We set up a logger to record all the prompts and responses. This helps us debug any issues and analyze the LLM's behavior.\n3.  **Configuring Caching:** We set up a simple cache to store the LLM's responses. This saves us from having to make the same API calls multiple times.\n4.  **Calling the LLM:** The `call_llm` function takes a prompt as input and sends it to the LLM. It handles authentication, error handling, and response parsing.\n5.  **Logging the Prompt and Response:** The function logs the prompt and response to the log file.\n6.  **Updating the Cache:** If caching is enabled, the function updates the cache with the prompt and response.\n7. **Choosing LLM Model:** The Gemini family of models are used by default, but the code has examples for how to call other models like Anthropic Claude or OpenAI.\n\n## Code Example: Writing a Chapter\n\nThe `WriteChapters` node uses the `call_llm` function to generate the content for each chapter of the tutorial.\n\n```python\nclass WriteChapters(BatchNode):\n    def prep(self, shared):\n        # ... (preparation logic) ...\n\n    def exec(self, item):\n        # ... (item preparation) ...\n\n        prompt = f\"\"\"\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `{project_name}` about the concept: \"{abstraction_name}\". This is Chapter {chapter_num}.\n\nConcept Details:\n- Description:\n{item[\"abstraction_details\"][\"description\"]}\n\nComplete Tutorial Structure:\n{item[\"full_chapter_listing\"]}\n\nContext from previous chapters (summary):\n{previous_chapters_summary if previous_chapters_summary else \"This is the first chapter.\"}\n\nRelevant Code Snippets:\n{file_context_str if file_context_str else \"No specific code snippets provided for this abstraction.\"}\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter {chapter_num}: {abstraction_name}`).\n        \n        # ... (more instructions) ...\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n\"\"\"\n        chapter_content = call_llm(prompt)\n        # ... (validation and cleanup) ...\n        return chapter_content # Return the Markdown string\n\n    def post(self, shared, prep_res, exec_res_list):\n        # ... (post-processing logic) ...\n```\n\nHere's a breakdown:\n\n1.  **Preparing the Prompt:** The `prep` and `exec` methods construct a detailed prompt that tells the LLM exactly what we want it to do:\n    *   Write a tutorial chapter about a specific concept.\n    *   Use a beginner-friendly tone.\n    *   Follow a specific structure.\n    *   Incorporate relevant code snippets.\n    *   Reference previous chapters.\n2.  **Calling the LLM:** The `exec` method calls the `call_llm` function to send the prompt to the LLM.\n3.  **Receiving the Response:** The `call_llm` function returns the LLM's response, which is the generated Markdown content for the chapter.\n4.  **Validating and Cleaning Up:** The `exec` method validates the LLM's response to make sure it's in the correct format and cleans up any unwanted characters.\n5.  **Returning the Result:** The `exec` method returns the validated and cleaned-up Markdown content for the chapter.\n\n## Setting Up the LLM API Key\n\nTo use the LLM Interaction module, you'll need to set up an API key for the LLM you want to use. The default call_llm uses Google's Gemini API, but code samples are shown for Anthropic Claude and OpenAI. The Gemini API can be set up with the Google AI Studio key or using Google Cloud.\n\nFor Google AI Studio:\n\n1.  Go to the [Google AI Studio website](https://makersuite.google.com/).\n2.  Create an account or log in.\n3.  Follow the instructions to create an API key.\n4.  Set the `GEMINI_API_KEY` environment variable to your API key.\n\n    ```bash\n    export GEMINI_API_KEY=<your_api_key>\n    ```\n\nFor Google Cloud:\n\n1.  Create a Google Cloud project.\n2.  Enable the Gemini API for your project.\n3.  Create a service account with the necessary permissions.\n4.  Download the service account key file.\n5.  Set the `GEMINI_PROJECT_ID` environment variable to your project ID.\n6.  Set the `GEMINI_LOCATION` environment variable to your project location.\n\n## Caching and Logging\n\nThe `call_llm` function automatically caches the LLM's responses to a JSON file called `llm_cache.json`. This helps to speed up the tutorial generation process and reduce the cost of calling the LLM.\n\nThe `call_llm` function also logs all the prompts and responses to a log file in the `logs` directory. This helps to debug any issues and analyze the LLM's behavior.\n\n## Why is LLM Interaction Important?\n\n*   **Content Generation:** It allows us to automatically generate the content for our tutorials, saving us a lot of time and effort.\n*   **Scalability:** It allows us to generate tutorials for any codebase, no matter how large or complex.\n*   **Consistency:** It ensures that the tutorials are written in a consistent style and tone.\n*   **Beginner-Friendly:** By providing detailed instructions to the LLM, we can ensure that the tutorials are easy for beginners to understand.\n\n## Conclusion\n\nIn this chapter, we've learned about LLM Interaction, which is the process of using a Large Language Model (LLM) to generate the content for our tutorials. We've seen how we prepare the prompts, call the LLM, receive the responses, and validate the results.\n\nIn the next chapter, [Chapter Writing](08_chapter_writing.md), we'll delve deeper into the techniques for crafting effective and engaging tutorial chapters.\n```\n---\n# Chapter 8: Chapter Writing\n\n```markdown\n# Chapter 8: Chapter Writing\n\nIn the previous chapter, [LLM Interaction](07_llm_interaction.md), we learned how to communicate with Large Language Models (LLMs) to generate content for our tutorial. Now, let's dive deeper into the art and science of *crafting* those tutorial chapters! This is where we refine the raw output from the LLM into something truly engaging and educational.\n\n## What is Chapter Writing?\n\nChapter Writing is the process of taking the information and insights gathered from the previous steps (Codebase Crawling, Abstraction Identification, Relationship Analysis, and LLM Interaction) and transforming them into clear, concise, and beginner-friendly tutorial content. It's about weaving a compelling narrative around different concepts and code snippets to educate and entertain the audience.\n\nThink of it like a skilled storyteller weaving a compelling narrative around different characters and events. We're taking complex technical information and making it accessible and engaging for newcomers. It's not just about writing code; it's about *teaching* code.\n\n## Key Concepts\n\n*   **Beginner-Friendliness:**  Writing in a way that is easy for newcomers to understand, avoiding jargon and complex explanations.\n*   **Clear Structure:**  Organizing the chapter content in a logical and easy-to-follow manner, with clear headings, subheadings, and transitions.\n*   **Code Snippets:**  Incorporating relevant code snippets to illustrate the concepts being explained.\n*   **Diagrams:**  Using diagrams (like Mermaid diagrams) to visualize complex relationships and processes.\n*   **Analogies:**  Employing analogies and real-world examples to make abstract concepts more concrete.\n*   **Markdown Formatting:** Using Markdown to format the chapter content, making it readable and visually appealing.\n\n## How Chapter Writing Works\n\nThe actual \"writing\" is largely handled by the LLM in the `WriteChapters` node (as seen in the [LLM Interaction](07_llm_interaction.md) chapter). However, the *prompt engineering* that guides the LLM is crucial.\n\nThe `WriteChapters` node in `nodes.py` is responsible for generating the content for each chapter. Let's revisit the prompt:\n\n```python\nprompt = f\"\"\"\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `{project_name}` about the concept: \"{abstraction_name}\". This is Chapter {chapter_num}.\n\nConcept Details:\n- Description:\n{item[\"abstraction_details\"][\"description\"]}\n\nComplete Tutorial Structure:\n{item[\"full_chapter_listing\"]}\n\nContext from previous chapters (summary):\n{previous_chapters_summary if previous_chapters_summary else \"This is the first chapter.\"}\n\nRelevant Code Snippets:\n{file_context_str if file_context_str else \"No specific code snippets provided for this abstraction.\"}\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter {chapter_num}: {abstraction_name}`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n\"\"\"\n```\n\nLet's break down the key elements of this prompt:\n\n1.  **Context:** We provide the LLM with all the necessary context, including:\n    *   The project name.\n    *   The concept name.\n    *   The concept description.\n    *   A complete tutorial structure (list of chapters).\n    *   A summary of previous chapters (to maintain continuity).\n    *   Relevant code snippets.\n2.  **Instructions:** We give the LLM very specific instructions on how to write the chapter, including:\n    *   Start with a clear heading.\n    *   Provide a transition from the previous chapter.\n    *   Explain the motivation behind the concept.\n    *   Break down complex concepts into smaller parts.\n    *   Use code snippets to illustrate the concepts.\n    *   Explain the internal implementation.\n    *   Use Mermaid diagrams.\n    *   Use analogies and examples.\n    *   End with a conclusion and transition to the next chapter.\n    *   Maintain a welcoming and beginner-friendly tone.\n3.  **Formatting:** We specify that the output should be in Markdown format and that it should *only* include the Markdown content (no extra text or formatting).\n\n## Key Techniques for Effective Chapter Writing\n\nHere are some key techniques to keep in mind when crafting your prompts:\n\n1.  **Start with a Clear Heading:** Every chapter should begin with a clear and descriptive heading that tells the reader what the chapter is about. This helps the reader quickly understand the chapter's purpose and scope.\n\n    ```markdown\n    # Chapter 8: Chapter Writing\n    ```\n\n2.  **Provide a Transition from the Previous Chapter:** If the chapter is not the first one, it's important to provide a brief transition from the previous chapter. This helps the reader understand how the current chapter relates to what they've already learned.\n\n    ```markdown\n    In the previous chapter, [LLM Interaction](07_llm_interaction.md), we learned how to communicate with Large Language Models (LLMs) to generate content for our tutorial.\n    ```\n\n3.  **Explain the Motivation Behind the Concept:** Every chapter should explain why the concept being discussed is important. What problem does it solve? Why should the reader care?\n\n    ```markdown\n    Chapter Writing is the process of taking the information and insights gathered from the previous steps... and transforming them into clear, concise, and beginner-friendly tutorial content.\n    ```\n\n4.  **Break Down Complex Concepts:** If the concept is complex, break it down into smaller, more manageable parts. Explain each part one at a time, using clear and simple language.\n\n    ```markdown\n    ## Key Concepts\n\n    *   Beginner-Friendliness\n    *   Clear Structure\n    *   Code Snippets\n    *   Diagrams\n    *   Analogies\n    *   Markdown Formatting\n    ```\n\n5.  **Use Code Snippets to Illustrate the Concepts:** Code snippets are a powerful tool for teaching programming concepts. Use them to show the reader how the concept is used in practice.\n\n    ```python\n    def hello_world():\n        print(\"Hello, world!\")\n\n    hello_world() # Prints \"Hello, world!\"\n    ```\n\n6.  **Explain the Internal Implementation:** Understanding how something works under the hood can be very helpful for beginners. Provide a high-level overview of the internal implementation, using diagrams and code snippets.\n\n    ```mermaid\n    sequenceDiagram\n        participant User\n        participant Application\n        participant Database\n\n        User->>Application: Requests data\n        Application->>Database: Queries database\n        Database->>Application: Returns data\n        Application->>User: Displays data\n    ```\n\n7.  **Use Analogies and Examples:** Analogies and real-world examples can help make abstract concepts more concrete and easier to understand.\n\n    ```markdown\n    Think of it like a skilled storyteller weaving a compelling narrative around different characters and events.\n    ```\n\n8.  **End with a Conclusion and Transition to the Next Chapter:** Every chapter should end with a brief conclusion that summarizes what was learned and provides a transition to the next chapter.\n\n    ```markdown\n    In the next chapter, [Tutorial Combination](09_tutorial_combination.md), we'll learn how to combine all the individual chapters into a complete and polished tutorial.\n    ```\n\n## Example: Crafting a Section About Code Snippets\n\nLet's say we want to write a section about how to effectively use code snippets in a tutorial chapter. We might start with an introduction:\n\n```markdown\n## Using Code Snippets\n\nCode snippets are essential for illustrating the concepts you're explaining. They provide concrete examples of how the code works in practice, making it easier for beginners to understand.\n```\n\nThen, we might provide some guidelines on how to choose and format code snippets:\n\n```markdown\n*   **Keep it short:** Code snippets should be as short and focused as possible. Avoid including irrelevant code that might distract the reader.\n*   **Use comments:** Add comments to explain what the code is doing. This helps the reader understand the purpose of each line of code.\n*   **Provide context:** Explain where the code snippet fits within the larger project. This helps the reader understand how the code relates to the overall system.\n*   **Format consistently:** Use a consistent code style throughout the tutorial. This makes the code easier to read and understand.\n```\n\nFinally, we might provide an example of a well-formatted code snippet:\n\n```python\n# This function calculates the sum of two numbers.\ndef add(x, y):\n    return x + y\n\n# Example usage:\nresult = add(5, 3)\nprint(result)  # Output: 8\n```\n\n## Why is Chapter Writing Important?\n\n*   **Clarity:** It ensures that the tutorial content is clear, concise, and easy to understand.\n*   **Engagement:** It makes the tutorial more engaging and enjoyable to read.\n*   **Effectiveness:** It helps the reader learn the concepts more effectively.\n*   **Professionalism:** It gives the tutorial a polished and professional look.\n\n## Conclusion\n\nIn this chapter, we've explored the art and science of Chapter Writing. We've learned how to craft effective and engaging tutorial chapters by providing clear instructions to the LLM and incorporating key techniques like clear structure, code snippets, diagrams, and analogies. By following these guidelines, we can transform raw information into a valuable learning resource for beginners.\n\nIn the next chapter, [Tutorial Combination](09_tutorial_combination.md), we'll learn how to combine all the individual chapters into a complete and polished tutorial.\n```\n\n\nRelevant Code Snippets:\n--- File: converter/md_to_html.py ---\nfrom pathlib import Path\nimport markdown\nfrom md_mermaid import MermaidExtension\nimport sys\nfrom mermaid_extension import MermaidExtension as mermaid_ext\n\ndef convert_md_to_html(md_file_path, output_folder):\n    # Read the Markdown file\n    with open(md_file_path, \"r\", encoding=\"utf-8\") as md_file:\n        md_content = md_file.read()\n\n    # Initialize the Markdown object with the extensions\n    md = markdown.Markdown(extensions=[\n        # MermaidExtension(),  # Proper initialization of MermaidExtension\n        mermaid_ext(),\n        'fenced_code',\n        'codehilite',\n        'tables',\n        'toc'\n    ])\n\n    md_content = convert_hyperlink_to_html(md_content)\n    # Convert Markdown to HTML\n    html_content = md.convert(md_content)\n    \n    # wrap the HTML content in a basic HTML structure\n    html_content = f\"\"\"\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n<meta charset=\"UTF-8\">\n<title>{md_file_path}</title>\n        <script type=\"module\">\n            import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';\n            mermaid.initialize({{ startOnLoad: true }});\n        </script>\n       <style>\n            body {{\n                max-width: 800px;\n                margin: 2em auto;\n                font-family: system-ui, sans-serif;\n                line-height: 1.6;\n                color: #333;\n                padding: 0 1em;\n            }}\n            pre {{\n                background: #f6f8fa;\n                padding: 1em;\n                overflow-x: auto;\n            }}\n            code {{\n                background: #f0f0f0;\n                padding: 2px 4px;\n                border-radius: 4px;\n            }}\n            a {{\n                color: #0366d6;\n                text-decoration: none;\n            }}\n            a:hover {{\n                text-decoration: underline;\n            }}\n        </style>\n</head>\n<body>\n{html_content}\n</body>\n</html>\n\"\"\"   \n\n    # create the output file path\n    output_file_path = output_folder / md_file_path.relative_to(md_file_path.parent).with_suffix(\".html\")\n    \n    # write the HTML content to a file\n    with open(output_file_path, \"w\", encoding=\"utf-8\") as html_file:\n        html_file.write(html_content)\n        \n    print(f\"Converted {md_file_path} to {output_file_path}\")\n    \n\ndef convert_hyperlink_to_html(md_content):\n    # replace any hyperlinks with HTML links (.md to .html)\n    md_content = md_content.replace(\".md\", \".html\")\n    return md_content\n    \ndef convert_all_md_to_html(md_folder_path, output_folder=None):\n    md_folder_path = Path(md_folder_path)\n    output_folder = Path(output_folder) if output_folder else md_folder_path / \"html\"\n    output_folder.mkdir(parents=True, exist_ok=True)\n    \n    # iterate through all Markdown files in the folder\n    for md_file in md_folder_path.glob(\"**/*.md\"):\n        convert_md_to_html(md_file, output_folder)\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python md_to_html.py <md_folder_path> <output_folder>\")\n        sys.exit(1)\n    else:\n        md_folder_path = sys.argv[1]\n        output_folder = sys.argv[2] if len(sys.argv) > 2 else None\n        convert_all_md_to_html(md_folder_path, output_folder)\n\n--- File: nodes.py ---\nimport os\nimport yaml\nfrom pocketflow import Node, BatchNode\nfrom utils.crawl_github_files import crawl_github_files\nfrom utils.call_llm import call_llm # Assuming you have this utility\nfrom utils.crawl_local_files import crawl_local_files\nfrom converter.md_to_html import convert_all_md_to_html\n\n\n# Helper to get content for specific file indices\ndef get_content_for_indices(files_data, indices):\n    content_map = {}\n    for i in indices:\n        if 0 <= i < len(files_data):\n            path, content = files_data[i]\n            content_map[f\"{i} # {path}\"] = content # Use index + path as key for context\n    return content_map\n\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"local_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        if not project_name:\n            # Basic name derivation from URL or directory\n            if repo_url:\n                project_name = repo_url.split('/')[-1].replace('.git', '')\n            else:\n                project_name = os.path.basename(os.path.abspath(local_dir))\n            shared[\"project_name\"] = project_name\n\n        # Get file patterns directly from shared\n        include_patterns = shared[\"include_patterns\"]\n        exclude_patterns = shared[\"exclude_patterns\"]\n        max_file_size = shared[\"max_file_size\"]\n\n        return {\n            \"repo_url\": repo_url,\n            \"local_dir\": local_dir,\n            \"token\": shared.get(\"github_token\"),\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns,\n            \"max_file_size\": max_file_size,\n            \"use_relative_paths\": True\n        }\n\n    def exec(self, prep_res):\n        if prep_res[\"repo_url\"]:\n            print(f\"Crawling repository: {prep_res['repo_url']}...\")\n            result = crawl_github_files(\n                repo_url=prep_res[\"repo_url\"],\n                token=prep_res[\"token\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        else:\n            print(f\"Crawling directory: {prep_res['local_dir']}...\")\n            result = crawl_local_files(\n                directory=prep_res[\"local_dir\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n            \n        # Convert dict to list of tuples: [(path, content), ...]\n        files_list = list(result.get(\"files\", {}).items())\n        print(f\"Fetched {len(files_list)} files.\")\n        return files_list\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"files\"] = exec_res # List of (path, content) tuples\n\nclass IdentifyAbstractions(Node):\n    def prep(self, shared):\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        \n        # Helper to create context from files, respecting limits (basic example)\n        def create_llm_context(files_data):\n            context = \"\"\n            file_info = [] # Store tuples of (index, path)\n            for i, (path, content) in enumerate(files_data):\n                entry = f\"--- File Index {i}: {path} ---\\n{content}\\n\\n\"\n                context += entry\n                file_info.append((i, path))\n\n            return context, file_info # file_info is list of (index, path)\n\n        context, file_info = create_llm_context(files_data)\n        # Format file info for the prompt (comment is just a hint for LLM)\n        file_listing_for_prompt = \"\\n\".join([f\"- {idx} # {path}\" for idx, path in file_info])\n        return context, file_listing_for_prompt, len(files_data), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, file_listing_for_prompt, file_count, project_name = prep_res  # Unpack project name\n        print(\"Identifying abstractions using LLM...\")\n        prompt = f\"\"\"\nFor the project `{project_name}`:\n\nCodebase Context:\n{context}\n\nAnalyze the codebase context.\nIdentify the top 5-10 core most important abstractions to help those new to the codebase.\n\nFor each abstraction, provide:\n1. A concise `name`.\n2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\nList of file indices and paths present in the context:\n{file_listing_for_prompt}\n\nFormat the output as a YAML list of dictionaries:\n\n```yaml\n- name: Query Processing\n  description: | \n    Explains what the abstraction does.\n    It's like a central dispatcher routing requests.\n  file_indices:\n    - 0 # path/to/file1.py\n    - 3 # path/to/related.py\n- name: Query Optimization\n  description: |\n    Another core concept, similar to a blueprint for objects.\n  file_indices:\n    - 5 # path/to/another.js\n# ... up to 10 abstractions\n```\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        abstractions = yaml.safe_load(yaml_str)\n\n        if not isinstance(abstractions, list):\n            raise ValueError(\"LLM Output is not a list\")\n\n        validated_abstractions = []\n        for item in abstractions:\n            if not isinstance(item, dict) or not all(k in item for k in [\"name\", \"description\", \"file_indices\"]):\n                raise ValueError(f\"Missing keys in abstraction item: {item}\")\n            if not isinstance(item[\"description\"], str):\n                 raise ValueError(f\"description is not a string in item: {item}\")\n            if not isinstance(item[\"file_indices\"], list):\n                 raise ValueError(f\"file_indices is not a list in item: {item}\")\n\n            # Validate indices\n            validated_indices = []\n            for idx_entry in item[\"file_indices\"]:\n                 try:\n                     if isinstance(idx_entry, int):\n                         idx = idx_entry\n                     elif isinstance(idx_entry, str) and '#' in idx_entry:\n                          idx = int(idx_entry.split('#')[0].strip())\n                     else:\n                          idx = int(str(idx_entry).strip())\n\n                     if not (0 <= idx < file_count):\n                         raise ValueError(f\"Invalid file index {idx} found in item {item['name']}. Max index is {file_count - 1}.\")\n                     validated_indices.append(idx)\n                 except (ValueError, TypeError):\n                      raise ValueError(f\"Could not parse index from entry: {idx_entry} in item {item['name']}\")\n\n            item[\"files\"] = sorted(list(set(validated_indices)))\n            # Store only the required fields\n            validated_abstractions.append({\n                \"name\": item[\"name\"],\n                \"description\": item[\"description\"],\n                \"files\": item[\"files\"]\n            })\n\n        print(f\"Identified {len(validated_abstractions)} abstractions.\")\n        return validated_abstractions\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"abstractions\"] = exec_res # List of {\"name\": str, \"description\": str, \"files\": [int]}\n\nclass AnalyzeRelationships(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"] # Now contains 'files' list of indices\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n\n        # Create context with abstraction names, indices, descriptions, and relevant file snippets\n        context = \"Identified Abstractions:\\n\"\n        all_relevant_indices = set()\n        abstraction_info_for_prompt = []\n        for i, abstr in enumerate(abstractions):\n            # Use 'files' which contains indices directly\n            file_indices_str = \", \".join(map(str, abstr['files']))\n            info_line = f\"- Index {i}: {abstr['name']} (Relevant file indices: [{file_indices_str}])\\n  Description: {abstr['description']}\"\n            context += info_line + \"\\n\"\n            abstraction_info_for_prompt.append(f\"{i} # {abstr['name']}\")\n            all_relevant_indices.update(abstr['files'])\n\n        context += \"\\nRelevant File Snippets (Referenced by Index and Path):\\n\"\n        # Get content for relevant files using helper\n        relevant_files_content_map = get_content_for_indices(\n            files_data,\n            sorted(list(all_relevant_indices))\n        )\n        # Format file content for context\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path} ---\\n{content}\"\n            for idx_path, content in relevant_files_content_map.items()\n        )\n        context += file_context_str\n\n        return context, \"\\n\".join(abstraction_info_for_prompt), project_name  # Return project name\n\n    def exec(self, prep_res):\n        context, abstraction_listing, project_name = prep_res  # Unpack project name\n        print(\"Analyzing relationships using LLM...\")\n        prompt = f\"\"\"\nBased on the following abstractions and relevant code snippets from the project `{project_name}`:\n\nList of Abstraction Indices and Names:\n{abstraction_listing}\n\nContext (Abstractions, Descriptions, Code):\n{context}\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        relationships_data = yaml.safe_load(yaml_str)\n\n        if not isinstance(relationships_data, dict) or not all(k in relationships_data for k in [\"summary\", \"relationships\"]):\n            raise ValueError(\"LLM output is not a dict or missing keys ('summary', 'relationships')\")\n        if not isinstance(relationships_data[\"summary\"], str):\n             raise ValueError(\"summary is not a string\")\n        if not isinstance(relationships_data[\"relationships\"], list):\n             raise ValueError(\"relationships is not a list\")\n\n        # Validate relationships structure\n        validated_relationships = []\n        num_abstractions = len(abstraction_listing.split('\\n'))\n        for rel in relationships_data[\"relationships\"]:\n             # Check for 'label' key\n             if not isinstance(rel, dict) or not all(k in rel for k in [\"from_abstraction\", \"to_abstraction\", \"label\"]):\n                  raise ValueError(f\"Missing keys (expected from_abstraction, to_abstraction, label) in relationship item: {rel}\")\n             # Validate 'label' is a string\n             if not isinstance(rel[\"label\"], str):\n                  raise ValueError(f\"Relationship label is not a string: {rel}\")\n\n             # Validate indices\n             try:\n                 from_idx = int(str(rel[\"from_abstraction\"]).split('#')[0].strip())\n                 to_idx = int(str(rel[\"to_abstraction\"]).split('#')[0].strip())\n                 if not (0 <= from_idx < num_abstractions and 0 <= to_idx < num_abstractions):\n                      raise ValueError(f\"Invalid index in relationship: from={from_idx}, to={to_idx}. Max index is {num_abstractions-1}.\")\n                 validated_relationships.append({\n                     \"from\": from_idx,\n                     \"to\": to_idx,\n                     \"label\": rel[\"label\"] \n                 })\n             except (ValueError, TypeError):\n                  raise ValueError(f\"Could not parse indices from relationship: {rel}\")\n\n        print(\"Generated project summary and relationship details.\")\n        return {\n            \"summary\": relationships_data[\"summary\"],\n            \"details\": validated_relationships # Store validated, index-based relationships\n        }\n\n\n    def post(self, shared, prep_res, exec_res):\n        # Structure is now {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        shared[\"relationships\"] = exec_res\n\nclass OrderChapters(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"]\n        relationships = shared[\"relationships\"]\n        project_name = shared[\"project_name\"]  # Get project name\n\n        # Prepare context for the LLM\n        abstraction_info_for_prompt = []\n        for i, a in enumerate(abstractions):\n            abstraction_info_for_prompt.append(f\"- {i} # {a['name']}\")\n        abstraction_listing = \"\\n\".join(abstraction_info_for_prompt)\n\n        context = f\"Project Summary:\\n{relationships['summary']}\\n\\n\"\n        context += \"Relationships (Indices refer to abstractions above):\\n\"\n        for rel in relationships['details']:\n             from_name = abstractions[rel['from']]['name']\n             to_name = abstractions[rel['to']]['name']\n             # Use 'label' instead of 'desc'\n             context += f\"- From {rel['from']} ({from_name}) to {rel['to']} ({to_name}): {rel['label']}\\n\"\n\n        return abstraction_listing, context, len(abstractions), project_name\n\n    def exec(self, prep_res):\n        abstraction_listing, context, num_abstractions, project_name = prep_res\n        print(\"Determining chapter order using LLM...\")\n        prompt = f\"\"\"\nGiven the following project abstractions and their relationships for the project ```` {project_name} ````:\n\nAbstractions (Index # Name):\n{abstraction_listing}\n\nContext about relationships and project summary:\n{context}\n\nIf you are going to make a tutorial for ```` {project_name} ````, what is the best order to explain these abstractions, from first to last?\nIdeally, first explain those that are the most important or foundational, perhaps user-facing concepts or entry points. Then move to more detailed, lower-level implementation details or supporting concepts.\n\nOutput the ordered list of abstraction indices, including the name in a comment for clarity. Use the format `idx # AbstractionName`.\n\n```yaml\n- 2 # FoundationalConcept\n- 0 # CoreClassA\n- 1 # CoreClassB (uses CoreClassA)\n- ...\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt)\n\n        # --- Validation ---\n        # Rely on Node's built-in retry/fallback\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        ordered_indices_raw = yaml.safe_load(yaml_str)\n\n        if not isinstance(ordered_indices_raw, list):\n            raise ValueError(\"LLM output is not a list\")\n\n        ordered_indices = []\n        seen_indices = set()\n        for entry in ordered_indices_raw:\n            try:\n                 if isinstance(entry, int):\n                     idx = entry\n                 elif isinstance(entry, str) and '#' in entry:\n                      idx = int(entry.split('#')[0].strip())\n                 else:\n                      idx = int(str(entry).strip())\n\n                 if not (0 <= idx < num_abstractions):\n                      raise ValueError(f\"Invalid index {idx} in ordered list. Max index is {num_abstractions-1}.\")\n                 if idx in seen_indices:\n                     raise ValueError(f\"Duplicate index {idx} found in ordered list.\")\n                 ordered_indices.append(idx)\n                 seen_indices.add(idx)\n\n            except (ValueError, TypeError):\n                 raise ValueError(f\"Could not parse index from ordered list entry: {entry}\")\n\n        # Check if all abstractions are included\n        if len(ordered_indices) != num_abstractions:\n             raise ValueError(f\"Ordered list length ({len(ordered_indices)}) does not match number of abstractions ({num_abstractions}). Missing indices: {set(range(num_abstractions)) - seen_indices}\")\n\n        print(f\"Determined chapter order (indices): {ordered_indices}\")\n        return ordered_indices # Return the list of indices\n\n    def post(self, shared, prep_res, exec_res):\n        # exec_res is already the list of ordered indices\n        shared[\"chapter_order\"] = exec_res # List of indices\n\nclass WriteChapters(BatchNode):\n    def prep(self, shared):\n        chapter_order = shared[\"chapter_order\"] # List of indices\n        abstractions = shared[\"abstractions\"]   # List of dicts, now using 'files' with indices\n        files_data = shared[\"files\"]\n        # Get already written chapters to provide context\n        # We store them temporarily during the batch run, not in shared memory yet\n        # The 'previous_chapters_summary' will be built progressively in the exec context\n        self.chapters_written_so_far = [] # Use instance variable for temporary storage across exec calls\n\n        # Create a complete list of all chapters\n        all_chapters = []\n        chapter_filenames = {} # Store chapter filename mapping for linking\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                chapter_num = i + 1\n                chapter_name = abstractions[abstraction_index][\"name\"]\n                # Create safe filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in chapter_name).lower()\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                # Format with link\n                all_chapters.append(f\"{chapter_num}. [{chapter_name}]({filename})\")\n                # Store mapping of chapter index to filename for linking\n                chapter_filenames[abstraction_index] = {\"num\": chapter_num, \"name\": chapter_name, \"filename\": filename}\n        \n        # Create a formatted string with all chapters\n        full_chapter_listing = \"\\n\".join(all_chapters)\n\n        items_to_process = []\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                abstraction_details = abstractions[abstraction_index]\n                # Use 'files' (list of indices) directly\n                related_file_indices = abstraction_details.get(\"files\", [])\n                # Get content using helper, passing indices\n                related_files_content_map = get_content_for_indices(files_data, related_file_indices)\n                \n                # Get previous chapter info for transitions\n                prev_chapter = None\n                if i > 0:\n                    prev_idx = chapter_order[i-1]\n                    prev_chapter = chapter_filenames[prev_idx]\n                \n                # Get next chapter info for transitions\n                next_chapter = None\n                if i < len(chapter_order) - 1:\n                    next_idx = chapter_order[i+1]\n                    next_chapter = chapter_filenames[next_idx]\n\n                items_to_process.append({\n                    \"chapter_num\": i + 1,\n                    \"abstraction_index\": abstraction_index,\n                    \"abstraction_details\": abstraction_details,\n                    \"related_files_content_map\": related_files_content_map,\n                    \"project_name\": shared[\"project_name\"],  # Add project name\n                    \"full_chapter_listing\": full_chapter_listing,  # Add the full chapter listing\n                    \"chapter_filenames\": chapter_filenames,  # Add chapter filenames mapping\n                    \"prev_chapter\": prev_chapter,  # Add previous chapter info\n                    \"next_chapter\": next_chapter,  # Add next chapter info\n                    # previous_chapters_summary will be added dynamically in exec\n                })\n            else:\n                print(f\"Warning: Invalid abstraction index {abstraction_index} in chapter_order. Skipping.\")\n\n        print(f\"Preparing to write {len(items_to_process)} chapters...\")\n        return items_to_process # Iterable for BatchNode\n\n    def exec(self, item):\n        # This runs for each item prepared above\n        abstraction_name = item[\"abstraction_details\"][\"name\"]\n        chapter_num = item[\"chapter_num\"]\n        project_name = item.get(\"project_name\")  # Get from item\n        print(f\"Writing chapter {chapter_num} for: {abstraction_name} using LLM...\")\n\n        # Prepare file context string from the map\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path.split('# ')[1] if '# ' in idx_path else idx_path} ---\\n{content}\"\n            for idx_path, content in item[\"related_files_content_map\"].items()\n        )\n\n        # Get summary of chapters written *before* this one\n        # Use the temporary instance variable\n        previous_chapters_summary = \"\\n---\\n\".join(self.chapters_written_so_far)\n\n\n        prompt = f\"\"\"\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `{project_name}` about the concept: \"{abstraction_name}\". This is Chapter {chapter_num}.\n\nConcept Details:\n- Description:\n{item[\"abstraction_details\"][\"description\"]}\n\nComplete Tutorial Structure:\n{item[\"full_chapter_listing\"]}\n\nContext from previous chapters (summary):\n{previous_chapters_summary if previous_chapters_summary else \"This is the first chapter.\"}\n\nRelevant Code Snippets:\n{file_context_str if file_context_str else \"No specific code snippets provided for this abstraction.\"}\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter {chapter_num}: {abstraction_name}`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n\"\"\"\n        chapter_content = call_llm(prompt)\n        # Basic validation/cleanup\n        actual_heading = f\"# Chapter {chapter_num}: {abstraction_name}\"\n        if not chapter_content.strip().startswith(f\"# Chapter {chapter_num}\"):\n             # Add heading if missing or incorrect, trying to preserve content\n             lines = chapter_content.strip().split('\\n')\n             if lines and lines[0].strip().startswith(\"#\"): # If there's some heading, replace it\n                 lines[0] = actual_heading\n                 chapter_content = \"\\n\".join(lines)\n             else: # Otherwise, prepend it\n                 chapter_content = f\"{actual_heading}\\n\\n{chapter_content}\"\n\n        # Add the generated content to our temporary list for the next iteration's context\n        self.chapters_written_so_far.append(chapter_content)\n\n        return chapter_content # Return the Markdown string\n\n    def post(self, shared, prep_res, exec_res_list):\n        # exec_res_list contains the generated Markdown for each chapter, in order\n        shared[\"chapters\"] = exec_res_list\n        # Clean up the temporary instance variable\n        del self.chapters_written_so_far\n        print(f\"Finished writing {len(exec_res_list)} chapters.\")\n\nclass CombineTutorial(Node):\n    def prep(self, shared):\n        project_name = shared[\"project_name\"]\n        output_base_dir = shared.get(\"output_dir\", \"output\") # Default output dir\n        output_path = os.path.join(output_base_dir, project_name)\n        repo_url = shared[\"repo_url\"]  # Get the repository URL\n\n        # Use 'label' from relationships_data['details']\n        relationships_data = shared[\"relationships\"] # {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        chapter_order = shared[\"chapter_order\"] # indices\n        abstractions = shared[\"abstractions\"]   # list of dicts\n        chapters_content = shared[\"chapters\"]   # list of strings\n\n        # --- Generate Mermaid Diagram ---\n        mermaid_lines = [\"flowchart TD\"]\n        # Add nodes for each abstraction\n        for i, abstr in enumerate(abstractions):\n            # Sanitize name for Mermaid ID and label\n            node_id = f\"A{i}\"\n            sanitized_name = abstr['name'].replace('\"', '')\n            node_label = sanitized_name # Using sanitized name only, no index\n            mermaid_lines.append(f'    {node_id}[\"{node_label}\"]')\n        # Add edges for relationships using 'label'\n        for rel in relationships_data['details']:\n            from_node_id = f\"A{rel['from']}\"\n            to_node_id = f\"A{rel['to']}\"\n            # Sanitize 'label' for edge label\n            edge_label = rel['label'].replace('\"', '').replace('\\n', ' ') # Basic sanitization\n            # Limit edge label length for readability (optional, but good for diagrams)\n            max_label_len = 30 # Make it shorter for labels\n            if len(edge_label) > max_label_len:\n                edge_label = edge_label[:max_label_len-3] + \"...\"\n            mermaid_lines.append(f'    {from_node_id} -- \"{edge_label}\" --> {to_node_id}')\n\n        mermaid_diagram = \"\\n\".join(mermaid_lines)\n        # --- End Mermaid ---\n\n\n        # Prepare index.md content\n        index_content = f\"# Tutorial: {project_name}\\n\\n\"\n        index_content += f\"{relationships_data['summary']}\\n\\n\"\n        index_content += f\"**Source Repository:** [{repo_url}]({repo_url})\\n\\n\"\n\n        # Add Mermaid diagram for relationships\n        index_content += \"```mermaid\\n\"\n        index_content += mermaid_diagram + \"\\n\"\n        index_content += \"```\\n\\n\"\n\n        index_content += \"## Chapters\\n\\n\"\n\n        chapter_files = []\n        # Generate chapter links based on the determined order\n        for i, abstraction_index in enumerate(chapter_order):\n            # Ensure index is valid and we have content for it\n            if 0 <= abstraction_index < len(abstractions) and i < len(chapters_content):\n                abstraction_name = abstractions[abstraction_index][\"name\"]\n                # Sanitize name for filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in abstraction_name).lower()\n                # Use chapter number (i+1) for ordering filename\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                index_content += f\"{i+1}. [{abstraction_name}]({filename})\\n\"\n                \n                # Add attribution to chapter content\n                chapter_content = chapters_content[i]\n                if not chapter_content.endswith(\"\\n\\n\"):\n                    chapter_content += \"\\n\\n\"\n                chapter_content += \"---\\n\\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\"\n                \n                # Store filename and corresponding content\n                chapter_files.append({\"filename\": filename, \"content\": chapter_content})\n            else:\n                 print(f\"Warning: Mismatch between chapter order, abstractions, or content at index {i} (abstraction index {abstraction_index}). Skipping file generation for this entry.\")\n\n        # Add attribution to index content\n        index_content += \"\\n\\n---\\n\\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\"\n\n        return {\n            \"output_path\": output_path,\n            \"index_content\": index_content,\n            \"chapter_files\": chapter_files # List of {\"filename\": str, \"content\": str}\n        }\n\n    def exec(self, prep_res):\n        output_path = prep_res[\"output_path\"]\n        index_content = prep_res[\"index_content\"]\n        chapter_files = prep_res[\"chapter_files\"]\n\n        print(f\"Combining tutorial into directory: {output_path}\")\n        # Rely on Node's built-in retry/fallback\n        os.makedirs(output_path, exist_ok=True)\n\n        # Write index.md\n        index_filepath = os.path.join(output_path, \"index.md\")\n        with open(index_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(index_content)\n        print(f\"  - Wrote {index_filepath}\")\n\n        # Write chapter files\n        for chapter_info in chapter_files:\n            chapter_filepath = os.path.join(output_path, chapter_info[\"filename\"])\n            with open(chapter_filepath, \"w\", encoding=\"utf-8\") as f:\n                f.write(chapter_info[\"content\"])\n            print(f\"  - Wrote {chapter_filepath}\")\n\n        return output_path # Return the final path\n\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"final_output_dir\"] = exec_res # Store the output path\n        print(f\"\\nTutorial generation complete! Files are in: {exec_res}\")\n        convert_all_md_to_html(exec_res) # Convert all Markdown files to HTML\n        \n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 9: Tutorial Combination`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "```markdown\n# Chapter 9: Tutorial Combination\n\nIn the previous chapter, [Chapter Writing](08_chapter_writing.md), we learned how to craft effective and engaging tutorial chapters. Now, it's time to bring all those individual pieces together and assemble the final product! This is where Tutorial Combination comes in.\n\n## What is Tutorial Combination?\n\nTutorial Combination is the final step in the tutorial generation process, where all the individual chapters are combined into a single, cohesive tutorial. This involves:\n\n*   Generating an index page that acts as a table of contents.\n*   Creating a Mermaid diagram to visually represent the relationships between the abstractions.\n*   Converting the Markdown files to HTML for easy viewing in a web browser.\n\nThink of it like a bookbinder assembling all the chapters of a book and adding a cover and table of contents to create a polished final product. It's the culmination of all our hard work, transforming a collection of individual chapters into a complete and valuable learning resource.\n\n## Key Concepts\n\n*   **Index Page:** A central page that provides an overview of the tutorial and links to all the individual chapters. It's like the table of contents in a book.\n*   **Mermaid Diagram:** A visual representation of the relationships between the abstractions, using the Mermaid syntax for creating diagrams in Markdown.\n*   **Markdown to HTML Conversion:** Converting the Markdown files (including the index page and chapter files) to HTML files that can be viewed in a web browser.\n*   **Directory Structure:** Organizing the HTML files in a logical directory structure, making it easy to navigate the tutorial.\n\n## How Tutorial Combination Works\n\nThe Tutorial Combination process is handled by the `CombineTutorial` node in our flow.\n\nLet's look at the simplified steps in `nodes.py`:\n\n```python\nclass CombineTutorial(Node):\n    def prep(self, shared):\n        project_name = shared[\"project_name\"]\n        output_base_dir = shared.get(\"output_dir\", \"output\") # Default output dir\n        output_path = os.path.join(output_base_dir, project_name)\n        repo_url = shared[\"repo_url\"]  # Get the repository URL\n\n        # Use 'label' from relationships_data['details']\n        relationships_data = shared[\"relationships\"] # {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        chapter_order = shared[\"chapter_order\"] # indices\n        abstractions = shared[\"abstractions\"]   # list of dicts\n        chapters_content = shared[\"chapters\"]   # list of strings\n\n        # --- Generate Mermaid Diagram ---\n        mermaid_lines = [\"flowchart TD\"]\n        # Add nodes for each abstraction\n        for i, abstr in enumerate(abstractions):\n            # Sanitize name for Mermaid ID and label\n            node_id = f\"A{i}\"\n            sanitized_name = abstr['name'].replace('\"', '')\n            node_label = sanitized_name # Using sanitized name only, no index\n            mermaid_lines.append(f'    {node_id}[\"{node_label}\"]')\n        # Add edges for relationships using 'label'\n        for rel in relationships_data['details']:\n            from_node_id = f\"A{rel['from']}\"\n            to_node_id = f\"A{rel['to']}\"\n            # Sanitize 'label' for edge label\n            edge_label = rel['label'].replace('\"', '').replace('\\n', ' ') # Basic sanitization\n            # Limit edge label length for readability (optional, but good for diagrams)\n            max_label_len = 30 # Make it shorter for labels\n            if len(edge_label) > max_label_len:\n                edge_label = edge_label[:max_label_len-3] + \"...\"\n            mermaid_lines.append(f'    {from_node_id} -- \"{edge_label}\" --> {to_node_id}')\n\n        mermaid_diagram = \"\\n\".join(mermaid_lines)\n        # --- End Mermaid ---\n\n\n        # Prepare index.md content\n        index_content = f\"# Tutorial: {project_name}\\n\\n\"\n        index_content += f\"{relationships_data['summary']}\\n\\n\"\n        index_content += f\"**Source Repository:** [{repo_url}]({repo_url})\\n\\n\"\n\n        # Add Mermaid diagram for relationships\n        index_content += \"```mermaid\\n\"\n        index_content += mermaid_diagram + \"\\n\"\n        index_content += \"```\\n\\n\"\n\n        index_content += \"## Chapters\\n\\n\"\n\n        chapter_files = []\n        # Generate chapter links based on the determined order\n        for i, abstraction_index in enumerate(chapter_order):\n            # Ensure index is valid and we have content for it\n            if 0 <= abstraction_index < len(abstractions) and i < len(chapters_content):\n                abstraction_name = abstractions[abstraction_index][\"name\"]\n                # Sanitize name for filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in abstraction_name).lower()\n                # Use chapter number (i+1) for ordering filename\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                index_content += f\"{i+1}. [{abstraction_name}]({filename})\\n\"\n                \n                # Add attribution to chapter content\n                chapter_content = chapters_content[i]\n                if not chapter_content.endswith(\"\\n\\n\"):\n                    chapter_content += \"\\n\\n\"\n                chapter_content += \"---\\n\\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\"\n                \n                # Store filename and corresponding content\n                chapter_files.append({\"filename\": filename, \"content\": chapter_content})\n            else:\n                 print(f\"Warning: Mismatch between chapter order, abstractions, or content at index {i} (abstraction index {abstraction_index}). Skipping file generation for this entry.\")\n\n        # Add attribution to index content\n        index_content += \"\\n\\n---\\n\\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\"\n\n        return {\n            \"output_path\": output_path,\n            \"index_content\": index_content,\n            \"chapter_files\": chapter_files # List of {\"filename\": str, \"content\": str}\n        }\n\n    def exec(self, prep_res):\n        output_path = prep_res[\"output_path\"]\n        index_content = prep_res[\"index_content\"]\n        chapter_files = prep_res[\"chapter_files\"]\n\n        print(f\"Combining tutorial into directory: {output_path}\")\n        # Rely on Node's built-in retry/fallback\n        os.makedirs(output_path, exist_ok=True)\n\n        # Write index.md\n        index_filepath = os.path.join(output_path, \"index.md\")\n        with open(index_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(index_content)\n        print(f\"  - Wrote {index_filepath}\")\n\n        # Write chapter files\n        for chapter_info in chapter_files:\n            chapter_filepath = os.path.join(output_path, chapter_info[\"filename\"])\n            with open(chapter_filepath, \"w\", encoding=\"utf-8\") as f:\n                f.write(chapter_info[\"content\"])\n            print(f\"  - Wrote {chapter_filepath}\")\n\n        return output_path # Return the final path\n\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"final_output_dir\"] = exec_res # Store the output path\n        print(f\"\\nTutorial generation complete! Files are in: {exec_res}\")\n        convert_all_md_to_html(exec_res) # Convert all Markdown files to HTML\n```\n\nHere's a breakdown:\n\n1.  **`prep(shared)`:**\n    *   This method retrieves all the necessary information from the `shared` dictionary, including:\n        *   The project name.\n        *   The output directory.\n        *   The repository URL.\n        *   The relationships between the abstractions.\n        *   The chapter order.\n        *   The abstraction details.\n        *   The chapter content.\n    *   It generates the Mermaid diagram by iterating over the relationships and creating the appropriate Mermaid syntax.\n    *   It creates the index page content, including the tutorial title, project summary, repository link, Mermaid diagram, and a list of chapters with links to their respective files.\n    *   It prepares a list of chapter files with their corresponding content.\n\n2.  **`exec(prep_res)`:**\n    *   This method takes the prepared information and writes it to the appropriate files in the output directory.\n    *   It creates the output directory if it doesn't already exist.\n    *   It writes the index page content to a file named `index.md`.\n    *   It writes each chapter's content to a separate Markdown file.\n\n3.  **`post(shared, prep_res, exec_res)`:**\n    *   This method stores the output path in the `shared[\"final_output_dir\"]` variable.\n    *   It calls `convert_all_md_to_html` (defined in `converter/md_to_html.py`) to convert all the Markdown files in the output directory to HTML files.\n\n### Generating the Mermaid Diagram\n\nThe `CombineTutorial` node generates a Mermaid diagram to visualize the relationships between the abstractions. Here's a simplified example of how it works:\n\n```python\n# --- Generate Mermaid Diagram ---\nmermaid_lines = [\"flowchart TD\"]\n# Add nodes for each abstraction\nfor i, abstr in enumerate(abstractions):\n    # Sanitize name for Mermaid ID and label\n    node_id = f\"A{i}\"\n    sanitized_name = abstr['name'].replace('\"', '')\n    node_label = sanitized_name # Using sanitized name only, no index\n    mermaid_lines.append(f'    {node_id}[\"{node_label}\"]')\n# Add edges for relationships using 'label'\nfor rel in relationships_data['details']:\n    from_node_id = f\"A{rel['from']}\"\n    to_node_id = f\"A{rel['to']}\"\n    # Sanitize 'label' for edge label\n    edge_label = rel['label'].replace('\"', '').replace('\\n', ' ') # Basic sanitization\n    # Limit edge label length for readability (optional, but good for diagrams)\n    max_label_len = 30 # Make it shorter for labels\n    if len(edge_label) > max_label_len:\n        edge_label = edge_label[:max_label_len-3] + \"...\"\n    mermaid_lines.append(f'    {from_node_id} -- \"{edge_label}\" --> {to_node_id}')\n\nmermaid_diagram = \"\\n\".join(mermaid_lines)\n# --- End Mermaid ---\n```\n\nThis code iterates over the abstractions and relationships to generate the Mermaid syntax for the diagram. Each abstraction is represented as a node in the diagram, and each relationship is represented as an edge connecting two nodes.\n\nFor example, if we have the following abstractions and relationships:\n\n```json\n{\n  \"abstractions\": [\n    { \"name\": \"API Endpoint\" },\n    { \"name\": \"Flow Orchestration\" }\n  ],\n  \"relationships\": {\n    \"details\": [\n      { \"from\": 0, \"to\": 1, \"label\": \"Triggers\" }\n    ]\n  }\n}\n```\n\nThe generated Mermaid diagram would look like this:\n\n```mermaid\nflowchart TD\n    A0[\"API Endpoint\"]\n    A1[\"Flow Orchestration\"]\n    A0 -- \"Triggers\" --> A1\n```\n\n### Converting Markdown to HTML\n\nThe `convert_all_md_to_html` function in `converter/md_to_html.py` is responsible for converting all the Markdown files in the output directory to HTML files. Here's a simplified view of how it works:\n\n```python\nfrom pathlib import Path\nimport markdown\nfrom md_mermaid import MermaidExtension\nimport sys\nfrom mermaid_extension import MermaidExtension as mermaid_ext\n\ndef convert_md_to_html(md_file_path, output_folder):\n    # Read the Markdown file\n    with open(md_file_path, \"r\", encoding=\"utf-8\") as md_file:\n        md_content = md_file.read()\n\n    # Initialize the Markdown object with the extensions\n    md = markdown.Markdown(extensions=[\n        # MermaidExtension(),  # Proper initialization of MermaidExtension\n        mermaid_ext(),\n        'fenced_code',\n        'codehilite',\n        'tables',\n        'toc'\n    ])\n\n    md_content = convert_hyperlink_to_html(md_content)\n    # Convert Markdown to HTML\n    html_content = md.convert(md_content)\n    \n    # wrap the HTML content in a basic HTML structure\n    html_content = f\"\"\"\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n<meta charset=\"UTF-8\">\n<title>{md_file_path}</title>\n        <script type=\"module\">\n            import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';\n            mermaid.initialize({{ startOnLoad: true }});\n        </script>\n       <style>\n            body {{\n                max-width: 800px;\n                margin: 2em auto;\n                font-family: system-ui, sans-serif;\n                line-height: 1.6;\n                color: #333;\n                padding: 0 1em;\n            }}\n            pre {{\n                background: #f6f8fa;\n                padding: 1em;\n                overflow-x: auto;\n            }}\n            code {{\n                background: #f0f0f0;\n                padding: 2px 4px;\n                border-radius: 4px;\n            }}\n            a {{\n                color: #0366d6;\n                text-decoration: none;\n            }}\n            a:hover {{\n                text-decoration: underline;\n            }}\n        </style>\n</head>\n<body>\n{html_content}\n</body>\n</html>\n\"\"\"   \n\n    # create the output file path\n    output_file_path = output_folder / md_file_path.relative_to(md_file_path.parent).with_suffix(\".html\")\n    \n    # write the HTML content to a file\n    with open(output_file_path, \"w\", encoding=\"utf-8\") as html_file:\n        html_file.write(html_content)\n        \n    print(f\"Converted {md_file_path} to {output_file_path}\")\n    \n\ndef convert_hyperlink_to_html(md_content):\n    # replace any hyperlinks with HTML links (.md to .html)\n    md_content = md_content.replace(\".md\", \".html\")\n    return md_content\n    \ndef convert_all_md_to_html(md_folder_path, output_folder=None):\n    md_folder_path = Path(md_folder_path)\n    output_folder = Path(output_folder) if output_folder else md_folder_path / \"html\"\n    output_folder.mkdir(parents=True, exist_ok=True)\n    \n    # iterate through all Markdown files in the folder\n    for md_file in md_folder_path.glob(\"**/*.md\"):\n        convert_md_to_html(md_file, output_folder)\n```\n\nHere's a breakdown:\n\n1.  **Iterating Over Markdown Files:** The function iterates over all the Markdown files in the output directory using `glob(\"**/*.md\")`.\n2.  **Reading Markdown Content:** For each Markdown file, the function reads the content of the file.\n3.  **Converting Markdown to HTML:** The function uses the `markdown` library to convert the Markdown content to HTML.  The markdown extensions allows for rendering of code snippets, and mermaid diagrams.\n4.  **Writing HTML Content to File:** The function writes the HTML content to a file with the same name as the Markdown file, but with the `.html` extension.\n5.  **Updating Links to HTML:** The function also updates any links to markdown files in the content to be the links to the new HTML files.\n\n### Example: The Output Directory Structure\n\nAfter the Tutorial Combination process is complete, the output directory will contain the following files:\n\n```\noutput/\n\u2514\u2500\u2500 FastAPI/\n    \u251c\u2500\u2500 01_api_endpoint.html\n    \u251c\u2500\u2500 02_flow_orchestration.html\n    \u251c\u2500\u2500 03_configuration_management.html\n    \u251c\u2500\u2500 04_codebase_crawling.html\n    \u251c\u2500\u2500 05_abstraction_identification.html\n    \u251c\u2500\u2500 06_relationship_analysis.html\n    \u251c\u2500\u2500 07_llm_interaction.html\n    \u251c\u2500\u2500 08_chapter_writing.html\n    \u2514\u2500\u2500 09_tutorial_combination.html\n    \u2514\u2500\u2500 index.html\n```\n\n## How to Use Tutorial Combination\n\nYou don't directly call the `CombineTutorial` node yourself. Instead, it is automatically executed as the final step in the flow. After the flow is complete, the generated tutorial will be located in the output directory specified by the `-o` option when you run the `main.py` script.\n\n## Why is Tutorial Combination Important?\n\n*   **Completeness:** It ensures that all the individual chapters are combined into a single, cohesive tutorial.\n*   **Navigation:** It provides an index page that makes it easy to navigate the tutorial.\n*   **Visualization:** It generates a Mermaid diagram that visualizes the relationships between the abstractions, providing a high-level overview of the codebase.\n*   **Accessibility:** It converts the Markdown files to HTML, making the tutorial accessible to anyone with a web browser.\n*   **Readability:** It applies a default stylesheet to make the tutorial easily readable\n\n## Conclusion\n\nIn this chapter, we've learned about Tutorial Combination, which is the final step in the tutorial generation process. We've seen how it combines all the individual chapters into a single, cohesive tutorial, generates an index page, creates a Mermaid diagram, and converts the Markdown files to HTML.\n\nCongratulations! You've now reached the end of our tutorial on generating tutorials from codebases! We hope you've found this journey informative and helpful. By combining the power of code analysis, LLMs, and automated processes, we can create high-quality learning resources for developers of all skill levels.\n```"}